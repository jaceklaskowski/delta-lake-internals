# DeltaSQLConf

`DeltaSQLConf` contains [spark.databricks.delta](index.md#spark.databricks.delta)-prefixed configuration properties to configure behaviour of Delta Lake.

## autoCompact.enabled { #DELTA_AUTO_COMPACT_ENABLED }

[spark.databricks.delta.autoCompact.enabled](index.md#autoCompact.enabled)

## autoCompact.maxFileSize { #DELTA_AUTO_COMPACT_MAX_FILE_SIZE }

[spark.databricks.delta.autoCompact.maxFileSize](index.md#autoCompact.maxFileSize)

## autoCompact.minFileSize { #DELTA_AUTO_COMPACT_MIN_FILE_SIZE }

[spark.databricks.delta.autoCompact.minFileSize](index.md#autoCompact.minFileSize)

## autoCompact.modifiedPartitionsOnly.enabled { #DELTA_AUTO_COMPACT_MODIFIED_PARTITIONS_ONLY_ENABLED }

[spark.databricks.delta.autoCompact.modifiedPartitionsOnly.enabled](index.md#autoCompact.modifiedPartitionsOnly.enabled)

## autoCompact.nonBlindAppend.enabled { #DELTA_AUTO_COMPACT_NON_BLIND_APPEND_ENABLED }

[spark.databricks.delta.autoCompact.nonBlindAppend.enabled](index.md#autoCompact.nonBlindAppend.enabled)

## changeDataFeed.unsafeBatchReadOnIncompatibleSchemaChanges.enabled { #DELTA_CDF_UNSAFE_BATCH_READ_ON_INCOMPATIBLE_SCHEMA_CHANGES }

[spark.databricks.delta.changeDataFeed.unsafeBatchReadOnIncompatibleSchemaChanges.enabled](index.md#changeDataFeed.unsafeBatchReadOnIncompatibleSchemaChanges.enabled)

## checkpoint.partSize { #DELTA_CHECKPOINT_PART_SIZE }

[spark.databricks.delta.checkpoint.partSize](index.md#checkpoint.partSize)

## clusteredTable.enableClusteringTablePreview { #DELTA_CLUSTERING_TABLE_PREVIEW_ENABLED }

[spark.databricks.delta.clusteredTable.enableClusteringTablePreview](index.md#clusteredTable.enableClusteringTablePreview)

## delete.deletionVectors.persistent { #DELETE_USE_PERSISTENT_DELETION_VECTORS }

[spark.databricks.delta.delete.deletionVectors.persistent](index.md#delete.deletionVectors.persistent)

## history.maxKeysPerList { #DELTA_HISTORY_PAR_SEARCH_THRESHOLD }

[spark.databricks.delta.history.maxKeysPerList](index.md#history.maxKeysPerList)

## merge.materializeSource { #DELTA_COLLECT_STATS_USING_TABLE_SCHEMA }

[spark.databricks.delta.merge.materializeSource](index.md#merge.materializeSource)

## merge.materializeSource.maxAttempts { #MERGE_MATERIALIZE_SOURCE_MAX_ATTEMPTS }

[spark.databricks.delta.merge.materializeSource.maxAttempts](index.md#merge.materializeSource.maxAttempts)

## merge.materializeSource.rddStorageLevel { #MERGE_MATERIALIZE_SOURCE_RDD_STORAGE_LEVEL }

[spark.databricks.delta.merge.materializeSource.rddStorageLevel](index.md#merge.materializeSource.rddStorageLevel)

## merge.materializeSource.rddStorageLevelRetry { #MERGE_MATERIALIZE_SOURCE_RDD_STORAGE_LEVEL_RETRY }

[spark.databricks.delta.merge.materializeSource.rddStorageLevelRetry](index.md#merge.materializeSource.rddStorageLevelRetry)

## merge.materializeSource.rddStorageLevelRetry { #MERGE_MATERIALIZE_SOURCE_RDD_STORAGE_LEVEL_RETRY }

[spark.databricks.delta.merge.materializeSource.rddStorageLevelRetry](index.md#merge.materializeSource.rddStorageLevelRetry)

## merge.optimizeMatchedOnlyMerge.enabled { #MERGE_MATCHED_ONLY_ENABLED }

[spark.databricks.delta.merge.optimizeMatchedOnlyMerge.enabled](index.md#merge.optimizeMatchedOnlyMerge.enabled)

## optimize.maxFileSize { #DELTA_OPTIMIZE_MAX_FILE_SIZE }

[spark.databricks.delta.optimize.maxFileSize](index.md#optimize.maxFileSize)

## optimize.repartition.enabled { #DELTA_OPTIMIZE_REPARTITION_ENABLED }

[spark.databricks.delta.optimize.repartition.enabled](index.md#optimize.repartition.enabled)

## optimize.zorder.checkStatsCollection.enabled { #DELTA_OPTIMIZE_ZORDER_COL_STAT_CHECK }

[spark.databricks.delta.optimize.zorder.checkStatsCollection.enabled](index.md#optimize.zorder.checkStatsCollection.enabled)

## replaceWhere.constraintCheck.enabled { #REPLACEWHERE_CONSTRAINT_CHECK_ENABLED }

[spark.databricks.delta.replaceWhere.constraintCheck.enabled](index.md#replaceWhere.constraintCheck.enabled)

## schema.removeSparkInternalMetadata { #DELTA_SCHEMA_REMOVE_SPARK_INTERNAL_METADATA }

[spark.databricks.delta.schema.removeSparkInternalMetadata](index.md#schema.removeSparkInternalMetadata)

## <span id="DELTA_COLLECT_STATS"> stats.collect { #stats.collect }

[spark.databricks.delta.stats.collect](index.md#stats.collect)

## stats.collect.using.tableSchema { #MERGE_MATERIALIZE_SOURCE }

[spark.databricks.delta.stats.collect.using.tableSchema](index.md#stats.collect.using.tableSchema)

## streaming.schemaTracking.enabled { #DELTA_STREAMING_ENABLE_SCHEMA_TRACKING }

[spark.databricks.delta.streaming.schemaTracking.enabled](index.md#streaming.schemaTracking.enabled)

## streaming.unsafeReadOnIncompatibleColumnMappingSchemaChanges.enabled { #DELTA_STREAMING_UNSAFE_READ_ON_INCOMPATIBLE_COLUMN_MAPPING_SCHEMA_CHANGES }

[spark.databricks.delta.streaming.unsafeReadOnIncompatibleColumnMappingSchemaChanges.enabled](index.md#streaming.unsafeReadOnIncompatibleColumnMappingSchemaChanges.enabled)

## write.txnVersion.autoReset.enabled { #DELTA_IDEMPOTENT_DML_AUTO_RESET_ENABLED }

[spark.databricks.delta.write.txnVersion.autoReset.enabled](index.md#write.txnVersion.autoReset.enabled)
