{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Delta Lake 0.8.0 \u00b6 Welcome to The Internals of Delta Lake online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Delta Lake as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Delta Lake \ud83d\udd25","title":"Welcome"},{"location":"#the-internals-of-delta-lake-080","text":"Welcome to The Internals of Delta Lake online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Delta Lake as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Delta Lake \ud83d\udd25","title":"The Internals of Delta Lake 0.8.0"},{"location":"Action/","text":"= Action Action is an < > of < > of a change to (the state of) a Delta table. Action can be converted ( serialized ) to < > format for...FIXME [[logSchema]] Action object defines logSchema that is a schema ( StructType ) based on the < > case class. [source, scala] \u00b6 import org.apache.spark.sql.delta.actions.Action.logSchema scala> logSchema.printTreeString root |-- txn: struct (nullable = true) | |-- appId: string (nullable = true) | |-- version: long (nullable = false) | |-- lastUpdated: long (nullable = true) |-- add: struct (nullable = true) | |-- path: string (nullable = true) | |-- partitionValues: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- size: long (nullable = false) | |-- modificationTime: long (nullable = false) | |-- dataChange: boolean (nullable = false) | |-- stats: string (nullable = true) | |-- tags: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) |-- remove: struct (nullable = true) | |-- path: string (nullable = true) | |-- deletionTimestamp: long (nullable = true) | |-- dataChange: boolean (nullable = false) |-- metaData: struct (nullable = true) | |-- id: string (nullable = true) | |-- name: string (nullable = true) | |-- description: string (nullable = true) | |-- format: struct (nullable = true) | | |-- provider: string (nullable = true) | | |-- options: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) | |-- schemaString: string (nullable = true) | |-- partitionColumns: array (nullable = true) | | |-- element: string (containsNull = true) | |-- configuration: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- createdTime: long (nullable = true) |-- protocol: struct (nullable = true) | |-- minReaderVersion: integer (nullable = false) | |-- minWriterVersion: integer (nullable = false) |-- commitInfo: struct (nullable = true) | |-- version: long (nullable = true) | |-- timestamp: timestamp (nullable = true) | |-- userId: string (nullable = true) | |-- userName: string (nullable = true) | |-- operation: string (nullable = true) | |-- operationParameters: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- job: struct (nullable = true) | | |-- jobId: string (nullable = true) | | |-- jobName: string (nullable = true) | | |-- runId: string (nullable = true) | | |-- jobOwnerId: string (nullable = true) | | |-- triggerType: string (nullable = true) | |-- notebook: struct (nullable = true) | | |-- notebookId: string (nullable = true) | |-- clusterId: string (nullable = true) | |-- readVersion: long (nullable = true) | |-- isolationLevel: string (nullable = true) | |-- isBlindAppend: boolean (nullable = true) [[contract]] .Action Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | wrap a| [[wrap]] [source, scala] \u00b6 wrap: SingleAction \u00b6 Wraps the action into a < > for serialization Used when: Snapshot is created (and initializes the < > for the < > of a delta table) Action is requested to < > |=== [[implementations]] [[extensions]] .Actions (Direct Implementations and Extensions Only) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Action | Description | < > | [[CommitInfo]] | < > | [[FileAction]] | < > | [[Metadata]] | < > | [[Protocol]] | < > | [[SetTransaction]] |=== NOTE: Action is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file). == [[json]] Serializing to JSON Format -- json Method [source, scala] \u00b6 json: String \u00b6 json simply serializes ( converts ) the < > to JSON format. NOTE: json uses https://github.com/FasterXML/jackson[Jackson ] library (with https://github.com/FasterXML/jackson-module-scala[jackson-module-scala ]) as the JSON processor. [NOTE] \u00b6 json is used when: OptimisticTransactionImpl is requested to < > * ConvertToDeltaCommand is requested to < > \u00b6 == [[fromJson]] Deserializing Action (From JSON Format) -- fromJson Utility [source, scala] \u00b6 fromJson( json: String): Action fromJson ...FIXME [NOTE] \u00b6 fromJson is used when: DeltaHistoryManager utility is requested for the < > DeltaLog is requested for the < > * OptimisticTransactionImpl is requested to < > \u00b6","title":"Action"},{"location":"Action/#source-scala","text":"import org.apache.spark.sql.delta.actions.Action.logSchema scala> logSchema.printTreeString root |-- txn: struct (nullable = true) | |-- appId: string (nullable = true) | |-- version: long (nullable = false) | |-- lastUpdated: long (nullable = true) |-- add: struct (nullable = true) | |-- path: string (nullable = true) | |-- partitionValues: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- size: long (nullable = false) | |-- modificationTime: long (nullable = false) | |-- dataChange: boolean (nullable = false) | |-- stats: string (nullable = true) | |-- tags: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) |-- remove: struct (nullable = true) | |-- path: string (nullable = true) | |-- deletionTimestamp: long (nullable = true) | |-- dataChange: boolean (nullable = false) |-- metaData: struct (nullable = true) | |-- id: string (nullable = true) | |-- name: string (nullable = true) | |-- description: string (nullable = true) | |-- format: struct (nullable = true) | | |-- provider: string (nullable = true) | | |-- options: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) | |-- schemaString: string (nullable = true) | |-- partitionColumns: array (nullable = true) | | |-- element: string (containsNull = true) | |-- configuration: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- createdTime: long (nullable = true) |-- protocol: struct (nullable = true) | |-- minReaderVersion: integer (nullable = false) | |-- minWriterVersion: integer (nullable = false) |-- commitInfo: struct (nullable = true) | |-- version: long (nullable = true) | |-- timestamp: timestamp (nullable = true) | |-- userId: string (nullable = true) | |-- userName: string (nullable = true) | |-- operation: string (nullable = true) | |-- operationParameters: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- job: struct (nullable = true) | | |-- jobId: string (nullable = true) | | |-- jobName: string (nullable = true) | | |-- runId: string (nullable = true) | | |-- jobOwnerId: string (nullable = true) | | |-- triggerType: string (nullable = true) | |-- notebook: struct (nullable = true) | | |-- notebookId: string (nullable = true) | |-- clusterId: string (nullable = true) | |-- readVersion: long (nullable = true) | |-- isolationLevel: string (nullable = true) | |-- isBlindAppend: boolean (nullable = true) [[contract]] .Action Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | wrap a| [[wrap]]","title":"[source, scala]"},{"location":"Action/#source-scala_1","text":"","title":"[source, scala]"},{"location":"Action/#wrap-singleaction","text":"Wraps the action into a < > for serialization Used when: Snapshot is created (and initializes the < > for the < > of a delta table) Action is requested to < > |=== [[implementations]] [[extensions]] .Actions (Direct Implementations and Extensions Only) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Action | Description | < > | [[CommitInfo]] | < > | [[FileAction]] | < > | [[Metadata]] | < > | [[Protocol]] | < > | [[SetTransaction]] |=== NOTE: Action is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file). == [[json]] Serializing to JSON Format -- json Method","title":"wrap: SingleAction"},{"location":"Action/#source-scala_2","text":"","title":"[source, scala]"},{"location":"Action/#json-string","text":"json simply serializes ( converts ) the < > to JSON format. NOTE: json uses https://github.com/FasterXML/jackson[Jackson ] library (with https://github.com/FasterXML/jackson-module-scala[jackson-module-scala ]) as the JSON processor.","title":"json: String"},{"location":"Action/#note","text":"json is used when: OptimisticTransactionImpl is requested to < >","title":"[NOTE]"},{"location":"Action/#converttodeltacommand-is-requested-to","text":"== [[fromJson]] Deserializing Action (From JSON Format) -- fromJson Utility","title":"* ConvertToDeltaCommand is requested to &lt;&gt;"},{"location":"Action/#source-scala_3","text":"fromJson( json: String): Action fromJson ...FIXME","title":"[source, scala]"},{"location":"Action/#note_1","text":"fromJson is used when: DeltaHistoryManager utility is requested for the < > DeltaLog is requested for the < >","title":"[NOTE]"},{"location":"Action/#optimistictransactionimpl-is-requested-to","text":"","title":"* OptimisticTransactionImpl is requested to &lt;&gt;"},{"location":"AddFile/","text":"AddFile \u00b6 AddFile is a FileAction that represents an action of adding a file to a delta table . Creating Instance \u00b6 AddFile takes the following to be created: Path Partition values ( Map[String, String] ) Size (in bytes) Modification time dataChange flag Stats (default: null ) Tags ( Map[String, String] ) (default: null ) AddFile is created when: ConvertToDeltaCommand is executed (for every data file to import ) DelayedCommitProtocol is requested to commit a task (after successful write) (for optimistic transactional writers ) Converting to SingleAction \u00b6 wrap : SingleAction wrap is part of the Action abstraction. wrap creates a new SingleAction with the add field set to this AddFile . Converting to RemoveFile with Defaults \u00b6 remove : RemoveFile remove simply creates a RemoveFile for the path (with the current time and dataChange flag enabled). remove is used when: MergeIntoCommand is executed WriteIntoDelta is requested to write (with Overwrite mode) DeltaSink is requested to add a streaming micro-batch (for Complete output mode) Converting to RemoveFile \u00b6 removeWithTimestamp ( timestamp : Long = System . currentTimeMillis (), dataChange : Boolean = true ) : RemoveFile remove creates a new RemoveFile for the path with the given timestamp and dataChange flag. removeWithTimestamp is used when: AddFile is requested to create a RemoveFile action with the defaults CreateDeltaTableCommand , DeleteCommand and UpdateCommand commands are executed DeltaCommand is requested to removeFilesFromPaths","title":"AddFile"},{"location":"AddFile/#addfile","text":"AddFile is a FileAction that represents an action of adding a file to a delta table .","title":"AddFile"},{"location":"AddFile/#creating-instance","text":"AddFile takes the following to be created: Path Partition values ( Map[String, String] ) Size (in bytes) Modification time dataChange flag Stats (default: null ) Tags ( Map[String, String] ) (default: null ) AddFile is created when: ConvertToDeltaCommand is executed (for every data file to import ) DelayedCommitProtocol is requested to commit a task (after successful write) (for optimistic transactional writers )","title":"Creating Instance"},{"location":"AddFile/#converting-to-singleaction","text":"wrap : SingleAction wrap is part of the Action abstraction. wrap creates a new SingleAction with the add field set to this AddFile .","title":" Converting to SingleAction"},{"location":"AddFile/#converting-to-removefile-with-defaults","text":"remove : RemoveFile remove simply creates a RemoveFile for the path (with the current time and dataChange flag enabled). remove is used when: MergeIntoCommand is executed WriteIntoDelta is requested to write (with Overwrite mode) DeltaSink is requested to add a streaming micro-batch (for Complete output mode)","title":" Converting to RemoveFile with Defaults"},{"location":"AddFile/#converting-to-removefile","text":"removeWithTimestamp ( timestamp : Long = System . currentTimeMillis (), dataChange : Boolean = true ) : RemoveFile remove creates a new RemoveFile for the path with the given timestamp and dataChange flag. removeWithTimestamp is used when: AddFile is requested to create a RemoveFile action with the defaults CreateDeltaTableCommand , DeleteCommand and UpdateCommand commands are executed DeltaCommand is requested to removeFilesFromPaths","title":" Converting to RemoveFile"},{"location":"CachedDS/","text":"CachedDS \u2014 Cached Delta State \u00b6 CachedDS is used when StateCache is requested to cacheDS . When created, CachedDS immediately initializes the cachedDs internal registry that requests the Dataset to generate a RDD[InternalRow] and associates the RDD with the given name : Delta Table State for Snapshot Delta Source Snapshot for DeltaSourceSnapshot The RDD is marked to be persisted using StorageLevel.MEMORY_AND_DISK_SER storage level. Note CachedDS is an internal class of StateCache and has access to its internals. Creating Instance \u00b6 CachedDS takes the following to be created: Dataset[A] Name CachedDS is created when StateCache is requested to cacheDS . getDS Method \u00b6 getDS : Dataset [ A ] getDS ...FIXME getDS is used when: Snapshot is requested to state DeltaSourceSnapshot is requested to initialFiles","title":"CachedDS"},{"location":"CachedDS/#cachedds-cached-delta-state","text":"CachedDS is used when StateCache is requested to cacheDS . When created, CachedDS immediately initializes the cachedDs internal registry that requests the Dataset to generate a RDD[InternalRow] and associates the RDD with the given name : Delta Table State for Snapshot Delta Source Snapshot for DeltaSourceSnapshot The RDD is marked to be persisted using StorageLevel.MEMORY_AND_DISK_SER storage level. Note CachedDS is an internal class of StateCache and has access to its internals.","title":"CachedDS &mdash; Cached Delta State"},{"location":"CachedDS/#creating-instance","text":"CachedDS takes the following to be created: Dataset[A] Name CachedDS is created when StateCache is requested to cacheDS .","title":"Creating Instance"},{"location":"CachedDS/#getds-method","text":"getDS : Dataset [ A ] getDS ...FIXME getDS is used when: Snapshot is requested to state DeltaSourceSnapshot is requested to initialFiles","title":" getDS Method"},{"location":"Checkpoints/","text":"Checkpoints \u00b6 Checkpoints is an < > of < > that can < > the current state of a delta table (represented by the < >). [[contract]] .Checkpoints Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | logPath a| [[logPath]] [source, scala] \u00b6 logPath: Path \u00b6 Used when...FIXME | dataPath a| [[dataPath]] [source, scala] \u00b6 dataPath: Path \u00b6 Used when...FIXME | snapshot a| [[snapshot]] [source, scala] \u00b6 snapshot: Snapshot \u00b6 Used when...FIXME | store a| [[store]] [source, scala] \u00b6 store: LogStore \u00b6 Used when...FIXME | metadata a| [[metadata]] [source, scala] \u00b6 metadata: Metadata \u00b6 < > of (the current state of) the < > Used when...FIXME | doLogCleanup a| [[doLogCleanup]] [source, scala] \u00b6 doLogCleanup(): Unit \u00b6 Used when...FIXME |=== [[implementations]][[self]] NOTE: < > is the default and only known Checkpoints in Delta Lake. == [[LAST_CHECKPOINT]][[_last_checkpoint]] _last_checkpoint Metadata File Checkpoints uses _last_checkpoint metadata file (under the < >) for the following: < > < > == [[checkpoint]] Checkpointing -- checkpoint Method [source, scala] \u00b6 checkpoint(): Unit checkpoint( snapshotToCheckpoint: Snapshot): CheckpointMetaData checkpoint ...FIXME NOTE: checkpoint is used when...FIXME Loading Latest Checkpoint Metadata \u00b6 lastCheckpoint : Option [ CheckpointMetaData ] lastCheckpoint simply loadMetadataFromFile (allowing for 3 retries). lastCheckpoint is used when: SnapshotManagement is requested to load the latest snapshot MetadataCleanup is requested to listExpiredDeltaLogs loadMetadataFromFile Helper Method \u00b6 loadMetadataFromFile ( tries : Int ) : Option [ CheckpointMetaData ] loadMetadataFromFile loads the _last_checkpoint file (in JSON format) and converts it to CheckpointMetaData (with a version, size and parts). loadMetadataFromFile uses the LogStore to read the _last_checkpoint file. In case the _last_checkpoint file is corrupted, loadMetadataFromFile ...FIXME == [[manuallyLoadCheckpoint]] manuallyLoadCheckpoint Method [source, scala] \u00b6 manuallyLoadCheckpoint(cv: CheckpointInstance): CheckpointMetaData \u00b6 manuallyLoadCheckpoint ...FIXME NOTE: manuallyLoadCheckpoint is used when...FIXME == [[findLastCompleteCheckpoint]] findLastCompleteCheckpoint Method [source, scala] \u00b6 findLastCompleteCheckpoint(cv: CheckpointInstance): Option[CheckpointInstance] \u00b6 findLastCompleteCheckpoint ...FIXME NOTE: findLastCompleteCheckpoint is used when...FIXME == [[getLatestCompleteCheckpointFromList]] getLatestCompleteCheckpointFromList Method [source, scala] \u00b6 getLatestCompleteCheckpointFromList( instances: Array[CheckpointInstance], notLaterThan: CheckpointInstance): Option[CheckpointInstance] getLatestCompleteCheckpointFromList ...FIXME NOTE: getLatestCompleteCheckpointFromList is used when...FIXME == [[writeCheckpoint]] writeCheckpoint Utility [source, scala] \u00b6 writeCheckpoint( spark: SparkSession, deltaLog: DeltaLog, snapshot: Snapshot): CheckpointMetaData writeCheckpoint ...FIXME NOTE: writeCheckpoint is used when Checkpoints is requested to < >.","title":"Checkpoints"},{"location":"Checkpoints/#checkpoints","text":"Checkpoints is an < > of < > that can < > the current state of a delta table (represented by the < >). [[contract]] .Checkpoints Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | logPath a| [[logPath]]","title":"Checkpoints"},{"location":"Checkpoints/#source-scala","text":"","title":"[source, scala]"},{"location":"Checkpoints/#logpath-path","text":"Used when...FIXME | dataPath a| [[dataPath]]","title":"logPath: Path"},{"location":"Checkpoints/#source-scala_1","text":"","title":"[source, scala]"},{"location":"Checkpoints/#datapath-path","text":"Used when...FIXME | snapshot a| [[snapshot]]","title":"dataPath: Path"},{"location":"Checkpoints/#source-scala_2","text":"","title":"[source, scala]"},{"location":"Checkpoints/#snapshot-snapshot","text":"Used when...FIXME | store a| [[store]]","title":"snapshot: Snapshot"},{"location":"Checkpoints/#source-scala_3","text":"","title":"[source, scala]"},{"location":"Checkpoints/#store-logstore","text":"Used when...FIXME | metadata a| [[metadata]]","title":"store: LogStore"},{"location":"Checkpoints/#source-scala_4","text":"","title":"[source, scala]"},{"location":"Checkpoints/#metadata-metadata","text":"< > of (the current state of) the < > Used when...FIXME | doLogCleanup a| [[doLogCleanup]]","title":"metadata: Metadata"},{"location":"Checkpoints/#source-scala_5","text":"","title":"[source, scala]"},{"location":"Checkpoints/#dologcleanup-unit","text":"Used when...FIXME |=== [[implementations]][[self]] NOTE: < > is the default and only known Checkpoints in Delta Lake. == [[LAST_CHECKPOINT]][[_last_checkpoint]] _last_checkpoint Metadata File Checkpoints uses _last_checkpoint metadata file (under the < >) for the following: < > < > == [[checkpoint]] Checkpointing -- checkpoint Method","title":"doLogCleanup(): Unit"},{"location":"Checkpoints/#source-scala_6","text":"checkpoint(): Unit checkpoint( snapshotToCheckpoint: Snapshot): CheckpointMetaData checkpoint ...FIXME NOTE: checkpoint is used when...FIXME","title":"[source, scala]"},{"location":"Checkpoints/#loading-latest-checkpoint-metadata","text":"lastCheckpoint : Option [ CheckpointMetaData ] lastCheckpoint simply loadMetadataFromFile (allowing for 3 retries). lastCheckpoint is used when: SnapshotManagement is requested to load the latest snapshot MetadataCleanup is requested to listExpiredDeltaLogs","title":" Loading Latest Checkpoint Metadata"},{"location":"Checkpoints/#loadmetadatafromfile-helper-method","text":"loadMetadataFromFile ( tries : Int ) : Option [ CheckpointMetaData ] loadMetadataFromFile loads the _last_checkpoint file (in JSON format) and converts it to CheckpointMetaData (with a version, size and parts). loadMetadataFromFile uses the LogStore to read the _last_checkpoint file. In case the _last_checkpoint file is corrupted, loadMetadataFromFile ...FIXME == [[manuallyLoadCheckpoint]] manuallyLoadCheckpoint Method","title":" loadMetadataFromFile Helper Method"},{"location":"Checkpoints/#source-scala_7","text":"","title":"[source, scala]"},{"location":"Checkpoints/#manuallyloadcheckpointcv-checkpointinstance-checkpointmetadata","text":"manuallyLoadCheckpoint ...FIXME NOTE: manuallyLoadCheckpoint is used when...FIXME == [[findLastCompleteCheckpoint]] findLastCompleteCheckpoint Method","title":"manuallyLoadCheckpoint(cv: CheckpointInstance): CheckpointMetaData"},{"location":"Checkpoints/#source-scala_8","text":"","title":"[source, scala]"},{"location":"Checkpoints/#findlastcompletecheckpointcv-checkpointinstance-optioncheckpointinstance","text":"findLastCompleteCheckpoint ...FIXME NOTE: findLastCompleteCheckpoint is used when...FIXME == [[getLatestCompleteCheckpointFromList]] getLatestCompleteCheckpointFromList Method","title":"findLastCompleteCheckpoint(cv: CheckpointInstance): Option[CheckpointInstance]"},{"location":"Checkpoints/#source-scala_9","text":"getLatestCompleteCheckpointFromList( instances: Array[CheckpointInstance], notLaterThan: CheckpointInstance): Option[CheckpointInstance] getLatestCompleteCheckpointFromList ...FIXME NOTE: getLatestCompleteCheckpointFromList is used when...FIXME == [[writeCheckpoint]] writeCheckpoint Utility","title":"[source, scala]"},{"location":"Checkpoints/#source-scala_10","text":"writeCheckpoint( spark: SparkSession, deltaLog: DeltaLog, snapshot: Snapshot): CheckpointMetaData writeCheckpoint ...FIXME NOTE: writeCheckpoint is used when Checkpoints is requested to < >.","title":"[source, scala]"},{"location":"CommitInfo/","text":"= CommitInfo CommitInfo is an Action.md[] with the following: [[version]] Version [[timestamp]] Timestamp [[userId]] User ID [[userName]] User Name [[operation]] Operation.md#name[Name of the operation] [[operationParameters]] Operation.md#parameters[Parameters of the operation] [[job]] JobInfo [[notebook]] NotebookInfo [[clusterId]] Cluster ID [[readVersion]] Read Version [[isolationLevel]] Isolation Level [[isBlindAppend]] isBlindAppend flag (to indicate whether a commit has blindly appended without caring about existing files) [[operationMetrics]] Metrics of the operation [[userMetadata]] User metadata CommitInfo is created (using < > utility) when: OptimisticTransactionImpl is requested to OptimisticTransactionImpl.md#commit[commit] ConvertToDeltaCommand command is requested to ConvertToDeltaCommand.md#streamWrite[streamWrite] (when executed) CommitInfo is used in OptimisticTransactionImpl.md#commitInfo[OptimisticTransactionImpl] and CommitStats. CommitInfo is added ( logged ) to a Delta log only for DeltaSQLConf.md#commitInfo.enabled[spark.databricks.delta.commitInfo.enabled] configuration enabled. == [[apply]] apply Utility [source,scala] \u00b6 apply( time: Long, operation: String, operationParameters: Map[String, String], commandContext: Map[String, String], readVersion: Option[Long], isolationLevel: Option[String], isBlindAppend: Option[Boolean], operationMetrics: Option[Map[String, String]], userMetadata: Option[String]): CommitInfo apply creates a CommitInfo (for the given arguments and based on the given commandContext for the user ID, user name, job, notebook, cluster). NOTE: commandContext is always empty, but could be customized using ConvertToDeltaCommand.md#ConvertToDeltaCommandBase[ConvertToDeltaCommandBase]. apply is used when: OptimisticTransactionImpl is requested to OptimisticTransactionImpl.md#commit[commit] ConvertToDeltaCommand command is requested to ConvertToDeltaCommand.md#streamWrite[streamWrite] (when executed)","title":"CommitInfo"},{"location":"CommitInfo/#sourcescala","text":"apply( time: Long, operation: String, operationParameters: Map[String, String], commandContext: Map[String, String], readVersion: Option[Long], isolationLevel: Option[String], isBlindAppend: Option[Boolean], operationMetrics: Option[Map[String, String]], userMetadata: Option[String]): CommitInfo apply creates a CommitInfo (for the given arguments and based on the given commandContext for the user ID, user name, job, notebook, cluster). NOTE: commandContext is always empty, but could be customized using ConvertToDeltaCommand.md#ConvertToDeltaCommandBase[ConvertToDeltaCommandBase]. apply is used when: OptimisticTransactionImpl is requested to OptimisticTransactionImpl.md#commit[commit] ConvertToDeltaCommand command is requested to ConvertToDeltaCommand.md#streamWrite[streamWrite] (when executed)","title":"[source,scala]"},{"location":"DelayedCommitProtocol/","text":"DelayedCommitProtocol \u00b6 DelayedCommitProtocol is used to model a distributed write that is orchestrated by the Spark driver with the write itself happening on executors. DelayedCommitProtocol is a concrete FileCommitProtocol (Spark Core) to write out a result of a structured query to a < > and return a < >. FileCommitProtocol (Spark Core) allows to track a write job (with a write task per partition) and inform the driver when all the write tasks finished successfully (and were < >) to consider the write job < >. TaskCommitMessage (Spark Core) allows to \"transfer\" the files added (written out) on the executors to the driver for the < >. TIP: Read up on https://books.japila.pl/apache-spark-internals/apache-spark-internals/2.4.4/spark-internal-io-FileCommitProtocol.html[FileCommitProtocol ] in https://books.japila.pl/apache-spark-internals[The Internals Of Apache Spark] online book. DelayedCommitProtocol is < > exclusively when TransactionalWrite is requested for a < > to < > to the < >. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.delta.files.DelayedCommitProtocol logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.files.DelayedCommitProtocol=ALL Refer to Logging . \u00b6 == [[creating-instance]] Creating DelayedCommitProtocol Instance DelayedCommitProtocol takes the following to be created: [[jobId]] Job ID (seems always < >) [[path]] Directory (to write files to) [[randomPrefixLength]] Optional length of a random prefix (seems always < >) DelayedCommitProtocol initializes the < >. == [[setupTask]] setupTask Method [source, scala] \u00b6 setupTask( taskContext: TaskAttemptContext): Unit NOTE: setupTask is part of the FileCommitProtocol contract to set up a task for a writing job. setupTask simply initializes the < > internal registry to be empty. == [[newTaskTempFile]] newTaskTempFile Method [source, scala] \u00b6 newTaskTempFile( taskContext: TaskAttemptContext, dir: Option[String], ext: String): String NOTE: newTaskTempFile is part of the FileCommitProtocol contract to inform the committer to add a new file. newTaskTempFile < > for the given TaskAttemptContext and ext . newTaskTempFile tries to < > with the given dir or falls back to an empty partitionValues . NOTE: The given dir defines a partition directory if the streaming query (and hence the write) is partitioned. newTaskTempFile builds a path (based on the given randomPrefixLength and the dir , or uses the file name directly). NOTE: FIXME When would the optional dir and the < > be defined? newTaskTempFile adds the partition values and the relative path to the < > internal registry. In the end, newTaskTempFile returns the absolute path of the (relative) path in the < >. == [[commitTask]] Committing Task (After Successful Write) -- commitTask Method [source, scala] \u00b6 commitTask( taskContext: TaskAttemptContext): TaskCommitMessage NOTE: commitTask is part of the FileCommitProtocol contract to commit a task after the writes succeed. commitTask simply creates a TaskCommitMessage with an < > for every < > if there were any. Otherwise, the TaskCommitMessage is empty. NOTE: A file is added (to < > internal registry) when DelayedCommitProtocol is requested for a < >. == [[commitJob]] Committing Spark Job (After Successful Write) -- commitJob Method [source, scala] \u00b6 commitJob( jobContext: JobContext, taskCommits: Seq[TaskCommitMessage]): Unit NOTE: commitJob is part of the FileCommitProtocol contract to commit a job after the writes succeed. commitJob simply adds the < > (from the given taskCommits from every < >) to the < > internal registry. == [[parsePartitions]] parsePartitions Method [source, scala] \u00b6 parsePartitions( dir: String): Map[String, String] parsePartitions ...FIXME NOTE: parsePartitions is used exclusively when DelayedCommitProtocol is requested to < >. == [[setupJob]] setupJob Method [source, scala] \u00b6 setupJob( jobContext: JobContext): Unit NOTE: setupJob is part of the FileCommitProtocol contract to set up a Spark job. setupJob does nothing. == [[abortJob]] abortJob Method [source, scala] \u00b6 abortJob( jobContext: JobContext): Unit NOTE: abortJob is part of the FileCommitProtocol contract to abort a Spark job. abortJob does nothing. == [[getFileName]] getFileName Method [source, scala] \u00b6 getFileName( taskContext: TaskAttemptContext, ext: String): String getFileName takes the task ID from the given TaskAttemptContext (for the split part below). getFileName generates a random UUID (for the uuid part below). In the end, getFileName returns a file name of the format: part-[split]%05d-[uuid][ext] NOTE: getFileName is used exclusively when DelayedCommitProtocol is requested to < >. == [[addedFiles]] addedFiles Internal Registry [source, scala] \u00b6 addedFiles: ArrayBuffer[(Map[String, String], String)] \u00b6 addedFiles tracks the files < > (that runs on an executor). addedFiles is initialized (as an empty collection) in < >. NOTE: addedFiles is used when DelayedCommitProtocol is requested to < > (on an executor and create a TaskCommitMessage with the files added while a task was writing out a partition of a streaming query). == [[addedStatuses]] addedStatuses Internal Registry [source, scala] \u00b6 addedStatuses = new ArrayBuffer[AddFile] \u00b6 addedStatuses is the files that were added by < > (on executors) once all they finish successfully and the < > (on a driver). [NOTE] \u00b6 addedStatuses is used when: DelayedCommitProtocol is requested to < > (on a driver) * TransactionalWrite is requested to < > \u00b6","title":"DelayedCommitProtocol"},{"location":"DelayedCommitProtocol/#delayedcommitprotocol","text":"DelayedCommitProtocol is used to model a distributed write that is orchestrated by the Spark driver with the write itself happening on executors. DelayedCommitProtocol is a concrete FileCommitProtocol (Spark Core) to write out a result of a structured query to a < > and return a < >. FileCommitProtocol (Spark Core) allows to track a write job (with a write task per partition) and inform the driver when all the write tasks finished successfully (and were < >) to consider the write job < >. TaskCommitMessage (Spark Core) allows to \"transfer\" the files added (written out) on the executors to the driver for the < >. TIP: Read up on https://books.japila.pl/apache-spark-internals/apache-spark-internals/2.4.4/spark-internal-io-FileCommitProtocol.html[FileCommitProtocol ] in https://books.japila.pl/apache-spark-internals[The Internals Of Apache Spark] online book. DelayedCommitProtocol is < > exclusively when TransactionalWrite is requested for a < > to < > to the < >. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.delta.files.DelayedCommitProtocol logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.files.DelayedCommitProtocol=ALL","title":"DelayedCommitProtocol"},{"location":"DelayedCommitProtocol/#refer-to-logging","text":"== [[creating-instance]] Creating DelayedCommitProtocol Instance DelayedCommitProtocol takes the following to be created: [[jobId]] Job ID (seems always < >) [[path]] Directory (to write files to) [[randomPrefixLength]] Optional length of a random prefix (seems always < >) DelayedCommitProtocol initializes the < >. == [[setupTask]] setupTask Method","title":"Refer to Logging."},{"location":"DelayedCommitProtocol/#source-scala","text":"setupTask( taskContext: TaskAttemptContext): Unit NOTE: setupTask is part of the FileCommitProtocol contract to set up a task for a writing job. setupTask simply initializes the < > internal registry to be empty. == [[newTaskTempFile]] newTaskTempFile Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_1","text":"newTaskTempFile( taskContext: TaskAttemptContext, dir: Option[String], ext: String): String NOTE: newTaskTempFile is part of the FileCommitProtocol contract to inform the committer to add a new file. newTaskTempFile < > for the given TaskAttemptContext and ext . newTaskTempFile tries to < > with the given dir or falls back to an empty partitionValues . NOTE: The given dir defines a partition directory if the streaming query (and hence the write) is partitioned. newTaskTempFile builds a path (based on the given randomPrefixLength and the dir , or uses the file name directly). NOTE: FIXME When would the optional dir and the < > be defined? newTaskTempFile adds the partition values and the relative path to the < > internal registry. In the end, newTaskTempFile returns the absolute path of the (relative) path in the < >. == [[commitTask]] Committing Task (After Successful Write) -- commitTask Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_2","text":"commitTask( taskContext: TaskAttemptContext): TaskCommitMessage NOTE: commitTask is part of the FileCommitProtocol contract to commit a task after the writes succeed. commitTask simply creates a TaskCommitMessage with an < > for every < > if there were any. Otherwise, the TaskCommitMessage is empty. NOTE: A file is added (to < > internal registry) when DelayedCommitProtocol is requested for a < >. == [[commitJob]] Committing Spark Job (After Successful Write) -- commitJob Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_3","text":"commitJob( jobContext: JobContext, taskCommits: Seq[TaskCommitMessage]): Unit NOTE: commitJob is part of the FileCommitProtocol contract to commit a job after the writes succeed. commitJob simply adds the < > (from the given taskCommits from every < >) to the < > internal registry. == [[parsePartitions]] parsePartitions Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_4","text":"parsePartitions( dir: String): Map[String, String] parsePartitions ...FIXME NOTE: parsePartitions is used exclusively when DelayedCommitProtocol is requested to < >. == [[setupJob]] setupJob Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_5","text":"setupJob( jobContext: JobContext): Unit NOTE: setupJob is part of the FileCommitProtocol contract to set up a Spark job. setupJob does nothing. == [[abortJob]] abortJob Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_6","text":"abortJob( jobContext: JobContext): Unit NOTE: abortJob is part of the FileCommitProtocol contract to abort a Spark job. abortJob does nothing. == [[getFileName]] getFileName Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_7","text":"getFileName( taskContext: TaskAttemptContext, ext: String): String getFileName takes the task ID from the given TaskAttemptContext (for the split part below). getFileName generates a random UUID (for the uuid part below). In the end, getFileName returns a file name of the format: part-[split]%05d-[uuid][ext] NOTE: getFileName is used exclusively when DelayedCommitProtocol is requested to < >. == [[addedFiles]] addedFiles Internal Registry","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_8","text":"","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#addedfiles-arraybuffermapstring-string-string","text":"addedFiles tracks the files < > (that runs on an executor). addedFiles is initialized (as an empty collection) in < >. NOTE: addedFiles is used when DelayedCommitProtocol is requested to < > (on an executor and create a TaskCommitMessage with the files added while a task was writing out a partition of a streaming query). == [[addedStatuses]] addedStatuses Internal Registry","title":"addedFiles: ArrayBuffer[(Map[String, String], String)]"},{"location":"DelayedCommitProtocol/#source-scala_9","text":"","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#addedstatuses-new-arraybufferaddfile","text":"addedStatuses is the files that were added by < > (on executors) once all they finish successfully and the < > (on a driver).","title":"addedStatuses = new ArrayBuffer[AddFile]"},{"location":"DelayedCommitProtocol/#note","text":"addedStatuses is used when: DelayedCommitProtocol is requested to < > (on a driver)","title":"[NOTE]"},{"location":"DelayedCommitProtocol/#transactionalwrite-is-requested-to","text":"","title":"* TransactionalWrite is requested to &lt;&gt;"},{"location":"DeltaAnalysis/","text":"DeltaAnalysis Logical Resolution Rule \u00b6 DeltaAnalysis is a logical resolution rule (Spark SQL's Rule[LogicalPlan] ) for INSERT INTO and INSERT OVERWRITE SQL commands (and DeleteFromTable, UpdateTable, MergeIntoTable). == [[creating-instance]] Creating Instance DeltaAnalysis takes the following to be created: [[session]] SparkSession [[conf]] SQLConf DeltaAnalysis is created when DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support]. == [[apply]] Executing Rule [source, scala] \u00b6 apply( plan: LogicalPlan): LogicalPlan apply...FIXME apply is part of the Rule (Spark SQL) abstraction.","title":"DeltaAnalysis"},{"location":"DeltaAnalysis/#deltaanalysis-logical-resolution-rule","text":"DeltaAnalysis is a logical resolution rule (Spark SQL's Rule[LogicalPlan] ) for INSERT INTO and INSERT OVERWRITE SQL commands (and DeleteFromTable, UpdateTable, MergeIntoTable). == [[creating-instance]] Creating Instance DeltaAnalysis takes the following to be created: [[session]] SparkSession [[conf]] SQLConf DeltaAnalysis is created when DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support]. == [[apply]] Executing Rule","title":"DeltaAnalysis Logical Resolution Rule"},{"location":"DeltaAnalysis/#source-scala","text":"apply( plan: LogicalPlan): LogicalPlan apply...FIXME apply is part of the Rule (Spark SQL) abstraction.","title":"[source, scala]"},{"location":"DeltaCatalog/","text":"DeltaCatalog \u00b6 DeltaCatalog is a DelegatingCatalogExtension ( Spark SQL ) and a StagingTableCatalog ( Spark SQL ). DeltaCatalog is registered using spark.sql.catalog.spark_catalog configuration property (while creating a SparkSession in a Spark application). Altering Table \u00b6 alterTable ( ident : Identifier , changes : TableChange* ) : Table alterTable ...FIXME alterTable is part of the TableCatalog ( Spark SQL ) abstraction. Creating Table \u00b6 createTable ( ident : Identifier , schema : StructType , partitions : Array [ Transform ], properties : util.Map [ String , String ]) : Table createTable ...FIXME createTable is part of the TableCatalog ( Spark SQL ) abstraction. Loading Table \u00b6 loadTable ( ident : Identifier ) : Table loadTable loads a table by the given identifier from a catalog. If found and the table is a delta table (Spark SQL's V1Table with delta provider), loadTable creates a DeltaTableV2 . loadTable is part of the TableCatalog ( Spark SQL ) abstraction. Creating Delta Table \u00b6 createDeltaTable ( ident : Identifier , schema : StructType , partitions : Array [ Transform ], properties : util.Map [ String , String ], sourceQuery : Option [ LogicalPlan ], operation : TableCreationModes.CreationMode ) : Table createDeltaTable ...FIXME createDeltaTable is used when: DeltaCatalog is requested to createTable StagedDeltaTableV2 is requested to commitStagedChanges","title":"DeltaCatalog"},{"location":"DeltaCatalog/#deltacatalog","text":"DeltaCatalog is a DelegatingCatalogExtension ( Spark SQL ) and a StagingTableCatalog ( Spark SQL ). DeltaCatalog is registered using spark.sql.catalog.spark_catalog configuration property (while creating a SparkSession in a Spark application).","title":"DeltaCatalog"},{"location":"DeltaCatalog/#altering-table","text":"alterTable ( ident : Identifier , changes : TableChange* ) : Table alterTable ...FIXME alterTable is part of the TableCatalog ( Spark SQL ) abstraction.","title":" Altering Table"},{"location":"DeltaCatalog/#creating-table","text":"createTable ( ident : Identifier , schema : StructType , partitions : Array [ Transform ], properties : util.Map [ String , String ]) : Table createTable ...FIXME createTable is part of the TableCatalog ( Spark SQL ) abstraction.","title":" Creating Table"},{"location":"DeltaCatalog/#loading-table","text":"loadTable ( ident : Identifier ) : Table loadTable loads a table by the given identifier from a catalog. If found and the table is a delta table (Spark SQL's V1Table with delta provider), loadTable creates a DeltaTableV2 . loadTable is part of the TableCatalog ( Spark SQL ) abstraction.","title":" Loading Table"},{"location":"DeltaCatalog/#creating-delta-table","text":"createDeltaTable ( ident : Identifier , schema : StructType , partitions : Array [ Transform ], properties : util.Map [ String , String ], sourceQuery : Option [ LogicalPlan ], operation : TableCreationModes.CreationMode ) : Table createDeltaTable ...FIXME createDeltaTable is used when: DeltaCatalog is requested to createTable StagedDeltaTableV2 is requested to commitStagedChanges","title":" Creating Delta Table"},{"location":"DeltaConfig/","text":"= DeltaConfig -- Configuration Property Of Delta Table (Metadata) [[T]] DeltaConfig (of type T ) represents a < > of a delta table with values (of type T ) in an < >. DeltaConfig can be < >. == [[creating-instance]] Creating DeltaConfig Instance DeltaConfig takes the following to be created: [[key]] Key [[defaultValue]] Default value [[fromString]] Conversion function (from text representation of the DeltaConfig to the < >, i.e. String => T ) [[validationFunction]] Validation function (that guards from incorrect values, i.e. T => Boolean ) [[helpMessage]] Help message [[minimumProtocolVersion]] (optional) Minimum version of < > supported DeltaConfig initializes the < >. == [[fromMetaData]] Reading Configuration Property From Metadata -- fromMetaData Method [source, scala] \u00b6 fromMetaData( metadata: Metadata): T fromMetaData looks up the < > in the < > of the given < >. If not found, fromMetaData gives the < >. In the end, fromMetaData converts the text representation to the proper type using < > conversion function. [NOTE] \u00b6 fromMetaData is used when: DeltaLog is requested for < > and < > table properties, and to < > MetadataCleanup is requested for the < > and the < > * Snapshot is requested for the < > \u00b6","title":"DeltaConfig"},{"location":"DeltaConfig/#source-scala","text":"fromMetaData( metadata: Metadata): T fromMetaData looks up the < > in the < > of the given < >. If not found, fromMetaData gives the < >. In the end, fromMetaData converts the text representation to the proper type using < > conversion function.","title":"[source, scala]"},{"location":"DeltaConfig/#note","text":"fromMetaData is used when: DeltaLog is requested for < > and < > table properties, and to < > MetadataCleanup is requested for the < > and the < >","title":"[NOTE]"},{"location":"DeltaConfig/#snapshot-is-requested-for-the","text":"","title":"* Snapshot is requested for the &lt;&gt;"},{"location":"DeltaConfigs/","text":"= DeltaConfigs DeltaConfigs defines < > (aka table properties ). Table properties can be assigned a value using ALTER TABLE SQL command: ALTER TABLE <table_name> SET TBLPROPERTIES (<key>=<value>) [[sqlConfPrefix]][[spark.databricks.delta.properties.defaults]] DeltaConfigs uses spark.databricks.delta.properties.defaults prefix for < >. [[configuration-properties]] .Reservoir Configuration Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Key | Description | appendOnly a| [[appendOnly]][[IS_APPEND_ONLY]] Whether a delta table is append-only ( true ) or not ( false ). When enabled, a table allows appends only and no updates or deletes. Default: false | autoOptimize a| [[autoOptimize]][[AUTO_OPTIMIZE]] Whether this delta table will automagically optimize the layout of files during writes. Default: false | checkpointInterval a| [[checkpointInterval]][[CHECKPOINT_INTERVAL]] How often to checkpoint the state of a delta table Default: 10 | checkpointRetentionDuration a| [[checkpointRetentionDuration]][[CHECKPOINT_RETENTION_DURATION]] How long to keep checkpoint files around before deleting them Default: interval 2 days The most recent checkpoint is never deleted. It is acceptable to keep checkpoint files beyond this duration until the next calendar day. | compatibility.symlinkFormatManifest.enabled a| [[compatibility.symlinkFormatManifest]][[SYMLINK_FORMAT_MANIFEST_ENABLED]] Whether to register the < > post-commit hook while < > or not Default: false | dataSkippingNumIndexedCols a| [[dataSkippingNumIndexedCols]][[DATA_SKIPPING_NUM_INDEXED_COLS]] The number of columns to collect stats on for data skipping. -1 means collecting stats for all columns. Default: 32 | deletedFileRetentionDuration a| [[deletedFileRetentionDuration]][[TOMBSTONE_RETENTION]] How long to keep logically deleted data files around before deleting them physically (to prevent failures in stale readers after compactions or partition overwrites) Default: interval 1 week | enableExpiredLogCleanup a| [[enableExpiredLogCleanup]][[ENABLE_EXPIRED_LOG_CLEANUP]] Whether to clean up expired log files and checkpoints Default: true | enableFullRetentionRollback a| [[enableFullRetentionRollback]][[ENABLE_FULL_RETENTION_ROLLBACK]] When enabled (default), a delta table can be rolled back to any point within < >. When disabled, the table can be rolled back < > only. Default: true | logRetentionDuration a| [[logRetentionDuration]][[LOG_RETENTION]] How long to keep obsolete logs around before deleting them. Delta can keep logs beyond the duration until the next calendar day to avoid constantly creating checkpoints. Default: interval 30 days | randomizeFilePrefixes a| [[randomizeFilePrefixes]][[RANDOMIZE_FILE_PREFIXES]] Whether to use a random prefix in a file path instead of partition information (may be required for very high volume S3 calls to better be partitioned across S3 servers) Default: false | randomPrefixLength a| [[randomPrefixLength]][[RANDOM_PREFIX_LENGTH]] The length of the random prefix in a file path for < > Default: 2 | sampleRetentionDuration a| [[sampleRetentionDuration]][[SAMPLE_RETENTION]] How long to keep delta sample files around before deleting them Default: interval 7 days |=== == [[mergeGlobalConfigs]] mergeGlobalConfigs Utility [source, scala] \u00b6 mergeGlobalConfigs( sqlConfs: SQLConf, tableConf: Map[String, String], protocol: Protocol): Map[String, String] mergeGlobalConfigs finds all < >-prefixed configuration properties among the < >. NOTE: mergeGlobalConfigs is used when OptimisticTransactionImpl is requested for the OptimisticTransactionImpl.md#snapshotMetadata[metadata], to OptimisticTransactionImpl.md#updateMetadata[update the metadata], and OptimisticTransactionImpl.md#prepareCommit[prepare a commit] (for new delta tables). == [[verifyProtocolVersionRequirements]] verifyProtocolVersionRequirements Utility [source, scala] \u00b6 verifyProtocolVersionRequirements( configurations: Map[String, String], current: Protocol): Unit verifyProtocolVersionRequirements...FIXME verifyProtocolVersionRequirements is used when...FIXME == [[buildConfig]] Creating DeltaConfig Instance -- buildConfig Internal Utility [source, scala] \u00b6 buildConfig T : DeltaConfig[T] buildConfig creates a DeltaConfig.md[DeltaConfig] for the given key (with delta prefix) and adds it to the < > internal registry. NOTE: buildConfig is used to define all of the < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | entries a| [[entries]] [source, scala] \u00b6 HashMap[String, DeltaConfig[_]] \u00b6 |===","title":"DeltaConfigs"},{"location":"DeltaConfigs/#source-scala","text":"mergeGlobalConfigs( sqlConfs: SQLConf, tableConf: Map[String, String], protocol: Protocol): Map[String, String] mergeGlobalConfigs finds all < >-prefixed configuration properties among the < >. NOTE: mergeGlobalConfigs is used when OptimisticTransactionImpl is requested for the OptimisticTransactionImpl.md#snapshotMetadata[metadata], to OptimisticTransactionImpl.md#updateMetadata[update the metadata], and OptimisticTransactionImpl.md#prepareCommit[prepare a commit] (for new delta tables). == [[verifyProtocolVersionRequirements]] verifyProtocolVersionRequirements Utility","title":"[source, scala]"},{"location":"DeltaConfigs/#source-scala_1","text":"verifyProtocolVersionRequirements( configurations: Map[String, String], current: Protocol): Unit verifyProtocolVersionRequirements...FIXME verifyProtocolVersionRequirements is used when...FIXME == [[buildConfig]] Creating DeltaConfig Instance -- buildConfig Internal Utility","title":"[source, scala]"},{"location":"DeltaConfigs/#source-scala_2","text":"buildConfig T : DeltaConfig[T] buildConfig creates a DeltaConfig.md[DeltaConfig] for the given key (with delta prefix) and adds it to the < > internal registry. NOTE: buildConfig is used to define all of the < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | entries a| [[entries]]","title":"[source, scala]"},{"location":"DeltaConfigs/#source-scala_3","text":"","title":"[source, scala]"},{"location":"DeltaConfigs/#hashmapstring-deltaconfig_","text":"|===","title":"HashMap[String, DeltaConfig[_]]"},{"location":"DeltaConvert/","text":"DeltaConvert Utility \u00b6 DeltaConvert utility is used exclusively for < >. DeltaConvert utility can be used directly or indirectly via < > utility. import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) // CONVERT TO DELTA only supports parquet tables // TableIdentifier should be parquet.`users` import org.apache.spark.sql.catalyst.TableIdentifier val table = TableIdentifier(table = \"users\", database = Some(\"parquet\")) import org.apache.spark.sql.types.{StringType, StructField, StructType} val partitionSchema: Option[StructType] = Some( new StructType().add(StructField(\"country\", StringType))) val deltaPath: Option[String] = None // Use web UI to monitor execution, e.g. http://localhost:4040 import io.delta.tables.execution.DeltaConvert DeltaConvert.executeConvert(spark, table, partitionSchema, deltaPath) DeltaConvert utility is a concrete DeltaConvertBase . Importing Parquet Table Into Delta Lake (Converting Parquet Table To Delta Format) \u00b6 executeConvert ( spark : SparkSession , tableIdentifier : TableIdentifier , partitionSchema : Option [ StructType ], deltaPath : Option [ String ]) : DeltaTable executeConvert creates a new ConvertToDeltaCommand and executes it. In the end, executeConvert creates a DeltaTable . Note executeConvert can convert a Spark table (to Delta) that is registered in a metastore. executeConvert is used when DeltaTable utility is requested to convert a parquet table to delta format (DeltaTable.convertToDelta) .","title":"DeltaConvert"},{"location":"DeltaConvert/#deltaconvert-utility","text":"DeltaConvert utility is used exclusively for < >. DeltaConvert utility can be used directly or indirectly via < > utility. import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) // CONVERT TO DELTA only supports parquet tables // TableIdentifier should be parquet.`users` import org.apache.spark.sql.catalyst.TableIdentifier val table = TableIdentifier(table = \"users\", database = Some(\"parquet\")) import org.apache.spark.sql.types.{StringType, StructField, StructType} val partitionSchema: Option[StructType] = Some( new StructType().add(StructField(\"country\", StringType))) val deltaPath: Option[String] = None // Use web UI to monitor execution, e.g. http://localhost:4040 import io.delta.tables.execution.DeltaConvert DeltaConvert.executeConvert(spark, table, partitionSchema, deltaPath) DeltaConvert utility is a concrete DeltaConvertBase .","title":"DeltaConvert Utility"},{"location":"DeltaConvert/#importing-parquet-table-into-delta-lake-converting-parquet-table-to-delta-format","text":"executeConvert ( spark : SparkSession , tableIdentifier : TableIdentifier , partitionSchema : Option [ StructType ], deltaPath : Option [ String ]) : DeltaTable executeConvert creates a new ConvertToDeltaCommand and executes it. In the end, executeConvert creates a DeltaTable . Note executeConvert can convert a Spark table (to Delta) that is registered in a metastore. executeConvert is used when DeltaTable utility is requested to convert a parquet table to delta format (DeltaTable.convertToDelta) .","title":" Importing Parquet Table Into Delta Lake (Converting Parquet Table To Delta Format)"},{"location":"DeltaDataSource/","text":"DeltaDataSource \u00b6 DeltaDataSource is a DataSourceRegister and is the entry point to all the features provided by delta data source. DeltaDataSource is a RelationProvider . DeltaDataSource is a StreamSinkProvider for a streaming sink for streaming queries (Structured Streaming). DataSourceRegister and delta Alias \u00b6 DeltaDataSource is a DataSourceRegister and registers delta alias. Tip Read up on DataSourceRegister in The Internals of Spark SQL online book. DeltaDataSource is registered using META-INF/services/org.apache.spark.sql.sources.DataSourceRegister : org.apache.spark.sql.delta.sources.DeltaDataSource RelationProvider for Batch Queries \u00b6 DeltaDataSource is a RelationProvider for reading ( loading ) data from a delta table in a structured query. Tip Read up on RelationProvider in The Internals of Spark SQL online book. createRelation ( sqlContext : SQLContext , parameters : Map [ String , String ]) : BaseRelation createRelation reads the path option from the given parameters. createRelation verifies the given parameters . createRelation extracts time travel specification from the given parameters. In the end, createRelation creates a DeltaTableV2 (for the path option and the time travel specification) and requests it for an insertable HadoopFsRelation . createRelation throws an IllegalArgumentException when path option is not specified: 'path' is not specified Source Schema \u00b6 sourceSchema ( sqlContext : SQLContext , schema : Option [ StructType ], providerName : String , parameters : Map [ String , String ]) : ( String , StructType ) sourceSchema creates a DeltaLog for a Delta table in the directory specified by the required path option (in the parameters) and returns the delta name with the schema (of the Delta table). sourceSchema throws an IllegalArgumentException when the path option has not been specified: 'path' is not specified sourceSchema throws an AnalysisException when the path option uses time travel : Cannot time travel views, subqueries or streams. sourceSchema is part of the StreamSourceProvider abstraction ( Spark Structured Streaming ). CreatableRelationProvider \u00b6 DeltaDataSource is a CreatableRelationProvider for writing out the result of a structured query. Tip Read up on CreatableRelationProvider in The Internals of Spark SQL online book. Creating Streaming Source \u00b6 DeltaDataSource is a StreamSourceProvider . Tip Read up on StreamSourceProvider in The Internals of Spark Structured Streaming online book. Creating Streaming Sink \u00b6 DeltaDataSource is a StreamSinkProvider for a streaming sink for Structured Streaming. Tip Read up on StreamSinkProvider in The Internals of Spark Structured Streaming online book. DeltaDataSource supports Append and Complete output modes only. In the end, DeltaDataSource creates a DeltaSink . Tip Consult the demo Using Delta Lake (as Streaming Sink) in Streaming Queries . Loading Table \u00b6 getTable ( schema : StructType , partitioning : Array [ Transform ], properties : java.util.Map [ String , String ]) : Table getTable ...FIXME getTable is part of the TableProvider (Spark SQL 3.0.0) abstraction. Utilities \u00b6 getTimeTravelVersion \u00b6 getTimeTravelVersion ( parameters : Map [ String , String ]) : Option [ DeltaTimeTravelSpec ] getTimeTravelVersion ...FIXME getTimeTravelVersion is used when DeltaDataSource is requested to create a relation (as a RelationProvider) . parsePathIdentifier \u00b6 parsePathIdentifier ( spark : SparkSession , userPath : String ) : ( Path , Seq [( String , String )], Option [ DeltaTimeTravelSpec ]) parsePathIdentifier ...FIXME parsePathIdentifier is used when DeltaTableV2 is requested for metadata (for a non-catalog table).","title":"DeltaDataSource"},{"location":"DeltaDataSource/#deltadatasource","text":"DeltaDataSource is a DataSourceRegister and is the entry point to all the features provided by delta data source. DeltaDataSource is a RelationProvider . DeltaDataSource is a StreamSinkProvider for a streaming sink for streaming queries (Structured Streaming).","title":"DeltaDataSource"},{"location":"DeltaDataSource/#datasourceregister-and-delta-alias","text":"DeltaDataSource is a DataSourceRegister and registers delta alias. Tip Read up on DataSourceRegister in The Internals of Spark SQL online book. DeltaDataSource is registered using META-INF/services/org.apache.spark.sql.sources.DataSourceRegister : org.apache.spark.sql.delta.sources.DeltaDataSource","title":" DataSourceRegister and delta Alias"},{"location":"DeltaDataSource/#relationprovider-for-batch-queries","text":"DeltaDataSource is a RelationProvider for reading ( loading ) data from a delta table in a structured query. Tip Read up on RelationProvider in The Internals of Spark SQL online book. createRelation ( sqlContext : SQLContext , parameters : Map [ String , String ]) : BaseRelation createRelation reads the path option from the given parameters. createRelation verifies the given parameters . createRelation extracts time travel specification from the given parameters. In the end, createRelation creates a DeltaTableV2 (for the path option and the time travel specification) and requests it for an insertable HadoopFsRelation . createRelation throws an IllegalArgumentException when path option is not specified: 'path' is not specified","title":" RelationProvider for Batch Queries"},{"location":"DeltaDataSource/#source-schema","text":"sourceSchema ( sqlContext : SQLContext , schema : Option [ StructType ], providerName : String , parameters : Map [ String , String ]) : ( String , StructType ) sourceSchema creates a DeltaLog for a Delta table in the directory specified by the required path option (in the parameters) and returns the delta name with the schema (of the Delta table). sourceSchema throws an IllegalArgumentException when the path option has not been specified: 'path' is not specified sourceSchema throws an AnalysisException when the path option uses time travel : Cannot time travel views, subqueries or streams. sourceSchema is part of the StreamSourceProvider abstraction ( Spark Structured Streaming ).","title":" Source Schema"},{"location":"DeltaDataSource/#creatablerelationprovider","text":"DeltaDataSource is a CreatableRelationProvider for writing out the result of a structured query. Tip Read up on CreatableRelationProvider in The Internals of Spark SQL online book.","title":" CreatableRelationProvider"},{"location":"DeltaDataSource/#creating-streaming-source","text":"DeltaDataSource is a StreamSourceProvider . Tip Read up on StreamSourceProvider in The Internals of Spark Structured Streaming online book.","title":" Creating Streaming Source"},{"location":"DeltaDataSource/#creating-streaming-sink","text":"DeltaDataSource is a StreamSinkProvider for a streaming sink for Structured Streaming. Tip Read up on StreamSinkProvider in The Internals of Spark Structured Streaming online book. DeltaDataSource supports Append and Complete output modes only. In the end, DeltaDataSource creates a DeltaSink . Tip Consult the demo Using Delta Lake (as Streaming Sink) in Streaming Queries .","title":" Creating Streaming Sink"},{"location":"DeltaDataSource/#loading-table","text":"getTable ( schema : StructType , partitioning : Array [ Transform ], properties : java.util.Map [ String , String ]) : Table getTable ...FIXME getTable is part of the TableProvider (Spark SQL 3.0.0) abstraction.","title":" Loading Table"},{"location":"DeltaDataSource/#utilities","text":"","title":"Utilities"},{"location":"DeltaDataSource/#gettimetravelversion","text":"getTimeTravelVersion ( parameters : Map [ String , String ]) : Option [ DeltaTimeTravelSpec ] getTimeTravelVersion ...FIXME getTimeTravelVersion is used when DeltaDataSource is requested to create a relation (as a RelationProvider) .","title":" getTimeTravelVersion"},{"location":"DeltaDataSource/#parsepathidentifier","text":"parsePathIdentifier ( spark : SparkSession , userPath : String ) : ( Path , Seq [( String , String )], Option [ DeltaTimeTravelSpec ]) parsePathIdentifier ...FIXME parsePathIdentifier is used when DeltaTableV2 is requested for metadata (for a non-catalog table).","title":" parsePathIdentifier"},{"location":"DeltaErrors/","text":"= DeltaErrors Utility DeltaErrors utility is...FIXME == [[postCommitHookFailedException]] Reporting Post-Commit Hook Failure (RuntimeException) -- postCommitHookFailedException Method [source, scala] \u00b6 postCommitHookFailedException( failedHook: PostCommitHook, failedOnCommitVersion: Long, extraErrorMessage: String, error: Throwable): Throwable postCommitHookFailedException ...FIXME NOTE: postCommitHookFailedException is used when...FIXME","title":"DeltaErrors"},{"location":"DeltaErrors/#source-scala","text":"postCommitHookFailedException( failedHook: PostCommitHook, failedOnCommitVersion: Long, extraErrorMessage: String, error: Throwable): Throwable postCommitHookFailedException ...FIXME NOTE: postCommitHookFailedException is used when...FIXME","title":"[source, scala]"},{"location":"DeltaFileFormat/","text":"= [[DeltaFileFormat]] DeltaFileFormat Contract -- Spark FileFormat Of Delta Table DeltaFileFormat is the < > of < > that can < >. [[contract]] .DeltaFileFormat Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | fileFormat a| [[fileFormat]] [source, scala] \u00b6 fileFormat: FileFormat \u00b6 Spark SQL's FileFormat of a delta table Default: ParquetFileFormat Used when: DeltaLog is requested for a < > (in batch queries) and < > DeltaCommand is requested for a < > TransactionalWrite is requested to < > |=== [[implementations]] NOTE: < > is the only known DeltaFileFormat .","title":"DeltaFileFormat"},{"location":"DeltaFileFormat/#source-scala","text":"","title":"[source, scala]"},{"location":"DeltaFileFormat/#fileformat-fileformat","text":"Spark SQL's FileFormat of a delta table Default: ParquetFileFormat Used when: DeltaLog is requested for a < > (in batch queries) and < > DeltaCommand is requested for a < > TransactionalWrite is requested to < > |=== [[implementations]] NOTE: < > is the only known DeltaFileFormat .","title":"fileFormat: FileFormat"},{"location":"DeltaFileOperations/","text":"= DeltaFileOperations Utilities :navtitle: DeltaFileOperations == [[utilities]] Utilities === [[recursiveListDirs]] recursiveListDirs [source, scala] \u00b6 recursiveListDirs( spark: SparkSession, subDirs: Seq[String], hadoopConf: Broadcast[SerializableConfiguration], hiddenFileNameFilter: String => Boolean = defaultHiddenFileFilter, fileListingParallelism: Option[Int] = None): Dataset[SerializableFileStatus] recursiveListDirs...FIXME recursiveListDirs is used when: ManualListingFileManifest (of ConvertToDeltaCommandBase) is requested to doList VacuumCommand utility is used to VacuumCommand.md#gc[gc] === [[tryDeleteNonRecursive]] tryDeleteNonRecursive [source,scala] \u00b6 tryDeleteNonRecursive( fs: FileSystem, path: Path, tries: Int = 3): Boolean tryDeleteNonRecursive...FIXME tryDeleteNonRecursive is used when VacuumCommandImpl is requested to VacuumCommandImpl.md#delete[delete] == [[internal-methods]] Internal Methods === [[recurseDirectories]] recurseDirectories [source,scala] \u00b6 recurseDirectories( logStore: LogStore, filesAndDirs: Iterator[SerializableFileStatus], hiddenFileNameFilter: String => Boolean): Iterator[SerializableFileStatus] recurseDirectories...FIXME recurseDirectories is used when DeltaFileOperations is requested to < > and < >. === [[listUsingLogStore]] listUsingLogStore [source,scala] \u00b6 listUsingLogStore( logStore: LogStore, subDirs: Iterator[String], recurse: Boolean, hiddenFileNameFilter: String => Boolean): Iterator[SerializableFileStatus] listUsingLogStore...FIXME listUsingLogStore is used when DeltaFileOperations is requested to < > and < >. === [[isThrottlingError]] isThrottlingError [source,scala] \u00b6 isThrottlingError( t: Throwable): Boolean isThrottlingError returns true when the Throwable contains slow down . isThrottlingError is used when DeltaFileOperations is requested to < > and < >.","title":"DeltaFileOperations"},{"location":"DeltaFileOperations/#source-scala","text":"recursiveListDirs( spark: SparkSession, subDirs: Seq[String], hadoopConf: Broadcast[SerializableConfiguration], hiddenFileNameFilter: String => Boolean = defaultHiddenFileFilter, fileListingParallelism: Option[Int] = None): Dataset[SerializableFileStatus] recursiveListDirs...FIXME recursiveListDirs is used when: ManualListingFileManifest (of ConvertToDeltaCommandBase) is requested to doList VacuumCommand utility is used to VacuumCommand.md#gc[gc] === [[tryDeleteNonRecursive]] tryDeleteNonRecursive","title":"[source, scala]"},{"location":"DeltaFileOperations/#sourcescala","text":"tryDeleteNonRecursive( fs: FileSystem, path: Path, tries: Int = 3): Boolean tryDeleteNonRecursive...FIXME tryDeleteNonRecursive is used when VacuumCommandImpl is requested to VacuumCommandImpl.md#delete[delete] == [[internal-methods]] Internal Methods === [[recurseDirectories]] recurseDirectories","title":"[source,scala]"},{"location":"DeltaFileOperations/#sourcescala_1","text":"recurseDirectories( logStore: LogStore, filesAndDirs: Iterator[SerializableFileStatus], hiddenFileNameFilter: String => Boolean): Iterator[SerializableFileStatus] recurseDirectories...FIXME recurseDirectories is used when DeltaFileOperations is requested to < > and < >. === [[listUsingLogStore]] listUsingLogStore","title":"[source,scala]"},{"location":"DeltaFileOperations/#sourcescala_2","text":"listUsingLogStore( logStore: LogStore, subDirs: Iterator[String], recurse: Boolean, hiddenFileNameFilter: String => Boolean): Iterator[SerializableFileStatus] listUsingLogStore...FIXME listUsingLogStore is used when DeltaFileOperations is requested to < > and < >. === [[isThrottlingError]] isThrottlingError","title":"[source,scala]"},{"location":"DeltaFileOperations/#sourcescala_3","text":"isThrottlingError( t: Throwable): Boolean isThrottlingError returns true when the Throwable contains slow down . isThrottlingError is used when DeltaFileOperations is requested to < > and < >.","title":"[source,scala]"},{"location":"DeltaGenerateCommandBase/","text":"= DeltaGenerateCommandBase DeltaGenerateCommandBase is an extension of the RunnableCommand contract (from Spark SQL) for < > that < >. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-RunnableCommand.html[RunnableCommand ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. [[implementations]] NOTE: < > is the default and only known DeltaGenerateCommandBase in Delta Lake. == [[getPath]] Getting Path Of Delta Table From Table Identifier -- getPath Method [source, scala] \u00b6 getPath( spark: SparkSession, tableId: TableIdentifier): Path getPath ...FIXME NOTE: getPath is used when DeltaGenerateCommand is requested to < >.","title":"DeltaGenerateCommandBase"},{"location":"DeltaGenerateCommandBase/#source-scala","text":"getPath( spark: SparkSession, tableId: TableIdentifier): Path getPath ...FIXME NOTE: getPath is used when DeltaGenerateCommand is requested to < >.","title":"[source, scala]"},{"location":"DeltaHistoryManager/","text":"= DeltaHistoryManager DeltaHistoryManager is...FIXME == [[getHistory]] getHistory Method [source, scala] \u00b6 getHistory( limitOpt: Option[Int]): Seq[CommitInfo] getHistory( start: Long, end: Option[Long]): Seq[CommitInfo] getHistory ...FIXME NOTE: getHistory is used when...FIXME == [[getActiveCommitAtTime]] getActiveCommitAtTime Method [source, scala] \u00b6 getActiveCommitAtTime( timestamp: Timestamp, canReturnLastCommit: Boolean, mustBeRecreatable: Boolean = true): Commit getActiveCommitAtTime ...FIXME NOTE: getActiveCommitAtTime is used exclusively when DeltaTableUtils utility is requested to < >. == [[checkVersionExists]] checkVersionExists Method [source, scala] \u00b6 checkVersionExists(version: Long): Unit \u00b6 checkVersionExists ...FIXME NOTE: checkVersionExists is used when...FIXME == [[parallelSearch]] parallelSearch Internal Method [source, scala] \u00b6 parallelSearch( time: Long, start: Long, end: Long): Commit parallelSearch ...FIXME NOTE: parallelSearch is used exclusively when DeltaHistoryManager is requested to < >. == [[parallelSearch0]] parallelSearch0 Internal Utility [source, scala] \u00b6 parallelSearch0( spark: SparkSession, conf: SerializableConfiguration, logPath: String, time: Long, start: Long, end: Long, step: Long): Commit parallelSearch0 ...FIXME NOTE: parallelSearch0 is used exclusively when DeltaHistoryManager is requested to < >. == [[getCommitInfo]] CommitInfo Of Delta File -- getCommitInfo Internal Utility [source, scala] \u00b6 getCommitInfo( logStore: LogStore, basePath: Path, version: Long): CommitInfo getCommitInfo ...FIXME NOTE: getCommitInfo is used when...FIXME","title":"DeltaHistoryManager"},{"location":"DeltaHistoryManager/#source-scala","text":"getHistory( limitOpt: Option[Int]): Seq[CommitInfo] getHistory( start: Long, end: Option[Long]): Seq[CommitInfo] getHistory ...FIXME NOTE: getHistory is used when...FIXME == [[getActiveCommitAtTime]] getActiveCommitAtTime Method","title":"[source, scala]"},{"location":"DeltaHistoryManager/#source-scala_1","text":"getActiveCommitAtTime( timestamp: Timestamp, canReturnLastCommit: Boolean, mustBeRecreatable: Boolean = true): Commit getActiveCommitAtTime ...FIXME NOTE: getActiveCommitAtTime is used exclusively when DeltaTableUtils utility is requested to < >. == [[checkVersionExists]] checkVersionExists Method","title":"[source, scala]"},{"location":"DeltaHistoryManager/#source-scala_2","text":"","title":"[source, scala]"},{"location":"DeltaHistoryManager/#checkversionexistsversion-long-unit","text":"checkVersionExists ...FIXME NOTE: checkVersionExists is used when...FIXME == [[parallelSearch]] parallelSearch Internal Method","title":"checkVersionExists(version: Long): Unit"},{"location":"DeltaHistoryManager/#source-scala_3","text":"parallelSearch( time: Long, start: Long, end: Long): Commit parallelSearch ...FIXME NOTE: parallelSearch is used exclusively when DeltaHistoryManager is requested to < >. == [[parallelSearch0]] parallelSearch0 Internal Utility","title":"[source, scala]"},{"location":"DeltaHistoryManager/#source-scala_4","text":"parallelSearch0( spark: SparkSession, conf: SerializableConfiguration, logPath: String, time: Long, start: Long, end: Long, step: Long): Commit parallelSearch0 ...FIXME NOTE: parallelSearch0 is used exclusively when DeltaHistoryManager is requested to < >. == [[getCommitInfo]] CommitInfo Of Delta File -- getCommitInfo Internal Utility","title":"[source, scala]"},{"location":"DeltaHistoryManager/#source-scala_5","text":"getCommitInfo( logStore: LogStore, basePath: Path, version: Long): CommitInfo getCommitInfo ...FIXME NOTE: getCommitInfo is used when...FIXME","title":"[source, scala]"},{"location":"DeltaInvariantCheckerExec/","text":"= [[DeltaInvariantCheckerExec]] DeltaInvariantCheckerExec Unary Physical Operator DeltaInvariantCheckerExec is...FIXME","title":"DeltaInvariantCheckerExec"},{"location":"DeltaLog/","text":"DeltaLog \u00b6 DeltaLog is a transaction log ( change log ) of changes to the state of a Delta table (in the given data directory ). Creating Instance \u00b6 DeltaLog takes the following to be created: Log directory (Hadoop Path ) Data directory (Hadoop Path ) Clock DeltaLog is created (indirectly via DeltaLog.apply utility) when: DeltaLog.forTable utility is used _delta_log Metadata Directory \u00b6 DeltaLog uses _delta_log metadata directory for the transaction log of a Delta table. The _delta_log directory is in the given data path directory (when created using DeltaLog.forTable utility). The _delta_log directory is resolved (in the DeltaLog.apply utility) using the application-wide Hadoop Configuration . Once resolved and turned into a qualified path, the _delta_log directory is cached . DeltaLog.forTable Utility \u00b6 forTable ( spark : SparkSession , table : CatalogTable ) : DeltaLog forTable ( spark : SparkSession , table : CatalogTable , clock : Clock ) : DeltaLog forTable ( spark : SparkSession , deltaTable : DeltaTableIdentifier ) : DeltaLog forTable ( spark : SparkSession , dataPath : File ) : DeltaLog forTable ( spark : SparkSession , dataPath : File , clock : Clock ) : DeltaLog forTable ( spark : SparkSession , dataPath : Path ) : DeltaLog forTable ( spark : SparkSession , dataPath : Path , clock : Clock ) : DeltaLog forTable ( spark : SparkSession , dataPath : String ) : DeltaLog forTable ( spark : SparkSession , dataPath : String , clock : Clock ) : DeltaLog forTable ( spark : SparkSession , tableName : TableIdentifier ) : DeltaLog forTable ( spark : SparkSession , tableName : TableIdentifier , clock : Clock ) : DeltaLog forTable creates a DeltaLog with _delta_log directory (in the given dataPath directory). forTable is used when: AlterTableSetLocationDeltaCommand , ConvertToDeltaCommand , VacuumTableCommand , CreateDeltaTableCommand , DeltaGenerateCommand , DescribeDeltaDetailCommand , DescribeDeltaHistoryCommand commands are executed DeltaDataSource is requested for the source schema , a source , and a relation DeltaTable.isDeltaTable utility is used DeltaTableUtils.combineWithCatalogMetadata utility is used DeltaTableIdentifier is requested to getDeltaLog DeltaCatalog is requested to createDeltaTable DeltaTableV2 is requested for the DeltaLog DeltaSink is created Looking Up Or Creating DeltaLog Instance \u00b6 apply ( spark : SparkSession , rawPath : Path , clock : Clock = new SystemClock ) : DeltaLog Note rawPath is a Hadoop Path to the _delta_log directory at the root of the data of a delta table. apply ...FIXME isValid \u00b6 isValid () : Boolean isValid ...FIXME Demo: Creating DeltaLog \u00b6 import org.apache.spark.sql.SparkSession assert ( spark . isInstanceOf [ SparkSession ]) val dataPath = \"/tmp/delta/t1\" import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog . forTable ( spark , dataPath ) import org.apache.hadoop.fs.Path val expected = new Path ( s\"file: $dataPath /_delta_log/_last_checkpoint\" ) assert ( deltaLog . LAST_CHECKPOINT == expected ) Accessing Current Version \u00b6 A common idiom (if not the only way) to know the current version of the delta table is to request the DeltaLog for the current state (snapshot) and then for the version . import org.apache.spark.sql.delta.DeltaLog assert(deltaLog.isInstanceOf[DeltaLog]) val deltaVersion = deltaLog.snapshot.version scala> println(deltaVersion) 5 Initialization \u00b6 When created, DeltaLog does the following: Creates the LogStore based on spark.delta.logStore.class configuration property Initializes the current snapshot Updates state of the delta table when there is no metadata checkpoint (e.g. the version of the state is -1 ) In other words, the version of (the DeltaLog of) a delta table is at version 0 at the very minimum. assert ( deltaLog . snapshot . version >= 0 ) filterFileList Utility \u00b6 filterFileList ( partitionSchema : StructType , files : DataFrame , partitionFilters : Seq [ Expression ], partitionColumnPrefixes : Seq [ String ] = Nil ) : DataFrame filterFileList ...FIXME filterFileList is used when: OptimisticTransactionImpl is requested to checkAndRetry PartitionFiltering is requested to filesForScan WriteIntoDelta is requested to write SnapshotIterator is requested to iterator TahoeBatchFileIndex is requested to matchingFiles DeltaDataSource utility is requested to verifyAndCreatePartitionFilters FileFormats \u00b6 DeltaLog defines two FileFormat s ( Spark SQL ): ParquetFileFormat for indices of delta files JsonFileFormat for indices of checkpoint files These FileFormat s are used to create DeltaLogFileIndex es for Snapshots that in turn used them for stateReconstruction . LogStore \u00b6 DeltaLog uses a LogStore for...FIXME Transaction Logs (DeltaLogs) per Fully-Qualified Path \u00b6 deltaLogCache : Cache [ Path , DeltaLog ] deltaLogCache is part of DeltaLog Scala object which makes it an application-wide cache \"for free\". Once used, deltaLogCache will only be one until the application that uses it stops. deltaLogCache is a registry of DeltaLogs by their fully-qualified _delta_log directories. A new instance of DeltaLog is added when DeltaLog.apply utility is used and the instance hasn't been created before for a path. deltaLogCache is invalidated: For a delta table using DeltaLog.invalidateCache utility (and DeltaLog.apply when the cached reference is no longer valid ) For all delta tables using DeltaLog.clearCache utility Executing Single-Threaded Operation in New Transaction \u00b6 withNewTransaction [ T ]( thunk : OptimisticTransaction => T ) : T withNewTransaction starts a new transaction (that is active for the whole thread) and executes the given thunk block. In the end, withNewTransaction makes the transaction no longer active . withNewTransaction is used when: DeleteCommand , MergeIntoCommand , UpdateCommand , and WriteIntoDelta commands are executed DeltaSink is requested to add a streaming micro-batch Starting New Transaction \u00b6 startTransaction () : OptimisticTransaction startTransaction updates and creates a new OptimisticTransaction (for this DeltaLog ). Note startTransaction is a \"subset\" of withNewTransaction . startTransaction is used when: DeltaLog is requested to upgradeProtocol AlterDeltaTableCommand is requested to startTransaction ConvertToDeltaCommand and CreateDeltaTableCommand are executed Throwing UnsupportedOperationException For appendOnly Table Property Enabled \u00b6 assertRemovable () : Unit assertRemovable throws an UnsupportedOperationException for the appendOnly table property ( in the Metadata ) enabled ( true ): This table is configured to only allow appends. If you would like to permit updates or deletes, use 'ALTER TABLE <table_name> SET TBLPROPERTIES (appendOnly=false)'. assertRemovable is used when: FIXME metadata \u00b6 metadata : Metadata metadata is part of the Checkpoints abstraction. metadata requests the current Snapshot for the metadata or creates a new one (if the current Snapshot is not initialized). update \u00b6 update ( stalenessAcceptable : Boolean = false ) : Snapshot update branches off based on a combination of flags: the given stalenessAcceptable and isSnapshotStale flags. For the stalenessAcceptable not acceptable (default) and the snapshot not stale , update simply acquires the deltaLogLock lock and updateInternal (with isAsync flag off). For all other cases, update ...FIXME update is used when: DeltaHistoryManager is requested to getHistory , getActiveCommitAtTime , and checkVersionExists DeltaLog is created (with no checkpoint created), and requested to startTransaction and withNewTransaction OptimisticTransactionImpl is requested to doCommit and checkAndRetry ConvertToDeltaCommand is requested to run and streamWrite VacuumCommand utility is used to gc TahoeLogFileIndex is requested for the (historical or latest) snapshot DeltaDataSource is requested for a relation tryUpdate \u00b6 tryUpdate ( isAsync : Boolean = false ) : Snapshot tryUpdate ...FIXME Current State Snapshot \u00b6 snapshot : Snapshot snapshot returns the current snapshot . snapshot is used when: OptimisticTransaction is created Checkpoints is requested to checkpoint DeltaLog is requested for the metadata , to upgradeProtocol , getSnapshotAt , createRelation OptimisticTransactionImpl is requested to getNextAttemptVersion DeleteCommand , DeltaGenerateCommand , DescribeDeltaDetailCommand , UpdateCommand commands are executed GenerateSymlinkManifest is executed DeltaCommand is requested to buildBaseRelation TahoeFileIndex is requested for the table version , partitionSchema TahoeLogFileIndex is requested for the table size DeltaDataSource is requested for the schema of the streaming delta source DeltaSource is created and requested for the getStartingOffset , getBatch Current State Snapshot \u00b6 currentSnapshot : Snapshot currentSnapshot is a Snapshot based on the metadata checkpoint if available or a new Snapshot instance (with version being -1 ). Note For a new Snapshot instance (with version being -1 ) DeltaLog immediately updates the state . Internally, currentSnapshot ...FIXME currentSnapshot is available using snapshot method. currentSnapshot is used when: DeltaLog is requested to updateInternal , update , tryUpdate , and isValid Creating Insertable HadoopFsRelation For Batch Queries \u00b6 createRelation ( partitionFilters : Seq [ Expression ] = Nil , timeTravel : Option [ DeltaTimeTravelSpec ] = None ) : BaseRelation createRelation ...FIXME createRelation creates a TahoeLogFileIndex for the data path , the given partitionFilters and a version (if defined). createRelation ...FIXME In the end, createRelation creates a HadoopFsRelation for the TahoeLogFileIndex and...FIXME. The HadoopFsRelation is also an InsertableRelation . createRelation is used when DeltaDataSource is requested for a relation as a CreatableRelationProvider and a RelationProvider (for batch queries). insert \u00b6 insert ( data : DataFrame , overwrite : Boolean ) : Unit insert ...FIXME insert is part of the InsertableRelation ( Spark SQL ) abstraction. Retrieving State Of Delta Table At Given Version \u00b6 getSnapshotAt ( version : Long , commitTimestamp : Option [ Long ] = None , lastCheckpointHint : Option [ CheckpointInstance ] = None ) : Snapshot getSnapshotAt ...FIXME getSnapshotAt is used when: DeltaLog is requested for a relation , and to updateInternal DeltaSource is requested to getSnapshotAt TahoeLogFileIndex is requested for historicalSnapshotOpt checkpointInterval \u00b6 checkpointInterval : Int checkpointInterval gives the value of checkpointInterval table property ( from the Metadata ). checkpointInterval is used when...FIXME Changes (Actions) Of Delta Version And Later \u00b6 getChanges ( startVersion : Long ) : Iterator [( Long , Seq [ Action ])] getChanges gives all action s ( changes ) per delta log file for the given startVersion of a delta table and later. val dataPath = \"/tmp/delta/users\" import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog . forTable ( spark , dataPath ) assert ( deltaLog . isInstanceOf [ DeltaLog ]) val changesPerVersion = deltaLog . getChanges ( startVersion = 0 ) Internally, getChanges requests the LogStore for files that are lexicographically greater or equal to the delta log file for the given startVersion (in the logPath ) and leaves only delta log files (e.g. files with numbers only as file name and .json file extension). For every delta file, getChanges requests the LogStore to read the JSON content (every line is an action ), and then deserializes it to an action . getChanges is used when: DeltaSource is requested for the indexed file additions (FileAdd actions) Creating DataFrame For Given AddFiles \u00b6 createDataFrame ( snapshot : Snapshot , addFiles : Seq [ AddFile ], isStreaming : Boolean = false , actionTypeOpt : Option [ String ] = None ) : DataFrame createDataFrame uses the action type based on the optional action type (if defined) or uses the following based on the isStreaming flag: streaming when isStreaming flag is enabled ( true ) batch when isStreaming flag is disabled ( false ) Note actionTypeOpt seems not to be defined ever. createDataFrame creates a new TahoeBatchFileIndex (for the action type, and the given AddFile s and Snapshot ). createDataFrame creates a HadoopFsRelation with the TahoeBatchFileIndex and the other properties based on the given Snapshot (and the associated Metadata ). Tip Learn more on HadoopFsRelation in The Internals of Spark SQL online book. In the end, createDataFrame creates a DataFrame with a logical query plan with a LogicalRelation over the HadoopFsRelation . Tip Learn more on LogicalRelation in The Internals of Spark SQL online book. createDataFrame is used when: MergeIntoCommand is executed DeltaSource is requested for a DataFrame for data between start and end offsets minFileRetentionTimestamp Method \u00b6 minFileRetentionTimestamp : Long minFileRetentionTimestamp is the timestamp that is tombstoneRetentionMillis before the current time (per the given Clock ). minFileRetentionTimestamp is used when: DeltaLog is requested for the currentSnapshot , to updateInternal , and to getSnapshotAt VacuumCommand is requested for garbage collecting of a delta table tombstoneRetentionMillis Method \u00b6 tombstoneRetentionMillis : Long tombstoneRetentionMillis gives the value of deletedFileRetentionDuration table property ( from the Metadata ). tombstoneRetentionMillis is used when: DeltaLog is requested for minFileRetentionTimestamp VacuumCommand is requested for garbage collecting of a delta table updateInternal Internal Method \u00b6 updateInternal ( isAsync : Boolean ) : Snapshot updateInternal ...FIXME updateInternal is used when: DeltaLog is requested to update (directly or via tryUpdate ) Invalidating Cached DeltaLog Instance By Path \u00b6 invalidateCache ( spark : SparkSession , dataPath : Path ) : Unit invalidateCache ...FIXME invalidateCache is a public API and does not seem to be used at all. protocolRead \u00b6 protocolRead ( protocol : Protocol ) : Unit protocolRead ...FIXME protocolRead is used when: OptimisticTransactionImpl is requested to validate and retry a commit Snapshot is created DeltaSource is requested to verifyStreamHygieneAndFilterAddFiles LogStoreProvider \u00b6 DeltaLog is a LogStoreProvider . Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.delta.DeltaLog logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.DeltaLog=ALL Refer to Logging .","title":"DeltaLog"},{"location":"DeltaLog/#deltalog","text":"DeltaLog is a transaction log ( change log ) of changes to the state of a Delta table (in the given data directory ).","title":"DeltaLog"},{"location":"DeltaLog/#creating-instance","text":"DeltaLog takes the following to be created: Log directory (Hadoop Path ) Data directory (Hadoop Path ) Clock DeltaLog is created (indirectly via DeltaLog.apply utility) when: DeltaLog.forTable utility is used","title":"Creating Instance"},{"location":"DeltaLog/#_delta_log-metadata-directory","text":"DeltaLog uses _delta_log metadata directory for the transaction log of a Delta table. The _delta_log directory is in the given data path directory (when created using DeltaLog.forTable utility). The _delta_log directory is resolved (in the DeltaLog.apply utility) using the application-wide Hadoop Configuration . Once resolved and turned into a qualified path, the _delta_log directory is cached .","title":" _delta_log Metadata Directory"},{"location":"DeltaLog/#deltalogfortable-utility","text":"forTable ( spark : SparkSession , table : CatalogTable ) : DeltaLog forTable ( spark : SparkSession , table : CatalogTable , clock : Clock ) : DeltaLog forTable ( spark : SparkSession , deltaTable : DeltaTableIdentifier ) : DeltaLog forTable ( spark : SparkSession , dataPath : File ) : DeltaLog forTable ( spark : SparkSession , dataPath : File , clock : Clock ) : DeltaLog forTable ( spark : SparkSession , dataPath : Path ) : DeltaLog forTable ( spark : SparkSession , dataPath : Path , clock : Clock ) : DeltaLog forTable ( spark : SparkSession , dataPath : String ) : DeltaLog forTable ( spark : SparkSession , dataPath : String , clock : Clock ) : DeltaLog forTable ( spark : SparkSession , tableName : TableIdentifier ) : DeltaLog forTable ( spark : SparkSession , tableName : TableIdentifier , clock : Clock ) : DeltaLog forTable creates a DeltaLog with _delta_log directory (in the given dataPath directory). forTable is used when: AlterTableSetLocationDeltaCommand , ConvertToDeltaCommand , VacuumTableCommand , CreateDeltaTableCommand , DeltaGenerateCommand , DescribeDeltaDetailCommand , DescribeDeltaHistoryCommand commands are executed DeltaDataSource is requested for the source schema , a source , and a relation DeltaTable.isDeltaTable utility is used DeltaTableUtils.combineWithCatalogMetadata utility is used DeltaTableIdentifier is requested to getDeltaLog DeltaCatalog is requested to createDeltaTable DeltaTableV2 is requested for the DeltaLog DeltaSink is created","title":" DeltaLog.forTable Utility"},{"location":"DeltaLog/#looking-up-or-creating-deltalog-instance","text":"apply ( spark : SparkSession , rawPath : Path , clock : Clock = new SystemClock ) : DeltaLog Note rawPath is a Hadoop Path to the _delta_log directory at the root of the data of a delta table. apply ...FIXME","title":" Looking Up Or Creating DeltaLog Instance"},{"location":"DeltaLog/#isvalid","text":"isValid () : Boolean isValid ...FIXME","title":" isValid"},{"location":"DeltaLog/#demo-creating-deltalog","text":"import org.apache.spark.sql.SparkSession assert ( spark . isInstanceOf [ SparkSession ]) val dataPath = \"/tmp/delta/t1\" import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog . forTable ( spark , dataPath ) import org.apache.hadoop.fs.Path val expected = new Path ( s\"file: $dataPath /_delta_log/_last_checkpoint\" ) assert ( deltaLog . LAST_CHECKPOINT == expected )","title":"Demo: Creating DeltaLog"},{"location":"DeltaLog/#accessing-current-version","text":"A common idiom (if not the only way) to know the current version of the delta table is to request the DeltaLog for the current state (snapshot) and then for the version . import org.apache.spark.sql.delta.DeltaLog assert(deltaLog.isInstanceOf[DeltaLog]) val deltaVersion = deltaLog.snapshot.version scala> println(deltaVersion) 5","title":"Accessing Current Version"},{"location":"DeltaLog/#initialization","text":"When created, DeltaLog does the following: Creates the LogStore based on spark.delta.logStore.class configuration property Initializes the current snapshot Updates state of the delta table when there is no metadata checkpoint (e.g. the version of the state is -1 ) In other words, the version of (the DeltaLog of) a delta table is at version 0 at the very minimum. assert ( deltaLog . snapshot . version >= 0 )","title":"Initialization"},{"location":"DeltaLog/#filterfilelist-utility","text":"filterFileList ( partitionSchema : StructType , files : DataFrame , partitionFilters : Seq [ Expression ], partitionColumnPrefixes : Seq [ String ] = Nil ) : DataFrame filterFileList ...FIXME filterFileList is used when: OptimisticTransactionImpl is requested to checkAndRetry PartitionFiltering is requested to filesForScan WriteIntoDelta is requested to write SnapshotIterator is requested to iterator TahoeBatchFileIndex is requested to matchingFiles DeltaDataSource utility is requested to verifyAndCreatePartitionFilters","title":" filterFileList Utility"},{"location":"DeltaLog/#fileformats","text":"DeltaLog defines two FileFormat s ( Spark SQL ): ParquetFileFormat for indices of delta files JsonFileFormat for indices of checkpoint files These FileFormat s are used to create DeltaLogFileIndex es for Snapshots that in turn used them for stateReconstruction .","title":"FileFormats"},{"location":"DeltaLog/#logstore","text":"DeltaLog uses a LogStore for...FIXME","title":" LogStore"},{"location":"DeltaLog/#transaction-logs-deltalogs-per-fully-qualified-path","text":"deltaLogCache : Cache [ Path , DeltaLog ] deltaLogCache is part of DeltaLog Scala object which makes it an application-wide cache \"for free\". Once used, deltaLogCache will only be one until the application that uses it stops. deltaLogCache is a registry of DeltaLogs by their fully-qualified _delta_log directories. A new instance of DeltaLog is added when DeltaLog.apply utility is used and the instance hasn't been created before for a path. deltaLogCache is invalidated: For a delta table using DeltaLog.invalidateCache utility (and DeltaLog.apply when the cached reference is no longer valid ) For all delta tables using DeltaLog.clearCache utility","title":" Transaction Logs (DeltaLogs) per Fully-Qualified Path"},{"location":"DeltaLog/#executing-single-threaded-operation-in-new-transaction","text":"withNewTransaction [ T ]( thunk : OptimisticTransaction => T ) : T withNewTransaction starts a new transaction (that is active for the whole thread) and executes the given thunk block. In the end, withNewTransaction makes the transaction no longer active . withNewTransaction is used when: DeleteCommand , MergeIntoCommand , UpdateCommand , and WriteIntoDelta commands are executed DeltaSink is requested to add a streaming micro-batch","title":" Executing Single-Threaded Operation in New Transaction"},{"location":"DeltaLog/#starting-new-transaction","text":"startTransaction () : OptimisticTransaction startTransaction updates and creates a new OptimisticTransaction (for this DeltaLog ). Note startTransaction is a \"subset\" of withNewTransaction . startTransaction is used when: DeltaLog is requested to upgradeProtocol AlterDeltaTableCommand is requested to startTransaction ConvertToDeltaCommand and CreateDeltaTableCommand are executed","title":" Starting New Transaction"},{"location":"DeltaLog/#throwing-unsupportedoperationexception-for-appendonly-table-property-enabled","text":"assertRemovable () : Unit assertRemovable throws an UnsupportedOperationException for the appendOnly table property ( in the Metadata ) enabled ( true ): This table is configured to only allow appends. If you would like to permit updates or deletes, use 'ALTER TABLE <table_name> SET TBLPROPERTIES (appendOnly=false)'. assertRemovable is used when: FIXME","title":" Throwing UnsupportedOperationException For appendOnly Table Property Enabled"},{"location":"DeltaLog/#metadata","text":"metadata : Metadata metadata is part of the Checkpoints abstraction. metadata requests the current Snapshot for the metadata or creates a new one (if the current Snapshot is not initialized).","title":" metadata"},{"location":"DeltaLog/#update","text":"update ( stalenessAcceptable : Boolean = false ) : Snapshot update branches off based on a combination of flags: the given stalenessAcceptable and isSnapshotStale flags. For the stalenessAcceptable not acceptable (default) and the snapshot not stale , update simply acquires the deltaLogLock lock and updateInternal (with isAsync flag off). For all other cases, update ...FIXME update is used when: DeltaHistoryManager is requested to getHistory , getActiveCommitAtTime , and checkVersionExists DeltaLog is created (with no checkpoint created), and requested to startTransaction and withNewTransaction OptimisticTransactionImpl is requested to doCommit and checkAndRetry ConvertToDeltaCommand is requested to run and streamWrite VacuumCommand utility is used to gc TahoeLogFileIndex is requested for the (historical or latest) snapshot DeltaDataSource is requested for a relation","title":" update"},{"location":"DeltaLog/#tryupdate","text":"tryUpdate ( isAsync : Boolean = false ) : Snapshot tryUpdate ...FIXME","title":" tryUpdate"},{"location":"DeltaLog/#current-state-snapshot","text":"snapshot : Snapshot snapshot returns the current snapshot . snapshot is used when: OptimisticTransaction is created Checkpoints is requested to checkpoint DeltaLog is requested for the metadata , to upgradeProtocol , getSnapshotAt , createRelation OptimisticTransactionImpl is requested to getNextAttemptVersion DeleteCommand , DeltaGenerateCommand , DescribeDeltaDetailCommand , UpdateCommand commands are executed GenerateSymlinkManifest is executed DeltaCommand is requested to buildBaseRelation TahoeFileIndex is requested for the table version , partitionSchema TahoeLogFileIndex is requested for the table size DeltaDataSource is requested for the schema of the streaming delta source DeltaSource is created and requested for the getStartingOffset , getBatch","title":" Current State Snapshot"},{"location":"DeltaLog/#current-state-snapshot_1","text":"currentSnapshot : Snapshot currentSnapshot is a Snapshot based on the metadata checkpoint if available or a new Snapshot instance (with version being -1 ). Note For a new Snapshot instance (with version being -1 ) DeltaLog immediately updates the state . Internally, currentSnapshot ...FIXME currentSnapshot is available using snapshot method. currentSnapshot is used when: DeltaLog is requested to updateInternal , update , tryUpdate , and isValid","title":" Current State Snapshot"},{"location":"DeltaLog/#creating-insertable-hadoopfsrelation-for-batch-queries","text":"createRelation ( partitionFilters : Seq [ Expression ] = Nil , timeTravel : Option [ DeltaTimeTravelSpec ] = None ) : BaseRelation createRelation ...FIXME createRelation creates a TahoeLogFileIndex for the data path , the given partitionFilters and a version (if defined). createRelation ...FIXME In the end, createRelation creates a HadoopFsRelation for the TahoeLogFileIndex and...FIXME. The HadoopFsRelation is also an InsertableRelation . createRelation is used when DeltaDataSource is requested for a relation as a CreatableRelationProvider and a RelationProvider (for batch queries).","title":" Creating Insertable HadoopFsRelation For Batch Queries"},{"location":"DeltaLog/#insert","text":"insert ( data : DataFrame , overwrite : Boolean ) : Unit insert ...FIXME insert is part of the InsertableRelation ( Spark SQL ) abstraction.","title":" insert"},{"location":"DeltaLog/#retrieving-state-of-delta-table-at-given-version","text":"getSnapshotAt ( version : Long , commitTimestamp : Option [ Long ] = None , lastCheckpointHint : Option [ CheckpointInstance ] = None ) : Snapshot getSnapshotAt ...FIXME getSnapshotAt is used when: DeltaLog is requested for a relation , and to updateInternal DeltaSource is requested to getSnapshotAt TahoeLogFileIndex is requested for historicalSnapshotOpt","title":" Retrieving State Of Delta Table At Given Version"},{"location":"DeltaLog/#checkpointinterval","text":"checkpointInterval : Int checkpointInterval gives the value of checkpointInterval table property ( from the Metadata ). checkpointInterval is used when...FIXME","title":" checkpointInterval"},{"location":"DeltaLog/#changes-actions-of-delta-version-and-later","text":"getChanges ( startVersion : Long ) : Iterator [( Long , Seq [ Action ])] getChanges gives all action s ( changes ) per delta log file for the given startVersion of a delta table and later. val dataPath = \"/tmp/delta/users\" import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog . forTable ( spark , dataPath ) assert ( deltaLog . isInstanceOf [ DeltaLog ]) val changesPerVersion = deltaLog . getChanges ( startVersion = 0 ) Internally, getChanges requests the LogStore for files that are lexicographically greater or equal to the delta log file for the given startVersion (in the logPath ) and leaves only delta log files (e.g. files with numbers only as file name and .json file extension). For every delta file, getChanges requests the LogStore to read the JSON content (every line is an action ), and then deserializes it to an action . getChanges is used when: DeltaSource is requested for the indexed file additions (FileAdd actions)","title":" Changes (Actions) Of Delta Version And Later"},{"location":"DeltaLog/#creating-dataframe-for-given-addfiles","text":"createDataFrame ( snapshot : Snapshot , addFiles : Seq [ AddFile ], isStreaming : Boolean = false , actionTypeOpt : Option [ String ] = None ) : DataFrame createDataFrame uses the action type based on the optional action type (if defined) or uses the following based on the isStreaming flag: streaming when isStreaming flag is enabled ( true ) batch when isStreaming flag is disabled ( false ) Note actionTypeOpt seems not to be defined ever. createDataFrame creates a new TahoeBatchFileIndex (for the action type, and the given AddFile s and Snapshot ). createDataFrame creates a HadoopFsRelation with the TahoeBatchFileIndex and the other properties based on the given Snapshot (and the associated Metadata ). Tip Learn more on HadoopFsRelation in The Internals of Spark SQL online book. In the end, createDataFrame creates a DataFrame with a logical query plan with a LogicalRelation over the HadoopFsRelation . Tip Learn more on LogicalRelation in The Internals of Spark SQL online book. createDataFrame is used when: MergeIntoCommand is executed DeltaSource is requested for a DataFrame for data between start and end offsets","title":" Creating DataFrame For Given AddFiles"},{"location":"DeltaLog/#minfileretentiontimestamp-method","text":"minFileRetentionTimestamp : Long minFileRetentionTimestamp is the timestamp that is tombstoneRetentionMillis before the current time (per the given Clock ). minFileRetentionTimestamp is used when: DeltaLog is requested for the currentSnapshot , to updateInternal , and to getSnapshotAt VacuumCommand is requested for garbage collecting of a delta table","title":" minFileRetentionTimestamp Method"},{"location":"DeltaLog/#tombstoneretentionmillis-method","text":"tombstoneRetentionMillis : Long tombstoneRetentionMillis gives the value of deletedFileRetentionDuration table property ( from the Metadata ). tombstoneRetentionMillis is used when: DeltaLog is requested for minFileRetentionTimestamp VacuumCommand is requested for garbage collecting of a delta table","title":" tombstoneRetentionMillis Method"},{"location":"DeltaLog/#updateinternal-internal-method","text":"updateInternal ( isAsync : Boolean ) : Snapshot updateInternal ...FIXME updateInternal is used when: DeltaLog is requested to update (directly or via tryUpdate )","title":" updateInternal Internal Method"},{"location":"DeltaLog/#invalidating-cached-deltalog-instance-by-path","text":"invalidateCache ( spark : SparkSession , dataPath : Path ) : Unit invalidateCache ...FIXME invalidateCache is a public API and does not seem to be used at all.","title":" Invalidating Cached DeltaLog Instance By Path"},{"location":"DeltaLog/#protocolread","text":"protocolRead ( protocol : Protocol ) : Unit protocolRead ...FIXME protocolRead is used when: OptimisticTransactionImpl is requested to validate and retry a commit Snapshot is created DeltaSource is requested to verifyStreamHygieneAndFilterAddFiles","title":" protocolRead"},{"location":"DeltaLog/#logstoreprovider","text":"DeltaLog is a LogStoreProvider .","title":"LogStoreProvider"},{"location":"DeltaLog/#logging","text":"Enable ALL logging level for org.apache.spark.sql.delta.DeltaLog logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.DeltaLog=ALL Refer to Logging .","title":"Logging"},{"location":"DeltaLogFileIndex/","text":"DeltaLogFileIndex \u00b6 DeltaLogFileIndex is a FileIndex for Snapshot (for the commit and checkpoint files). Note Learn more on FileIndex in The Internals of Spark SQL online book. Creating Instance \u00b6 DeltaLogFileIndex takes the following to be created: FileFormat Files (as Hadoop FileStatus es) While being created, DeltaLogFileIndex prints out the following INFO message to the logs: Created [this] DeltaLogFileIndex is created (indirectly using apply utility) when Snapshot is requested for DeltaLogFileIndex for commit or checkpoint files. FileFormat \u00b6 DeltaLogFileIndex is given a FileFormat ( Spark SQL ) when created : JsonFileFormat ( Spark SQL ) for commit files ParquetFileFormat ( Spark SQL ) for checkpoint files Text Representation \u00b6 toString : String toString returns the following (using the given FileFormat , the number of files and their estimated size): DeltaLogFileIndex([format], numFilesInSegment: [files], totalFileSize: [sizeInBytes]) Creating DeltaLogFileIndex \u00b6 apply ( format : FileFormat , files : Seq [ FileStatus ]) : Option [ DeltaLogFileIndex ] apply creates a new DeltaLogFileIndex (for a non-empty collection of files). apply is used when Snapshot is requested for DeltaLogFileIndex for commit or checkpoint files. Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.delta.DeltaLogFileIndex logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.DeltaLogFileIndex=ALL Refer to Logging .","title":"DeltaLogFileIndex"},{"location":"DeltaLogFileIndex/#deltalogfileindex","text":"DeltaLogFileIndex is a FileIndex for Snapshot (for the commit and checkpoint files). Note Learn more on FileIndex in The Internals of Spark SQL online book.","title":"DeltaLogFileIndex"},{"location":"DeltaLogFileIndex/#creating-instance","text":"DeltaLogFileIndex takes the following to be created: FileFormat Files (as Hadoop FileStatus es) While being created, DeltaLogFileIndex prints out the following INFO message to the logs: Created [this] DeltaLogFileIndex is created (indirectly using apply utility) when Snapshot is requested for DeltaLogFileIndex for commit or checkpoint files.","title":"Creating Instance"},{"location":"DeltaLogFileIndex/#fileformat","text":"DeltaLogFileIndex is given a FileFormat ( Spark SQL ) when created : JsonFileFormat ( Spark SQL ) for commit files ParquetFileFormat ( Spark SQL ) for checkpoint files","title":" FileFormat"},{"location":"DeltaLogFileIndex/#text-representation","text":"toString : String toString returns the following (using the given FileFormat , the number of files and their estimated size): DeltaLogFileIndex([format], numFilesInSegment: [files], totalFileSize: [sizeInBytes])","title":" Text Representation"},{"location":"DeltaLogFileIndex/#creating-deltalogfileindex","text":"apply ( format : FileFormat , files : Seq [ FileStatus ]) : Option [ DeltaLogFileIndex ] apply creates a new DeltaLogFileIndex (for a non-empty collection of files). apply is used when Snapshot is requested for DeltaLogFileIndex for commit or checkpoint files.","title":" Creating DeltaLogFileIndex"},{"location":"DeltaLogFileIndex/#logging","text":"Enable ALL logging level for org.apache.spark.sql.delta.DeltaLogFileIndex logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.DeltaLogFileIndex=ALL Refer to Logging .","title":"Logging"},{"location":"DeltaOptions/","text":"DeltaOptions \u00b6 DeltaOptions is an extension of DeltaWriteOptions and DeltaReadOptions for all supported options of the DeltaDataSource . DeltaOptions is used to create WriteIntoDelta command, DeltaSink , and DeltaSource . DeltaOptions can be verified . Options \u00b6 checkpointLocation \u00b6 dataChange \u00b6 excludeRegex \u00b6 ignoreChanges \u00b6 ignoreDeletes \u00b6 ignoreFileDeletion \u00b6 maxBytesPerTrigger \u00b6 maxFilesPerTrigger \u00b6 Maximum number of files ( AddFiles ) that DeltaSource is supposed to scan ( read ) in a streaming micro-batch ( trigger ) Default: 1000 Must be at least 1 mergeSchema \u00b6 Enables schema migration (and allows automatic schema merging during a write operation for WriteIntoDelta and DeltaSink ) Equivalent SQL Session configuration: spark.databricks.delta.schema.autoMerge.enabled optimizeWrite \u00b6 overwriteSchema \u00b6 path \u00b6 (required) Directory on a Hadoop DFS-compliant file system with an optional time travel identifier Default: (undefined) Note Can also be specified using load method of DataFrameReader and DataStreamReader . queryName \u00b6 replaceWhere \u00b6 timestampAsOf \u00b6 Timestamp of the version of a Delta table for Time Travel Mutually exclusive with versionAsOf option and the time travel identifier of the path option. userMetadata \u00b6 Defines a user-defined commit metadata Take precedence over spark.databricks.delta.commitInfo.userMetadata Available by inspecting CommitInfo s using DESCRIBE HISTORY or DeltaTable.history . Demo Learn more in Demo: User Metadata for Labelling Commits . versionAsOf \u00b6 Version of a Delta table for Time Travel Mutually exclusive with timestampAsOf option and the time travel identifier of the path option. Used when: DeltaDataSource is requested for a relation Creating Instance \u00b6 DeltaOptions takes the following to be created: Case-Insensitive Options SQLConf ( Spark SQL ) When created, DeltaOptions verifies the input options. DeltaOptions is created when: DeltaLog is requested for a relation (for DeltaDataSource as a CreatableRelationProvider and a RelationProvider ) DeltaDataSource is requested for a streaming source (to create a DeltaSource for Structured Streaming), a streaming sink (to create a DeltaSink for Structured Streaming), and for an insertable HadoopFsRelation WriteIntoDeltaBuilder is requested to buildForV1Write CreateDeltaTableCommand is requested to run How to Define Options \u00b6 The options can be defined using option method of the following: DataFrameReader and DataFrameWriter for batch queries (Spark SQL) DataStreamReader and DataStreamWriter for streaming queries (Spark Structured Streaming) Verifying Options \u00b6 verifyOptions ( options : CaseInsensitiveMap [ String ]) : Unit verifyOptions finds invalid options among the input options . Note In the open-source version verifyOptions does nothing. The underlying objects ( recordDeltaEvent and the others) are no-ops. verifyOptions is used when: DeltaOptions is created DeltaDataSource is requested for a relation (for loading data in batch queries)","title":"DeltaOptions"},{"location":"DeltaOptions/#deltaoptions","text":"DeltaOptions is an extension of DeltaWriteOptions and DeltaReadOptions for all supported options of the DeltaDataSource . DeltaOptions is used to create WriteIntoDelta command, DeltaSink , and DeltaSource . DeltaOptions can be verified .","title":"DeltaOptions"},{"location":"DeltaOptions/#options","text":"","title":" Options"},{"location":"DeltaOptions/#checkpointlocation","text":"","title":" checkpointLocation"},{"location":"DeltaOptions/#datachange","text":"","title":" dataChange"},{"location":"DeltaOptions/#excluderegex","text":"","title":" excludeRegex"},{"location":"DeltaOptions/#ignorechanges","text":"","title":" ignoreChanges"},{"location":"DeltaOptions/#ignoredeletes","text":"","title":" ignoreDeletes"},{"location":"DeltaOptions/#ignorefiledeletion","text":"","title":" ignoreFileDeletion"},{"location":"DeltaOptions/#maxbytespertrigger","text":"","title":" maxBytesPerTrigger"},{"location":"DeltaOptions/#maxfilespertrigger","text":"Maximum number of files ( AddFiles ) that DeltaSource is supposed to scan ( read ) in a streaming micro-batch ( trigger ) Default: 1000 Must be at least 1","title":" maxFilesPerTrigger"},{"location":"DeltaOptions/#mergeschema","text":"Enables schema migration (and allows automatic schema merging during a write operation for WriteIntoDelta and DeltaSink ) Equivalent SQL Session configuration: spark.databricks.delta.schema.autoMerge.enabled","title":" mergeSchema"},{"location":"DeltaOptions/#optimizewrite","text":"","title":" optimizeWrite"},{"location":"DeltaOptions/#overwriteschema","text":"","title":" overwriteSchema"},{"location":"DeltaOptions/#path","text":"(required) Directory on a Hadoop DFS-compliant file system with an optional time travel identifier Default: (undefined) Note Can also be specified using load method of DataFrameReader and DataStreamReader .","title":" path"},{"location":"DeltaOptions/#queryname","text":"","title":" queryName"},{"location":"DeltaOptions/#replacewhere","text":"","title":" replaceWhere"},{"location":"DeltaOptions/#timestampasof","text":"Timestamp of the version of a Delta table for Time Travel Mutually exclusive with versionAsOf option and the time travel identifier of the path option.","title":" timestampAsOf"},{"location":"DeltaOptions/#usermetadata","text":"Defines a user-defined commit metadata Take precedence over spark.databricks.delta.commitInfo.userMetadata Available by inspecting CommitInfo s using DESCRIBE HISTORY or DeltaTable.history . Demo Learn more in Demo: User Metadata for Labelling Commits .","title":" userMetadata"},{"location":"DeltaOptions/#versionasof","text":"Version of a Delta table for Time Travel Mutually exclusive with timestampAsOf option and the time travel identifier of the path option. Used when: DeltaDataSource is requested for a relation","title":" versionAsOf"},{"location":"DeltaOptions/#creating-instance","text":"DeltaOptions takes the following to be created: Case-Insensitive Options SQLConf ( Spark SQL ) When created, DeltaOptions verifies the input options. DeltaOptions is created when: DeltaLog is requested for a relation (for DeltaDataSource as a CreatableRelationProvider and a RelationProvider ) DeltaDataSource is requested for a streaming source (to create a DeltaSource for Structured Streaming), a streaming sink (to create a DeltaSink for Structured Streaming), and for an insertable HadoopFsRelation WriteIntoDeltaBuilder is requested to buildForV1Write CreateDeltaTableCommand is requested to run","title":"Creating Instance"},{"location":"DeltaOptions/#how-to-define-options","text":"The options can be defined using option method of the following: DataFrameReader and DataFrameWriter for batch queries (Spark SQL) DataStreamReader and DataStreamWriter for streaming queries (Spark Structured Streaming)","title":"How to Define Options"},{"location":"DeltaOptions/#verifying-options","text":"verifyOptions ( options : CaseInsensitiveMap [ String ]) : Unit verifyOptions finds invalid options among the input options . Note In the open-source version verifyOptions does nothing. The underlying objects ( recordDeltaEvent and the others) are no-ops. verifyOptions is used when: DeltaOptions is created DeltaDataSource is requested for a relation (for loading data in batch queries)","title":" Verifying Options"},{"location":"DeltaReadOptions/","text":"DeltaReadOptions \u00b6 DeltaReadOptions is...FIXME","title":"DeltaReadOptions"},{"location":"DeltaReadOptions/#deltareadoptions","text":"DeltaReadOptions is...FIXME","title":"DeltaReadOptions"},{"location":"DeltaSQLConf/","text":"DeltaSQLConf \u2014 spark.databricks.delta Configuration Properties \u00b6 DeltaSQLConf contains spark.databricks.delta -prefixed configuration properties to configure behaviour of Delta Lake. alterLocation.bypassSchemaCheck \u00b6 spark.databricks.delta.alterLocation.bypassSchemaCheck enables Alter Table Set Location on Delta to go through even if the Delta table in the new location has a different schema from the original Delta table Default: false checkLatestSchemaOnRead \u00b6 spark.databricks.delta.checkLatestSchemaOnRead (internal) enables a check that ensures that users won't read corrupt data if the source schema changes in an incompatible way. Default: true In Delta, we always try to give users the latest version of their data without having to call REFRESH TABLE or redefine their DataFrames when used in the context of streaming. There is a possibility that the schema of the latest version of the table may be incompatible with the schema at the time of DataFrame creation. checkpoint.partSize \u00b6 spark.databricks.delta.checkpoint.partSize (internal) is the limit at which we will start parallelizing the checkpoint. We will attempt to write maximum of this many actions per checkpoint. Default: 5000000 commitInfo.enabled \u00b6 spark.databricks.delta.commitInfo.enabled controls whether to log commit information into a Delta log . Default: true commitInfo.userMetadata \u00b6 spark.databricks.delta.commitInfo.userMetadata is an arbitrary user-defined metadata to include in CommitInfo (requires spark.databricks.delta.commitInfo.enabled ). Default: (empty) commitValidation.enabled \u00b6 spark.databricks.delta.commitValidation.enabled (internal) controls whether to perform validation checks before commit or not Default: true convert.metadataCheck.enabled \u00b6 spark.databricks.delta.convert.metadataCheck.enabled enables validation during convert to delta, if there is a difference between the catalog table's properties and the Delta table's configuration, we should error. If disabled, merge the two configurations with the same semantics as update and merge Default: true dummyFileManager.numOfFiles \u00b6 spark.databricks.delta.dummyFileManager.numOfFiles (internal) controls how many dummy files to write in DummyFileManager Default: 3 dummyFileManager.prefix \u00b6 spark.databricks.delta.dummyFileManager.prefix (internal) is the file prefix to use in DummyFileManager Default: .s3-optimization- history.maxKeysPerList \u00b6 spark.databricks.delta.history.maxKeysPerList (internal) controls how many commits to list when performing a parallel search. The default is the maximum keys returned by S3 per list call. Azure can return 5000, therefore we choose 1000. Default: 1000 history.metricsEnabled \u00b6 spark.databricks.delta.history.metricsEnabled enables Metrics reporting in Describe History. CommitInfo will now record the Operation Metrics. Default: true Used when: OptimisticTransactionImpl is requested to getOperationMetrics ConvertToDeltaCommand is requested to streamWrite SQLMetricsReporting is requested to registerSQLMetrics TransactionalWrite is requested to writeFiles import.batchSize.schemaInference \u00b6 spark.databricks.delta.import.batchSize.schemaInference (internal) is the number of files per batch for schema inference during import . Default: 1000000 import.batchSize.statsCollection \u00b6 spark.databricks.delta.import.batchSize.statsCollection (internal) is the number of files per batch for stats collection during import . Default: 50000 maxSnapshotLineageLength \u00b6 spark.databricks.delta.maxSnapshotLineageLength (internal) is the maximum lineage length of a Snapshot before Delta forces to build a Snapshot from scratch Default: 50 merge.maxInsertCount \u00b6 spark.databricks.delta.merge.maxInsertCount (internal) is the maximum row count of inserts in each MERGE execution Default: 10000L merge.optimizeInsertOnlyMerge.enabled \u00b6 spark.databricks.delta.merge.optimizeInsertOnlyMerge.enabled (internal) controls merge without any matched clause (i.e., insert-only merge) will be optimized by avoiding rewriting old files and just inserting new files Default: true merge.optimizeMatchedOnlyMerge.enabled \u00b6 spark.databricks.delta.merge.optimizeMatchedOnlyMerge.enabled (internal) controls merge without 'when not matched' clause will be optimized to use a right outer join instead of a full outer join Default: true merge.repartitionBeforeWrite.enabled \u00b6 spark.databricks.delta.merge.repartitionBeforeWrite.enabled (internal) controls merge will repartition the output by the table's partition columns before writing the files Default: false partitionColumnValidity.enabled \u00b6 spark.databricks.delta.partitionColumnValidity.enabled (internal) enables validation of the partition column names (just like the data columns) Default: true retentionDurationCheck.enabled \u00b6 spark.databricks.delta.retentionDurationCheck.enabled adds a check preventing users from running vacuum with a very short retention period, which may end up corrupting a Delta log. Default: true sampling.enabled \u00b6 spark.databricks.delta.sampling.enabled (internal) enables sample-based estimation Default: false schema.autoMerge.enabled \u00b6 spark.databricks.delta.schema.autoMerge.enabled enables schema merging on appends and overwrites. Default: false Equivalent DataFrame option: mergeSchema snapshotIsolation.enabled \u00b6 spark.databricks.delta.snapshotIsolation.enabled (internal) controls whether queries on Delta tables are guaranteed to have snapshot isolation Default: true snapshotPartitions \u00b6 spark.databricks.delta.snapshotPartitions (internal) is the number of partitions to use for state reconstruction (when building a snapshot of a Delta table). Default: 50 stalenessLimit \u00b6 spark.databricks.delta.stalenessLimit (in millis) allows you to query the last loaded state of the Delta table without blocking on a table update. You can use this configuration to reduce the latency on queries when up-to-date results are not a requirement. Table updates will be scheduled on a separate scheduler pool in a FIFO queue, and will share cluster resources fairly with your query. If a table hasn't updated past this time limit, we will block on a synchronous state update before running the query. Default: 0 (no tables can be stale) state.corruptionIsFatal \u00b6 spark.databricks.delta.state.corruptionIsFatal (internal) throws a fatal error when the recreated Delta State doesn't match committed checksum file Default: true stateReconstructionValidation.enabled \u00b6 spark.databricks.delta.stateReconstructionValidation.enabled (internal) controls whether to perform validation checks on the reconstructed state Default: true stats.collect \u00b6 spark.databricks.delta.stats.collect (internal) enables statistics to be collected while writing files into a Delta table Default: true stats.limitPushdown.enabled \u00b6 spark.databricks.delta.stats.limitPushdown.enabled (internal) enables using the limit clause and file statistics to prune files before they are collected to the driver Default: true stats.localCache.maxNumFiles \u00b6 spark.databricks.delta.stats.localCache.maxNumFiles (internal) is the maximum number of files for a table to be considered a delta small table . Some metadata operations (such as using data skipping) are optimized for small tables using driver local caching and local execution. Default: 2000 stats.skipping \u00b6 spark.databricks.delta.stats.skipping (internal) enables statistics used for skipping Default: true timeTravel.resolveOnIdentifier.enabled \u00b6 spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled (internal) controls whether to resolve patterns as @v123 in identifiers as time travel nodes. Default: true","title":"Configuration Properties"},{"location":"DeltaSQLConf/#deltasqlconf-sparkdatabricksdelta-configuration-properties","text":"DeltaSQLConf contains spark.databricks.delta -prefixed configuration properties to configure behaviour of Delta Lake.","title":"DeltaSQLConf &mdash; spark.databricks.delta Configuration Properties"},{"location":"DeltaSQLConf/#alterlocationbypassschemacheck","text":"spark.databricks.delta.alterLocation.bypassSchemaCheck enables Alter Table Set Location on Delta to go through even if the Delta table in the new location has a different schema from the original Delta table Default: false","title":" alterLocation.bypassSchemaCheck"},{"location":"DeltaSQLConf/#checklatestschemaonread","text":"spark.databricks.delta.checkLatestSchemaOnRead (internal) enables a check that ensures that users won't read corrupt data if the source schema changes in an incompatible way. Default: true In Delta, we always try to give users the latest version of their data without having to call REFRESH TABLE or redefine their DataFrames when used in the context of streaming. There is a possibility that the schema of the latest version of the table may be incompatible with the schema at the time of DataFrame creation.","title":" checkLatestSchemaOnRead"},{"location":"DeltaSQLConf/#checkpointpartsize","text":"spark.databricks.delta.checkpoint.partSize (internal) is the limit at which we will start parallelizing the checkpoint. We will attempt to write maximum of this many actions per checkpoint. Default: 5000000","title":" checkpoint.partSize"},{"location":"DeltaSQLConf/#commitinfoenabled","text":"spark.databricks.delta.commitInfo.enabled controls whether to log commit information into a Delta log . Default: true","title":" commitInfo.enabled"},{"location":"DeltaSQLConf/#commitinfousermetadata","text":"spark.databricks.delta.commitInfo.userMetadata is an arbitrary user-defined metadata to include in CommitInfo (requires spark.databricks.delta.commitInfo.enabled ). Default: (empty)","title":" commitInfo.userMetadata"},{"location":"DeltaSQLConf/#commitvalidationenabled","text":"spark.databricks.delta.commitValidation.enabled (internal) controls whether to perform validation checks before commit or not Default: true","title":" commitValidation.enabled"},{"location":"DeltaSQLConf/#convertmetadatacheckenabled","text":"spark.databricks.delta.convert.metadataCheck.enabled enables validation during convert to delta, if there is a difference between the catalog table's properties and the Delta table's configuration, we should error. If disabled, merge the two configurations with the same semantics as update and merge Default: true","title":" convert.metadataCheck.enabled"},{"location":"DeltaSQLConf/#dummyfilemanagernumoffiles","text":"spark.databricks.delta.dummyFileManager.numOfFiles (internal) controls how many dummy files to write in DummyFileManager Default: 3","title":" dummyFileManager.numOfFiles"},{"location":"DeltaSQLConf/#dummyfilemanagerprefix","text":"spark.databricks.delta.dummyFileManager.prefix (internal) is the file prefix to use in DummyFileManager Default: .s3-optimization-","title":" dummyFileManager.prefix"},{"location":"DeltaSQLConf/#historymaxkeysperlist","text":"spark.databricks.delta.history.maxKeysPerList (internal) controls how many commits to list when performing a parallel search. The default is the maximum keys returned by S3 per list call. Azure can return 5000, therefore we choose 1000. Default: 1000","title":" history.maxKeysPerList"},{"location":"DeltaSQLConf/#historymetricsenabled","text":"spark.databricks.delta.history.metricsEnabled enables Metrics reporting in Describe History. CommitInfo will now record the Operation Metrics. Default: true Used when: OptimisticTransactionImpl is requested to getOperationMetrics ConvertToDeltaCommand is requested to streamWrite SQLMetricsReporting is requested to registerSQLMetrics TransactionalWrite is requested to writeFiles","title":" history.metricsEnabled"},{"location":"DeltaSQLConf/#importbatchsizeschemainference","text":"spark.databricks.delta.import.batchSize.schemaInference (internal) is the number of files per batch for schema inference during import . Default: 1000000","title":" import.batchSize.schemaInference"},{"location":"DeltaSQLConf/#importbatchsizestatscollection","text":"spark.databricks.delta.import.batchSize.statsCollection (internal) is the number of files per batch for stats collection during import . Default: 50000","title":" import.batchSize.statsCollection"},{"location":"DeltaSQLConf/#maxsnapshotlineagelength","text":"spark.databricks.delta.maxSnapshotLineageLength (internal) is the maximum lineage length of a Snapshot before Delta forces to build a Snapshot from scratch Default: 50","title":" maxSnapshotLineageLength"},{"location":"DeltaSQLConf/#mergemaxinsertcount","text":"spark.databricks.delta.merge.maxInsertCount (internal) is the maximum row count of inserts in each MERGE execution Default: 10000L","title":" merge.maxInsertCount"},{"location":"DeltaSQLConf/#mergeoptimizeinsertonlymergeenabled","text":"spark.databricks.delta.merge.optimizeInsertOnlyMerge.enabled (internal) controls merge without any matched clause (i.e., insert-only merge) will be optimized by avoiding rewriting old files and just inserting new files Default: true","title":" merge.optimizeInsertOnlyMerge.enabled"},{"location":"DeltaSQLConf/#mergeoptimizematchedonlymergeenabled","text":"spark.databricks.delta.merge.optimizeMatchedOnlyMerge.enabled (internal) controls merge without 'when not matched' clause will be optimized to use a right outer join instead of a full outer join Default: true","title":" merge.optimizeMatchedOnlyMerge.enabled"},{"location":"DeltaSQLConf/#mergerepartitionbeforewriteenabled","text":"spark.databricks.delta.merge.repartitionBeforeWrite.enabled (internal) controls merge will repartition the output by the table's partition columns before writing the files Default: false","title":" merge.repartitionBeforeWrite.enabled"},{"location":"DeltaSQLConf/#partitioncolumnvalidityenabled","text":"spark.databricks.delta.partitionColumnValidity.enabled (internal) enables validation of the partition column names (just like the data columns) Default: true","title":" partitionColumnValidity.enabled"},{"location":"DeltaSQLConf/#retentiondurationcheckenabled","text":"spark.databricks.delta.retentionDurationCheck.enabled adds a check preventing users from running vacuum with a very short retention period, which may end up corrupting a Delta log. Default: true","title":" retentionDurationCheck.enabled"},{"location":"DeltaSQLConf/#samplingenabled","text":"spark.databricks.delta.sampling.enabled (internal) enables sample-based estimation Default: false","title":" sampling.enabled"},{"location":"DeltaSQLConf/#schemaautomergeenabled","text":"spark.databricks.delta.schema.autoMerge.enabled enables schema merging on appends and overwrites. Default: false Equivalent DataFrame option: mergeSchema","title":" schema.autoMerge.enabled"},{"location":"DeltaSQLConf/#snapshotisolationenabled","text":"spark.databricks.delta.snapshotIsolation.enabled (internal) controls whether queries on Delta tables are guaranteed to have snapshot isolation Default: true","title":" snapshotIsolation.enabled"},{"location":"DeltaSQLConf/#snapshotpartitions","text":"spark.databricks.delta.snapshotPartitions (internal) is the number of partitions to use for state reconstruction (when building a snapshot of a Delta table). Default: 50","title":" snapshotPartitions"},{"location":"DeltaSQLConf/#stalenesslimit","text":"spark.databricks.delta.stalenessLimit (in millis) allows you to query the last loaded state of the Delta table without blocking on a table update. You can use this configuration to reduce the latency on queries when up-to-date results are not a requirement. Table updates will be scheduled on a separate scheduler pool in a FIFO queue, and will share cluster resources fairly with your query. If a table hasn't updated past this time limit, we will block on a synchronous state update before running the query. Default: 0 (no tables can be stale)","title":" stalenessLimit"},{"location":"DeltaSQLConf/#statecorruptionisfatal","text":"spark.databricks.delta.state.corruptionIsFatal (internal) throws a fatal error when the recreated Delta State doesn't match committed checksum file Default: true","title":" state.corruptionIsFatal"},{"location":"DeltaSQLConf/#statereconstructionvalidationenabled","text":"spark.databricks.delta.stateReconstructionValidation.enabled (internal) controls whether to perform validation checks on the reconstructed state Default: true","title":" stateReconstructionValidation.enabled"},{"location":"DeltaSQLConf/#statscollect","text":"spark.databricks.delta.stats.collect (internal) enables statistics to be collected while writing files into a Delta table Default: true","title":" stats.collect"},{"location":"DeltaSQLConf/#statslimitpushdownenabled","text":"spark.databricks.delta.stats.limitPushdown.enabled (internal) enables using the limit clause and file statistics to prune files before they are collected to the driver Default: true","title":" stats.limitPushdown.enabled"},{"location":"DeltaSQLConf/#statslocalcachemaxnumfiles","text":"spark.databricks.delta.stats.localCache.maxNumFiles (internal) is the maximum number of files for a table to be considered a delta small table . Some metadata operations (such as using data skipping) are optimized for small tables using driver local caching and local execution. Default: 2000","title":" stats.localCache.maxNumFiles"},{"location":"DeltaSQLConf/#statsskipping","text":"spark.databricks.delta.stats.skipping (internal) enables statistics used for skipping Default: true","title":" stats.skipping"},{"location":"DeltaSQLConf/#timetravelresolveonidentifierenabled","text":"spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled (internal) controls whether to resolve patterns as @v123 in identifiers as time travel nodes. Default: true","title":" timeTravel.resolveOnIdentifier.enabled"},{"location":"DeltaSink/","text":"= DeltaSink DeltaSink is the sink of < > for streaming queries in Spark Structured Streaming. TIP: Read up on https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Sink.html[Streaming Sink] in https://bit.ly/spark-structured-streaming[The Internals of Spark Structured Streaming] online book. DeltaSink is < > exclusively when DeltaDataSource is requested for a < > (Structured Streaming). [[toString]] DeltaSink uses the following text representation (with the < >): DeltaSink[path] [[ImplicitMetadataOperation]] DeltaSink is an < > of a < >. == [[creating-instance]] Creating Instance DeltaSink takes the following to be created: [[sqlContext]] SQLContext [[path]] Hadoop Path of the delta table (to < > as configured by the < > option) [[partitionColumns]] Names of the partition columns ( Seq[String] ) [[outputMode]] OutputMode [[options]] < > == [[deltaLog]] deltaLog Internal Property [source, scala] \u00b6 deltaLog: DeltaLog \u00b6 deltaLog is a < > that is < > for the < > when DeltaSink is created (when DeltaDataSource is requested for a < >). deltaLog is used exclusively when DeltaSink is requested to < >. == [[addBatch]] Adding Streaming Micro-Batch [source, scala] \u00b6 addBatch( batchId: Long, data: DataFrame): Unit NOTE: addBatch is part of the Sink contract (in Spark Structured Streaming) to add a batch of data to the sink. addBatch requests the < > to < >. addBatch ...FIXME In the end, addBatch requests the OptimisticTransaction to < >.","title":"DeltaSink"},{"location":"DeltaSink/#source-scala","text":"","title":"[source, scala]"},{"location":"DeltaSink/#deltalog-deltalog","text":"deltaLog is a < > that is < > for the < > when DeltaSink is created (when DeltaDataSource is requested for a < >). deltaLog is used exclusively when DeltaSink is requested to < >. == [[addBatch]] Adding Streaming Micro-Batch","title":"deltaLog: DeltaLog"},{"location":"DeltaSink/#source-scala_1","text":"addBatch( batchId: Long, data: DataFrame): Unit NOTE: addBatch is part of the Sink contract (in Spark Structured Streaming) to add a batch of data to the sink. addBatch requests the < > to < >. addBatch ...FIXME In the end, addBatch requests the OptimisticTransaction to < >.","title":"[source, scala]"},{"location":"DeltaSource/","text":"DeltaSource \u2014 Streaming Source of Delta Data Source \u00b6 DeltaSource is the streaming source of < > for streaming queries in Spark Structured Streaming. TIP: Read up on https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Source.html[Streaming Source] in https://bit.ly/spark-structured-streaming[The Internals of Spark Structured Streaming] online book. DeltaSource is < > when DeltaDataSource is requested for a < >. val q = spark .readStream // Creating a streaming query .format(\"delta\") // Using delta data source .load(\"/tmp/delta/users\") // Over data in a delta table .writeStream .format(\"memory\") .option(\"queryName\", \"demo\") .start import org.apache.spark.sql.execution.streaming.{MicroBatchExecution, StreamingQueryWrapper} val plan = q.asInstanceOf[StreamingQueryWrapper] .streamingQuery .asInstanceOf[MicroBatchExecution] .logicalPlan import org.apache.spark.sql.execution.streaming.StreamingExecutionRelation val relation = plan.collect { case r: StreamingExecutionRelation => r }.head import org.apache.spark.sql.delta.sources.DeltaSource assert(relation.source.asInstanceOf[DeltaSource]) scala> println(relation.source) DeltaSource[file:/tmp/delta/users] [[maxFilesPerTrigger]] DeltaSource uses < > option to limit the number of files to process when requested for the < >. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.delta.sources.DeltaSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.sources.DeltaSource=ALL Refer to Logging . \u00b6 == [[creating-instance]] Creating DeltaSource Instance DeltaSource takes the following to be created: [[spark]] SparkSession [[deltaLog]] < > of the delta table to read data (as < >) from [[options]] < > [[filters]] Filter expressions (default: no filters) DeltaSource initializes the < >. == [[getBatch]] Micro-Batch With Data Between Start And End Offsets (Streaming DataFrame) -- getBatch Method [source, scala] \u00b6 getBatch( start: Option[Offset], end: Offset): DataFrame NOTE: getBatch is part of the Source contract ( https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Source.html[Spark Structured Streaming]) for a streaming DataFrame with data between the start and end offsets. getBatch creates an < > for the < > (aka < >) and the given end offset. getBatch < > as follows...FIXME == [[getOffset]] Latest Available Offset -- getOffset Method [source, scala] \u00b6 getOffset: Option[Offset] \u00b6 NOTE: getOffset is part of the Source abstraction ( https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Source.html[Spark Structured Streaming]) for the latest available offset of this streaming source. [[getOffset-currentOffset]] getOffset calculates the latest offset (that a streaming query can use for the data of the next micro-batch) based on the < > internal registry. For no < >, getOffset simply < > (based on the latest version of the delta table). When the < > is defined (which means that the DeltaSource is requested for another micro-batch), getOffset takes the < > from < > for the < >. getOffset returns the < > when the last element was not available. With the < > and the < > both available, getOffset creates a new < > for the version, index, and isLast flag from the last indexed < >. [NOTE] \u00b6 isStartingVersion local value is enabled ( true ) when the following holds: Version of the last indexed < > is equal to the < > of the < > * < > flag of the < > is enabled ( true ) \u00b6 In the end, getOffset prints out the following DEBUG message to the logs (using the < > internal registry): previousOffset -> currentOffset: [previousOffset] -> [currentOffset] == [[stop]] Stopping -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 NOTE: stop is part of the streaming Source contract ( https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Source.html[Spark Structured Streaming]) to stop this source and free up any resources allocated. stop simply < >. == [[getStartingOffset]] Retrieving Starting Offset -- getStartingOffset Internal Method [source, scala] \u00b6 getStartingOffset(): Option[Offset] \u00b6 getStartingOffset requests the < > for the version of the delta table (by requesting for the < > and then for the < >). getStartingOffset < > from the < > for the version of the delta table, -1L as the fromIndex , and the isStartingVersion flag enabled ( true ). getStartingOffset returns a new < > for the < >, the version and the index of the last file added, and whether the last file added is the last file of its version. getStartingOffset returns None ( offset not available ) when either happens: the version of the delta table is negative (below 0 ) no files were added in the version getStartingOffset throws an AssertionError when the version of the last file added is smaller than the delta table's version: assertion failed: getChangesWithRateLimit returns an invalid version: [v] (expected: >= [version]) NOTE: getStartingOffset is used exclusively when DeltaSource is requested for the < >. == [[getChanges]] getChanges Internal Method [source, scala] \u00b6 getChanges( fromVersion: Long, fromIndex: Long, isStartingVersion: Boolean): Iterator[IndexedFile] getChanges branches per the given isStartingVersion flag (enabled or not): For isStartingVersion flag enabled ( true ), getChanges < > for the given fromVersion followed by < > for the next version after the given fromVersion For isStartingVersion flag disabled ( false ), getChanges simply gives < > for the given fromVersion [NOTE] \u00b6 isStartingVersion flag simply adds < > before < > when enabled ( true ). isStartingVersion flag is enabled when DeltaSource is requested for the following: < > and the start offset is not given or is for the < > * < > with no < > or the < > for the < > \u00b6 In the end, getChanges filters out ( excludes ) indexed < > that are not with the version later than the given fromVersion or the index greater than the given fromIndex . NOTE: getChanges is used when DeltaSource is requested for the < > (when requested for the < >) and < >. === [[getChanges-filterAndIndexDeltaLogs]] filterAndIndexDeltaLogs Internal Method [source, scala] \u00b6 filterAndIndexDeltaLogs( startVersion: Long): Iterator[IndexedFile] filterAndIndexDeltaLogs ...FIXME == [[getChangesWithRateLimit]] Retrieving File Additions (With Rate Limit) -- getChangesWithRateLimit Internal Method [source, scala] \u00b6 getChangesWithRateLimit( fromVersion: Long, fromIndex: Long, isStartingVersion: Boolean): Iterator[IndexedFile] getChangesWithRateLimit < > (as indexed < >) for the given fromVersion , fromIndex , and isStartingVersion flag. getChangesWithRateLimit takes the configured number of AddFiles (up to the < > option (if defined) or < >). NOTE: getChangesWithRateLimit is used when DeltaSource is requested for the < >. == [[getSnapshotAt]] Retrieving State Of Delta Table At Given Version -- getSnapshotAt Internal Method [source, scala] \u00b6 getSnapshotAt( version: Long): Iterator[IndexedFile] getSnapshotAt requests the < > for the < > (as indexed < >). In case the < > hasn't been initialized yet ( null ) or the requested version is different from the < >, getSnapshotAt does the following: . < > . Requests the < > for the < > at the version . Creates a new < > for the state (snapshot) as the current < > . Changes the < > internal registry to the requested version NOTE: getSnapshotAt is used when DeltaSource is requested to < > (with isStartingVersion flag enabled). == [[verifyStreamHygieneAndFilterAddFiles]] verifyStreamHygieneAndFilterAddFiles Internal Method [source, scala] \u00b6 verifyStreamHygieneAndFilterAddFiles( actions: Seq[Action]): Seq[Action] verifyStreamHygieneAndFilterAddFiles ...FIXME NOTE: verifyStreamHygieneAndFilterAddFiles is used when DeltaSource is requested to < >. == [[cleanUpSnapshotResources]] cleanUpSnapshotResources Internal Method [source, scala] \u00b6 cleanUpSnapshotResources(): Unit \u00b6 cleanUpSnapshotResources ...FIXME NOTE: cleanUpSnapshotResources is used when DeltaSource is requested to < >, < > and < >. == [[iteratorLast]] Retrieving Last Element From Iterator -- iteratorLast Internal Method [source, scala] \u00b6 iteratorLast T : Option[T] iteratorLast simply returns the last element in the given Iterator or None . NOTE: iteratorLast is used when DeltaSource is requested to < > and < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | initialState a| [[initialState]] < > Initially uninitialized ( null ). Changes (along with the < >) when DeltaSource is requested for the < > (only when the versions are different) Used when DeltaSource is requested for the < > Closed and dereferenced ( null ) when DeltaSource is requested to < > | initialStateVersion a| [[initialStateVersion]] Version of the < > Initially -1L and changes (along with the < >) to the version requested when DeltaSource is requested for the < > (only when the versions are different) Used when DeltaSource is requested to < > (and unpersist the current snapshot) | previousOffset a| [[previousOffset]] Ending < > of the latest < > Starts uninitialized ( null ). Used when DeltaSource is requested for the < >. | tableId a| [[tableId]] Table ID Used when...FIXME |===","title":"DeltaSource"},{"location":"DeltaSource/#deltasource-streaming-source-of-delta-data-source","text":"DeltaSource is the streaming source of < > for streaming queries in Spark Structured Streaming. TIP: Read up on https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Source.html[Streaming Source] in https://bit.ly/spark-structured-streaming[The Internals of Spark Structured Streaming] online book. DeltaSource is < > when DeltaDataSource is requested for a < >. val q = spark .readStream // Creating a streaming query .format(\"delta\") // Using delta data source .load(\"/tmp/delta/users\") // Over data in a delta table .writeStream .format(\"memory\") .option(\"queryName\", \"demo\") .start import org.apache.spark.sql.execution.streaming.{MicroBatchExecution, StreamingQueryWrapper} val plan = q.asInstanceOf[StreamingQueryWrapper] .streamingQuery .asInstanceOf[MicroBatchExecution] .logicalPlan import org.apache.spark.sql.execution.streaming.StreamingExecutionRelation val relation = plan.collect { case r: StreamingExecutionRelation => r }.head import org.apache.spark.sql.delta.sources.DeltaSource assert(relation.source.asInstanceOf[DeltaSource]) scala> println(relation.source) DeltaSource[file:/tmp/delta/users] [[maxFilesPerTrigger]] DeltaSource uses < > option to limit the number of files to process when requested for the < >. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.delta.sources.DeltaSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.sources.DeltaSource=ALL","title":"DeltaSource &mdash; Streaming Source of Delta Data Source"},{"location":"DeltaSource/#refer-to-logging","text":"== [[creating-instance]] Creating DeltaSource Instance DeltaSource takes the following to be created: [[spark]] SparkSession [[deltaLog]] < > of the delta table to read data (as < >) from [[options]] < > [[filters]] Filter expressions (default: no filters) DeltaSource initializes the < >. == [[getBatch]] Micro-Batch With Data Between Start And End Offsets (Streaming DataFrame) -- getBatch Method","title":"Refer to Logging."},{"location":"DeltaSource/#source-scala","text":"getBatch( start: Option[Offset], end: Offset): DataFrame NOTE: getBatch is part of the Source contract ( https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Source.html[Spark Structured Streaming]) for a streaming DataFrame with data between the start and end offsets. getBatch creates an < > for the < > (aka < >) and the given end offset. getBatch < > as follows...FIXME == [[getOffset]] Latest Available Offset -- getOffset Method","title":"[source, scala]"},{"location":"DeltaSource/#source-scala_1","text":"","title":"[source, scala]"},{"location":"DeltaSource/#getoffset-optionoffset","text":"NOTE: getOffset is part of the Source abstraction ( https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Source.html[Spark Structured Streaming]) for the latest available offset of this streaming source. [[getOffset-currentOffset]] getOffset calculates the latest offset (that a streaming query can use for the data of the next micro-batch) based on the < > internal registry. For no < >, getOffset simply < > (based on the latest version of the delta table). When the < > is defined (which means that the DeltaSource is requested for another micro-batch), getOffset takes the < > from < > for the < >. getOffset returns the < > when the last element was not available. With the < > and the < > both available, getOffset creates a new < > for the version, index, and isLast flag from the last indexed < >.","title":"getOffset: Option[Offset]"},{"location":"DeltaSource/#note","text":"isStartingVersion local value is enabled ( true ) when the following holds: Version of the last indexed < > is equal to the < > of the < >","title":"[NOTE]"},{"location":"DeltaSource/#flag-of-the-is-enabled-true","text":"In the end, getOffset prints out the following DEBUG message to the logs (using the < > internal registry): previousOffset -> currentOffset: [previousOffset] -> [currentOffset] == [[stop]] Stopping -- stop Method","title":"* &lt;&gt; flag of the &lt;&gt; is enabled (true)"},{"location":"DeltaSource/#source-scala_2","text":"","title":"[source, scala]"},{"location":"DeltaSource/#stop-unit","text":"NOTE: stop is part of the streaming Source contract ( https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Source.html[Spark Structured Streaming]) to stop this source and free up any resources allocated. stop simply < >. == [[getStartingOffset]] Retrieving Starting Offset -- getStartingOffset Internal Method","title":"stop(): Unit"},{"location":"DeltaSource/#source-scala_3","text":"","title":"[source, scala]"},{"location":"DeltaSource/#getstartingoffset-optionoffset","text":"getStartingOffset requests the < > for the version of the delta table (by requesting for the < > and then for the < >). getStartingOffset < > from the < > for the version of the delta table, -1L as the fromIndex , and the isStartingVersion flag enabled ( true ). getStartingOffset returns a new < > for the < >, the version and the index of the last file added, and whether the last file added is the last file of its version. getStartingOffset returns None ( offset not available ) when either happens: the version of the delta table is negative (below 0 ) no files were added in the version getStartingOffset throws an AssertionError when the version of the last file added is smaller than the delta table's version: assertion failed: getChangesWithRateLimit returns an invalid version: [v] (expected: >= [version]) NOTE: getStartingOffset is used exclusively when DeltaSource is requested for the < >. == [[getChanges]] getChanges Internal Method","title":"getStartingOffset(): Option[Offset]"},{"location":"DeltaSource/#source-scala_4","text":"getChanges( fromVersion: Long, fromIndex: Long, isStartingVersion: Boolean): Iterator[IndexedFile] getChanges branches per the given isStartingVersion flag (enabled or not): For isStartingVersion flag enabled ( true ), getChanges < > for the given fromVersion followed by < > for the next version after the given fromVersion For isStartingVersion flag disabled ( false ), getChanges simply gives < > for the given fromVersion","title":"[source, scala]"},{"location":"DeltaSource/#note_1","text":"isStartingVersion flag simply adds < > before < > when enabled ( true ). isStartingVersion flag is enabled when DeltaSource is requested for the following: < > and the start offset is not given or is for the < >","title":"[NOTE]"},{"location":"DeltaSource/#with-no-or-the-for-the","text":"In the end, getChanges filters out ( excludes ) indexed < > that are not with the version later than the given fromVersion or the index greater than the given fromIndex . NOTE: getChanges is used when DeltaSource is requested for the < > (when requested for the < >) and < >. === [[getChanges-filterAndIndexDeltaLogs]] filterAndIndexDeltaLogs Internal Method","title":"* &lt;&gt; with no &lt;&gt; or the &lt;&gt; for the &lt;&gt;"},{"location":"DeltaSource/#source-scala_5","text":"filterAndIndexDeltaLogs( startVersion: Long): Iterator[IndexedFile] filterAndIndexDeltaLogs ...FIXME == [[getChangesWithRateLimit]] Retrieving File Additions (With Rate Limit) -- getChangesWithRateLimit Internal Method","title":"[source, scala]"},{"location":"DeltaSource/#source-scala_6","text":"getChangesWithRateLimit( fromVersion: Long, fromIndex: Long, isStartingVersion: Boolean): Iterator[IndexedFile] getChangesWithRateLimit < > (as indexed < >) for the given fromVersion , fromIndex , and isStartingVersion flag. getChangesWithRateLimit takes the configured number of AddFiles (up to the < > option (if defined) or < >). NOTE: getChangesWithRateLimit is used when DeltaSource is requested for the < >. == [[getSnapshotAt]] Retrieving State Of Delta Table At Given Version -- getSnapshotAt Internal Method","title":"[source, scala]"},{"location":"DeltaSource/#source-scala_7","text":"getSnapshotAt( version: Long): Iterator[IndexedFile] getSnapshotAt requests the < > for the < > (as indexed < >). In case the < > hasn't been initialized yet ( null ) or the requested version is different from the < >, getSnapshotAt does the following: . < > . Requests the < > for the < > at the version . Creates a new < > for the state (snapshot) as the current < > . Changes the < > internal registry to the requested version NOTE: getSnapshotAt is used when DeltaSource is requested to < > (with isStartingVersion flag enabled). == [[verifyStreamHygieneAndFilterAddFiles]] verifyStreamHygieneAndFilterAddFiles Internal Method","title":"[source, scala]"},{"location":"DeltaSource/#source-scala_8","text":"verifyStreamHygieneAndFilterAddFiles( actions: Seq[Action]): Seq[Action] verifyStreamHygieneAndFilterAddFiles ...FIXME NOTE: verifyStreamHygieneAndFilterAddFiles is used when DeltaSource is requested to < >. == [[cleanUpSnapshotResources]] cleanUpSnapshotResources Internal Method","title":"[source, scala]"},{"location":"DeltaSource/#source-scala_9","text":"","title":"[source, scala]"},{"location":"DeltaSource/#cleanupsnapshotresources-unit","text":"cleanUpSnapshotResources ...FIXME NOTE: cleanUpSnapshotResources is used when DeltaSource is requested to < >, < > and < >. == [[iteratorLast]] Retrieving Last Element From Iterator -- iteratorLast Internal Method","title":"cleanUpSnapshotResources(): Unit"},{"location":"DeltaSource/#source-scala_10","text":"iteratorLast T : Option[T] iteratorLast simply returns the last element in the given Iterator or None . NOTE: iteratorLast is used when DeltaSource is requested to < > and < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | initialState a| [[initialState]] < > Initially uninitialized ( null ). Changes (along with the < >) when DeltaSource is requested for the < > (only when the versions are different) Used when DeltaSource is requested for the < > Closed and dereferenced ( null ) when DeltaSource is requested to < > | initialStateVersion a| [[initialStateVersion]] Version of the < > Initially -1L and changes (along with the < >) to the version requested when DeltaSource is requested for the < > (only when the versions are different) Used when DeltaSource is requested to < > (and unpersist the current snapshot) | previousOffset a| [[previousOffset]] Ending < > of the latest < > Starts uninitialized ( null ). Used when DeltaSource is requested for the < >. | tableId a| [[tableId]] Table ID Used when...FIXME |===","title":"[source, scala]"},{"location":"DeltaSourceOffset/","text":"= DeltaSourceOffset -- Streaming Offset Of DeltaSource DeltaSourceOffset is a streaming Offset for DeltaSource.md[DeltaSource]. TIP: Read up on https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-Offset.html[Offset ] in https://bit.ly/spark-structured-streaming[The Internals of Spark Structured Streaming] online book. DeltaSourceOffset is < > (via < > utility) when DeltaSource is requested for the DeltaSource.md#getOffset[latest offset] and a DeltaSource.md#getBatch[batch (for the given starting and ending offsets)]. [[VERSION]] DeltaSourceOffset uses the version 1 . == [[creating-instance]] Creating DeltaSourceOffset Instance DeltaSourceOffset takes the following to be created: [[sourceVersion]] Source Version (always < >) [[reservoirId]] Reservoir ID (aka DeltaSource.md#tableId[Table ID]) [[reservoirVersion]] Reservoir Version [[index]] Index [[isStartingVersion]] isStartingVersion flag == [[apply]] Creating DeltaSourceOffset Instance -- apply Utility [source, scala] \u00b6 apply( reservoirId: String, reservoirVersion: Long, index: Long, isStartingVersion: Boolean): DeltaSourceOffset apply( reservoirId: String, offset: Offset): DeltaSourceOffset apply creates a new DeltaSourceOffset (for the < > and the given arguments) or converts the given Offset to a DeltaSourceOffset . NOTE: apply is used when DeltaSource is requested for the DeltaSource.md#getOffset[latest offset] and a DeltaSource.md#getBatch[batch (for the given starting and ending offsets)]. == [[json]] json Method [source, scala] \u00b6 json: String \u00b6 NOTE: json is part of the Offset contract to serialize an offset to JSON. json ...FIXME == [[validateSourceVersion]] validateSourceVersion Internal Utility [source, scala] \u00b6 validateSourceVersion( json: String): Unit validateSourceVersion ...FIXME NOTE: validateSourceVersion is used when...FIXME","title":"DeltaSourceOffset"},{"location":"DeltaSourceOffset/#source-scala","text":"apply( reservoirId: String, reservoirVersion: Long, index: Long, isStartingVersion: Boolean): DeltaSourceOffset apply( reservoirId: String, offset: Offset): DeltaSourceOffset apply creates a new DeltaSourceOffset (for the < > and the given arguments) or converts the given Offset to a DeltaSourceOffset . NOTE: apply is used when DeltaSource is requested for the DeltaSource.md#getOffset[latest offset] and a DeltaSource.md#getBatch[batch (for the given starting and ending offsets)]. == [[json]] json Method","title":"[source, scala]"},{"location":"DeltaSourceOffset/#source-scala_1","text":"","title":"[source, scala]"},{"location":"DeltaSourceOffset/#json-string","text":"NOTE: json is part of the Offset contract to serialize an offset to JSON. json ...FIXME == [[validateSourceVersion]] validateSourceVersion Internal Utility","title":"json: String"},{"location":"DeltaSourceOffset/#source-scala_2","text":"validateSourceVersion( json: String): Unit validateSourceVersion ...FIXME NOTE: validateSourceVersion is used when...FIXME","title":"[source, scala]"},{"location":"DeltaSourceSnapshot/","text":"= DeltaSourceSnapshot [[SnapshotIterator]][[StateCache]] DeltaSourceSnapshot is a < > with < > DeltaSourceSnapshot is < > when DeltaSource is requested for the < >. [[version]] When < >, DeltaSourceSnapshot requests the < > for the < > that it uses for the < > (a new column and the name of the cached RDD). == [[creating-instance]] Creating DeltaSourceSnapshot Instance DeltaSourceSnapshot takes the following to be created: [[spark]] SparkSession [[snapshot]] < > [[filters]] Filter expressions ( Seq[Expression] ) == [[initialFiles]] Initial Files (Indexed AddFiles) -- initialFiles Method [source, scala] \u00b6 initialFiles: Dataset[IndexedFile] \u00b6 initialFiles requests the < > for < > ( Dataset[AddFile] ) and sorts them by < > and < > in ascending order. initialFiles zips the < > with indices (using RDD.zipWithIndex operator), adds two new columns with the < > and isLast as false , and finally creates a Dataset[IndexedFile] . In the end, initialFiles < > with the following name (with the < > and the < > of the < >) Delta Source Snapshot #[version] - [redactedPath] NOTE: initialFiles is used exclusively when SnapshotIterator is requested for a < >.","title":"DeltaSourceSnapshot"},{"location":"DeltaSourceSnapshot/#source-scala","text":"","title":"[source, scala]"},{"location":"DeltaSourceSnapshot/#initialfiles-datasetindexedfile","text":"initialFiles requests the < > for < > ( Dataset[AddFile] ) and sorts them by < > and < > in ascending order. initialFiles zips the < > with indices (using RDD.zipWithIndex operator), adds two new columns with the < > and isLast as false , and finally creates a Dataset[IndexedFile] . In the end, initialFiles < > with the following name (with the < > and the < > of the < >) Delta Source Snapshot #[version] - [redactedPath] NOTE: initialFiles is used exclusively when SnapshotIterator is requested for a < >.","title":"initialFiles: Dataset[IndexedFile]"},{"location":"DeltaSparkSessionExtension/","text":"DeltaSparkSessionExtension \u00b6 DeltaSparkSessionExtension is used to register ( inject ) the following extensions: Delta SQL support (using DeltaSqlParser ) DeltaAnalysis logical resolution rule DeltaUnsupportedOperationsCheck PreprocessTableUpdate logical resolution rule PreprocessTableMerge logical resolution rule PreprocessTableDelete logical resolution rule DeltaSparkSessionExtension is registered using spark.sql.extensions configuration property (while creating a SparkSession in a Spark application).","title":"DeltaSparkSessionExtension"},{"location":"DeltaSparkSessionExtension/#deltasparksessionextension","text":"DeltaSparkSessionExtension is used to register ( inject ) the following extensions: Delta SQL support (using DeltaSqlParser ) DeltaAnalysis logical resolution rule DeltaUnsupportedOperationsCheck PreprocessTableUpdate logical resolution rule PreprocessTableMerge logical resolution rule PreprocessTableDelete logical resolution rule DeltaSparkSessionExtension is registered using spark.sql.extensions configuration property (while creating a SparkSession in a Spark application).","title":"DeltaSparkSessionExtension"},{"location":"DeltaTable/","text":"DeltaTable \u00b6 DeltaTable is the management interface of a Delta table. DeltaTable instances are created using utilities (e.g. DeltaTable.forName , DeltaTable.convertToDelta ). Utilities (Static Methods) \u00b6 convertToDelta \u00b6 convertToDelta ( spark : SparkSession , identifier : String ) : DeltaTable convertToDelta ( spark : SparkSession , identifier : String , partitionSchema : String ) : DeltaTable convertToDelta ( spark : SparkSession , identifier : String , partitionSchema : StructType ) : DeltaTable convertToDelta converts a parquet table to delta format (and makes the table available in Delta Lake). Note Refer to Demo: Converting Parquet Dataset Into Delta Format for a demo of DeltaTable.convertToDelta . Internally, convertToDelta requests the SparkSession for the SQL parser ( ParserInterface ) that is in turn requested to parse the given table identifier (to get a TableIdentifier ). Tip Read up on ParserInterface in The Internals of Spark SQL online book. In the end, convertToDelta uses the DeltaConvert utility to convert the parquet table to delta format and creates a DeltaTable . forName \u00b6 forName ( sparkSession : SparkSession , tableName : String ) : DeltaTable forName ( tableOrViewName : String ) : DeltaTable forName uses ParserInterface (of the given SparkSession ) to parse the given table name. forName checks whether the given table name is of a Delta table and, if so, creates a DeltaTable with the following: Dataset that represents loading data from the specified table name (using SparkSession.table operator) DeltaLog of the specified table forName throws an AnalysisException when the given table name is for non-Delta table: [deltaTableIdentifier] is not a Delta table. forName is used internally when DeltaConvert utility is used to executeConvert . forPath \u00b6 forPath ( sparkSession : SparkSession , path : String ) : DeltaTable forPath ( path : String ) : DeltaTable forPath creates a DeltaTable instance for data in the given directory ( path ) when the given directory is part of a delta table already (as the root or a child directory). assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession]) val tableId = \"/tmp/delta-table/users\" import io.delta.tables.DeltaTable assert(DeltaTable.isDeltaTable(tableId), s\"$tableId should be a Delta table\") val dt = DeltaTable.forPath(\"delta-table\") forPath throws an AnalysisException when the given path does not belong to a delta table: [deltaTableIdentifier] is not a Delta table. Internally, forPath creates a new DeltaTable with the following: Dataset that represents loading data from the specified path using delta data source DeltaLog for the (transaction log in) the specified path forPath is used internally in DeltaTable.convertToDelta (via DeltaConvert utility). isDeltaTable \u00b6 isDeltaTable ( sparkSession : SparkSession , identifier : String ) : Boolean isDeltaTable ( identifier : String ) : Boolean isDeltaTable checks whether the provided identifier string is a file path that points to the root of a Delta table or one of the subdirectories. Internally, isDeltaTable simply relays to DeltaTableUtils.isDeltaTable utility. Creating Instance \u00b6 DeltaTable takes the following to be created: Table Data ( Dataset[Row] ) DeltaLog DeltaTable is created using DeltaTable.forPath or DeltaTable.forName utilities. Operators \u00b6 alias \u00b6 alias ( alias : String ) : DeltaTable Applies an alias to the DeltaTable (equivalent to as ) as \u00b6 as ( alias : String ) : DeltaTable Applies an alias to the DeltaTable delete \u00b6 delete () : Unit delete ( condition : Column ) : Unit delete ( condition : String ) : Unit Deletes data from the DeltaTable that matches the given condition . generate \u00b6 generate ( mode : String ) : Unit Generates a manifest for the delta table generate executeGenerate with the table ID of the format ++delta. path ++ (where the path is the data directory of the DeltaLog ) and the given mode. history \u00b6 history () : DataFrame history ( limit : Int ) : DataFrame Gets available commits ( history ) of the DeltaTable merge \u00b6 merge ( source : DataFrame , condition : Column ) : DeltaMergeBuilder merge ( source : DataFrame , condition : String ) : DeltaMergeBuilder Creates a DeltaMergeBuilder toDF \u00b6 toDF : Dataset [ Row ] Returns the DataFrame representation of the DeltaTable update \u00b6 update ( condition : Column , set : Map [ String , Column ]) : Unit update ( set : Map [ String , Column ]) : Unit Updates data in the DeltaTable on the rows that match the given condition based on the rules defined by set updateExpr \u00b6 updateExpr ( set : Map [ String , String ]) : Unit updateExpr ( condition : String , set : Map [ String , String ]) : Unit Updates data in the DeltaTable on the rows that match the given condition based on the rules defined by set vacuum \u00b6 vacuum () : DataFrame vacuum ( retentionHours : Double ) : DataFrame Deletes files and directories (recursively) in the DeltaTable that are not needed by the table (and maintains older versions up to the given retention threshold). vacuum executes vacuum command . Demo \u00b6 import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val path = \"/tmp/delta/t1\" // random data to create a delta table from scratch val data = spark.range(5) data.write.format(\"delta\").save(path) import io.delta.tables.DeltaTable val dt = DeltaTable.forPath(spark, path) val history = dt.history.select('version, 'timestamp, 'operation, 'operationParameters, 'isBlindAppend) scala> history.show(truncate = false) +-------+-------------------+---------+------------------------------------------+-------------+ |version|timestamp |operation|operationParameters |isBlindAppend| +-------+-------------------+---------+------------------------------------------+-------------+ |0 |2019-12-23 22:24:40|WRITE |[mode -> ErrorIfExists, partitionBy -> []]|true | +-------+-------------------+---------+------------------------------------------+-------------+","title":"DeltaTable"},{"location":"DeltaTable/#deltatable","text":"DeltaTable is the management interface of a Delta table. DeltaTable instances are created using utilities (e.g. DeltaTable.forName , DeltaTable.convertToDelta ).","title":"DeltaTable"},{"location":"DeltaTable/#utilities-static-methods","text":"","title":" Utilities (Static Methods)"},{"location":"DeltaTable/#converttodelta","text":"convertToDelta ( spark : SparkSession , identifier : String ) : DeltaTable convertToDelta ( spark : SparkSession , identifier : String , partitionSchema : String ) : DeltaTable convertToDelta ( spark : SparkSession , identifier : String , partitionSchema : StructType ) : DeltaTable convertToDelta converts a parquet table to delta format (and makes the table available in Delta Lake). Note Refer to Demo: Converting Parquet Dataset Into Delta Format for a demo of DeltaTable.convertToDelta . Internally, convertToDelta requests the SparkSession for the SQL parser ( ParserInterface ) that is in turn requested to parse the given table identifier (to get a TableIdentifier ). Tip Read up on ParserInterface in The Internals of Spark SQL online book. In the end, convertToDelta uses the DeltaConvert utility to convert the parquet table to delta format and creates a DeltaTable .","title":" convertToDelta"},{"location":"DeltaTable/#forname","text":"forName ( sparkSession : SparkSession , tableName : String ) : DeltaTable forName ( tableOrViewName : String ) : DeltaTable forName uses ParserInterface (of the given SparkSession ) to parse the given table name. forName checks whether the given table name is of a Delta table and, if so, creates a DeltaTable with the following: Dataset that represents loading data from the specified table name (using SparkSession.table operator) DeltaLog of the specified table forName throws an AnalysisException when the given table name is for non-Delta table: [deltaTableIdentifier] is not a Delta table. forName is used internally when DeltaConvert utility is used to executeConvert .","title":" forName"},{"location":"DeltaTable/#forpath","text":"forPath ( sparkSession : SparkSession , path : String ) : DeltaTable forPath ( path : String ) : DeltaTable forPath creates a DeltaTable instance for data in the given directory ( path ) when the given directory is part of a delta table already (as the root or a child directory). assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession]) val tableId = \"/tmp/delta-table/users\" import io.delta.tables.DeltaTable assert(DeltaTable.isDeltaTable(tableId), s\"$tableId should be a Delta table\") val dt = DeltaTable.forPath(\"delta-table\") forPath throws an AnalysisException when the given path does not belong to a delta table: [deltaTableIdentifier] is not a Delta table. Internally, forPath creates a new DeltaTable with the following: Dataset that represents loading data from the specified path using delta data source DeltaLog for the (transaction log in) the specified path forPath is used internally in DeltaTable.convertToDelta (via DeltaConvert utility).","title":" forPath"},{"location":"DeltaTable/#isdeltatable","text":"isDeltaTable ( sparkSession : SparkSession , identifier : String ) : Boolean isDeltaTable ( identifier : String ) : Boolean isDeltaTable checks whether the provided identifier string is a file path that points to the root of a Delta table or one of the subdirectories. Internally, isDeltaTable simply relays to DeltaTableUtils.isDeltaTable utility.","title":" isDeltaTable"},{"location":"DeltaTable/#creating-instance","text":"DeltaTable takes the following to be created: Table Data ( Dataset[Row] ) DeltaLog DeltaTable is created using DeltaTable.forPath or DeltaTable.forName utilities.","title":"Creating Instance"},{"location":"DeltaTable/#operators","text":"","title":"Operators"},{"location":"DeltaTable/#alias","text":"alias ( alias : String ) : DeltaTable Applies an alias to the DeltaTable (equivalent to as )","title":" alias"},{"location":"DeltaTable/#as","text":"as ( alias : String ) : DeltaTable Applies an alias to the DeltaTable","title":" as"},{"location":"DeltaTable/#delete","text":"delete () : Unit delete ( condition : Column ) : Unit delete ( condition : String ) : Unit Deletes data from the DeltaTable that matches the given condition .","title":" delete"},{"location":"DeltaTable/#generate","text":"generate ( mode : String ) : Unit Generates a manifest for the delta table generate executeGenerate with the table ID of the format ++delta. path ++ (where the path is the data directory of the DeltaLog ) and the given mode.","title":" generate"},{"location":"DeltaTable/#history","text":"history () : DataFrame history ( limit : Int ) : DataFrame Gets available commits ( history ) of the DeltaTable","title":" history"},{"location":"DeltaTable/#merge","text":"merge ( source : DataFrame , condition : Column ) : DeltaMergeBuilder merge ( source : DataFrame , condition : String ) : DeltaMergeBuilder Creates a DeltaMergeBuilder","title":" merge"},{"location":"DeltaTable/#todf","text":"toDF : Dataset [ Row ] Returns the DataFrame representation of the DeltaTable","title":" toDF"},{"location":"DeltaTable/#update","text":"update ( condition : Column , set : Map [ String , Column ]) : Unit update ( set : Map [ String , Column ]) : Unit Updates data in the DeltaTable on the rows that match the given condition based on the rules defined by set","title":" update"},{"location":"DeltaTable/#updateexpr","text":"updateExpr ( set : Map [ String , String ]) : Unit updateExpr ( condition : String , set : Map [ String , String ]) : Unit Updates data in the DeltaTable on the rows that match the given condition based on the rules defined by set","title":" updateExpr"},{"location":"DeltaTable/#vacuum","text":"vacuum () : DataFrame vacuum ( retentionHours : Double ) : DataFrame Deletes files and directories (recursively) in the DeltaTable that are not needed by the table (and maintains older versions up to the given retention threshold). vacuum executes vacuum command .","title":" vacuum"},{"location":"DeltaTable/#demo","text":"import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val path = \"/tmp/delta/t1\" // random data to create a delta table from scratch val data = spark.range(5) data.write.format(\"delta\").save(path) import io.delta.tables.DeltaTable val dt = DeltaTable.forPath(spark, path) val history = dt.history.select('version, 'timestamp, 'operation, 'operationParameters, 'isBlindAppend) scala> history.show(truncate = false) +-------+-------------------+---------+------------------------------------------+-------------+ |version|timestamp |operation|operationParameters |isBlindAppend| +-------+-------------------+---------+------------------------------------------+-------------+ |0 |2019-12-23 22:24:40|WRITE |[mode -> ErrorIfExists, partitionBy -> []]|true | +-------+-------------------+---------+------------------------------------------+-------------+","title":"Demo"},{"location":"DeltaTableOperations/","text":"DeltaTableOperations \u2014 Delta DML Operations \u00b6 DeltaTableOperations is an abstraction of management services for executing delete , generate , history , update , and vacuum operations ( commands ). DeltaTableOperations is assumed to be associated with a DeltaTable . Implementations \u00b6 DeltaTable Executing DeleteFromTable Command \u00b6 executeDelete ( condition : Option [ Expression ]) : Unit executeDelete creates a DataFrame for a DeleteFromTable logical operator (from Spark SQL) with (the analyzed logical plan of the DeltaTable and the given condition ). Note DeleteFromTable is a Command (Spark SQL) that represents DELETE FROM SQL statement for v2 tables. As a Command it is executed eagerly. executeDelete is used for DeltaTable.delete operator. Executing DeltaGenerateCommand Command \u00b6 executeGenerate ( tblIdentifier : String , mode : String ) : Unit executeGenerate requests the SQL parser (of the SparkSession ) to parse the given table identifier, creates a DeltaGenerateCommand and runs it. executeGenerate is used for DeltaTable.generate operator. Executing History Command \u00b6 executeHistory ( deltaLog : DeltaLog , limit : Option [ Int ]) : DataFrame executeHistory creates DeltaHistoryManager (for the given DeltaLog ) and requests it for the number of commits to match the limit . In the end, executeHistory creates a DataFrame for the commits. executeHistory is used for DeltaTable.history operator. Executing UpdateTable Command \u00b6 executeUpdate ( set : Map [ String , Column ], condition : Option [ Column ]) : Unit executeUpdate creates a DataFrame for a UpdateTable logical operator (from Spark SQL) with (the analyzed logical plan of the DeltaTable and the given condition ). Note UpdateTable is a Command (Spark SQL) that represents UPDATE TABLE SQL statement for v2 tables. As a Command it is executed eagerly. executeUpdate is used for DeltaTable.update and DeltaTable.updateExpr operators. Executing VacuumCommand \u00b6 executeVacuum ( deltaLog : DeltaLog , retentionHours : Option [ Double ]) : DataFrame executeVacuum uses the VacuumCommand utility to gc (with the dryRun flag off and the given retentionHours ). In the end, executeVacuum returns an empty DataFrame . Note executeVacuum returns an empty DataFrame not the one from VacuumCommand.gc . NOTE: executeVacuum is used exclusively in < > operator.","title":"DeltaTableOperations"},{"location":"DeltaTableOperations/#deltatableoperations-delta-dml-operations","text":"DeltaTableOperations is an abstraction of management services for executing delete , generate , history , update , and vacuum operations ( commands ). DeltaTableOperations is assumed to be associated with a DeltaTable .","title":"DeltaTableOperations &mdash; Delta DML Operations"},{"location":"DeltaTableOperations/#implementations","text":"DeltaTable","title":"Implementations"},{"location":"DeltaTableOperations/#executing-deletefromtable-command","text":"executeDelete ( condition : Option [ Expression ]) : Unit executeDelete creates a DataFrame for a DeleteFromTable logical operator (from Spark SQL) with (the analyzed logical plan of the DeltaTable and the given condition ). Note DeleteFromTable is a Command (Spark SQL) that represents DELETE FROM SQL statement for v2 tables. As a Command it is executed eagerly. executeDelete is used for DeltaTable.delete operator.","title":" Executing DeleteFromTable Command"},{"location":"DeltaTableOperations/#executing-deltageneratecommand-command","text":"executeGenerate ( tblIdentifier : String , mode : String ) : Unit executeGenerate requests the SQL parser (of the SparkSession ) to parse the given table identifier, creates a DeltaGenerateCommand and runs it. executeGenerate is used for DeltaTable.generate operator.","title":" Executing DeltaGenerateCommand Command"},{"location":"DeltaTableOperations/#executing-history-command","text":"executeHistory ( deltaLog : DeltaLog , limit : Option [ Int ]) : DataFrame executeHistory creates DeltaHistoryManager (for the given DeltaLog ) and requests it for the number of commits to match the limit . In the end, executeHistory creates a DataFrame for the commits. executeHistory is used for DeltaTable.history operator.","title":" Executing History Command"},{"location":"DeltaTableOperations/#executing-updatetable-command","text":"executeUpdate ( set : Map [ String , Column ], condition : Option [ Column ]) : Unit executeUpdate creates a DataFrame for a UpdateTable logical operator (from Spark SQL) with (the analyzed logical plan of the DeltaTable and the given condition ). Note UpdateTable is a Command (Spark SQL) that represents UPDATE TABLE SQL statement for v2 tables. As a Command it is executed eagerly. executeUpdate is used for DeltaTable.update and DeltaTable.updateExpr operators.","title":" Executing UpdateTable Command"},{"location":"DeltaTableOperations/#executing-vacuumcommand","text":"executeVacuum ( deltaLog : DeltaLog , retentionHours : Option [ Double ]) : DataFrame executeVacuum uses the VacuumCommand utility to gc (with the dryRun flag off and the given retentionHours ). In the end, executeVacuum returns an empty DataFrame . Note executeVacuum returns an empty DataFrame not the one from VacuumCommand.gc . NOTE: executeVacuum is used exclusively in < > operator.","title":" Executing VacuumCommand"},{"location":"DeltaTableUtils/","text":"DeltaTableUtils Utility \u00b6 extractIfPathContainsTimeTravel \u00b6 extractIfPathContainsTimeTravel ( session : SparkSession , path : String ) : ( String , Option [ DeltaTimeTravelSpec ]) extractIfPathContainsTimeTravel ...FIXME extractIfPathContainsTimeTravel is used when: DeltaDataSource is requested to sourceSchema and parsePathIdentifier resolveTimeTravelVersion Utility \u00b6 resolveTimeTravelVersion ( conf : SQLConf , deltaLog : DeltaLog , tt : DeltaTimeTravelSpec ) : ( Long , String ) resolveTimeTravelVersion ...FIXME resolveTimeTravelVersion is used when: DeltaLog is requested to create a relation (per partition filters and time travel) DeltaTableV2 is requested for a Snapshot == [[isDeltaTable]] isDeltaTable Utility [source, scala] \u00b6 isDeltaTable( spark: SparkSession, path: Path): Boolean isDeltaTable tries to < > for the given path and returns whether it was successful or not. NOTE: isDeltaTable is used when DeltaTable utility is used to < > or < >. == [[findDeltaTableRoot]] findDeltaTableRoot Utility [source, scala] \u00b6 findDeltaTableRoot( spark: SparkSession, path: Path): Option[Path] findDeltaTableRoot traverses the Hadoop DFS-compliant path upwards (to the root directory of the file system) until _delta_log or _samples directories are found, or the root directory is reached. For _delta_log or _samples directories, findDeltaTableRoot returns the parent directory. [NOTE] \u00b6 findDeltaTableRoot is used when: < > is executed DeltaTableUtils utility is used to < > * DeltaDataSource is requested to < > \u00b6 == [[splitMetadataAndDataPredicates]] splitMetadataAndDataPredicates Utility [source, scala] \u00b6 splitMetadataAndDataPredicates( condition: Expression, partitionColumns: Seq[String], spark: SparkSession): (Seq[Expression], Seq[Expression]) splitMetadataAndDataPredicates ...FIXME [NOTE] \u00b6 splitMetadataAndDataPredicates is used when: PartitionFiltering is requested to PartitionFiltering.md#filesForScan[filesForScan] * DeleteCommand.md[DeleteCommand] and UpdateCommand.md[UpdateCommand] are executed \u00b6 == [[isPredicatePartitionColumnsOnly]] isPredicatePartitionColumnsOnly Utility [source, scala] \u00b6 isPredicatePartitionColumnsOnly( condition: Expression, partitionColumns: Seq[String], spark: SparkSession): Boolean isPredicatePartitionColumnsOnly ...FIXME [NOTE] \u00b6 isPredicatePartitionColumnsOnly is used when...FIXME \u00b6 == [[combineWithCatalogMetadata]] combineWithCatalogMetadata Utility [source, scala] \u00b6 combineWithCatalogMetadata( sparkSession: SparkSession, table: CatalogTable): CatalogTable combineWithCatalogMetadata ...FIXME NOTE: combineWithCatalogMetadata seems unused.","title":"DeltaTableUtils"},{"location":"DeltaTableUtils/#deltatableutils-utility","text":"","title":"DeltaTableUtils Utility"},{"location":"DeltaTableUtils/#extractifpathcontainstimetravel","text":"extractIfPathContainsTimeTravel ( session : SparkSession , path : String ) : ( String , Option [ DeltaTimeTravelSpec ]) extractIfPathContainsTimeTravel ...FIXME extractIfPathContainsTimeTravel is used when: DeltaDataSource is requested to sourceSchema and parsePathIdentifier","title":" extractIfPathContainsTimeTravel"},{"location":"DeltaTableUtils/#resolvetimetravelversion-utility","text":"resolveTimeTravelVersion ( conf : SQLConf , deltaLog : DeltaLog , tt : DeltaTimeTravelSpec ) : ( Long , String ) resolveTimeTravelVersion ...FIXME resolveTimeTravelVersion is used when: DeltaLog is requested to create a relation (per partition filters and time travel) DeltaTableV2 is requested for a Snapshot == [[isDeltaTable]] isDeltaTable Utility","title":" resolveTimeTravelVersion Utility"},{"location":"DeltaTableUtils/#source-scala","text":"isDeltaTable( spark: SparkSession, path: Path): Boolean isDeltaTable tries to < > for the given path and returns whether it was successful or not. NOTE: isDeltaTable is used when DeltaTable utility is used to < > or < >. == [[findDeltaTableRoot]] findDeltaTableRoot Utility","title":"[source, scala]"},{"location":"DeltaTableUtils/#source-scala_1","text":"findDeltaTableRoot( spark: SparkSession, path: Path): Option[Path] findDeltaTableRoot traverses the Hadoop DFS-compliant path upwards (to the root directory of the file system) until _delta_log or _samples directories are found, or the root directory is reached. For _delta_log or _samples directories, findDeltaTableRoot returns the parent directory.","title":"[source, scala]"},{"location":"DeltaTableUtils/#note","text":"findDeltaTableRoot is used when: < > is executed DeltaTableUtils utility is used to < >","title":"[NOTE]"},{"location":"DeltaTableUtils/#deltadatasource-is-requested-to","text":"== [[splitMetadataAndDataPredicates]] splitMetadataAndDataPredicates Utility","title":"* DeltaDataSource is requested to &lt;&gt;"},{"location":"DeltaTableUtils/#source-scala_2","text":"splitMetadataAndDataPredicates( condition: Expression, partitionColumns: Seq[String], spark: SparkSession): (Seq[Expression], Seq[Expression]) splitMetadataAndDataPredicates ...FIXME","title":"[source, scala]"},{"location":"DeltaTableUtils/#note_1","text":"splitMetadataAndDataPredicates is used when: PartitionFiltering is requested to PartitionFiltering.md#filesForScan[filesForScan]","title":"[NOTE]"},{"location":"DeltaTableUtils/#deletecommandmddeletecommand-and-updatecommandmdupdatecommand-are-executed","text":"== [[isPredicatePartitionColumnsOnly]] isPredicatePartitionColumnsOnly Utility","title":"* DeleteCommand.md[DeleteCommand] and UpdateCommand.md[UpdateCommand] are executed"},{"location":"DeltaTableUtils/#source-scala_3","text":"isPredicatePartitionColumnsOnly( condition: Expression, partitionColumns: Seq[String], spark: SparkSession): Boolean isPredicatePartitionColumnsOnly ...FIXME","title":"[source, scala]"},{"location":"DeltaTableUtils/#note_2","text":"","title":"[NOTE]"},{"location":"DeltaTableUtils/#ispredicatepartitioncolumnsonly-is-used-whenfixme","text":"== [[combineWithCatalogMetadata]] combineWithCatalogMetadata Utility","title":"isPredicatePartitionColumnsOnly is used when...FIXME"},{"location":"DeltaTableUtils/#source-scala_4","text":"combineWithCatalogMetadata( sparkSession: SparkSession, table: CatalogTable): CatalogTable combineWithCatalogMetadata ...FIXME NOTE: combineWithCatalogMetadata seems unused.","title":"[source, scala]"},{"location":"DeltaTableV2/","text":"DeltaTableV2 \u00b6 DeltaTableV2 is a logical representation of a writable Delta table. DeltaTableV2 by Spark SQL 3.0.0 Using the abstractions introduced in Spark SQL 3.0.0, DeltaTableV2 is a Table that SupportsWrite . Creating Instance \u00b6 DeltaTableV2 takes the following to be created: SparkSession Hadoop Path Optional Catalog Metadata ( Option[CatalogTable] ) Optional Table ID ( Option[String] ) Optional DeltaTimeTravelSpec DeltaTableV2 is created when: DeltaCatalog is requested to load a table DeltaDataSource is requested to load a table or create a table relation DeltaTimeTravelSpec \u00b6 DeltaTableV2 may be given a DeltaTimeTravelSpec to be created . DeltaTimeTravelSpec is assumed not to be defined. DeltaTableV2 is given a DeltaTimeTravelSpec only when DeltaDataSource is requested to create a BaseRelation . DeltaTimeTravelSpec is used for timeTravelSpec . Snapshot \u00b6 snapshot : Snapshot DeltaTableV2 has a Snapshot . In other words, DeltaTableV2 represents a Delta table at a specific version. Scala lazy value snapshot is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged. DeltaTableV2 uses the DeltaLog to load it at a given version (based on the optional timeTravelSpec ) or update to the latest version . snapshot is used when DeltaTableV2 is requested for the schema , partitioning and properties . DeltaTimeTravelSpec \u00b6 timeTravelSpec : Option [ DeltaTimeTravelSpec ] DeltaTableV2 may have a DeltaTimeTravelSpec specified that is either given or timeTravelByPath . timeTravelSpec throws an AnalysisException when timeTravelOpt and timeTravelByPath are both defined: Cannot specify time travel in multiple formats. timeTravelSpec is used when DeltaTableV2 is requested for a Snapshot and BaseRelation . DeltaTimeTravelSpec by Path \u00b6 timeTravelByPath : Option [ DeltaTimeTravelSpec ] Scala lazy value timeTravelByPath is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged. timeTravelByPath is undefined when CatalogTable is defined. With no CatalogTable defined, DeltaTableV2 parses the given Path for the timeTravelByPath (that resolvePath under the covers). Converting to Insertable HadoopFsRelation \u00b6 toBaseRelation : BaseRelation toBaseRelation verifyAndCreatePartitionFilters for the Path , the current Snapshot and partitionFilters . In the end, toBaseRelation requests the DeltaLog for an insertable HadoopFsRelation . toBaseRelation is used when: DeltaDataSource is requested to createRelation DeltaRelation utility is used to fromV2Relation","title":"DeltaTableV2"},{"location":"DeltaTableV2/#deltatablev2","text":"DeltaTableV2 is a logical representation of a writable Delta table. DeltaTableV2 by Spark SQL 3.0.0 Using the abstractions introduced in Spark SQL 3.0.0, DeltaTableV2 is a Table that SupportsWrite .","title":"DeltaTableV2"},{"location":"DeltaTableV2/#creating-instance","text":"DeltaTableV2 takes the following to be created: SparkSession Hadoop Path Optional Catalog Metadata ( Option[CatalogTable] ) Optional Table ID ( Option[String] ) Optional DeltaTimeTravelSpec DeltaTableV2 is created when: DeltaCatalog is requested to load a table DeltaDataSource is requested to load a table or create a table relation","title":"Creating Instance"},{"location":"DeltaTableV2/#deltatimetravelspec","text":"DeltaTableV2 may be given a DeltaTimeTravelSpec to be created . DeltaTimeTravelSpec is assumed not to be defined. DeltaTableV2 is given a DeltaTimeTravelSpec only when DeltaDataSource is requested to create a BaseRelation . DeltaTimeTravelSpec is used for timeTravelSpec .","title":" DeltaTimeTravelSpec"},{"location":"DeltaTableV2/#snapshot","text":"snapshot : Snapshot DeltaTableV2 has a Snapshot . In other words, DeltaTableV2 represents a Delta table at a specific version. Scala lazy value snapshot is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged. DeltaTableV2 uses the DeltaLog to load it at a given version (based on the optional timeTravelSpec ) or update to the latest version . snapshot is used when DeltaTableV2 is requested for the schema , partitioning and properties .","title":" Snapshot"},{"location":"DeltaTableV2/#deltatimetravelspec_1","text":"timeTravelSpec : Option [ DeltaTimeTravelSpec ] DeltaTableV2 may have a DeltaTimeTravelSpec specified that is either given or timeTravelByPath . timeTravelSpec throws an AnalysisException when timeTravelOpt and timeTravelByPath are both defined: Cannot specify time travel in multiple formats. timeTravelSpec is used when DeltaTableV2 is requested for a Snapshot and BaseRelation .","title":" DeltaTimeTravelSpec"},{"location":"DeltaTableV2/#deltatimetravelspec-by-path","text":"timeTravelByPath : Option [ DeltaTimeTravelSpec ] Scala lazy value timeTravelByPath is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged. timeTravelByPath is undefined when CatalogTable is defined. With no CatalogTable defined, DeltaTableV2 parses the given Path for the timeTravelByPath (that resolvePath under the covers).","title":" DeltaTimeTravelSpec by Path"},{"location":"DeltaTableV2/#converting-to-insertable-hadoopfsrelation","text":"toBaseRelation : BaseRelation toBaseRelation verifyAndCreatePartitionFilters for the Path , the current Snapshot and partitionFilters . In the end, toBaseRelation requests the DeltaLog for an insertable HadoopFsRelation . toBaseRelation is used when: DeltaDataSource is requested to createRelation DeltaRelation utility is used to fromV2Relation","title":" Converting to Insertable HadoopFsRelation"},{"location":"DeltaTimeTravelSpec/","text":"DeltaTimeTravelSpec \u00b6 Time Travel Patterns \u00b6 DeltaTimeTravelSpec defines regular expressions for timestamp- and version-based time travel identifiers: Version URI: (path)@[vV](some numbers) Timestamp URI: (path)@(yyyyMMddHHmmssSSS) Creating Instance \u00b6 DeltaTimeTravelSpec takes the following to be created: Timestamp Version creationSource identifier DeltaTimeTravelSpec asserts that either version or timestamp is provided (and throws an AssertionError ). DeltaTimeTravelSpec is created when: DeltaTimeTravelSpec utility is used to resolve a path DeltaDataSource utility is used to getTimeTravelVersion Resolving Path \u00b6 resolvePath ( conf : SQLConf , identifier : String ) : ( DeltaTimeTravelSpec , String ) resolvePath ...FIXME resolvePath is used when DeltaTableUtils utility is used to extractIfPathContainsTimeTravel . getTimestamp \u00b6 getTimestamp ( timeZone : String ) : Timestamp getTimestamp ...FIXME getTimestamp is used when DeltaTableUtils utility is used to resolveTimeTravelVersion . isApplicable \u00b6 isApplicable ( conf : SQLConf , identifier : String ) : Boolean isApplicable is true when all of the following hold: spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled is true identifierContainsTimeTravel is true isApplicable is used when DeltaTableUtils utility is used to extractIfPathContainsTimeTravel . identifierContainsTimeTravel \u00b6 identifierContainsTimeTravel ( identifier : String ) : Boolean identifierContainsTimeTravel is true when the given identifier is either timestamp or version time travel pattern.","title":"DeltaTimeTravelSpec"},{"location":"DeltaTimeTravelSpec/#deltatimetravelspec","text":"","title":"DeltaTimeTravelSpec"},{"location":"DeltaTimeTravelSpec/#time-travel-patterns","text":"DeltaTimeTravelSpec defines regular expressions for timestamp- and version-based time travel identifiers: Version URI: (path)@[vV](some numbers) Timestamp URI: (path)@(yyyyMMddHHmmssSSS)","title":" Time Travel Patterns"},{"location":"DeltaTimeTravelSpec/#creating-instance","text":"DeltaTimeTravelSpec takes the following to be created: Timestamp Version creationSource identifier DeltaTimeTravelSpec asserts that either version or timestamp is provided (and throws an AssertionError ). DeltaTimeTravelSpec is created when: DeltaTimeTravelSpec utility is used to resolve a path DeltaDataSource utility is used to getTimeTravelVersion","title":"Creating Instance"},{"location":"DeltaTimeTravelSpec/#resolving-path","text":"resolvePath ( conf : SQLConf , identifier : String ) : ( DeltaTimeTravelSpec , String ) resolvePath ...FIXME resolvePath is used when DeltaTableUtils utility is used to extractIfPathContainsTimeTravel .","title":" Resolving Path"},{"location":"DeltaTimeTravelSpec/#gettimestamp","text":"getTimestamp ( timeZone : String ) : Timestamp getTimestamp ...FIXME getTimestamp is used when DeltaTableUtils utility is used to resolveTimeTravelVersion .","title":" getTimestamp"},{"location":"DeltaTimeTravelSpec/#isapplicable","text":"isApplicable ( conf : SQLConf , identifier : String ) : Boolean isApplicable is true when all of the following hold: spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled is true identifierContainsTimeTravel is true isApplicable is used when DeltaTableUtils utility is used to extractIfPathContainsTimeTravel .","title":" isApplicable"},{"location":"DeltaTimeTravelSpec/#identifiercontainstimetravel","text":"identifierContainsTimeTravel ( identifier : String ) : Boolean identifierContainsTimeTravel is true when the given identifier is either timestamp or version time travel pattern.","title":" identifierContainsTimeTravel"},{"location":"DeltaUnsupportedOperationsCheck/","text":"DeltaUnsupportedOperationsCheck \u00b6 DeltaUnsupportedOperationsCheck is a logical check rule that adds helpful error messages when Delta is being used with unsupported Hive operations or if an unsupported operation is executed.","title":"DeltaUnsupportedOperationsCheck"},{"location":"DeltaUnsupportedOperationsCheck/#deltaunsupportedoperationscheck","text":"DeltaUnsupportedOperationsCheck is a logical check rule that adds helpful error messages when Delta is being used with unsupported Hive operations or if an unsupported operation is executed.","title":"DeltaUnsupportedOperationsCheck"},{"location":"DeltaWriteOptions/","text":"DeltaWriteOptions \u00b6 DeltaWriteOptions is...FIXME","title":"DeltaWriteOptions"},{"location":"DeltaWriteOptions/#deltawriteoptions","text":"DeltaWriteOptions is...FIXME","title":"DeltaWriteOptions"},{"location":"FileAction/","text":"= FileAction FileAction is an < > of the < > for < > with the < > and < > flag. [[contract]] .FileAction Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | dataChange a| [[dataChange]] [source, scala] \u00b6 dataChange: Boolean \u00b6 Used when...FIXME | path a| [[path]] [source, scala] \u00b6 path: String \u00b6 Used when...FIXME |=== [[implementations]] .FileActions [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | FileAction | Description | < > | [[AddFile]] | RemoveFile | [[RemoveFile]] |=== NOTE: FileAction is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file).","title":"FileAction"},{"location":"FileAction/#source-scala","text":"","title":"[source, scala]"},{"location":"FileAction/#datachange-boolean","text":"Used when...FIXME | path a| [[path]]","title":"dataChange: Boolean"},{"location":"FileAction/#source-scala_1","text":"","title":"[source, scala]"},{"location":"FileAction/#path-string","text":"Used when...FIXME |=== [[implementations]] .FileActions [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | FileAction | Description | < > | [[AddFile]] | RemoveFile | [[RemoveFile]] |=== NOTE: FileAction is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file).","title":"path: String"},{"location":"FileNames/","text":"= FileNames Utility [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | checkpointPrefix a| [[checkpointPrefix]] Creates a Hadoop Path for a file name with a given version : [version][%020d].checkpoint E.g. 00000000000000000005.checkpoint | isCheckpointFile a| [[isCheckpointFile]] | isDeltaFile a| [[isDeltaFile]] |=== == [[deltaFile]] Creating Hadoop Path To Delta File -- deltaFile Utility [source, scala] \u00b6 deltaFile( path: Path, version: Long): Path deltaFile creates a Hadoop Path to a file of the format [version][%020d].json in the path directory, e.g. 00000000000000000001.json . NOTE: deltaFile is used when...FIXME","title":"FileNames"},{"location":"FileNames/#source-scala","text":"deltaFile( path: Path, version: Long): Path deltaFile creates a Hadoop Path to a file of the format [version][%020d].json in the path directory, e.g. 00000000000000000001.json . NOTE: deltaFile is used when...FIXME","title":"[source, scala]"},{"location":"GenerateSymlinkManifest/","text":"GenerateSymlinkManifest (And GenerateSymlinkManifestImpl) \u00b6 [[GenerateSymlinkManifest]] GenerateSymlinkManifest is a concrete < > to generate < > and < > Hive-style manifests for delta tables. NOTE: You can generate a < > Hive-style manifest for delta tables using < > SQL command or < > operator. [[GenerateSymlinkManifestImpl]] GenerateSymlinkManifestImpl is a < > that...FIXME == [[generateFullManifest]] generateFullManifest Method [source, scala] \u00b6 generateFullManifest( spark: SparkSession, deltaLog: DeltaLog): Unit generateFullManifest ...FIXME NOTE: generateFullManifest is used when...FIXME == [[generateIncrementalManifest]] generateIncrementalManifest Method [source, scala] \u00b6 generateIncrementalManifest( spark: SparkSession, deltaLog: DeltaLog, txnReadSnapshot: Snapshot, actions: Seq[Action]): Unit generateIncrementalManifest ...FIXME NOTE: generateIncrementalManifest is used when...FIXME == [[run]] Running Post-Commit Hook -- run Method [source, scala] \u00b6 run( spark: SparkSession, txn: OptimisticTransactionImpl, committedActions: Seq[Action]): Unit NOTE: run is part of the < > to execute a post-commit hook. run simply < > for the < > and < > of the delta table (of the given < >) and the < >. == [[handleError]] Handling Errors -- handleError Method [source, scala] \u00b6 handleError( error: Throwable, version: Long): Unit NOTE: handleError is part of the < > to handle errors while < > handleError ...FIXME","title":"GenerateSymlinkManifest"},{"location":"GenerateSymlinkManifest/#generatesymlinkmanifest-and-generatesymlinkmanifestimpl","text":"[[GenerateSymlinkManifest]] GenerateSymlinkManifest is a concrete < > to generate < > and < > Hive-style manifests for delta tables. NOTE: You can generate a < > Hive-style manifest for delta tables using < > SQL command or < > operator. [[GenerateSymlinkManifestImpl]] GenerateSymlinkManifestImpl is a < > that...FIXME == [[generateFullManifest]] generateFullManifest Method","title":"GenerateSymlinkManifest (And GenerateSymlinkManifestImpl)"},{"location":"GenerateSymlinkManifest/#source-scala","text":"generateFullManifest( spark: SparkSession, deltaLog: DeltaLog): Unit generateFullManifest ...FIXME NOTE: generateFullManifest is used when...FIXME == [[generateIncrementalManifest]] generateIncrementalManifest Method","title":"[source, scala]"},{"location":"GenerateSymlinkManifest/#source-scala_1","text":"generateIncrementalManifest( spark: SparkSession, deltaLog: DeltaLog, txnReadSnapshot: Snapshot, actions: Seq[Action]): Unit generateIncrementalManifest ...FIXME NOTE: generateIncrementalManifest is used when...FIXME == [[run]] Running Post-Commit Hook -- run Method","title":"[source, scala]"},{"location":"GenerateSymlinkManifest/#source-scala_2","text":"run( spark: SparkSession, txn: OptimisticTransactionImpl, committedActions: Seq[Action]): Unit NOTE: run is part of the < > to execute a post-commit hook. run simply < > for the < > and < > of the delta table (of the given < >) and the < >. == [[handleError]] Handling Errors -- handleError Method","title":"[source, scala]"},{"location":"GenerateSymlinkManifest/#source-scala_3","text":"handleError( error: Throwable, version: Long): Unit NOTE: handleError is part of the < > to handle errors while < > handleError ...FIXME","title":"[source, scala]"},{"location":"HDFSLogStore/","text":"= HDFSLogStore HDFSLogStore is...FIXME","title":"HDFSLogStore"},{"location":"HadoopFileSystemLogStore/","text":"= HadoopFileSystemLogStore HadoopFileSystemLogStore is...FIXME","title":"HadoopFileSystemLogStore"},{"location":"ImplicitMetadataOperation/","text":"= [[ImplicitMetadataOperation]] ImplicitMetadataOperation -- Operations Updating Metadata (Schema And Partitioning) ImplicitMetadataOperation is an < > of < > that can < > of a delta table (while writing out a new data to a delta table). ImplicitMetadataOperation operations can update schema by < > and < > schema. [[contract]] .ImplicitMetadataOperation Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | canMergeSchema a| [[canMergeSchema]] [source, scala] \u00b6 canMergeSchema: Boolean \u00b6 Used when ImplicitMetadataOperation is requested to < > | canOverwriteSchema a| [[canOverwriteSchema]] [source, scala] \u00b6 canOverwriteSchema: Boolean \u00b6 Used when ImplicitMetadataOperation is requested to < > |=== [[implementations]] .ImplicitMetadataOperations [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | ImplicitMetadataOperation | Description | < > | [[WriteIntoDelta]] Delta command for batch queries (Spark SQL) | < > | [[DeltaSink]] Streaming sink for streaming queries (Spark Structured Streaming) |=== Updating Metadata \u00b6 updateMetadata ( txn : OptimisticTransaction , data : Dataset [ _ ], partitionColumns : Seq [ String ], configuration : Map [ String , String ], isOverwriteMode : Boolean ) : Unit updateMetadata ...FIXME updateMetadata is used when: WriteIntoDelta command is executed DeltaSink is requested to add a streaming micro-batch == [[normalizePartitionColumns]] Normalize Partition Columns -- normalizePartitionColumns Internal Method [source, scala] \u00b6 normalizePartitionColumns( spark: SparkSession, partitionCols: Seq[String], schema: StructType): Seq[String] normalizePartitionColumns ...FIXME NOTE: normalizePartitionColumns is used when ImplicitMetadataOperation is requested to < >.","title":"ImplicitMetadataOperation"},{"location":"ImplicitMetadataOperation/#source-scala","text":"","title":"[source, scala]"},{"location":"ImplicitMetadataOperation/#canmergeschema-boolean","text":"Used when ImplicitMetadataOperation is requested to < > | canOverwriteSchema a| [[canOverwriteSchema]]","title":"canMergeSchema: Boolean"},{"location":"ImplicitMetadataOperation/#source-scala_1","text":"","title":"[source, scala]"},{"location":"ImplicitMetadataOperation/#canoverwriteschema-boolean","text":"Used when ImplicitMetadataOperation is requested to < > |=== [[implementations]] .ImplicitMetadataOperations [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | ImplicitMetadataOperation | Description | < > | [[WriteIntoDelta]] Delta command for batch queries (Spark SQL) | < > | [[DeltaSink]] Streaming sink for streaming queries (Spark Structured Streaming) |===","title":"canOverwriteSchema: Boolean"},{"location":"ImplicitMetadataOperation/#updating-metadata","text":"updateMetadata ( txn : OptimisticTransaction , data : Dataset [ _ ], partitionColumns : Seq [ String ], configuration : Map [ String , String ], isOverwriteMode : Boolean ) : Unit updateMetadata ...FIXME updateMetadata is used when: WriteIntoDelta command is executed DeltaSink is requested to add a streaming micro-batch == [[normalizePartitionColumns]] Normalize Partition Columns -- normalizePartitionColumns Internal Method","title":" Updating Metadata"},{"location":"ImplicitMetadataOperation/#source-scala_2","text":"normalizePartitionColumns( spark: SparkSession, partitionCols: Seq[String], schema: StructType): Seq[String] normalizePartitionColumns ...FIXME NOTE: normalizePartitionColumns is used when ImplicitMetadataOperation is requested to < >.","title":"[source, scala]"},{"location":"InMemoryLogReplay/","text":"InMemoryLogReplay \u2014 Delta Log Replay \u00b6 InMemoryLogReplay is used at the very last phase of < > (of a < >). InMemoryLogReplay is < > for every partition of the < > dataset ( Dataset[SingleAction] ) that is based on the < > configuration property (default: 50 ). The lifecycle of InMemoryLogReplay is as follows: . < > (with < >) . < > (with all < > of a partition) . < > == [[creating-instance]] Creating InMemoryLogReplay Instance InMemoryLogReplay takes the following to be created: [[minFileRetentionTimestamp]] minFileRetentionTimestamp (that is exactly < >) InMemoryLogReplay initializes the < >. == [[append]] Appending Actions -- append Method [source, scala] \u00b6 append( version: Long, actions: Iterator[Action]): Unit append sets the < > as the given version . append adds < > to respective registries: Every < > is registered in the < > by < > < > is registered as the < > < > is registered as the < > Every < > is registered as follows: ** Added to < > by pathAsUri (with dataChange flag turned off) ** Removed from < > by pathAsUri Every < > is registered as follows: ** Removed from < > by pathAsUri ** Added to < > by pathAsUri (with dataChange flag turned off) < > are ignored append throws an AssertionError when the < > is neither -1 (the default) nor one before the given version : Attempted to replay version [version], but state is at [currentVersion] NOTE: append is used when Snapshot is created (and initializes the < > for the < >). == [[checkpoint]] Current State Of Delta Table -- checkpoint Method [source, scala] \u00b6 checkpoint: Iterator[Action] \u00b6 checkpoint simply builds a sequence ( Iterator[Action] ) of the following (in that order): < > if defined (non- null ) < > if defined (non- null ) < > < > and < > (after the < >) sorted by < > (lexicographically) NOTE: checkpoint is used when Snapshot is created (and initializes the < > for the < >). == [[getTombstones]] getTombstones Internal Method [source, scala] \u00b6 getTombstones: Iterable[FileAction] \u00b6 getTombstones returns < > (from the < >) with their delTimestamp after the < >. NOTE: getTombstones is used when InMemoryLogReplay is requested to < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | currentProtocolVersion a| [[currentProtocolVersion]] < > (default: null ) Used when...FIXME | currentVersion a| [[currentVersion]] Version (default: -1 ) Used when...FIXME | currentMetaData a| [[currentMetaData]] < > (default: null ) Used when...FIXME | transactions a| [[transactions]] < > per ID ( HashMap[String, SetTransaction] ) Used when...FIXME | activeFiles a| [[activeFiles]] < > per URI ( HashMap[URI, AddFile] ) Used when...FIXME | tombstones a| [[tombstones]] < > per URI ( HashMap[URI, RemoveFile] ) Used when...FIXME |===","title":"InMemoryLogReplay"},{"location":"InMemoryLogReplay/#inmemorylogreplay-delta-log-replay","text":"InMemoryLogReplay is used at the very last phase of < > (of a < >). InMemoryLogReplay is < > for every partition of the < > dataset ( Dataset[SingleAction] ) that is based on the < > configuration property (default: 50 ). The lifecycle of InMemoryLogReplay is as follows: . < > (with < >) . < > (with all < > of a partition) . < > == [[creating-instance]] Creating InMemoryLogReplay Instance InMemoryLogReplay takes the following to be created: [[minFileRetentionTimestamp]] minFileRetentionTimestamp (that is exactly < >) InMemoryLogReplay initializes the < >. == [[append]] Appending Actions -- append Method","title":"InMemoryLogReplay &mdash; Delta Log Replay"},{"location":"InMemoryLogReplay/#source-scala","text":"append( version: Long, actions: Iterator[Action]): Unit append sets the < > as the given version . append adds < > to respective registries: Every < > is registered in the < > by < > < > is registered as the < > < > is registered as the < > Every < > is registered as follows: ** Added to < > by pathAsUri (with dataChange flag turned off) ** Removed from < > by pathAsUri Every < > is registered as follows: ** Removed from < > by pathAsUri ** Added to < > by pathAsUri (with dataChange flag turned off) < > are ignored append throws an AssertionError when the < > is neither -1 (the default) nor one before the given version : Attempted to replay version [version], but state is at [currentVersion] NOTE: append is used when Snapshot is created (and initializes the < > for the < >). == [[checkpoint]] Current State Of Delta Table -- checkpoint Method","title":"[source, scala]"},{"location":"InMemoryLogReplay/#source-scala_1","text":"","title":"[source, scala]"},{"location":"InMemoryLogReplay/#checkpoint-iteratoraction","text":"checkpoint simply builds a sequence ( Iterator[Action] ) of the following (in that order): < > if defined (non- null ) < > if defined (non- null ) < > < > and < > (after the < >) sorted by < > (lexicographically) NOTE: checkpoint is used when Snapshot is created (and initializes the < > for the < >). == [[getTombstones]] getTombstones Internal Method","title":"checkpoint: Iterator[Action]"},{"location":"InMemoryLogReplay/#source-scala_2","text":"","title":"[source, scala]"},{"location":"InMemoryLogReplay/#gettombstones-iterablefileaction","text":"getTombstones returns < > (from the < >) with their delTimestamp after the < >. NOTE: getTombstones is used when InMemoryLogReplay is requested to < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | currentProtocolVersion a| [[currentProtocolVersion]] < > (default: null ) Used when...FIXME | currentVersion a| [[currentVersion]] Version (default: -1 ) Used when...FIXME | currentMetaData a| [[currentMetaData]] < > (default: null ) Used when...FIXME | transactions a| [[transactions]] < > per ID ( HashMap[String, SetTransaction] ) Used when...FIXME | activeFiles a| [[activeFiles]] < > per URI ( HashMap[URI, AddFile] ) Used when...FIXME | tombstones a| [[tombstones]] < > per URI ( HashMap[URI, RemoveFile] ) Used when...FIXME |===","title":"getTombstones: Iterable[FileAction]"},{"location":"Invariants/","text":"= [[Invariants]] Invariants Invariants is...FIXME","title":"Invariants"},{"location":"LogStore/","text":"= LogStore LogStore is an < > of < > that can < > and < > actions to a directory (among other things). [[contract]] .LogStore Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | invalidateCache a| [[invalidateCache]] [source, scala] \u00b6 invalidateCache(): Unit \u00b6 Used when...FIXME | isPartialWriteVisible a| [[isPartialWriteVisible]] [source, scala] \u00b6 isPartialWriteVisible(path: Path): Boolean = true \u00b6 Used when...FIXME | listFrom a| [[listFrom]] [source, scala] \u00b6 listFrom( path: Path): Iterator[FileStatus] listFrom( path: String): Iterator[FileStatus] Used when...FIXME | read a| [[read]] [source, scala] \u00b6 read(path: String): Seq[String] read(path: Path): Seq[String] Used when: Checkpoints is requested to < > DeltaHistoryManager utility is requested to < > DeltaLog is requested to < > VerifyChecksum is requested to < > OptimisticTransactionImpl is requested to < > | write a| [[write]] [source, scala] \u00b6 write( path: Path, actions: Iterator[String], overwrite: Boolean = false): Unit write( path: String, actions: Iterator[String]): Unit Writes the actions out to the given path (with or without overwrite as indicated). Used when: Checkpoints is requested to < > ConvertToDeltaCommand is < > (and does < >) OptimisticTransactionImpl is requested to < > |=== [[implementations]] .LogStores (Direct Implementations and Extensions Only) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | LogStore | Description | < > | [[HDFSLogStore]] | < > | [[HadoopFileSystemLogStore]] |=== == [[resolvePathOnPhysicalStorage]] resolvePathOnPhysicalStorage Method [source, scala] \u00b6 resolvePathOnPhysicalStorage(path: Path): Path \u00b6 resolvePathOnPhysicalStorage ...FIXME NOTE: resolvePathOnPhysicalStorage is used when...FIXME == [[apply]] Creating LogStore -- apply Utility [source, scala] \u00b6 apply( sc: SparkContext): LogStore apply( sparkConf: SparkConf, hadoopConf: Configuration): LogStore apply ...FIXME [NOTE] \u00b6 apply is used when: DeltaHistoryManager is requested to < > and < > * DeltaFileOperations utility is used to < > \u00b6","title":"LogStore"},{"location":"LogStore/#source-scala","text":"","title":"[source, scala]"},{"location":"LogStore/#invalidatecache-unit","text":"Used when...FIXME | isPartialWriteVisible a| [[isPartialWriteVisible]]","title":"invalidateCache(): Unit"},{"location":"LogStore/#source-scala_1","text":"","title":"[source, scala]"},{"location":"LogStore/#ispartialwritevisiblepath-path-boolean-true","text":"Used when...FIXME | listFrom a| [[listFrom]]","title":"isPartialWriteVisible(path: Path): Boolean = true"},{"location":"LogStore/#source-scala_2","text":"listFrom( path: Path): Iterator[FileStatus] listFrom( path: String): Iterator[FileStatus] Used when...FIXME | read a| [[read]]","title":"[source, scala]"},{"location":"LogStore/#source-scala_3","text":"read(path: String): Seq[String] read(path: Path): Seq[String] Used when: Checkpoints is requested to < > DeltaHistoryManager utility is requested to < > DeltaLog is requested to < > VerifyChecksum is requested to < > OptimisticTransactionImpl is requested to < > | write a| [[write]]","title":"[source, scala]"},{"location":"LogStore/#source-scala_4","text":"write( path: Path, actions: Iterator[String], overwrite: Boolean = false): Unit write( path: String, actions: Iterator[String]): Unit Writes the actions out to the given path (with or without overwrite as indicated). Used when: Checkpoints is requested to < > ConvertToDeltaCommand is < > (and does < >) OptimisticTransactionImpl is requested to < > |=== [[implementations]] .LogStores (Direct Implementations and Extensions Only) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | LogStore | Description | < > | [[HDFSLogStore]] | < > | [[HadoopFileSystemLogStore]] |=== == [[resolvePathOnPhysicalStorage]] resolvePathOnPhysicalStorage Method","title":"[source, scala]"},{"location":"LogStore/#source-scala_5","text":"","title":"[source, scala]"},{"location":"LogStore/#resolvepathonphysicalstoragepath-path-path","text":"resolvePathOnPhysicalStorage ...FIXME NOTE: resolvePathOnPhysicalStorage is used when...FIXME == [[apply]] Creating LogStore -- apply Utility","title":"resolvePathOnPhysicalStorage(path: Path): Path"},{"location":"LogStore/#source-scala_6","text":"apply( sc: SparkContext): LogStore apply( sparkConf: SparkConf, hadoopConf: Configuration): LogStore apply ...FIXME","title":"[source, scala]"},{"location":"LogStore/#note","text":"apply is used when: DeltaHistoryManager is requested to < > and < >","title":"[NOTE]"},{"location":"LogStore/#deltafileoperations-utility-is-used-to","text":"","title":"* DeltaFileOperations utility is used to &lt;&gt;"},{"location":"LogStoreProvider/","text":"LogStoreProvider \u00b6 LogStoreProvider is an abstraction of < > of < >. [[logStoreClassConfKey]][[defaultLogStoreClass]][[spark.delta.logStore.class]] LogStoreProvider uses the spark.delta.logStore.class configuration property (default: < >) for the fully-qualified class name of the < > to < > (for a < >, a < >, and < >). == [[createLogStore]] Creating LogStore -- createLogStore Method [source, scala] \u00b6 createLogStore( spark: SparkSession): LogStore createLogStore( sparkConf: SparkConf, hadoopConf: Configuration): LogStore createLogStore ...FIXME [NOTE] \u00b6 createLogStore is used when: < > is created * < > utility is used (to create a < >) \u00b6","title":"LogStoreProvider"},{"location":"LogStoreProvider/#logstoreprovider","text":"LogStoreProvider is an abstraction of < > of < >. [[logStoreClassConfKey]][[defaultLogStoreClass]][[spark.delta.logStore.class]] LogStoreProvider uses the spark.delta.logStore.class configuration property (default: < >) for the fully-qualified class name of the < > to < > (for a < >, a < >, and < >). == [[createLogStore]] Creating LogStore -- createLogStore Method","title":"LogStoreProvider"},{"location":"LogStoreProvider/#source-scala","text":"createLogStore( spark: SparkSession): LogStore createLogStore( sparkConf: SparkConf, hadoopConf: Configuration): LogStore createLogStore ...FIXME","title":"[source, scala]"},{"location":"LogStoreProvider/#note","text":"createLogStore is used when: < > is created","title":"[NOTE]"},{"location":"LogStoreProvider/#utility-is-used-to-create-a","text":"","title":"* &lt;&gt; utility is used (to create a &lt;&gt;)"},{"location":"Metadata/","text":"Metadata \u00b6 Metadata is an < > that describes metadata (change) of a < > (indirectly via < >). import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, \"/tmp/delta/users\") scala> :type deltaLog.snapshot.metadata org.apache.spark.sql.delta.actions.Metadata Metadata contains all the non-data information ( metadata ) like < >, < >, < >, < >, < >, < > and < >. These can be changed (e.g., schema evolution). TIP: Use < > to review the metadata of a delta table. Metadata uses < > to uniquely identify a delta table. The ID is never going to change through the history of the table (unless the entire directory, along with the transaction log is deleted). It is known as tableId or < >. [NOTE] \u00b6 When I asked the question https://groups.google.com/forum/#!topic/delta-users/5OKEFvVKiew[tableId and reservoirId - Why two different names for metadata ID?] on delta-users mailing list, Tathagata Das wrote: Any reference to \"reservoir\" is just legacy code. In the early days of this project, the project was called \"Tahoe\" and each table is called a \"reservoir\" (Tahoe is one of the 2 nd deepest lake in US, and is a very large reservoir of water ;) ). So you may still find those two terms all around the codebase. In some cases, like DeltaSourceOffset, the term reservoirId is in the json that is written to the streaming checkpoint directory. So we cannot change that for backward compatibility. ==== Metadata can be < > in a OptimisticTransactionImpl.md[transaction] once (and only when created for an uninitialized table, when < > is -1 ). [source,scala] \u00b6 txn.metadata \u00b6 Metadata is < > when: DeltaLog is requested for the < > OptimisticTransactionImpl is requested for the < > ConvertToDeltaCommand is requested to < > ImplicitMetadataOperation is requested to < > == [[creating-instance]] Creating Metadata Instance Metadata takes the following to be created: [[id]] Table ID (default: a random UUID) [[name]] Name of the delta table (default: null ) [[description]] Description (default: null ) [[format]] Format [[schemaString]] Schema (default: null ) [[partitionColumns]] Partition columns (default: Nil ) [[configuration]] Configuration (default: empty ) [[createdTime]] Created time (in millis since the epoch) == [[wrap]] wrap Method [source, scala] \u00b6 wrap: SingleAction \u00b6 NOTE: wrap is part of the < > contract to wrap the action into a < >. wrap simply creates a new < > with the Metadata field set to this Metadata. == [[partitionSchema]] partitionSchema (Lazy) Property [source, scala] \u00b6 partitionSchema: StructType \u00b6 partitionSchema is the < > as StructFields (and defined in the < >). NOTE: partitionSchema throws an IllegalArgumentException for undefined fields that were used for the < > but not defined in the < >. NOTE: partitionSchema is used when...FIXME == [[dataSchema]] dataSchema (Lazy) Property [source, scala] \u00b6 dataSchema: StructType \u00b6 dataSchema ...FIXME NOTE: dataSchema is used when...FIXME == [[schema]] schema (Lazy) Property [source, scala] \u00b6 schema: StructType \u00b6 schema is a deserialized < > (from JSON format) to StructType . [NOTE] \u00b6 schema is used when: Metadata is requested for the schema of the < > and the < > DeltaLog is requested for an DeltaLog.md#createRelation[insertable HadoopFsRelation for batch queries] (for the data schema), to DeltaLog.md#upgradeProtocol[upgrade protocol], a DeltaLog.md#createDataFrame[DataFrame for given AddFiles] DeltaTableUtils utility is used to DeltaTableUtils.md#combineWithCatalogMetadata[combineWithCatalogMetadata] OptimisticTransactionImpl is requested to OptimisticTransactionImpl.md#verifyNewMetadata[verifyNewMetadata] * ...FIXME (there are other uses) \u00b6","title":"Metadata"},{"location":"Metadata/#metadata","text":"Metadata is an < > that describes metadata (change) of a < > (indirectly via < >). import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, \"/tmp/delta/users\") scala> :type deltaLog.snapshot.metadata org.apache.spark.sql.delta.actions.Metadata Metadata contains all the non-data information ( metadata ) like < >, < >, < >, < >, < >, < > and < >. These can be changed (e.g., schema evolution). TIP: Use < > to review the metadata of a delta table. Metadata uses < > to uniquely identify a delta table. The ID is never going to change through the history of the table (unless the entire directory, along with the transaction log is deleted). It is known as tableId or < >.","title":"Metadata"},{"location":"Metadata/#note","text":"When I asked the question https://groups.google.com/forum/#!topic/delta-users/5OKEFvVKiew[tableId and reservoirId - Why two different names for metadata ID?] on delta-users mailing list, Tathagata Das wrote: Any reference to \"reservoir\" is just legacy code. In the early days of this project, the project was called \"Tahoe\" and each table is called a \"reservoir\" (Tahoe is one of the 2 nd deepest lake in US, and is a very large reservoir of water ;) ). So you may still find those two terms all around the codebase. In some cases, like DeltaSourceOffset, the term reservoirId is in the json that is written to the streaming checkpoint directory. So we cannot change that for backward compatibility. ==== Metadata can be < > in a OptimisticTransactionImpl.md[transaction] once (and only when created for an uninitialized table, when < > is -1 ).","title":"[NOTE]"},{"location":"Metadata/#sourcescala","text":"","title":"[source,scala]"},{"location":"Metadata/#txnmetadata","text":"Metadata is < > when: DeltaLog is requested for the < > OptimisticTransactionImpl is requested for the < > ConvertToDeltaCommand is requested to < > ImplicitMetadataOperation is requested to < > == [[creating-instance]] Creating Metadata Instance Metadata takes the following to be created: [[id]] Table ID (default: a random UUID) [[name]] Name of the delta table (default: null ) [[description]] Description (default: null ) [[format]] Format [[schemaString]] Schema (default: null ) [[partitionColumns]] Partition columns (default: Nil ) [[configuration]] Configuration (default: empty ) [[createdTime]] Created time (in millis since the epoch) == [[wrap]] wrap Method","title":"txn.metadata"},{"location":"Metadata/#source-scala","text":"","title":"[source, scala]"},{"location":"Metadata/#wrap-singleaction","text":"NOTE: wrap is part of the < > contract to wrap the action into a < >. wrap simply creates a new < > with the Metadata field set to this Metadata. == [[partitionSchema]] partitionSchema (Lazy) Property","title":"wrap: SingleAction"},{"location":"Metadata/#source-scala_1","text":"","title":"[source, scala]"},{"location":"Metadata/#partitionschema-structtype","text":"partitionSchema is the < > as StructFields (and defined in the < >). NOTE: partitionSchema throws an IllegalArgumentException for undefined fields that were used for the < > but not defined in the < >. NOTE: partitionSchema is used when...FIXME == [[dataSchema]] dataSchema (Lazy) Property","title":"partitionSchema: StructType"},{"location":"Metadata/#source-scala_2","text":"","title":"[source, scala]"},{"location":"Metadata/#dataschema-structtype","text":"dataSchema ...FIXME NOTE: dataSchema is used when...FIXME == [[schema]] schema (Lazy) Property","title":"dataSchema: StructType"},{"location":"Metadata/#source-scala_3","text":"","title":"[source, scala]"},{"location":"Metadata/#schema-structtype","text":"schema is a deserialized < > (from JSON format) to StructType .","title":"schema: StructType"},{"location":"Metadata/#note_1","text":"schema is used when: Metadata is requested for the schema of the < > and the < > DeltaLog is requested for an DeltaLog.md#createRelation[insertable HadoopFsRelation for batch queries] (for the data schema), to DeltaLog.md#upgradeProtocol[upgrade protocol], a DeltaLog.md#createDataFrame[DataFrame for given AddFiles] DeltaTableUtils utility is used to DeltaTableUtils.md#combineWithCatalogMetadata[combineWithCatalogMetadata] OptimisticTransactionImpl is requested to OptimisticTransactionImpl.md#verifyNewMetadata[verifyNewMetadata]","title":"[NOTE]"},{"location":"Metadata/#fixme-there-are-other-uses","text":"","title":"* ...FIXME (there are other uses)"},{"location":"MetadataCleanup/","text":"MetadataCleanup \u00b6 MetadataCleanup is an abstraction of < > that can < > the < >. [[implementations]][[self]] NOTE: < > is the default and only known MetadataCleanup in Delta Lake. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.delta.MetadataCleanup logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.MetadataCleanup=ALL Refer to Logging .. \u00b6 == [[doLogCleanup]] doLogCleanup Method [source, scala] \u00b6 doLogCleanup(): Unit \u00b6 [NOTE] \u00b6 doLogCleanup is part of the < > to...FIXME. Interestingly, this MetadataCleanup and < > abstractions require to be used with < > only. \u00b6 doLogCleanup < > when the < > table property is enabled. == [[enableExpiredLogCleanup]] enableExpiredLogCleanup Table Property -- enableExpiredLogCleanup Method [source, scala] \u00b6 enableExpiredLogCleanup: Boolean \u00b6 enableExpiredLogCleanup gives the value of < > table property (< > the < >). NOTE: enableExpiredLogCleanup is used exclusively when MetadataCleanup is requested to < >. == [[deltaRetentionMillis]] logRetentionDuration Table Property -- deltaRetentionMillis Method [source, scala] \u00b6 deltaRetentionMillis: Long \u00b6 deltaRetentionMillis gives the value of < > table property (< > the < >). NOTE: deltaRetentionMillis is used when...FIXME == [[cleanUpExpiredLogs]] cleanUpExpiredLogs Internal Method [source, scala] \u00b6 cleanUpExpiredLogs(): Unit \u00b6 cleanUpExpiredLogs calculates a so-called fileCutOffTime based on the < > and the < > table property. cleanUpExpiredLogs prints out the following INFO message to the logs: Starting the deletion of log files older than [date] cleanUpExpiredLogs < > (based on the fileCutOffTime ) and deletes the files (using Hadoop's FileSystem.delete non-recursively). In the end, cleanUpExpiredLogs prints out the following INFO message to the logs: Deleted numDeleted log files older than [date] NOTE: cleanUpExpiredLogs is used exclusively when MetadataCleanup is requested to < >. == [[listExpiredDeltaLogs]] Finding Expired Delta Logs -- listExpiredDeltaLogs Internal Method [source, scala] \u00b6 listExpiredDeltaLogs( fileCutOffTime: Long): Iterator[FileStatus] listExpiredDeltaLogs ...FIXME requests the < > for the < > that are (lexicographically) greater or equal to the 0 th checkpoint file (per < > format) of the < > and < > files in the < > (of the < >). In the end, listExpiredDeltaLogs creates a BufferingLogDeletionIterator that...FIXME NOTE: listExpiredDeltaLogs is used exclusively when MetadataCleanup is requested to < >.","title":"MetadataCleanup"},{"location":"MetadataCleanup/#metadatacleanup","text":"MetadataCleanup is an abstraction of < > that can < > the < >. [[implementations]][[self]] NOTE: < > is the default and only known MetadataCleanup in Delta Lake. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.delta.MetadataCleanup logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.MetadataCleanup=ALL","title":"MetadataCleanup"},{"location":"MetadataCleanup/#refer-to-logging","text":"== [[doLogCleanup]] doLogCleanup Method","title":"Refer to Logging.."},{"location":"MetadataCleanup/#source-scala","text":"","title":"[source, scala]"},{"location":"MetadataCleanup/#dologcleanup-unit","text":"","title":"doLogCleanup(): Unit"},{"location":"MetadataCleanup/#note","text":"doLogCleanup is part of the < > to...FIXME.","title":"[NOTE]"},{"location":"MetadataCleanup/#interestingly-this-metadatacleanup-and-abstractions-require-to-be-used-with-only","text":"doLogCleanup < > when the < > table property is enabled. == [[enableExpiredLogCleanup]] enableExpiredLogCleanup Table Property -- enableExpiredLogCleanup Method","title":"Interestingly, this MetadataCleanup and &lt;&gt; abstractions require to be used with &lt;&gt; only."},{"location":"MetadataCleanup/#source-scala_1","text":"","title":"[source, scala]"},{"location":"MetadataCleanup/#enableexpiredlogcleanup-boolean","text":"enableExpiredLogCleanup gives the value of < > table property (< > the < >). NOTE: enableExpiredLogCleanup is used exclusively when MetadataCleanup is requested to < >. == [[deltaRetentionMillis]] logRetentionDuration Table Property -- deltaRetentionMillis Method","title":"enableExpiredLogCleanup: Boolean"},{"location":"MetadataCleanup/#source-scala_2","text":"","title":"[source, scala]"},{"location":"MetadataCleanup/#deltaretentionmillis-long","text":"deltaRetentionMillis gives the value of < > table property (< > the < >). NOTE: deltaRetentionMillis is used when...FIXME == [[cleanUpExpiredLogs]] cleanUpExpiredLogs Internal Method","title":"deltaRetentionMillis: Long"},{"location":"MetadataCleanup/#source-scala_3","text":"","title":"[source, scala]"},{"location":"MetadataCleanup/#cleanupexpiredlogs-unit","text":"cleanUpExpiredLogs calculates a so-called fileCutOffTime based on the < > and the < > table property. cleanUpExpiredLogs prints out the following INFO message to the logs: Starting the deletion of log files older than [date] cleanUpExpiredLogs < > (based on the fileCutOffTime ) and deletes the files (using Hadoop's FileSystem.delete non-recursively). In the end, cleanUpExpiredLogs prints out the following INFO message to the logs: Deleted numDeleted log files older than [date] NOTE: cleanUpExpiredLogs is used exclusively when MetadataCleanup is requested to < >. == [[listExpiredDeltaLogs]] Finding Expired Delta Logs -- listExpiredDeltaLogs Internal Method","title":"cleanUpExpiredLogs(): Unit"},{"location":"MetadataCleanup/#source-scala_4","text":"listExpiredDeltaLogs( fileCutOffTime: Long): Iterator[FileStatus] listExpiredDeltaLogs ...FIXME requests the < > for the < > that are (lexicographically) greater or equal to the 0 th checkpoint file (per < > format) of the < > and < > files in the < > (of the < >). In the end, listExpiredDeltaLogs creates a BufferingLogDeletionIterator that...FIXME NOTE: listExpiredDeltaLogs is used exclusively when MetadataCleanup is requested to < >.","title":"[source, scala]"},{"location":"Operation/","text":"Operation \u00b6 Operation is an abstraction of operations that can be executed on Delta tables. Operation is described by a name and parameters (that are simply used to create a CommitInfo for OptimisticTransactionImpl when committed and, as a way to bypass a transaction, ConvertToDeltaCommand ). Operation may have performance metrics . Contract \u00b6 parameters \u00b6 parameters : Map [ String , Any ] Parameters of the operation (to create a CommitInfo with the JSON-encoded values ) Used when Operation is requested for parameters with the values in JSON format Implementations \u00b6 Sealed Abstract Class Operation is a Scala sealed abstract class which means that all of the implementations are in the same compilation unit (a single file). AddColumns ChangeColumn ComputeStats Convert CreateTable Delete FileNotificationRetention Fsck ManualUpdate Optimize ReplaceColumns ReplaceTable ResetZCubeInfo SetTableProperties StreamingUpdate Truncate UnsetTableProperties Update UpdateColumnMetadata UpdateSchema UpgradeProtocol Write Merge \u00b6 Recorded when a merge operation is committed to a Delta table (when MergeIntoCommand is executed) Creating Instance \u00b6 Operation takes the following to be created: Name Abstract Class Operation is an abstract class and cannot be created directly. It is created indirectly for the concrete Operations . Serializing Parameter Values (to JSON Format) \u00b6 jsonEncodedValues : Map [ String , String ] jsonEncodedValues converts the values of the parameters to JSON format. jsonEncodedValues is used when: OptimisticTransactionImpl is requested to commit ConvertToDeltaCommand command is requested to streamWrite operationMetrics Registry \u00b6 operationMetrics : Set [ String ] operationMetrics is empty by default (and is expected to be overriden by concrete operations ) operationMetrics is used when Operation is requested to transformMetrics . transformMetrics Method \u00b6 transformMetrics ( metrics : Map [ String , SQLMetric ]) : Map [ String , String ] transformMetrics returns a collection of performance metrics ( SQLMetric ) and their values (as a text) that are defined as the operationMetrics . transformMetrics is used when SQLMetricsReporting is requested to getMetricsForOperation .","title":"Operation"},{"location":"Operation/#operation","text":"Operation is an abstraction of operations that can be executed on Delta tables. Operation is described by a name and parameters (that are simply used to create a CommitInfo for OptimisticTransactionImpl when committed and, as a way to bypass a transaction, ConvertToDeltaCommand ). Operation may have performance metrics .","title":"Operation"},{"location":"Operation/#contract","text":"","title":"Contract"},{"location":"Operation/#parameters","text":"parameters : Map [ String , Any ] Parameters of the operation (to create a CommitInfo with the JSON-encoded values ) Used when Operation is requested for parameters with the values in JSON format","title":" parameters"},{"location":"Operation/#implementations","text":"Sealed Abstract Class Operation is a Scala sealed abstract class which means that all of the implementations are in the same compilation unit (a single file). AddColumns ChangeColumn ComputeStats Convert CreateTable Delete FileNotificationRetention Fsck ManualUpdate Optimize ReplaceColumns ReplaceTable ResetZCubeInfo SetTableProperties StreamingUpdate Truncate UnsetTableProperties Update UpdateColumnMetadata UpdateSchema UpgradeProtocol Write","title":"Implementations"},{"location":"Operation/#merge","text":"Recorded when a merge operation is committed to a Delta table (when MergeIntoCommand is executed)","title":"Merge"},{"location":"Operation/#creating-instance","text":"Operation takes the following to be created: Name Abstract Class Operation is an abstract class and cannot be created directly. It is created indirectly for the concrete Operations .","title":"Creating Instance"},{"location":"Operation/#serializing-parameter-values-to-json-format","text":"jsonEncodedValues : Map [ String , String ] jsonEncodedValues converts the values of the parameters to JSON format. jsonEncodedValues is used when: OptimisticTransactionImpl is requested to commit ConvertToDeltaCommand command is requested to streamWrite","title":" Serializing Parameter Values (to JSON Format)"},{"location":"Operation/#operationmetrics-registry","text":"operationMetrics : Set [ String ] operationMetrics is empty by default (and is expected to be overriden by concrete operations ) operationMetrics is used when Operation is requested to transformMetrics .","title":" operationMetrics Registry"},{"location":"Operation/#transformmetrics-method","text":"transformMetrics ( metrics : Map [ String , SQLMetric ]) : Map [ String , String ] transformMetrics returns a collection of performance metrics ( SQLMetric ) and their values (as a text) that are defined as the operationMetrics . transformMetrics is used when SQLMetricsReporting is requested to getMetricsForOperation .","title":" transformMetrics Method"},{"location":"OptimisticTransaction/","text":"OptimisticTransaction \u00b6 OptimisticTransaction is an OptimisticTransactionImpl (which seems more of a class name change than anything more important). When OptimisticTransaction (as a < >) is attempted to be < > (that does < > internally), the < > (of the < >) is requested to < >, e.g. _delta_log/00000000000000000001.json for the attempt version 1 . Only when a FileAlreadyExistsException is thrown a commit is considered unsuccessful and < >. OptimisticTransaction can be associated with a thread as an < >. Creating Instance \u00b6 OptimisticTransaction takes the following to be created: [[deltaLog]] DeltaLog.md[] [[snapshot]] Snapshot.md[] [[clock]] Clock NOTE: The < > and < > are part of the < > contract (which in turn inherits them as a TransactionalWrite.md[] and changes to val from def ). OptimisticTransaction is created for changes to a < > at a given < >. OptimisticTransaction is created when DeltaLog is used for the following: DeltaLog.md#startTransaction[Starting a new transaction] DeltaLog.md#withNewTransaction[Executing a single-threaded operation (in a new transaction)] (for < >, < >, < >, and < > commands as well as for < > for < >) == [[active]] Active Thread-Local OptimisticTransaction [source, scala] \u00b6 active: ThreadLocal[OptimisticTransaction] \u00b6 active is a Java https://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html[ThreadLocal ] with the < > of the current thread. ThreadLocal provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its get or set method) has its own, independently initialized copy of the variable. ThreadLocal instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID). active is assigned to the current thread using < > utility and cleared in < >. active is available using < > utility. There can only be one active OptimisticTransaction (or an IllegalStateException is thrown). == [[utilities]] Utilities === [[setActive]] setActive [source, scala] \u00b6 setActive( txn: OptimisticTransaction): Unit setActive simply associates the given OptimisticTransaction as < > with the current thread. setActive throws an IllegalStateException if there is an active OptimisticTransaction already associated: Cannot set a new txn as active when one is already active setActive is used when DeltaLog is requested to < >. === [[clearActive]] clearActive [source, scala] \u00b6 clearActive(): Unit \u00b6 clearActive simply clears the < > transaction (so no transaction is associated with a thread). clearActive is used when DeltaLog is requested to < >. === [[getActive]] getActive [source, scala] \u00b6 getActive(): Option[OptimisticTransaction] \u00b6 getActive simply returns the < > transaction. getActive seems unused. == [[logging]] Logging Enable ALL logging level for org.apache.spark.sql.delta.OptimisticTransaction logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.sql.delta.OptimisticTransaction=ALL \u00b6 Refer to Logging . == [[demo]] Demo [source,scala] \u00b6 import org.apache.spark.sql.delta.DeltaLog val dir = \"/tmp/delta/users\" val log = DeltaLog.forTable(spark, dir) val txn = log.startTransaction() // ...changes to a delta table... val addFile = AddFile(\"foo\", Map.empty, 1L, System.currentTimeMillis(), dataChange = true) val removeFile = addFile.remove val actions = addFile :: removeFile :: Nil txn.commit(actions, op) // You could do the following instead deltaLog.withNewTransaction { txn => // ...transactional changes to a delta table }","title":"OptimisticTransaction"},{"location":"OptimisticTransaction/#optimistictransaction","text":"OptimisticTransaction is an OptimisticTransactionImpl (which seems more of a class name change than anything more important). When OptimisticTransaction (as a < >) is attempted to be < > (that does < > internally), the < > (of the < >) is requested to < >, e.g. _delta_log/00000000000000000001.json for the attempt version 1 . Only when a FileAlreadyExistsException is thrown a commit is considered unsuccessful and < >. OptimisticTransaction can be associated with a thread as an < >.","title":"OptimisticTransaction"},{"location":"OptimisticTransaction/#creating-instance","text":"OptimisticTransaction takes the following to be created: [[deltaLog]] DeltaLog.md[] [[snapshot]] Snapshot.md[] [[clock]] Clock NOTE: The < > and < > are part of the < > contract (which in turn inherits them as a TransactionalWrite.md[] and changes to val from def ). OptimisticTransaction is created for changes to a < > at a given < >. OptimisticTransaction is created when DeltaLog is used for the following: DeltaLog.md#startTransaction[Starting a new transaction] DeltaLog.md#withNewTransaction[Executing a single-threaded operation (in a new transaction)] (for < >, < >, < >, and < > commands as well as for < > for < >) == [[active]] Active Thread-Local OptimisticTransaction","title":"Creating Instance"},{"location":"OptimisticTransaction/#source-scala","text":"","title":"[source, scala]"},{"location":"OptimisticTransaction/#active-threadlocaloptimistictransaction","text":"active is a Java https://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html[ThreadLocal ] with the < > of the current thread. ThreadLocal provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its get or set method) has its own, independently initialized copy of the variable. ThreadLocal instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID). active is assigned to the current thread using < > utility and cleared in < >. active is available using < > utility. There can only be one active OptimisticTransaction (or an IllegalStateException is thrown). == [[utilities]] Utilities === [[setActive]] setActive","title":"active: ThreadLocal[OptimisticTransaction]"},{"location":"OptimisticTransaction/#source-scala_1","text":"setActive( txn: OptimisticTransaction): Unit setActive simply associates the given OptimisticTransaction as < > with the current thread. setActive throws an IllegalStateException if there is an active OptimisticTransaction already associated: Cannot set a new txn as active when one is already active setActive is used when DeltaLog is requested to < >. === [[clearActive]] clearActive","title":"[source, scala]"},{"location":"OptimisticTransaction/#source-scala_2","text":"","title":"[source, scala]"},{"location":"OptimisticTransaction/#clearactive-unit","text":"clearActive simply clears the < > transaction (so no transaction is associated with a thread). clearActive is used when DeltaLog is requested to < >. === [[getActive]] getActive","title":"clearActive(): Unit"},{"location":"OptimisticTransaction/#source-scala_3","text":"","title":"[source, scala]"},{"location":"OptimisticTransaction/#getactive-optionoptimistictransaction","text":"getActive simply returns the < > transaction. getActive seems unused. == [[logging]] Logging Enable ALL logging level for org.apache.spark.sql.delta.OptimisticTransaction logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"getActive(): Option[OptimisticTransaction]"},{"location":"OptimisticTransaction/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"OptimisticTransaction/#log4jloggerorgapachesparksqldeltaoptimistictransactionall","text":"Refer to Logging . == [[demo]] Demo","title":"log4j.logger.org.apache.spark.sql.delta.OptimisticTransaction=ALL"},{"location":"OptimisticTransaction/#sourcescala","text":"import org.apache.spark.sql.delta.DeltaLog val dir = \"/tmp/delta/users\" val log = DeltaLog.forTable(spark, dir) val txn = log.startTransaction() // ...changes to a delta table... val addFile = AddFile(\"foo\", Map.empty, 1L, System.currentTimeMillis(), dataChange = true) val removeFile = addFile.remove val actions = addFile :: removeFile :: Nil txn.commit(actions, op) // You could do the following instead deltaLog.withNewTransaction { txn => // ...transactional changes to a delta table }","title":"[source,scala]"},{"location":"OptimisticTransactionImpl/","text":"OptimisticTransactionImpl \u00b6 OptimisticTransactionImpl is an < > of the TransactionalWrite.md[] abstraction for < > that can modify a < > (at a given < >) and can be < > eventually. In other words, OptimisticTransactionImpl is a set of Action.md[actions] as part of an Operation.md[]. == [[contract]] Contract === [[clock]] clock [source,scala] \u00b6 clock: Clock \u00b6 === [[deltaLog]] deltaLog [source,scala] \u00b6 deltaLog: DeltaLog \u00b6 DeltaLog.md[] (of the delta table) that this transaction is changing deltaLog is part of the TransactionalWrite.md#deltaLog[TransactionalWrite] contract and seems to change it to val (from def ). === [[snapshot]] snapshot [source,scala] \u00b6 snapshot: Snapshot \u00b6 Snapshot.md[] (of the < >) that this transaction is changing snapshot is part of the TransactionalWrite.md#deltaLog[TransactionalWrite] contract and seems to change it to val (from def ). == [[implementations]] Implementations OptimisticTransaction.md[] is the default and only known OptimisticTransactionImpl in Delta Lake. == [[metadata]] metadata Method [source, scala] \u00b6 metadata: Metadata \u00b6 metadata is either the < > (if defined) or the < >. metadata is part of the TransactionalWrite.md#metadata[TransactionalWrite] abstraction. == [[readVersion]] readVersion Method [source, scala] \u00b6 readVersion: Long \u00b6 readVersion simply requests the < > for the < >. readVersion is used when: OptimisticTransactionImpl is requested for < >, to < > and < > ConvertToDeltaCommand is requested to < > WriteIntoDelta is requested to < > ImplicitMetadataOperation is requested to < > == [[updateMetadata]] Updating Metadata [source, scala] \u00b6 updateMetadata( metadata: Metadata): Unit updateMetadata updates the < > internal property based on the < >: For -1 , updateMetadata updates the < > of the given metadata with a < > based on the SQLConf (of the active SparkSession ), the < > of the given metadata and a new < > For other versions, updateMetadata leaves the given < > unchanged [[updateMetadata-AssertionError-hasWritten]] updateMetadata throws an AssertionError when the < > flag is enabled ( true ): Cannot update the metadata in a transaction that has already written data. updateMetadata throws an AssertionError when the < > is not empty: Cannot change the metadata more than once in a transaction. updateMetadata is used when: < > is executed (and requested to < >) ImplicitMetadataOperation is requested to < > Files To Scan Matching Given Predicates \u00b6 filterFiles () : Seq [ AddFile ] // Uses `true` literal to mean that all files match filterFiles ( filters : Seq [ Expression ]) : Seq [ AddFile ] filterFiles gives the files to scan based on the given predicates (filter expressions). Internally, filterFiles requests the Snapshot for the filesForScan (for no projection attributes and the given filters). filterFiles finds the partition predicates among the given filters (and the partition columns of the Metadata ). filterFiles registers ( adds ) the partition predicates (in the readPredicates internal registry) and the files to scan (in the readFiles internal registry). filterFiles is used when: WriteIntoDelta is requested to write DeltaSink is requested to add a streaming micro-batch (with Complete output mode) DeleteCommand , MergeIntoCommand and UpdateCommand are executed CreateDeltaTableCommand is executed == [[readWholeTable]] readWholeTable Method [source, scala] \u00b6 readWholeTable(): Unit \u00b6 readWholeTable simply adds True literal to the < > internal registry. readWholeTable is used when DeltaSink is requested to DeltaSink.md#addBatch[add a streaming micro-batch] (and the batch reads the same Delta table as this sink is going to write to). Committing Transaction \u00b6 commit ( actions : Seq [ Action ], op : DeltaOperations.Operation ) : Long commit commits the transaction (with the Action s and a given Operation ) [[commit-prepareCommit]] commit firstly < > (that gives the final actions to commit that may be different from the given < >). [[commit-isolationLevelToUse]] commit determines the isolation level for this commit by checking whether any < > (in the given < >) has the < > flag on ( true ). With no data changed, commit uses SnapshotIsolation else Serializable . [[commit-isBlindAppend]] commit...FIXME [[commit-commitInfo]] commit...FIXME [[commit-registerPostCommitHook]] commit < > the < > post-commit hook when there is a < > among the actions and the < > table property (< > the < >) is enabled ( true ). NOTE: < > table property defaults to false . [[commit-commitVersion]] commit < > with the next version, the actions, attempt number 0 , and the select isolation level. commit prints out the following INFO message to the logs: Committed delta #[commitVersion] to [logPath] [[commit-postCommit]] commit < > (with the version committed and the actions). [[commit-runPostCommitHooks]] In the end, commit < > and returns the version of the successful commit. == [[prepareCommit]] Preparing Commit [source, scala] \u00b6 prepareCommit( actions: Seq[Action], op: DeltaOperations.Operation): Seq[Action] prepareCommit adds the < > action (if available) to the given < >. prepareCommit < > if there was one. prepareCommit...FIXME prepareCommit requests the < > to < >. prepareCommit...FIXME prepareCommit throws an AssertionError when the number of metadata changes in the transaction (by means of < > actions) is above 1 : Cannot change the metadata more than once in a transaction. prepareCommit throws an AssertionError when the < > internal flag is turned on ( true ): Transaction already committed. prepareCommit is used when OptimisticTransactionImpl is requested to < > (at the beginning). == [[postCommit]] Performing Post-Commit Operations [source, scala] \u00b6 postCommit( commitVersion: Long, commitActions: Seq[Action]): Unit postCommit...FIXME postCommit is used when OptimisticTransactionImpl is requested to < > (at the end). == [[commitInfo]] CommitInfo OptimisticTransactionImpl creates a CommitInfo.md[] when requested to < > with DeltaSQLConf.md#commitInfo.enabled[spark.databricks.delta.commitInfo.enabled] configuration enabled. OptimisticTransactionImpl uses the CommitInfo to recordDeltaEvent (as a CommitStats). == [[registerPostCommitHook]] Registering Post-Commit Hook [source, scala] \u00b6 registerPostCommitHook( hook: PostCommitHook): Unit registerPostCommitHook registers ( adds ) the given < > to the < > internal registry. NOTE: registerPostCommitHook adds the hook only once. registerPostCommitHook is used when OptimisticTransactionImpl is requested to < > (to register the < > post-commit hook). == [[runPostCommitHooks]] Running Post-Commit Hooks [source, scala] \u00b6 runPostCommitHooks( version: Long, committedActions: Seq[Action]): Unit runPostCommitHooks simply < > every < > registered (in the < > internal registry). runPostCommitHooks < > (making all follow-up operations non-transactional). NOTE: Hooks may create new transactions. For any non-fatal exception, runPostCommitHooks prints out the following ERROR message to the logs, records the delta event, and requests the post-commit hook to < >. Error when executing post-commit hook [name] for commit [version] runPostCommitHooks throws an AssertionError when < > flag is turned off ( false ): Can't call post commit hooks before committing runPostCommitHooks is used when OptimisticTransactionImpl is requested to < >. == [[doCommit]] Attempting Commit [source, scala] \u00b6 doCommit( attemptVersion: Long, actions: Seq[Action], attemptNumber: Int): Long doCommit returns the given attemptVersion as the commit version if successful or < >. Internally, doCommit prints out the following DEBUG message to the logs: Attempting to commit version [attemptVersion] with [size] actions with [isolationLevel] isolation level [[doCommit-write]] doCommit requests the < > (of the < >) to < > the given < > (serialized to < >) to a < > (e.g. 00000000000000000001.json ) in the < > (of the < >) with the attemptVersion version. NOTE: < > must throw a java.nio.file.FileAlreadyExistsException exception if the delta file already exists. Any FileAlreadyExistsExceptions are caught by < > itself to < >. [[doCommit-postCommitSnapshot]] doCommit requests the < > to < >. [[doCommit-IllegalStateException]] doCommit throws an IllegalStateException if the version of the snapshot after update is smaller than the given attemptVersion version. The committed version is [attemptVersion] but the current version is [version]. [[doCommit-stats]] doCommit records a new CommitStats and returns the given attemptVersion as the commit version. [[doCommit-FileAlreadyExistsException]] doCommit catches FileAlreadyExistsExceptions and < >. doCommit is used when OptimisticTransactionImpl is requested to < > (and < >). Retrying Commit \u00b6 checkAndRetry ( checkVersion : Long , actions : Seq [ Action ], attemptNumber : Int ) : Long checkAndRetry ...FIXME checkAndRetry is used when OptimisticTransactionImpl is requested to commit (and attempts a commit that failed with an FileAlreadyExistsException ). == [[verifyNewMetadata]] verifyNewMetadata Method [source, scala] \u00b6 verifyNewMetadata( metadata: Metadata): Unit verifyNewMetadata...FIXME verifyNewMetadata is used when OptimisticTransactionImpl is requested to < > and < >. == [[txnVersion]] Looking Up Transaction Version For Given (Streaming Query) ID [source, scala] \u00b6 txnVersion( id: String): Long txnVersion simply registers ( adds ) the given ID in the < > internal registry. In the end, txnVersion requests the < > for the < > or assumes -1 . txnVersion is used when DeltaSink is requested to < >. getOperationMetrics Method \u00b6 getOperationMetrics ( op : Operation ) : Option [ Map [ String , String ]] getOperationMetrics ...FIXME getOperationMetrics is used when OptimisticTransactionImpl is requested to commit . == [[getUserMetadata]] User-Defined Metadata [source,scala] \u00b6 getUserMetadata( op: Operation): Option[String] getUserMetadata returns the Operation.md#userMetadata[userMetadata] of the given Operation.md[] (if defined) or the value of DeltaSQLConf.md#DELTA_USER_METADATA[spark.databricks.delta.commitInfo.userMetadata] configuration property. getUserMetadata is used when OptimisticTransactionImpl is requested to < > (and DeltaSQLConf.md#DELTA_COMMIT_INFO_ENABLED[spark.databricks.delta.commitInfo.enabled] configuration property is enabled). == [[getPrettyPartitionMessage]] getPrettyPartitionMessage Method [source,scala] \u00b6 getPrettyPartitionMessage( partitionValues: Map[String, String]): String getPrettyPartitionMessage...FIXME getPrettyPartitionMessage is used when...FIXME == [[getNextAttemptVersion]] getNextAttemptVersion Internal Method [source,scala] \u00b6 getNextAttemptVersion( previousAttemptVersion: Long): Long getNextAttemptVersion...FIXME getNextAttemptVersion is used when OptimisticTransactionImpl is requested to < >. == [[internal-registries]] Internal Registries === [[postCommitHooks]] Post-Commit Hooks [source, scala] \u00b6 postCommitHooks: ArrayBuffer[PostCommitHook] \u00b6 OptimisticTransactionImpl manages PostCommitHook.md[]s that will be < > right after a < > is successful. Post-commit hooks can be < >, but only the < > post-commit hook is supported (when...FIXME). === [[newMetadata]] newMetadata [source, scala] \u00b6 newMetadata: Option[Metadata] \u00b6 OptimisticTransactionImpl uses the newMetadata internal registry for a new < > that should be committed with this transaction. newMetadata is initially undefined ( None ). It can be < > only once and before the transaction < >. newMetadata is used when < > (and < > for statistics). newMetadata is available using < > method. === [[readPredicates]] readPredicates [source,scala] \u00b6 readPredicates: ArrayBuffer[Expression] \u00b6 readPredicates holds predicate expressions for partitions the transaction is modifying. readPredicates is added a new predicate expression when < > and < >. readPredicates is used when < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | committed a| [[committed]] Flag that controls whether the transaction is < > or not (and prevents < > from being executed again) Default: false Enabled (set to true ) exclusively in < > | dependsOnFiles a| [[dependsOnFiles]] Flag that...FIXME Default: false Enabled (set to true ) in < >, < > Used in < > and < > | readFiles a| [[readFiles]] | readTxn a| [[readTxn]] Streaming query IDs that have been seen by this transaction A new queryId is added when OptimisticTransactionImpl is requested for < > Used when OptimisticTransactionImpl is requested to < > (to fail with a ConcurrentTransactionException for idempotent transactions that have conflicted) | snapshotMetadata a| [[snapshotMetadata]] < > of the < > |===","title":"OptimisticTransactionImpl"},{"location":"OptimisticTransactionImpl/#optimistictransactionimpl","text":"OptimisticTransactionImpl is an < > of the TransactionalWrite.md[] abstraction for < > that can modify a < > (at a given < >) and can be < > eventually. In other words, OptimisticTransactionImpl is a set of Action.md[actions] as part of an Operation.md[]. == [[contract]] Contract === [[clock]] clock","title":"OptimisticTransactionImpl"},{"location":"OptimisticTransactionImpl/#sourcescala","text":"","title":"[source,scala]"},{"location":"OptimisticTransactionImpl/#clock-clock","text":"=== [[deltaLog]] deltaLog","title":"clock: Clock"},{"location":"OptimisticTransactionImpl/#sourcescala_1","text":"","title":"[source,scala]"},{"location":"OptimisticTransactionImpl/#deltalog-deltalog","text":"DeltaLog.md[] (of the delta table) that this transaction is changing deltaLog is part of the TransactionalWrite.md#deltaLog[TransactionalWrite] contract and seems to change it to val (from def ). === [[snapshot]] snapshot","title":"deltaLog: DeltaLog"},{"location":"OptimisticTransactionImpl/#sourcescala_2","text":"","title":"[source,scala]"},{"location":"OptimisticTransactionImpl/#snapshot-snapshot","text":"Snapshot.md[] (of the < >) that this transaction is changing snapshot is part of the TransactionalWrite.md#deltaLog[TransactionalWrite] contract and seems to change it to val (from def ). == [[implementations]] Implementations OptimisticTransaction.md[] is the default and only known OptimisticTransactionImpl in Delta Lake. == [[metadata]] metadata Method","title":"snapshot: Snapshot"},{"location":"OptimisticTransactionImpl/#source-scala","text":"","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#metadata-metadata","text":"metadata is either the < > (if defined) or the < >. metadata is part of the TransactionalWrite.md#metadata[TransactionalWrite] abstraction. == [[readVersion]] readVersion Method","title":"metadata: Metadata"},{"location":"OptimisticTransactionImpl/#source-scala_1","text":"","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#readversion-long","text":"readVersion simply requests the < > for the < >. readVersion is used when: OptimisticTransactionImpl is requested for < >, to < > and < > ConvertToDeltaCommand is requested to < > WriteIntoDelta is requested to < > ImplicitMetadataOperation is requested to < > == [[updateMetadata]] Updating Metadata","title":"readVersion: Long"},{"location":"OptimisticTransactionImpl/#source-scala_2","text":"updateMetadata( metadata: Metadata): Unit updateMetadata updates the < > internal property based on the < >: For -1 , updateMetadata updates the < > of the given metadata with a < > based on the SQLConf (of the active SparkSession ), the < > of the given metadata and a new < > For other versions, updateMetadata leaves the given < > unchanged [[updateMetadata-AssertionError-hasWritten]] updateMetadata throws an AssertionError when the < > flag is enabled ( true ): Cannot update the metadata in a transaction that has already written data. updateMetadata throws an AssertionError when the < > is not empty: Cannot change the metadata more than once in a transaction. updateMetadata is used when: < > is executed (and requested to < >) ImplicitMetadataOperation is requested to < >","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#files-to-scan-matching-given-predicates","text":"filterFiles () : Seq [ AddFile ] // Uses `true` literal to mean that all files match filterFiles ( filters : Seq [ Expression ]) : Seq [ AddFile ] filterFiles gives the files to scan based on the given predicates (filter expressions). Internally, filterFiles requests the Snapshot for the filesForScan (for no projection attributes and the given filters). filterFiles finds the partition predicates among the given filters (and the partition columns of the Metadata ). filterFiles registers ( adds ) the partition predicates (in the readPredicates internal registry) and the files to scan (in the readFiles internal registry). filterFiles is used when: WriteIntoDelta is requested to write DeltaSink is requested to add a streaming micro-batch (with Complete output mode) DeleteCommand , MergeIntoCommand and UpdateCommand are executed CreateDeltaTableCommand is executed == [[readWholeTable]] readWholeTable Method","title":" Files To Scan Matching Given Predicates"},{"location":"OptimisticTransactionImpl/#source-scala_3","text":"","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#readwholetable-unit","text":"readWholeTable simply adds True literal to the < > internal registry. readWholeTable is used when DeltaSink is requested to DeltaSink.md#addBatch[add a streaming micro-batch] (and the batch reads the same Delta table as this sink is going to write to).","title":"readWholeTable(): Unit"},{"location":"OptimisticTransactionImpl/#committing-transaction","text":"commit ( actions : Seq [ Action ], op : DeltaOperations.Operation ) : Long commit commits the transaction (with the Action s and a given Operation ) [[commit-prepareCommit]] commit firstly < > (that gives the final actions to commit that may be different from the given < >). [[commit-isolationLevelToUse]] commit determines the isolation level for this commit by checking whether any < > (in the given < >) has the < > flag on ( true ). With no data changed, commit uses SnapshotIsolation else Serializable . [[commit-isBlindAppend]] commit...FIXME [[commit-commitInfo]] commit...FIXME [[commit-registerPostCommitHook]] commit < > the < > post-commit hook when there is a < > among the actions and the < > table property (< > the < >) is enabled ( true ). NOTE: < > table property defaults to false . [[commit-commitVersion]] commit < > with the next version, the actions, attempt number 0 , and the select isolation level. commit prints out the following INFO message to the logs: Committed delta #[commitVersion] to [logPath] [[commit-postCommit]] commit < > (with the version committed and the actions). [[commit-runPostCommitHooks]] In the end, commit < > and returns the version of the successful commit. == [[prepareCommit]] Preparing Commit","title":" Committing Transaction"},{"location":"OptimisticTransactionImpl/#source-scala_4","text":"prepareCommit( actions: Seq[Action], op: DeltaOperations.Operation): Seq[Action] prepareCommit adds the < > action (if available) to the given < >. prepareCommit < > if there was one. prepareCommit...FIXME prepareCommit requests the < > to < >. prepareCommit...FIXME prepareCommit throws an AssertionError when the number of metadata changes in the transaction (by means of < > actions) is above 1 : Cannot change the metadata more than once in a transaction. prepareCommit throws an AssertionError when the < > internal flag is turned on ( true ): Transaction already committed. prepareCommit is used when OptimisticTransactionImpl is requested to < > (at the beginning). == [[postCommit]] Performing Post-Commit Operations","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#source-scala_5","text":"postCommit( commitVersion: Long, commitActions: Seq[Action]): Unit postCommit...FIXME postCommit is used when OptimisticTransactionImpl is requested to < > (at the end). == [[commitInfo]] CommitInfo OptimisticTransactionImpl creates a CommitInfo.md[] when requested to < > with DeltaSQLConf.md#commitInfo.enabled[spark.databricks.delta.commitInfo.enabled] configuration enabled. OptimisticTransactionImpl uses the CommitInfo to recordDeltaEvent (as a CommitStats). == [[registerPostCommitHook]] Registering Post-Commit Hook","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#source-scala_6","text":"registerPostCommitHook( hook: PostCommitHook): Unit registerPostCommitHook registers ( adds ) the given < > to the < > internal registry. NOTE: registerPostCommitHook adds the hook only once. registerPostCommitHook is used when OptimisticTransactionImpl is requested to < > (to register the < > post-commit hook). == [[runPostCommitHooks]] Running Post-Commit Hooks","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#source-scala_7","text":"runPostCommitHooks( version: Long, committedActions: Seq[Action]): Unit runPostCommitHooks simply < > every < > registered (in the < > internal registry). runPostCommitHooks < > (making all follow-up operations non-transactional). NOTE: Hooks may create new transactions. For any non-fatal exception, runPostCommitHooks prints out the following ERROR message to the logs, records the delta event, and requests the post-commit hook to < >. Error when executing post-commit hook [name] for commit [version] runPostCommitHooks throws an AssertionError when < > flag is turned off ( false ): Can't call post commit hooks before committing runPostCommitHooks is used when OptimisticTransactionImpl is requested to < >. == [[doCommit]] Attempting Commit","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#source-scala_8","text":"doCommit( attemptVersion: Long, actions: Seq[Action], attemptNumber: Int): Long doCommit returns the given attemptVersion as the commit version if successful or < >. Internally, doCommit prints out the following DEBUG message to the logs: Attempting to commit version [attemptVersion] with [size] actions with [isolationLevel] isolation level [[doCommit-write]] doCommit requests the < > (of the < >) to < > the given < > (serialized to < >) to a < > (e.g. 00000000000000000001.json ) in the < > (of the < >) with the attemptVersion version. NOTE: < > must throw a java.nio.file.FileAlreadyExistsException exception if the delta file already exists. Any FileAlreadyExistsExceptions are caught by < > itself to < >. [[doCommit-postCommitSnapshot]] doCommit requests the < > to < >. [[doCommit-IllegalStateException]] doCommit throws an IllegalStateException if the version of the snapshot after update is smaller than the given attemptVersion version. The committed version is [attemptVersion] but the current version is [version]. [[doCommit-stats]] doCommit records a new CommitStats and returns the given attemptVersion as the commit version. [[doCommit-FileAlreadyExistsException]] doCommit catches FileAlreadyExistsExceptions and < >. doCommit is used when OptimisticTransactionImpl is requested to < > (and < >).","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#retrying-commit","text":"checkAndRetry ( checkVersion : Long , actions : Seq [ Action ], attemptNumber : Int ) : Long checkAndRetry ...FIXME checkAndRetry is used when OptimisticTransactionImpl is requested to commit (and attempts a commit that failed with an FileAlreadyExistsException ). == [[verifyNewMetadata]] verifyNewMetadata Method","title":" Retrying Commit"},{"location":"OptimisticTransactionImpl/#source-scala_9","text":"verifyNewMetadata( metadata: Metadata): Unit verifyNewMetadata...FIXME verifyNewMetadata is used when OptimisticTransactionImpl is requested to < > and < >. == [[txnVersion]] Looking Up Transaction Version For Given (Streaming Query) ID","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#source-scala_10","text":"txnVersion( id: String): Long txnVersion simply registers ( adds ) the given ID in the < > internal registry. In the end, txnVersion requests the < > for the < > or assumes -1 . txnVersion is used when DeltaSink is requested to < >.","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#getoperationmetrics-method","text":"getOperationMetrics ( op : Operation ) : Option [ Map [ String , String ]] getOperationMetrics ...FIXME getOperationMetrics is used when OptimisticTransactionImpl is requested to commit . == [[getUserMetadata]] User-Defined Metadata","title":" getOperationMetrics Method"},{"location":"OptimisticTransactionImpl/#sourcescala_3","text":"getUserMetadata( op: Operation): Option[String] getUserMetadata returns the Operation.md#userMetadata[userMetadata] of the given Operation.md[] (if defined) or the value of DeltaSQLConf.md#DELTA_USER_METADATA[spark.databricks.delta.commitInfo.userMetadata] configuration property. getUserMetadata is used when OptimisticTransactionImpl is requested to < > (and DeltaSQLConf.md#DELTA_COMMIT_INFO_ENABLED[spark.databricks.delta.commitInfo.enabled] configuration property is enabled). == [[getPrettyPartitionMessage]] getPrettyPartitionMessage Method","title":"[source,scala]"},{"location":"OptimisticTransactionImpl/#sourcescala_4","text":"getPrettyPartitionMessage( partitionValues: Map[String, String]): String getPrettyPartitionMessage...FIXME getPrettyPartitionMessage is used when...FIXME == [[getNextAttemptVersion]] getNextAttemptVersion Internal Method","title":"[source,scala]"},{"location":"OptimisticTransactionImpl/#sourcescala_5","text":"getNextAttemptVersion( previousAttemptVersion: Long): Long getNextAttemptVersion...FIXME getNextAttemptVersion is used when OptimisticTransactionImpl is requested to < >. == [[internal-registries]] Internal Registries === [[postCommitHooks]] Post-Commit Hooks","title":"[source,scala]"},{"location":"OptimisticTransactionImpl/#source-scala_11","text":"","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#postcommithooks-arraybufferpostcommithook","text":"OptimisticTransactionImpl manages PostCommitHook.md[]s that will be < > right after a < > is successful. Post-commit hooks can be < >, but only the < > post-commit hook is supported (when...FIXME). === [[newMetadata]] newMetadata","title":"postCommitHooks: ArrayBuffer[PostCommitHook]"},{"location":"OptimisticTransactionImpl/#source-scala_12","text":"","title":"[source, scala]"},{"location":"OptimisticTransactionImpl/#newmetadata-optionmetadata","text":"OptimisticTransactionImpl uses the newMetadata internal registry for a new < > that should be committed with this transaction. newMetadata is initially undefined ( None ). It can be < > only once and before the transaction < >. newMetadata is used when < > (and < > for statistics). newMetadata is available using < > method. === [[readPredicates]] readPredicates","title":"newMetadata: Option[Metadata]"},{"location":"OptimisticTransactionImpl/#sourcescala_6","text":"","title":"[source,scala]"},{"location":"OptimisticTransactionImpl/#readpredicates-arraybufferexpression","text":"readPredicates holds predicate expressions for partitions the transaction is modifying. readPredicates is added a new predicate expression when < > and < >. readPredicates is used when < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | committed a| [[committed]] Flag that controls whether the transaction is < > or not (and prevents < > from being executed again) Default: false Enabled (set to true ) exclusively in < > | dependsOnFiles a| [[dependsOnFiles]] Flag that...FIXME Default: false Enabled (set to true ) in < >, < > Used in < > and < > | readFiles a| [[readFiles]] | readTxn a| [[readTxn]] Streaming query IDs that have been seen by this transaction A new queryId is added when OptimisticTransactionImpl is requested for < > Used when OptimisticTransactionImpl is requested to < > (to fail with a ConcurrentTransactionException for idempotent transactions that have conflicted) | snapshotMetadata a| [[snapshotMetadata]] < > of the < > |===","title":"readPredicates: ArrayBuffer[Expression]"},{"location":"PartitionFiltering/","text":"PartitionFiltering \u00b6 PartitionFiltering is an abstraction of snapshots with partition filtering for scan . Implementations \u00b6 Snapshot is the default and only known PartitionFiltering in Delta Lake. Files to Scan (Matching Projection Attributes and Predicates) \u00b6 filesForScan ( projection : Seq [ Attribute ], filters : Seq [ Expression ], keepStats : Boolean = false ) : DeltaScan filesForScan ...FIXME filesForScan is used when: OptimisticTransactionImpl is requested for the files to scan matching given predicates TahoeLogFileIndex is requested for the files matching predicates and the input files","title":"PartitionFiltering"},{"location":"PartitionFiltering/#partitionfiltering","text":"PartitionFiltering is an abstraction of snapshots with partition filtering for scan .","title":"PartitionFiltering"},{"location":"PartitionFiltering/#implementations","text":"Snapshot is the default and only known PartitionFiltering in Delta Lake.","title":"Implementations"},{"location":"PartitionFiltering/#files-to-scan-matching-projection-attributes-and-predicates","text":"filesForScan ( projection : Seq [ Attribute ], filters : Seq [ Expression ], keepStats : Boolean = false ) : DeltaScan filesForScan ...FIXME filesForScan is used when: OptimisticTransactionImpl is requested for the files to scan matching given predicates TahoeLogFileIndex is requested for the files matching predicates and the input files","title":" Files to Scan (Matching Projection Attributes and Predicates)"},{"location":"PostCommitHook/","text":"= PostCommitHook PostCommitHook is an < > of < > that have a < > and can be < > (when OptimisticTransactionImpl is < >). [[contract]] .PostCommitHook Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | handleError a| [[handleError]] [source, scala] \u00b6 handleError( error: Throwable, version: Long): Unit = {} Handles an error while < > Used when OptimisticTransactionImpl is requested to < > (when < >) | name a| [[name]] [source, scala] \u00b6 name: String \u00b6 User-friendly name of the hook for error reporting Used when: DeltaErrors utility is used to < > OptimisticTransactionImpl is requested to < > (when < >) GenerateSymlinkManifestImpl is requested to < > | run a| [[run]] [source, scala] \u00b6 run( spark: SparkSession, txn: OptimisticTransactionImpl, committedActions: Seq[Action]): Unit Executes the post-commit hook Used when OptimisticTransactionImpl is requested to < > (when < >) |=== [[implementations]] NOTE: < > is the default and only known PostCommitHook in Delta Lake.","title":"Post-Commit Hooks"},{"location":"PostCommitHook/#source-scala","text":"handleError( error: Throwable, version: Long): Unit = {} Handles an error while < > Used when OptimisticTransactionImpl is requested to < > (when < >) | name a| [[name]]","title":"[source, scala]"},{"location":"PostCommitHook/#source-scala_1","text":"","title":"[source, scala]"},{"location":"PostCommitHook/#name-string","text":"User-friendly name of the hook for error reporting Used when: DeltaErrors utility is used to < > OptimisticTransactionImpl is requested to < > (when < >) GenerateSymlinkManifestImpl is requested to < > | run a| [[run]]","title":"name: String"},{"location":"PostCommitHook/#source-scala_2","text":"run( spark: SparkSession, txn: OptimisticTransactionImpl, committedActions: Seq[Action]): Unit Executes the post-commit hook Used when OptimisticTransactionImpl is requested to < > (when < >) |=== [[implementations]] NOTE: < > is the default and only known PostCommitHook in Delta Lake.","title":"[source, scala]"},{"location":"PreprocessTableDelete/","text":"PreprocessTableDelete Logical Resolution Rule \u00b6 PreprocessTableDelete is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to < > in a query plan into DeleteCommand.md[]s. PreprocessTableDelete is installed (injected) into a SparkSession using DeltaSparkSessionExtension.md[]. == [[creating-instance]][[conf]] Creating Instance PreprocessTableDelete takes a single SQLConf to be created. PreprocessTableDelete is created when DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support]. == [[apply]] Executing Rule [source, scala] \u00b6 apply( plan: LogicalPlan): LogicalPlan apply resolves ( replaces ) DeltaDelete logical commands (in a logical query plan) into corresponding DeleteCommand.md[]s. apply is part of the Spark SQL's Rule abstraction.","title":"PreprocessTableDelete"},{"location":"PreprocessTableDelete/#preprocesstabledelete-logical-resolution-rule","text":"PreprocessTableDelete is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to < > in a query plan into DeleteCommand.md[]s. PreprocessTableDelete is installed (injected) into a SparkSession using DeltaSparkSessionExtension.md[]. == [[creating-instance]][[conf]] Creating Instance PreprocessTableDelete takes a single SQLConf to be created. PreprocessTableDelete is created when DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support]. == [[apply]] Executing Rule","title":"PreprocessTableDelete Logical Resolution Rule"},{"location":"PreprocessTableDelete/#source-scala","text":"apply( plan: LogicalPlan): LogicalPlan apply resolves ( replaces ) DeltaDelete logical commands (in a logical query plan) into corresponding DeleteCommand.md[]s. apply is part of the Spark SQL's Rule abstraction.","title":"[source, scala]"},{"location":"PreprocessTableMerge/","text":"PreprocessTableMerge Logical Resolution Rule \u00b6 PreprocessTableMerge is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to < > in a query plan into MergeIntoCommand.md[]s. PreprocessTableMerge is installed (injected) into a SparkSession using DeltaSparkSessionExtension.md[]. == [[creating-instance]][[conf]] Creating Instance PreprocessTableMerge takes a single SQLConf to be created. PreprocessTableMerge is created when: DeltaMergeBuilder is executed DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support] == [[apply]] Executing Rule [source, scala] \u00b6 apply( plan: LogicalPlan): LogicalPlan apply resolves ( replaces ) DeltaMergeInto.md[] logical commands (in a logical query plan) into corresponding MergeIntoCommand.md[]s. apply is part of the Spark SQL's Rule abstraction.","title":"PreprocessTableMerge"},{"location":"PreprocessTableMerge/#preprocesstablemerge-logical-resolution-rule","text":"PreprocessTableMerge is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to < > in a query plan into MergeIntoCommand.md[]s. PreprocessTableMerge is installed (injected) into a SparkSession using DeltaSparkSessionExtension.md[]. == [[creating-instance]][[conf]] Creating Instance PreprocessTableMerge takes a single SQLConf to be created. PreprocessTableMerge is created when: DeltaMergeBuilder is executed DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support] == [[apply]] Executing Rule","title":"PreprocessTableMerge Logical Resolution Rule"},{"location":"PreprocessTableMerge/#source-scala","text":"apply( plan: LogicalPlan): LogicalPlan apply resolves ( replaces ) DeltaMergeInto.md[] logical commands (in a logical query plan) into corresponding MergeIntoCommand.md[]s. apply is part of the Spark SQL's Rule abstraction.","title":"[source, scala]"},{"location":"PreprocessTableUpdate/","text":"PreprocessTableUpdate Logical Resolution Rule \u00b6 PreprocessTableUpdate is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to < > in a query plan into UpdateCommand.md[]s. PreprocessTableUpdate is installed (injected) into a SparkSession using DeltaSparkSessionExtension.md[]. == [[creating-instance]][[conf]] Creating Instance PreprocessTableUpdate takes a single SQLConf to be created. PreprocessTableUpdate is created when DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support]. == [[apply]] Executing Rule [source, scala] \u00b6 apply( plan: LogicalPlan): LogicalPlan apply resolves ( replaces ) DeltaUpdateTable logical commands (in a logical query plan) into corresponding UpdateCommand.md[]s. apply is part of the Spark SQL's Rule abstraction.","title":"PreprocessTableUpdate"},{"location":"PreprocessTableUpdate/#preprocesstableupdate-logical-resolution-rule","text":"PreprocessTableUpdate is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to < > in a query plan into UpdateCommand.md[]s. PreprocessTableUpdate is installed (injected) into a SparkSession using DeltaSparkSessionExtension.md[]. == [[creating-instance]][[conf]] Creating Instance PreprocessTableUpdate takes a single SQLConf to be created. PreprocessTableUpdate is created when DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support]. == [[apply]] Executing Rule","title":"PreprocessTableUpdate Logical Resolution Rule"},{"location":"PreprocessTableUpdate/#source-scala","text":"apply( plan: LogicalPlan): LogicalPlan apply resolves ( replaces ) DeltaUpdateTable logical commands (in a logical query plan) into corresponding UpdateCommand.md[]s. apply is part of the Spark SQL's Rule abstraction.","title":"[source, scala]"},{"location":"Protocol/","text":"= Protocol Protocol is...FIXME","title":"Protocol"},{"location":"ReadChecksum/","text":"ReadChecksum \u00b6 ReadChecksum is...FIXME","title":"ReadChecksum"},{"location":"ReadChecksum/#readchecksum","text":"ReadChecksum is...FIXME","title":"ReadChecksum"},{"location":"RemoveFile/","text":"RemoveFile \u00b6 RemoveFile is...FIXME","title":"RemoveFile"},{"location":"RemoveFile/#removefile","text":"RemoveFile is...FIXME","title":"RemoveFile"},{"location":"SQLMetricsReporting/","text":"SQLMetricsReporting \u00b6 SQLMetricsReporting is an extension for OptimisticTransactionImpl to track SQL metrics of Operations . Implementations \u00b6 OptimisticTransactionImpl operationSQLMetrics Registry \u00b6 operationSQLMetrics ( spark : SparkSession , metrics : Map [ String , SQLMetric ]) : Unit operationSQLMetrics ...FIXME operationSQLMetrics is used when...FIXME registerSQLMetrics \u00b6 registerSQLMetrics ( spark : SparkSession , metrics : Map [ String , SQLMetric ]) : Unit registerSQLMetrics ...FIXME registerSQLMetrics is used when...FIXME getMetricsForOperation \u00b6 getMetricsForOperation ( operation : Operation ) : Map [ String , String ] getMetricsForOperation ...FIXME getMetricsForOperation is used when OptimisticTransactionImpl is requested to getOperationMetrics .","title":"SQLMetricsReporting"},{"location":"SQLMetricsReporting/#sqlmetricsreporting","text":"SQLMetricsReporting is an extension for OptimisticTransactionImpl to track SQL metrics of Operations .","title":"SQLMetricsReporting"},{"location":"SQLMetricsReporting/#implementations","text":"OptimisticTransactionImpl","title":"Implementations"},{"location":"SQLMetricsReporting/#operationsqlmetrics-registry","text":"operationSQLMetrics ( spark : SparkSession , metrics : Map [ String , SQLMetric ]) : Unit operationSQLMetrics ...FIXME operationSQLMetrics is used when...FIXME","title":" operationSQLMetrics Registry"},{"location":"SQLMetricsReporting/#registersqlmetrics","text":"registerSQLMetrics ( spark : SparkSession , metrics : Map [ String , SQLMetric ]) : Unit registerSQLMetrics ...FIXME registerSQLMetrics is used when...FIXME","title":" registerSQLMetrics"},{"location":"SQLMetricsReporting/#getmetricsforoperation","text":"getMetricsForOperation ( operation : Operation ) : Map [ String , String ] getMetricsForOperation ...FIXME getMetricsForOperation is used when OptimisticTransactionImpl is requested to getOperationMetrics .","title":" getMetricsForOperation"},{"location":"SchemaUtils/","text":"= [[SchemaUtils]] SchemaUtils Utility SchemaUtils is...FIXME == [[mergeSchemas]] mergeSchemas Utility [source, scala] \u00b6 mergeSchemas( tableSchema: StructType, dataSchema: StructType): StructType mergeSchemas ...FIXME [NOTE] \u00b6 mergeSchemas is used when: ConvertToDeltaCommand is requested to ConvertToDeltaCommand.md#performConvert[performConvert] and ConvertToDeltaCommand.md#mergeSchemasInParallel[mergeSchemasInParallel] * ImplicitMetadataOperation is requested to ImplicitMetadataOperation.md#updateMetadata[update metadata] \u00b6","title":"SchemaUtils"},{"location":"SchemaUtils/#source-scala","text":"mergeSchemas( tableSchema: StructType, dataSchema: StructType): StructType mergeSchemas ...FIXME","title":"[source, scala]"},{"location":"SchemaUtils/#note","text":"mergeSchemas is used when: ConvertToDeltaCommand is requested to ConvertToDeltaCommand.md#performConvert[performConvert] and ConvertToDeltaCommand.md#mergeSchemasInParallel[mergeSchemasInParallel]","title":"[NOTE]"},{"location":"SchemaUtils/#implicitmetadataoperation-is-requested-to-implicitmetadataoperationmdupdatemetadataupdate-metadata","text":"","title":"* ImplicitMetadataOperation is requested to ImplicitMetadataOperation.md#updateMetadata[update metadata]"},{"location":"SetTransaction/","text":"= SetTransaction SetTransaction is an < > that denotes the committed < > for an < >. SetTransaction is < > when DeltaSink is requested to < > (for STREAMING UPDATE operation idempotence at query restart). == [[creating-instance]] Creating SetTransaction Instance SetTransaction takes the following to be created: [[appId]] Application ID (e.g. streaming query ID) [[version]] Version (e.g micro-batch ID) [[lastUpdated]] Last Updated (optional) (e.g. milliseconds since the epoch) == [[wrap]] wrap Method [source, scala] \u00b6 wrap: SingleAction \u00b6 NOTE: wrap is part of the < > contract to wrap the action into a < > for serialization. wrap simply creates a new < > with the txn field set to this SetTransaction .","title":"SetTransaction"},{"location":"SetTransaction/#source-scala","text":"","title":"[source, scala]"},{"location":"SetTransaction/#wrap-singleaction","text":"NOTE: wrap is part of the < > contract to wrap the action into a < > for serialization. wrap simply creates a new < > with the txn field set to this SetTransaction .","title":"wrap: SingleAction"},{"location":"SingleAction/","text":"SingleAction \u00b6 SingleAction is...FIXME","title":"SingleAction"},{"location":"SingleAction/#singleaction","text":"SingleAction is...FIXME","title":"SingleAction"},{"location":"Snapshot/","text":"Snapshot \u00b6 Snapshot is an immutable snapshot of the state of the Delta table at the version . Tip Use Demo: DeltaTable, DeltaLog And Snapshots to learn more. Creating Instance \u00b6 Snapshot takes the following to be created: Hadoop Path to the log directory Version LogSegment minFileRetentionTimestamp (that is exactly DeltaLog.minFileRetentionTimestamp ) DeltaLog Timestamp Optional VersionChecksum While being created, Snapshot prints out the following INFO message to the logs and initialize : Created snapshot [this] Snapshot is created when SnapshotManagement is requested for one . Initializing \u00b6 init () : Unit init requests the DeltaLog for the protocolRead for the Protocol . Computed State \u00b6 computedState : State Scala lazy value computedState is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val computedState: State computedState takes the current cached set of actions and reads the latest state (executes a state.select(...).first() query). Note The state.select(...).first() query uses aggregate standard functions (e.g. last , collect_set , sum , count ) and so uses groupBy over the whole dataset indirectly. computedState assumes that the protocol and metadata (actions) are defined. computedState throws an IllegalStateException when the actions are not defined and spark.databricks.delta.stateReconstructionValidation.enabled configuration property is enabled. The [action] of your Delta table couldn't be recovered while Reconstructing version: [version]. Did you manually delete files in the _delta_log directory? Note The state.select(...).first() query uses last with ignoreNulls flag true and so may give no rows for first() . computedState makes sure that the State to be returned has at least the default protocol and metadata (actions) defined. Configuration Properties \u00b6 spark.databricks.delta.snapshotPartitions \u00b6 Snapshot uses the spark.databricks.delta.snapshotPartitions configuration property for the number of partitions to use for state reconstruction . spark.databricks.delta.stateReconstructionValidation.enabled \u00b6 Snapshot uses the spark.databricks.delta.stateReconstructionValidation.enabled configuration property for reconstructing state . State Dataset of Actions \u00b6 state : Dataset [ SingleAction ] state simply requests the cached delta state to get the delta state from the cache . state is used when: Checkpoints utility is used to writeCheckpoint Snapshot is requested for computedState , all files and files removed (tombstones) VacuumCommand utility is requested for garbage collection All AddFile Actions \u00b6 allFiles : Dataset [ AddFile ] allFiles simply takes the state dataset and selects AddFiles (adds where clause for add IS NOT NULL and select over the fields of AddFiles ). Note allFiles simply adds where and select clauses. No computation happens yet as it is (a description of) a distributed computation as a Dataset[AddFile] . import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, \"/tmp/delta/users\") val files = deltaLog.snapshot.allFiles scala> :type files org.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.AddFile] scala> files.show(truncate = false) +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ |path |partitionValues|size|modificationTime|dataChange|stats|tags| +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ |part-00000-4050db39-e0f5-485d-ab3b-3ca72307f621-c000.snappy.parquet|[] |262 |1578083748000 |false |null |null| |part-00000-ba39f292-2970-4528-a40c-8f0aa5f796de-c000.snappy.parquet|[] |262 |1578083570000 |false |null |null| |part-00003-99f9d902-24a7-4f76-a15a-6971940bc245-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00007-03d987f1-5bb3-4b5b-8db9-97b6667107e2-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00011-a759a8c2-507d-46dd-9da7-dc722316214b-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00015-2e685d29-25ed-4262-90a7-5491847fd8d0-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00015-ee0ac1af-e1e0-4422-8245-12da91ced0a2-c000.snappy.parquet|[] |429 |1578083570000 |false |null |null| +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ allFiles is used when: PartitionFiltering is requested for the files to scan (matching projection attributes and predicates) DeltaSourceSnapshot is requested for the initial files (indexed AddFiles ) GenerateSymlinkManifestImpl is requested to generateIncrementalManifest and generateFullManifest DeltaDataSource is requested for an Insertable HadoopFsRelation stateReconstruction Dataset of Actions \u00b6 stateReconstruction : Dataset [ SingleAction ] Note stateReconstruction returns a Dataset[SingleAction] and so does not do any computation per se. stateReconstruction is a Dataset of SingleActions (that is the dataset part) of the cachedState . stateReconstruction loads the log file indices (that gives a Dataset[SingleAction] ). stateReconstruction maps over partitions (using Dataset.mapPartitions ) and canonicalize the paths for AddFile and RemoveFile actions. stateReconstruction adds file column that uses a UDF to assert that input_file_name() belongs to the Delta table. Note This UDF-based check is very clever. stateReconstruction repartitions the Dataset using the path of add or remove actions (with the configurable number of partitions ) and Dataset.sortWithinPartitions by the file column. In the end, stateReconstruction maps over partitions (using Dataset.mapPartitions ) that creates a InMemoryLogReplay , requests it to append the actions (as version 0 ) and checkpoint . stateReconstruction is used when Snapshot is requested for a cached state . Loading Log File Indices \u00b6 loadActions : Dataset [ SingleAction ] loadActions takes fileIndices and...FIXME fileIndices \u00b6 fileIndices : Seq [ DeltaLogFileIndex ] Scala lazy value fileIndices is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val fileIndices: Seq[DeltaLogFileIndex] fileIndices is a collection of the checkpointFileIndexOpt and the deltaFileIndexOpt (if they are available). Commit File Index \u00b6 deltaFileIndexOpt : Option [ DeltaLogFileIndex ] Scala lazy value deltaFileIndexOpt is a Scala lazy value and is initialized once when first accessed. Once computed, it stays unchanged for the Snapshot instance. lazy val deltaFileIndexOpt: Option[DeltaLogFileIndex] deltaFileIndexOpt is a DeltaLogFileIndex (in JsonFileFormat ) for the checkpoint file of the LogSegment . Checkpoint File Index \u00b6 checkpointFileIndexOpt : Option [ DeltaLogFileIndex ] Scala lazy value checkpointFileIndexOpt is a Scala lazy value and is initialized once when first accessed. Once computed, it stays unchanged for the Snapshot instance. lazy val checkpointFileIndexOpt: Option[DeltaLogFileIndex] checkpointFileIndexOpt is a DeltaLogFileIndex (in ParquetFileFormat ) for the delta files of the LogSegment . emptyActions Dataset (of Actions) \u00b6 emptyActions : Dataset [ SingleAction ] emptyActions is an empty dataset of SingleActions for stateReconstruction and load . Transaction Version By App ID \u00b6 transactions : Map [ String , Long ] transactions takes the SetTransaction actions (from the state dataset) and makes them a lookup table of transaction version by appId . Scala lazy value transactions is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val transactions: Map[String, Long] transactions is used when OptimisticTransactionImpl is requested for the transaction version for a given (streaming query) id . All RemoveFile Actions (Tombstones) \u00b6 tombstones : Dataset [ RemoveFile ] tombstones ...FIXME scala> deltaLog.snapshot.tombstones.show(false) +----+-----------------+----------+ |path|deletionTimestamp|dataChange| +----+-----------------+----------+ +----+-----------------+----------+ cachedState \u00b6 cachedState : CachedDS [ SingleAction ] Scala lazy value cachedState is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val cachedState: CachedDS[SingleAction] cachedState creates a Cached Delta State with the following: The dataset part is the stateReconstruction dataset of SingleAction s The name in the format Delta Table State #version - [redactedPath] (with the version and the path redacted) Used when Snapshot is requested for the state ( Dataset[SingleAction] )","title":"Snapshot"},{"location":"Snapshot/#snapshot","text":"Snapshot is an immutable snapshot of the state of the Delta table at the version . Tip Use Demo: DeltaTable, DeltaLog And Snapshots to learn more.","title":"Snapshot"},{"location":"Snapshot/#creating-instance","text":"Snapshot takes the following to be created: Hadoop Path to the log directory Version LogSegment minFileRetentionTimestamp (that is exactly DeltaLog.minFileRetentionTimestamp ) DeltaLog Timestamp Optional VersionChecksum While being created, Snapshot prints out the following INFO message to the logs and initialize : Created snapshot [this] Snapshot is created when SnapshotManagement is requested for one .","title":"Creating Instance"},{"location":"Snapshot/#initializing","text":"init () : Unit init requests the DeltaLog for the protocolRead for the Protocol .","title":" Initializing"},{"location":"Snapshot/#computed-state","text":"computedState : State Scala lazy value computedState is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val computedState: State computedState takes the current cached set of actions and reads the latest state (executes a state.select(...).first() query). Note The state.select(...).first() query uses aggregate standard functions (e.g. last , collect_set , sum , count ) and so uses groupBy over the whole dataset indirectly. computedState assumes that the protocol and metadata (actions) are defined. computedState throws an IllegalStateException when the actions are not defined and spark.databricks.delta.stateReconstructionValidation.enabled configuration property is enabled. The [action] of your Delta table couldn't be recovered while Reconstructing version: [version]. Did you manually delete files in the _delta_log directory? Note The state.select(...).first() query uses last with ignoreNulls flag true and so may give no rows for first() . computedState makes sure that the State to be returned has at least the default protocol and metadata (actions) defined.","title":" Computed State"},{"location":"Snapshot/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"Snapshot/#sparkdatabricksdeltasnapshotpartitions","text":"Snapshot uses the spark.databricks.delta.snapshotPartitions configuration property for the number of partitions to use for state reconstruction .","title":" spark.databricks.delta.snapshotPartitions"},{"location":"Snapshot/#sparkdatabricksdeltastatereconstructionvalidationenabled","text":"Snapshot uses the spark.databricks.delta.stateReconstructionValidation.enabled configuration property for reconstructing state .","title":"spark.databricks.delta.stateReconstructionValidation.enabled"},{"location":"Snapshot/#state-dataset-of-actions","text":"state : Dataset [ SingleAction ] state simply requests the cached delta state to get the delta state from the cache . state is used when: Checkpoints utility is used to writeCheckpoint Snapshot is requested for computedState , all files and files removed (tombstones) VacuumCommand utility is requested for garbage collection","title":" State Dataset of Actions"},{"location":"Snapshot/#all-addfile-actions","text":"allFiles : Dataset [ AddFile ] allFiles simply takes the state dataset and selects AddFiles (adds where clause for add IS NOT NULL and select over the fields of AddFiles ). Note allFiles simply adds where and select clauses. No computation happens yet as it is (a description of) a distributed computation as a Dataset[AddFile] . import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, \"/tmp/delta/users\") val files = deltaLog.snapshot.allFiles scala> :type files org.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.AddFile] scala> files.show(truncate = false) +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ |path |partitionValues|size|modificationTime|dataChange|stats|tags| +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ |part-00000-4050db39-e0f5-485d-ab3b-3ca72307f621-c000.snappy.parquet|[] |262 |1578083748000 |false |null |null| |part-00000-ba39f292-2970-4528-a40c-8f0aa5f796de-c000.snappy.parquet|[] |262 |1578083570000 |false |null |null| |part-00003-99f9d902-24a7-4f76-a15a-6971940bc245-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00007-03d987f1-5bb3-4b5b-8db9-97b6667107e2-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00011-a759a8c2-507d-46dd-9da7-dc722316214b-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00015-2e685d29-25ed-4262-90a7-5491847fd8d0-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00015-ee0ac1af-e1e0-4422-8245-12da91ced0a2-c000.snappy.parquet|[] |429 |1578083570000 |false |null |null| +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ allFiles is used when: PartitionFiltering is requested for the files to scan (matching projection attributes and predicates) DeltaSourceSnapshot is requested for the initial files (indexed AddFiles ) GenerateSymlinkManifestImpl is requested to generateIncrementalManifest and generateFullManifest DeltaDataSource is requested for an Insertable HadoopFsRelation","title":" All AddFile Actions"},{"location":"Snapshot/#statereconstruction-dataset-of-actions","text":"stateReconstruction : Dataset [ SingleAction ] Note stateReconstruction returns a Dataset[SingleAction] and so does not do any computation per se. stateReconstruction is a Dataset of SingleActions (that is the dataset part) of the cachedState . stateReconstruction loads the log file indices (that gives a Dataset[SingleAction] ). stateReconstruction maps over partitions (using Dataset.mapPartitions ) and canonicalize the paths for AddFile and RemoveFile actions. stateReconstruction adds file column that uses a UDF to assert that input_file_name() belongs to the Delta table. Note This UDF-based check is very clever. stateReconstruction repartitions the Dataset using the path of add or remove actions (with the configurable number of partitions ) and Dataset.sortWithinPartitions by the file column. In the end, stateReconstruction maps over partitions (using Dataset.mapPartitions ) that creates a InMemoryLogReplay , requests it to append the actions (as version 0 ) and checkpoint . stateReconstruction is used when Snapshot is requested for a cached state .","title":" stateReconstruction Dataset of Actions"},{"location":"Snapshot/#loading-log-file-indices","text":"loadActions : Dataset [ SingleAction ] loadActions takes fileIndices and...FIXME","title":" Loading Log File Indices"},{"location":"Snapshot/#fileindices","text":"fileIndices : Seq [ DeltaLogFileIndex ] Scala lazy value fileIndices is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val fileIndices: Seq[DeltaLogFileIndex] fileIndices is a collection of the checkpointFileIndexOpt and the deltaFileIndexOpt (if they are available).","title":" fileIndices"},{"location":"Snapshot/#commit-file-index","text":"deltaFileIndexOpt : Option [ DeltaLogFileIndex ] Scala lazy value deltaFileIndexOpt is a Scala lazy value and is initialized once when first accessed. Once computed, it stays unchanged for the Snapshot instance. lazy val deltaFileIndexOpt: Option[DeltaLogFileIndex] deltaFileIndexOpt is a DeltaLogFileIndex (in JsonFileFormat ) for the checkpoint file of the LogSegment .","title":" Commit File Index"},{"location":"Snapshot/#checkpoint-file-index","text":"checkpointFileIndexOpt : Option [ DeltaLogFileIndex ] Scala lazy value checkpointFileIndexOpt is a Scala lazy value and is initialized once when first accessed. Once computed, it stays unchanged for the Snapshot instance. lazy val checkpointFileIndexOpt: Option[DeltaLogFileIndex] checkpointFileIndexOpt is a DeltaLogFileIndex (in ParquetFileFormat ) for the delta files of the LogSegment .","title":" Checkpoint File Index"},{"location":"Snapshot/#emptyactions-dataset-of-actions","text":"emptyActions : Dataset [ SingleAction ] emptyActions is an empty dataset of SingleActions for stateReconstruction and load .","title":" emptyActions Dataset (of Actions)"},{"location":"Snapshot/#transaction-version-by-app-id","text":"transactions : Map [ String , Long ] transactions takes the SetTransaction actions (from the state dataset) and makes them a lookup table of transaction version by appId . Scala lazy value transactions is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val transactions: Map[String, Long] transactions is used when OptimisticTransactionImpl is requested for the transaction version for a given (streaming query) id .","title":" Transaction Version By App ID"},{"location":"Snapshot/#all-removefile-actions-tombstones","text":"tombstones : Dataset [ RemoveFile ] tombstones ...FIXME scala> deltaLog.snapshot.tombstones.show(false) +----+-----------------+----------+ |path|deletionTimestamp|dataChange| +----+-----------------+----------+ +----+-----------------+----------+","title":" All RemoveFile Actions (Tombstones)"},{"location":"Snapshot/#cachedstate","text":"cachedState : CachedDS [ SingleAction ] Scala lazy value cachedState is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val cachedState: CachedDS[SingleAction] cachedState creates a Cached Delta State with the following: The dataset part is the stateReconstruction dataset of SingleAction s The name in the format Delta Table State #version - [redactedPath] (with the version and the path redacted) Used when Snapshot is requested for the state ( Dataset[SingleAction] )","title":" cachedState"},{"location":"SnapshotIterator/","text":"= SnapshotIterator SnapshotIterator is...FIXME == [[iterator]] iterator Method [source, scala] \u00b6 iterator(): Iterator[IndexedFile] \u00b6 iterator ...FIXME NOTE: iterator is used exclusively when DeltaSource is requested for the < >.","title":"SnapshotIterator"},{"location":"SnapshotIterator/#source-scala","text":"","title":"[source, scala]"},{"location":"SnapshotIterator/#iterator-iteratorindexedfile","text":"iterator ...FIXME NOTE: iterator is used exclusively when DeltaSource is requested for the < >.","title":"iterator(): Iterator[IndexedFile]"},{"location":"SnapshotManagement/","text":"SnapshotManagement \u00b6 SnapshotManagement is an extension for DeltaLog to manage Snapshot s. Demo \u00b6 val name = \"employees\" val dataPath = s\"/tmp/delta/$name\" sql(s\"DROP TABLE $name\") sql(s\"\"\" | CREATE TABLE $name (id bigint, name string, city string) | USING delta | OPTIONS (path='$dataPath') \"\"\".stripMargin) import org.apache.spark.sql.delta.DeltaLog val log = DeltaLog.forTable(spark, dataPath) import org.apache.spark.sql.delta.SnapshotManagement assert(log.isInstanceOf[SnapshotManagement], \"DeltaLog is a SnapshotManagement\") val snapshot = log.update(stalenessAcceptable = false) scala> :type snapshot org.apache.spark.sql.delta.Snapshot assert(snapshot.version == 0) Current Snapshot \u00b6 currentSnapshot : Snapshot currentSnapshot is a registry with the current Snapshot of a Delta table. When DeltaLog is created, currentSnapshot is initialized as getSnapshotAtInit and changed every update . currentSnapshot ...FIXME currentSnapshot is used when: DeltaLog is requested to isValid SnapshotManagement is requested to...FIXME Updating Current Snapshot \u00b6 update ( stalenessAcceptable : Boolean = false ) : Snapshot update ...FIXME update is used when: DeltaLog is requested to start a transaction and withNewTransaction OptimisticTransactionImpl is requested to doCommit and getNextAttemptVersion DeltaTableV2 is requested for a Snapshot TahoeLogFileIndex is requested for a Snapshot DeltaHistoryManager is requested to getHistory , getActiveCommitAtTime , checkVersionExists In Delta commands ... tryUpdate \u00b6 tryUpdate ( isAsync : Boolean = false ) : Snapshot tryUpdate ...FIXME tryUpdate is used when SnapshotManagement is requested to update . updateInternal \u00b6 updateInternal ( isAsync : Boolean ) : Snapshot updateInternal ...FIXME updateInternal is used when SnapshotManagement is requested to update (and tryUpdate ). Loading Latest Snapshot \u00b6 getSnapshotAtInit : Snapshot getSnapshotAtInit getLogSegmentFrom for the last checkpoint . getSnapshotAtInit prints out the following INFO message to the logs: Loading version [version][startCheckpoint] getSnapshotAtInit creates a Snapshot for the log segment. getSnapshotAtInit records the current time in lastUpdateTimestamp registry. getSnapshotAtInit prints out the following INFO message to the logs: Returning initial snapshot [snapshot] getSnapshotAtInit is used when SnapshotManagement is created (and initializes the currentSnapshot registry). getLogSegmentFrom \u00b6 getLogSegmentFrom ( startingCheckpoint : Option [ CheckpointMetaData ]) : LogSegment getLogSegmentFrom getLogSegmentForVersion for the version of the given CheckpointMetaData (if specified) as a start checkpoint version or leaves it undefined. getLogSegmentFrom is used when SnapshotManagement is requested for getSnapshotAtInit . getLogSegmentForVersion \u00b6 getLogSegmentForVersion ( startCheckpoint : Option [ Long ], versionToLoad : Option [ Long ] = None ) : LogSegment getLogSegmentForVersion ...FIXME getLogSegmentForVersion is used when SnapshotManagement is requested for getLogSegmentFrom , updateInternal and getSnapshotAt . Creating Snapshot \u00b6 createSnapshot ( segment : LogSegment , minFileRetentionTimestamp : Long , timestamp : Long ) : Snapshot createSnapshot readChecksum (for the version of the given LogSegment ) and creates a Snapshot . createSnapshot is used when SnapshotManagement is requested for getSnapshotAtInit , getSnapshotAt and update . Last Successful Update Timestamp \u00b6 SnapshotManagement uses lastUpdateTimestamp internal registry for the timestamp of the last successful update.","title":"SnapshotManagement"},{"location":"SnapshotManagement/#snapshotmanagement","text":"SnapshotManagement is an extension for DeltaLog to manage Snapshot s.","title":"SnapshotManagement"},{"location":"SnapshotManagement/#demo","text":"val name = \"employees\" val dataPath = s\"/tmp/delta/$name\" sql(s\"DROP TABLE $name\") sql(s\"\"\" | CREATE TABLE $name (id bigint, name string, city string) | USING delta | OPTIONS (path='$dataPath') \"\"\".stripMargin) import org.apache.spark.sql.delta.DeltaLog val log = DeltaLog.forTable(spark, dataPath) import org.apache.spark.sql.delta.SnapshotManagement assert(log.isInstanceOf[SnapshotManagement], \"DeltaLog is a SnapshotManagement\") val snapshot = log.update(stalenessAcceptable = false) scala> :type snapshot org.apache.spark.sql.delta.Snapshot assert(snapshot.version == 0)","title":"Demo"},{"location":"SnapshotManagement/#current-snapshot","text":"currentSnapshot : Snapshot currentSnapshot is a registry with the current Snapshot of a Delta table. When DeltaLog is created, currentSnapshot is initialized as getSnapshotAtInit and changed every update . currentSnapshot ...FIXME currentSnapshot is used when: DeltaLog is requested to isValid SnapshotManagement is requested to...FIXME","title":" Current Snapshot"},{"location":"SnapshotManagement/#updating-current-snapshot","text":"update ( stalenessAcceptable : Boolean = false ) : Snapshot update ...FIXME update is used when: DeltaLog is requested to start a transaction and withNewTransaction OptimisticTransactionImpl is requested to doCommit and getNextAttemptVersion DeltaTableV2 is requested for a Snapshot TahoeLogFileIndex is requested for a Snapshot DeltaHistoryManager is requested to getHistory , getActiveCommitAtTime , checkVersionExists In Delta commands ...","title":" Updating Current Snapshot"},{"location":"SnapshotManagement/#tryupdate","text":"tryUpdate ( isAsync : Boolean = false ) : Snapshot tryUpdate ...FIXME tryUpdate is used when SnapshotManagement is requested to update .","title":" tryUpdate"},{"location":"SnapshotManagement/#updateinternal","text":"updateInternal ( isAsync : Boolean ) : Snapshot updateInternal ...FIXME updateInternal is used when SnapshotManagement is requested to update (and tryUpdate ).","title":" updateInternal"},{"location":"SnapshotManagement/#loading-latest-snapshot","text":"getSnapshotAtInit : Snapshot getSnapshotAtInit getLogSegmentFrom for the last checkpoint . getSnapshotAtInit prints out the following INFO message to the logs: Loading version [version][startCheckpoint] getSnapshotAtInit creates a Snapshot for the log segment. getSnapshotAtInit records the current time in lastUpdateTimestamp registry. getSnapshotAtInit prints out the following INFO message to the logs: Returning initial snapshot [snapshot] getSnapshotAtInit is used when SnapshotManagement is created (and initializes the currentSnapshot registry).","title":" Loading Latest Snapshot"},{"location":"SnapshotManagement/#getlogsegmentfrom","text":"getLogSegmentFrom ( startingCheckpoint : Option [ CheckpointMetaData ]) : LogSegment getLogSegmentFrom getLogSegmentForVersion for the version of the given CheckpointMetaData (if specified) as a start checkpoint version or leaves it undefined. getLogSegmentFrom is used when SnapshotManagement is requested for getSnapshotAtInit .","title":" getLogSegmentFrom"},{"location":"SnapshotManagement/#getlogsegmentforversion","text":"getLogSegmentForVersion ( startCheckpoint : Option [ Long ], versionToLoad : Option [ Long ] = None ) : LogSegment getLogSegmentForVersion ...FIXME getLogSegmentForVersion is used when SnapshotManagement is requested for getLogSegmentFrom , updateInternal and getSnapshotAt .","title":" getLogSegmentForVersion"},{"location":"SnapshotManagement/#creating-snapshot","text":"createSnapshot ( segment : LogSegment , minFileRetentionTimestamp : Long , timestamp : Long ) : Snapshot createSnapshot readChecksum (for the version of the given LogSegment ) and creates a Snapshot . createSnapshot is used when SnapshotManagement is requested for getSnapshotAtInit , getSnapshotAt and update .","title":" Creating Snapshot"},{"location":"SnapshotManagement/#last-successful-update-timestamp","text":"SnapshotManagement uses lastUpdateTimestamp internal registry for the timestamp of the last successful update.","title":" Last Successful Update Timestamp"},{"location":"StagedDeltaTableV2/","text":"StagedDeltaTableV2 \u00b6 StagedDeltaTableV2 is a StagedTable ( Spark SQL ) and a SupportsWrite ( Spark SQL ). Creating Instance \u00b6 StagedDeltaTableV2 takes the following to be created: Identifier Schema Partitions ( Array[Transform] ) Properties Operation (one of Create , CreateOrReplace , Replace ) StagedDeltaTableV2 is created when DeltaCatalog is requested to stageReplace , stageCreateOrReplace or stageCreate . commitStagedChanges \u00b6 commitStagedChanges () : Unit commitStagedChanges ...FIXME commitStagedChanges is part of the StagedTable ( Spark SQL ) abstraction. abortStagedChanges \u00b6 abortStagedChanges () : Unit abortStagedChanges does nothing. abortStagedChanges is part of the StagedTable ( Spark SQL ) abstraction. Creating WriteBuilder \u00b6 newWriteBuilder ( info : LogicalWriteInfo ) : V1WriteBuilder newWriteBuilder ...FIXME newWriteBuilder is part of the SupportsWrite ( Spark SQL ) abstraction.","title":"StagedDeltaTableV2"},{"location":"StagedDeltaTableV2/#stageddeltatablev2","text":"StagedDeltaTableV2 is a StagedTable ( Spark SQL ) and a SupportsWrite ( Spark SQL ).","title":"StagedDeltaTableV2"},{"location":"StagedDeltaTableV2/#creating-instance","text":"StagedDeltaTableV2 takes the following to be created: Identifier Schema Partitions ( Array[Transform] ) Properties Operation (one of Create , CreateOrReplace , Replace ) StagedDeltaTableV2 is created when DeltaCatalog is requested to stageReplace , stageCreateOrReplace or stageCreate .","title":"Creating Instance"},{"location":"StagedDeltaTableV2/#commitstagedchanges","text":"commitStagedChanges () : Unit commitStagedChanges ...FIXME commitStagedChanges is part of the StagedTable ( Spark SQL ) abstraction.","title":" commitStagedChanges"},{"location":"StagedDeltaTableV2/#abortstagedchanges","text":"abortStagedChanges () : Unit abortStagedChanges does nothing. abortStagedChanges is part of the StagedTable ( Spark SQL ) abstraction.","title":" abortStagedChanges"},{"location":"StagedDeltaTableV2/#creating-writebuilder","text":"newWriteBuilder ( info : LogicalWriteInfo ) : V1WriteBuilder newWriteBuilder ...FIXME newWriteBuilder is part of the SupportsWrite ( Spark SQL ) abstraction.","title":" Creating WriteBuilder"},{"location":"StateCache/","text":"StateCache \u00b6 StateCache is an abstraction of state caches that can cache a Dataset and uncache them all . Contract \u00b6 SparkSession \u00b6 spark : SparkSession SparkSession the cached RDDs belong to Implementations \u00b6 DeltaSourceSnapshot Snapshot Cached RDDs \u00b6 cached : ArrayBuffer [ RDD [ _ ]] StateCache tracks cached RDDs in cached internal registry. cached is given a new RDD when StateCache is requested to cache a Dataset . cached is used when StateCache is requested to get a cached Dataset and uncache . Caching Dataset \u00b6 cacheDS [ A ]( ds : Dataset [ A ], name : String ) : CachedDS [ A ] cacheDS creates a new CachedDS . cacheDS is used when: Snapshot is requested for a cached state ) DeltaSourceSnapshot is requested to initialFiles Uncaching All Cached Datasets \u00b6 uncache [ A ]( ds : Dataset [ A ], name : String ) : CachedDS [ A ] uncache uses the isCached internal flag to avoid multiple executions. uncache is used when: DeltaLog utility is used to access deltaLogCache and a cached entry expires SnapshotManagement is requested to update state of a Delta table DeltaSourceSnapshot is requested to close","title":"StateCache"},{"location":"StateCache/#statecache","text":"StateCache is an abstraction of state caches that can cache a Dataset and uncache them all .","title":"StateCache"},{"location":"StateCache/#contract","text":"","title":"Contract"},{"location":"StateCache/#sparksession","text":"spark : SparkSession SparkSession the cached RDDs belong to","title":" SparkSession"},{"location":"StateCache/#implementations","text":"DeltaSourceSnapshot Snapshot","title":"Implementations"},{"location":"StateCache/#cached-rdds","text":"cached : ArrayBuffer [ RDD [ _ ]] StateCache tracks cached RDDs in cached internal registry. cached is given a new RDD when StateCache is requested to cache a Dataset . cached is used when StateCache is requested to get a cached Dataset and uncache .","title":" Cached RDDs"},{"location":"StateCache/#caching-dataset","text":"cacheDS [ A ]( ds : Dataset [ A ], name : String ) : CachedDS [ A ] cacheDS creates a new CachedDS . cacheDS is used when: Snapshot is requested for a cached state ) DeltaSourceSnapshot is requested to initialFiles","title":" Caching Dataset"},{"location":"StateCache/#uncaching-all-cached-datasets","text":"uncache [ A ]( ds : Dataset [ A ], name : String ) : CachedDS [ A ] uncache uses the isCached internal flag to avoid multiple executions. uncache is used when: DeltaLog utility is used to access deltaLogCache and a cached entry expires SnapshotManagement is requested to update state of a Delta table DeltaSourceSnapshot is requested to close","title":" Uncaching All Cached Datasets"},{"location":"TahoeBatchFileIndex/","text":"TahoeBatchFileIndex \u00b6 TahoeBatchFileIndex is a file index of a delta table at a given version . Creating Instance \u00b6 TahoeBatchFileIndex takes the following to be created: SparkSession Action Type AddFile s DeltaLog Data directory (as Hadoop Path ) Snapshot TahoeBatchFileIndex is created when: DeltaLog is requested for a DataFrame for given AddFiles DeleteCommand and UpdateCommand are executed (and DeltaCommand is requested for a HadoopFsRelation ) Action Type \u00b6 TahoeBatchFileIndex is given an Action Type identifier when created : batch or streaming when DeltaLog is requested for a batch or streaming DataFrame for given AddFiles , respectively delete for DeleteCommand update for UpdateCommand Important Action Type seems not to be used ever. tableVersion \u00b6 tableVersion : Long tableVersion is always the version of the Snapshot . tableVersion is part of the TahoeFileIndex abstraction. matchingFiles \u00b6 matchingFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ], keepStats : Boolean = false ) : Seq [ AddFile ] matchingFiles filterFileList (that gives a DataFrame ) and collects the AddFile s (using Dataset.collect ). matchingFiles is part of the TahoeFileIndex abstraction. Input Files \u00b6 inputFiles : Array [ String ] inputFiles returns the paths of all the given AddFiles . inputFiles is part of the FileIndex abstraction ( Spark SQL ). Partitions \u00b6 partitionSchema : StructType partitionSchema requests the Snapshot for the metadata that is in turn requested for the partitionSchema . partitionSchema is part of the FileIndex abstraction ( Spark SQL ). Estimated Size of Relation \u00b6 sizeInBytes : Long sizeInBytes is a sum of the sizes of all the given AddFiles . sizeInBytes is part of the FileIndex abstraction ( Spark SQL ).","title":"TahoeBatchFileIndex"},{"location":"TahoeBatchFileIndex/#tahoebatchfileindex","text":"TahoeBatchFileIndex is a file index of a delta table at a given version .","title":"TahoeBatchFileIndex"},{"location":"TahoeBatchFileIndex/#creating-instance","text":"TahoeBatchFileIndex takes the following to be created: SparkSession Action Type AddFile s DeltaLog Data directory (as Hadoop Path ) Snapshot TahoeBatchFileIndex is created when: DeltaLog is requested for a DataFrame for given AddFiles DeleteCommand and UpdateCommand are executed (and DeltaCommand is requested for a HadoopFsRelation )","title":"Creating Instance"},{"location":"TahoeBatchFileIndex/#action-type","text":"TahoeBatchFileIndex is given an Action Type identifier when created : batch or streaming when DeltaLog is requested for a batch or streaming DataFrame for given AddFiles , respectively delete for DeleteCommand update for UpdateCommand Important Action Type seems not to be used ever.","title":" Action Type"},{"location":"TahoeBatchFileIndex/#tableversion","text":"tableVersion : Long tableVersion is always the version of the Snapshot . tableVersion is part of the TahoeFileIndex abstraction.","title":" tableVersion"},{"location":"TahoeBatchFileIndex/#matchingfiles","text":"matchingFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ], keepStats : Boolean = false ) : Seq [ AddFile ] matchingFiles filterFileList (that gives a DataFrame ) and collects the AddFile s (using Dataset.collect ). matchingFiles is part of the TahoeFileIndex abstraction.","title":" matchingFiles"},{"location":"TahoeBatchFileIndex/#input-files","text":"inputFiles : Array [ String ] inputFiles returns the paths of all the given AddFiles . inputFiles is part of the FileIndex abstraction ( Spark SQL ).","title":" Input Files"},{"location":"TahoeBatchFileIndex/#partitions","text":"partitionSchema : StructType partitionSchema requests the Snapshot for the metadata that is in turn requested for the partitionSchema . partitionSchema is part of the FileIndex abstraction ( Spark SQL ).","title":" Partitions"},{"location":"TahoeBatchFileIndex/#estimated-size-of-relation","text":"sizeInBytes : Long sizeInBytes is a sum of the sizes of all the given AddFiles . sizeInBytes is part of the FileIndex abstraction ( Spark SQL ).","title":" Estimated Size of Relation"},{"location":"TahoeFileIndex/","text":"TahoeFileIndex \u00b6 TahoeFileIndex is an extension of the FileIndex abstraction ( Spark SQL ) for file indices of delta tables that can list data files to scan (based on partition and data filters ). The aim of TahoeFileIndex (and FileIndex in general) is to reduce usage of very expensive disk access for file-related information using Hadoop FileSystem API. Contract \u00b6 matchingFiles \u00b6 matchingFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ]) : Seq [ AddFile ] AddFile s matching given partition and data filters (predicates) Used for listing data files Implementations \u00b6 TahoeBatchFileIndex TahoeLogFileIndex Creating Instance \u00b6 TahoeFileIndex takes the following to be created: SparkSession DeltaLog Hadoop Path Abstract Class TahoeFileIndex is an abstract class and cannot be created directly. It is created indirectly for the concrete TahoeFileIndexes . Root Paths \u00b6 rootPaths : Seq [ Path ] rootPaths is the path only. rootPaths is part of the FileIndex abstraction ( Spark SQL ). Listing Files \u00b6 listFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ]) : Seq [ PartitionDirectory ] listFiles is the path only. listFiles is part of the FileIndex abstraction ( Spark SQL ). Partitions \u00b6 partitionSchema : StructType partitionSchema is the partition schema of (the Metadata of the Snapshot ) of the DeltaLog . partitionSchema is part of the FileIndex abstraction ( Spark SQL ). Version of Delta Table \u00b6 tableVersion : Long tableVersion is the version of (the snapshot of) the DeltaLog . tableVersion is used when TahoeFileIndex is requested for the human-friendly textual representation . Textual Representation \u00b6 toString : String toString returns the following text (using the version and the path of the Delta table): Delta[version=[tableVersion], [truncatedPath]] toString is part of the java.lang.Object contract for a string representation of the object.","title":"TahoeFileIndex"},{"location":"TahoeFileIndex/#tahoefileindex","text":"TahoeFileIndex is an extension of the FileIndex abstraction ( Spark SQL ) for file indices of delta tables that can list data files to scan (based on partition and data filters ). The aim of TahoeFileIndex (and FileIndex in general) is to reduce usage of very expensive disk access for file-related information using Hadoop FileSystem API.","title":"TahoeFileIndex"},{"location":"TahoeFileIndex/#contract","text":"","title":"Contract"},{"location":"TahoeFileIndex/#matchingfiles","text":"matchingFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ]) : Seq [ AddFile ] AddFile s matching given partition and data filters (predicates) Used for listing data files","title":" matchingFiles"},{"location":"TahoeFileIndex/#implementations","text":"TahoeBatchFileIndex TahoeLogFileIndex","title":"Implementations"},{"location":"TahoeFileIndex/#creating-instance","text":"TahoeFileIndex takes the following to be created: SparkSession DeltaLog Hadoop Path Abstract Class TahoeFileIndex is an abstract class and cannot be created directly. It is created indirectly for the concrete TahoeFileIndexes .","title":"Creating Instance"},{"location":"TahoeFileIndex/#root-paths","text":"rootPaths : Seq [ Path ] rootPaths is the path only. rootPaths is part of the FileIndex abstraction ( Spark SQL ).","title":" Root Paths"},{"location":"TahoeFileIndex/#listing-files","text":"listFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ]) : Seq [ PartitionDirectory ] listFiles is the path only. listFiles is part of the FileIndex abstraction ( Spark SQL ).","title":" Listing Files"},{"location":"TahoeFileIndex/#partitions","text":"partitionSchema : StructType partitionSchema is the partition schema of (the Metadata of the Snapshot ) of the DeltaLog . partitionSchema is part of the FileIndex abstraction ( Spark SQL ).","title":" Partitions"},{"location":"TahoeFileIndex/#version-of-delta-table","text":"tableVersion : Long tableVersion is the version of (the snapshot of) the DeltaLog . tableVersion is used when TahoeFileIndex is requested for the human-friendly textual representation .","title":" Version of Delta Table"},{"location":"TahoeFileIndex/#textual-representation","text":"toString : String toString returns the following text (using the version and the path of the Delta table): Delta[version=[tableVersion], [truncatedPath]] toString is part of the java.lang.Object contract for a string representation of the object.","title":" Textual Representation"},{"location":"TahoeLogFileIndex/","text":"TahoeLogFileIndex \u00b6 TahoeLogFileIndex is a file index . Creating Instance \u00b6 TahoeLogFileIndex takes the following to be created: SparkSession DeltaLog Data directory of the Delta table (as a Hadoop Path ) Schema at analysis ( StructType ) Catalyst Expressions for the partition filters (default: empty ) Snapshot version (default: undefined ) ( Option[Long] ) TahoeLogFileIndex is created when DeltaLog is requested for an Insertable HadoopFsRelation . Demo \u00b6 val q = spark.read.format(\"delta\").load(\"/tmp/delta/users\") val plan = q.queryExecution.executedPlan import org.apache.spark.sql.execution.FileSourceScanExec val scan = plan.collect { case e: FileSourceScanExec => e }.head import org.apache.spark.sql.delta.files.TahoeLogFileIndex val index = scan.relation.location.asInstanceOf[TahoeLogFileIndex] scala> println(index) Delta[version=1, file:/tmp/delta/users] matchingFiles Method \u00b6 matchingFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ], keepStats : Boolean = false ) : Seq [ AddFile ] matchingFiles gets the snapshot (with stalenessAcceptable flag off) and requests it for the files to scan (for the index's partition filters , the given partitionFilters and dataFilters ). Note inputFiles and matchingFiles are similar. Both get the snapshot (of the delta table), but they use different filtering expressions and return value types. matchingFiles is part of the TahoeFileIndex abstraction. inputFiles Method \u00b6 inputFiles : Array [ String ] inputFiles gets the snapshot (with stalenessAcceptable flag off) and requests it for the files to scan (for the index's partition filters only). Note inputFiles and matchingFiles are similar. Both get the snapshot , but they use different filtering expressions and return value types. inputFiles is part of the FileIndex contract (Spark SQL). Historical Or Latest Snapshot \u00b6 getSnapshot ( stalenessAcceptable : Boolean ) : Snapshot getSnapshot returns a Snapshot that is either the historical snapshot (for the snapshot version if specified) or requests the DeltaLog to update (and give one). getSnapshot is used when TahoeLogFileIndex is requested for the matching files and the input files . Internal Properties \u00b6 historicalSnapshotOpt \u00b6 Historical snapshot that is the Snapshot for the versionToUse if defined. Used when TahoeLogFileIndex is requested for the (historical or latest) snapshot and the schema of the partition columns","title":"TahoeLogFileIndex"},{"location":"TahoeLogFileIndex/#tahoelogfileindex","text":"TahoeLogFileIndex is a file index .","title":"TahoeLogFileIndex"},{"location":"TahoeLogFileIndex/#creating-instance","text":"TahoeLogFileIndex takes the following to be created: SparkSession DeltaLog Data directory of the Delta table (as a Hadoop Path ) Schema at analysis ( StructType ) Catalyst Expressions for the partition filters (default: empty ) Snapshot version (default: undefined ) ( Option[Long] ) TahoeLogFileIndex is created when DeltaLog is requested for an Insertable HadoopFsRelation .","title":"Creating Instance"},{"location":"TahoeLogFileIndex/#demo","text":"val q = spark.read.format(\"delta\").load(\"/tmp/delta/users\") val plan = q.queryExecution.executedPlan import org.apache.spark.sql.execution.FileSourceScanExec val scan = plan.collect { case e: FileSourceScanExec => e }.head import org.apache.spark.sql.delta.files.TahoeLogFileIndex val index = scan.relation.location.asInstanceOf[TahoeLogFileIndex] scala> println(index) Delta[version=1, file:/tmp/delta/users]","title":"Demo"},{"location":"TahoeLogFileIndex/#matchingfiles-method","text":"matchingFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ], keepStats : Boolean = false ) : Seq [ AddFile ] matchingFiles gets the snapshot (with stalenessAcceptable flag off) and requests it for the files to scan (for the index's partition filters , the given partitionFilters and dataFilters ). Note inputFiles and matchingFiles are similar. Both get the snapshot (of the delta table), but they use different filtering expressions and return value types. matchingFiles is part of the TahoeFileIndex abstraction.","title":" matchingFiles Method"},{"location":"TahoeLogFileIndex/#inputfiles-method","text":"inputFiles : Array [ String ] inputFiles gets the snapshot (with stalenessAcceptable flag off) and requests it for the files to scan (for the index's partition filters only). Note inputFiles and matchingFiles are similar. Both get the snapshot , but they use different filtering expressions and return value types. inputFiles is part of the FileIndex contract (Spark SQL).","title":" inputFiles Method"},{"location":"TahoeLogFileIndex/#historical-or-latest-snapshot","text":"getSnapshot ( stalenessAcceptable : Boolean ) : Snapshot getSnapshot returns a Snapshot that is either the historical snapshot (for the snapshot version if specified) or requests the DeltaLog to update (and give one). getSnapshot is used when TahoeLogFileIndex is requested for the matching files and the input files .","title":" Historical Or Latest Snapshot"},{"location":"TahoeLogFileIndex/#internal-properties","text":"","title":"Internal Properties"},{"location":"TahoeLogFileIndex/#historicalsnapshotopt","text":"Historical snapshot that is the Snapshot for the versionToUse if defined. Used when TahoeLogFileIndex is requested for the (historical or latest) snapshot and the schema of the partition columns","title":" historicalSnapshotOpt"},{"location":"TransactionalWrite/","text":"TransactionalWrite \u00b6 TransactionalWrite is an < > of < > that can < > to a < >. == [[contract]] Contract === [[deltaLog]] deltaLog [source,scala] \u00b6 deltaLog: DeltaLog \u00b6 DeltaLog.md[] (of a delta table) that this transaction is changing Used when: OptimisticTransactionImpl is requested to < >, < > (after < >), < >, and < > (and execute < >) < > and < > are executed DeltaCommand is requested to < > DeltaLog is requested to < > TransactionalWrite is requested to < > === [[metadata]] metadata [source, scala] \u00b6 metadata: Metadata \u00b6 Metadata.md[] (of the < >) that this transaction is changing === [[protocol]] protocol [source, scala] \u00b6 protocol: Protocol \u00b6 Protocol.md[] (of the < >) that this transaction is changing Used when AlterTableSetPropertiesDeltaCommand.md[] is executed (to DeltaConfigs.md#verifyProtocolVersionRequirements[verifyProtocolVersionRequirements]) === [[snapshot]] snapshot [source, scala] \u00b6 snapshot: Snapshot \u00b6 Snapshot.md[] (of the < >) that this transaction is < > == [[implementations]][[self]] Implementations OptimisticTransaction.md[] is the default and only known TransactionalWrite in Delta Lake (indirectly as a OptimisticTransactionImpl.md[]). Writing Data Out (Result Of Structured Query) \u00b6 writeFiles ( data : Dataset [ _ ]) : Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], writeOptions : Option [ DeltaOptions ]) : Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], isOptimize : Boolean ) : Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], writeOptions : Option [ DeltaOptions ], isOptimize : Boolean ) : Seq [ AddFile ] writeFiles creates a DeltaInvariantCheckerExec and a DelayedCommitProtocol to write out files to the data path (of the DeltaLog ). Note writeFiles uses Spark SQL's FileFormatWriter utility to write out a result of a streaming query. Read up on FileFormatWriter in The Internals of Spark SQL online book. writeFiles is executed within SQLExecution.withNewExecutionId . Note writeFiles can be tracked using web UI or SQLAppStatusListener (using SparkListenerSQLExecutionStart and SparkListenerSQLExecutionEnd events). In the end, writeFiles returns the addedStatuses of the DelayedCommitProtocol committer. Internally, writeFiles turns the hasWritten flag on ( true ). NOTE: After writeFiles , no metadata updates in the transaction are permitted. writeFiles normalize the given data dataset (based on the partitionColumns of the Metadata ). writeFiles getPartitioningColumns based on the partitionSchema of the Metadata . writeFiles creates a DelayedCommitProtocol committer for the data path of the DeltaLog . writeFiles gets the invariants from the schema of the Metadata . writeFiles requests a new Execution ID (that is used to track all Spark jobs of FileFormatWriter.write in Spark SQL) with a physical query plan of a new DeltaInvariantCheckerExec unary physical operator (with the executed plan of the normalized query execution as the child operator). writeFiles is used when: DeleteCommand , MergeIntoCommand , UpdateCommand , and WriteIntoDelta commands are executed DeltaSink is requested to add a streaming micro-batch == [[getCommitter]] Creating Committer [source, scala] \u00b6 getCommitter( outputPath: Path): DelayedCommitProtocol getCommitter creates a new < > with the delta job ID and the given outputPath (and no random prefix). getCommitter is used when TransactionalWrite is requested to < >. == [[makeOutputNullable]] makeOutputNullable Method [source, scala] \u00b6 makeOutputNullable( output: Seq[Attribute]): Seq[Attribute] makeOutputNullable...FIXME makeOutputNullable is used when...FIXME == [[normalizeData]] normalizeData Method [source, scala] \u00b6 normalizeData( data: Dataset[_], partitionCols: Seq[String]): (QueryExecution, Seq[Attribute]) normalizeData...FIXME normalizeData is used when...FIXME == [[getPartitioningColumns]] getPartitioningColumns Method [source, scala] \u00b6 getPartitioningColumns( partitionSchema: StructType, output: Seq[Attribute], colsDropped: Boolean): Seq[Attribute] getPartitioningColumns...FIXME getPartitioningColumns is used when...FIXME == [[hasWritten]] hasWritten Flag [source, scala] \u00b6 hasWritten: Boolean = false \u00b6 TransactionalWrite uses the hasWritten internal registry to prevent OptimisticTransactionImpl from < > after < >. hasWritten is initially turned off ( false ). It can be turned on ( true ) when TransactionalWrite is requested to < >.","title":"TransactionalWrite"},{"location":"TransactionalWrite/#transactionalwrite","text":"TransactionalWrite is an < > of < > that can < > to a < >. == [[contract]] Contract === [[deltaLog]] deltaLog","title":"TransactionalWrite"},{"location":"TransactionalWrite/#sourcescala","text":"","title":"[source,scala]"},{"location":"TransactionalWrite/#deltalog-deltalog","text":"DeltaLog.md[] (of a delta table) that this transaction is changing Used when: OptimisticTransactionImpl is requested to < >, < > (after < >), < >, and < > (and execute < >) < > and < > are executed DeltaCommand is requested to < > DeltaLog is requested to < > TransactionalWrite is requested to < > === [[metadata]] metadata","title":"deltaLog: DeltaLog"},{"location":"TransactionalWrite/#source-scala","text":"","title":"[source, scala]"},{"location":"TransactionalWrite/#metadata-metadata","text":"Metadata.md[] (of the < >) that this transaction is changing === [[protocol]] protocol","title":"metadata: Metadata"},{"location":"TransactionalWrite/#source-scala_1","text":"","title":"[source, scala]"},{"location":"TransactionalWrite/#protocol-protocol","text":"Protocol.md[] (of the < >) that this transaction is changing Used when AlterTableSetPropertiesDeltaCommand.md[] is executed (to DeltaConfigs.md#verifyProtocolVersionRequirements[verifyProtocolVersionRequirements]) === [[snapshot]] snapshot","title":"protocol: Protocol"},{"location":"TransactionalWrite/#source-scala_2","text":"","title":"[source, scala]"},{"location":"TransactionalWrite/#snapshot-snapshot","text":"Snapshot.md[] (of the < >) that this transaction is < > == [[implementations]][[self]] Implementations OptimisticTransaction.md[] is the default and only known TransactionalWrite in Delta Lake (indirectly as a OptimisticTransactionImpl.md[]).","title":"snapshot: Snapshot"},{"location":"TransactionalWrite/#writing-data-out-result-of-structured-query","text":"writeFiles ( data : Dataset [ _ ]) : Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], writeOptions : Option [ DeltaOptions ]) : Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], isOptimize : Boolean ) : Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], writeOptions : Option [ DeltaOptions ], isOptimize : Boolean ) : Seq [ AddFile ] writeFiles creates a DeltaInvariantCheckerExec and a DelayedCommitProtocol to write out files to the data path (of the DeltaLog ). Note writeFiles uses Spark SQL's FileFormatWriter utility to write out a result of a streaming query. Read up on FileFormatWriter in The Internals of Spark SQL online book. writeFiles is executed within SQLExecution.withNewExecutionId . Note writeFiles can be tracked using web UI or SQLAppStatusListener (using SparkListenerSQLExecutionStart and SparkListenerSQLExecutionEnd events). In the end, writeFiles returns the addedStatuses of the DelayedCommitProtocol committer. Internally, writeFiles turns the hasWritten flag on ( true ). NOTE: After writeFiles , no metadata updates in the transaction are permitted. writeFiles normalize the given data dataset (based on the partitionColumns of the Metadata ). writeFiles getPartitioningColumns based on the partitionSchema of the Metadata . writeFiles creates a DelayedCommitProtocol committer for the data path of the DeltaLog . writeFiles gets the invariants from the schema of the Metadata . writeFiles requests a new Execution ID (that is used to track all Spark jobs of FileFormatWriter.write in Spark SQL) with a physical query plan of a new DeltaInvariantCheckerExec unary physical operator (with the executed plan of the normalized query execution as the child operator). writeFiles is used when: DeleteCommand , MergeIntoCommand , UpdateCommand , and WriteIntoDelta commands are executed DeltaSink is requested to add a streaming micro-batch == [[getCommitter]] Creating Committer","title":" Writing Data Out (Result Of Structured Query)"},{"location":"TransactionalWrite/#source-scala_3","text":"getCommitter( outputPath: Path): DelayedCommitProtocol getCommitter creates a new < > with the delta job ID and the given outputPath (and no random prefix). getCommitter is used when TransactionalWrite is requested to < >. == [[makeOutputNullable]] makeOutputNullable Method","title":"[source, scala]"},{"location":"TransactionalWrite/#source-scala_4","text":"makeOutputNullable( output: Seq[Attribute]): Seq[Attribute] makeOutputNullable...FIXME makeOutputNullable is used when...FIXME == [[normalizeData]] normalizeData Method","title":"[source, scala]"},{"location":"TransactionalWrite/#source-scala_5","text":"normalizeData( data: Dataset[_], partitionCols: Seq[String]): (QueryExecution, Seq[Attribute]) normalizeData...FIXME normalizeData is used when...FIXME == [[getPartitioningColumns]] getPartitioningColumns Method","title":"[source, scala]"},{"location":"TransactionalWrite/#source-scala_6","text":"getPartitioningColumns( partitionSchema: StructType, output: Seq[Attribute], colsDropped: Boolean): Seq[Attribute] getPartitioningColumns...FIXME getPartitioningColumns is used when...FIXME == [[hasWritten]] hasWritten Flag","title":"[source, scala]"},{"location":"TransactionalWrite/#source-scala_7","text":"","title":"[source, scala]"},{"location":"TransactionalWrite/#haswritten-boolean-false","text":"TransactionalWrite uses the hasWritten internal registry to prevent OptimisticTransactionImpl from < > after < >. hasWritten is initially turned off ( false ). It can be turned on ( true ) when TransactionalWrite is requested to < >.","title":"hasWritten: Boolean = false"},{"location":"VerifyChecksum/","text":"= VerifyChecksum VerifyChecksum is...FIXME == [[validateChecksum]] validateChecksum Method [source, scala] \u00b6 validateChecksum(snapshot: Snapshot): Unit \u00b6 validateChecksum ...FIXME NOTE: validateChecksum is used when...FIXME","title":"VerifyChecksum"},{"location":"VerifyChecksum/#source-scala","text":"","title":"[source, scala]"},{"location":"VerifyChecksum/#validatechecksumsnapshot-snapshot-unit","text":"validateChecksum ...FIXME NOTE: validateChecksum is used when...FIXME","title":"validateChecksum(snapshot: Snapshot): Unit"},{"location":"WriteIntoDeltaBuilder/","text":"WriteIntoDeltaBuilder \u00b6 WriteIntoDeltaBuilder is...FIXME","title":"WriteIntoDeltaBuilder"},{"location":"WriteIntoDeltaBuilder/#writeintodeltabuilder","text":"WriteIntoDeltaBuilder is...FIXME","title":"WriteIntoDeltaBuilder"},{"location":"installation/","text":"Installation \u00b6 Delta Lake is a Spark data source and as such installation boils down to using spark-submit's --packages command-line option. Delta Lake also requires DeltaSparkSessionExtension and DeltaCatalog to be registered (using respective configuration properties). Spark SQL Application \u00b6 import org.apache.spark.sql.SparkSession val spark = SparkSession . builder () . appName ( \"...\" ) . config ( \"spark.sql.extensions\" , \"io.delta.sql.DeltaSparkSessionExtension\" ) . config ( \"spark.sql.catalog.spark_catalog\" , \"org.apache.spark.sql.delta.catalog.DeltaCatalog\" ) . getOrCreate Spark Shell \u00b6 ./bin/spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog","title":"Installation"},{"location":"installation/#installation","text":"Delta Lake is a Spark data source and as such installation boils down to using spark-submit's --packages command-line option. Delta Lake also requires DeltaSparkSessionExtension and DeltaCatalog to be registered (using respective configuration properties).","title":"Installation"},{"location":"installation/#spark-sql-application","text":"import org.apache.spark.sql.SparkSession val spark = SparkSession . builder () . appName ( \"...\" ) . config ( \"spark.sql.extensions\" , \"io.delta.sql.DeltaSparkSessionExtension\" ) . config ( \"spark.sql.catalog.spark_catalog\" , \"org.apache.spark.sql.delta.catalog.DeltaCatalog\" ) . getOrCreate","title":" Spark SQL Application"},{"location":"installation/#spark-shell","text":"./bin/spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog","title":" Spark Shell"},{"location":"overview/","text":"Overview \u00b6 Delta Lake is an open-source storage management system (storage layer) that brings ACID transactions and time travel to Apache Spark and big data workloads. Important As of 0.7.0 Delta Lake requires Spark 3 (starting from the first 3.0.0 release). Delta Lake is a table format. It introduces DeltaTable abstraction that is simply a parquet table with a transactional log . Changes to (the state of) a delta table are reflected as actions and persisted to the transactional log (in JSON format ). Delta Lake uses OptimisticTransaction for transactional writes . A commit is successful when the transaction can write the actions to a delta file (in the transactional log ). In case the delta file for the commit version already exists, the transaction is retried . Structured queries can write (transactionally) to a delta table using the following interfaces: WriteIntoDelta command for batch queries (Spark SQL) DeltaSink for streaming queries (Spark Structured Streaming) More importantly, multiple queries can write to the same delta table simultaneously (at exactly the same time). Delta Lake provides DeltaTable API to programmatically access Delta tables. A delta table can be created based on a parquet table or from scratch . Delta Lake supports batch and streaming queries (Spark SQL and Structured Streaming, respectively) using delta format. In order to fine tune queries over data in Delta Lake use options . Among the options path option is mandatory. Delta Lake supports reading and writing in batch queries: Batch reads (as a RelationProvider ) Batch writes (as a CreatableRelationProvider ) Delta Lake supports reading and writing in streaming queries: Stream reads (as a Source ) Stream writes (as a Sink ) Delta Lake uses LogStore abstraction to read and write physical log files and checkpoints (using Hadoop FileSystem API ).","title":"Overview"},{"location":"overview/#overview","text":"Delta Lake is an open-source storage management system (storage layer) that brings ACID transactions and time travel to Apache Spark and big data workloads. Important As of 0.7.0 Delta Lake requires Spark 3 (starting from the first 3.0.0 release). Delta Lake is a table format. It introduces DeltaTable abstraction that is simply a parquet table with a transactional log . Changes to (the state of) a delta table are reflected as actions and persisted to the transactional log (in JSON format ). Delta Lake uses OptimisticTransaction for transactional writes . A commit is successful when the transaction can write the actions to a delta file (in the transactional log ). In case the delta file for the commit version already exists, the transaction is retried . Structured queries can write (transactionally) to a delta table using the following interfaces: WriteIntoDelta command for batch queries (Spark SQL) DeltaSink for streaming queries (Spark Structured Streaming) More importantly, multiple queries can write to the same delta table simultaneously (at exactly the same time). Delta Lake provides DeltaTable API to programmatically access Delta tables. A delta table can be created based on a parquet table or from scratch . Delta Lake supports batch and streaming queries (Spark SQL and Structured Streaming, respectively) using delta format. In order to fine tune queries over data in Delta Lake use options . Among the options path option is mandatory. Delta Lake supports reading and writing in batch queries: Batch reads (as a RelationProvider ) Batch writes (as a CreatableRelationProvider ) Delta Lake supports reading and writing in streaming queries: Stream reads (as a Source ) Stream writes (as a Sink ) Delta Lake uses LogStore abstraction to read and write physical log files and checkpoints (using Hadoop FileSystem API ).","title":"Overview"},{"location":"spark-logging/","text":"= Logging Delta Lake uses http://logging.apache.org/log4j[log4j ] for logging (as does Apache Spark itself). == [[levels]] Logging Levels The valid logging levels are http://logging.apache.org/log4j/2.x/log4j-api/apidocs/index.html[log4j's Levels] (from most specific to least): OFF (most specific, no logging) FATAL (most specific, little data) ERROR WARN INFO DEBUG TRACE (least specific, a lot of data) ALL (least specific, all data) == [[log4j-properties]] conf/log4j.properties You can set up the default logging for Delta applications in conf/log4j.properties of the Spark installation. Use Spark's conf/log4j.properties.template as a starting point. == [[sbt]] sbt When running a Delta application from within sbt using run task, you can use the following build.sbt to configure logging levels: [source, scala] \u00b6 fork in run := true javaOptions in run ++= Seq( \"-Dlog4j.debug=true\", \"-Dlog4j.configuration=log4j.properties\") outputStrategy := Some(StdoutOutput) With the above configuration log4j.properties file should be on CLASSPATH which can be in src/main/resources directory (that is included in CLASSPATH by default). When run starts, you should see the following output in sbt: [spark-activator]> run [info] Running StreamingApp log4j: Trying to find [log4j.properties] using context classloader sun.misc.Launcher$AppClassLoader@1b6d3586. log4j: Using URL [file:.../classes/log4j.properties] for automatic log4j configuration. log4j: Reading configuration from URL file:.../classes/log4j.properties","title":"Logging"},{"location":"spark-logging/#source-scala","text":"fork in run := true javaOptions in run ++= Seq( \"-Dlog4j.debug=true\", \"-Dlog4j.configuration=log4j.properties\") outputStrategy := Some(StdoutOutput) With the above configuration log4j.properties file should be on CLASSPATH which can be in src/main/resources directory (that is included in CLASSPATH by default). When run starts, you should see the following output in sbt: [spark-activator]> run [info] Running StreamingApp log4j: Trying to find [log4j.properties] using context classloader sun.misc.Launcher$AppClassLoader@1b6d3586. log4j: Using URL [file:.../classes/log4j.properties] for automatic log4j configuration. log4j: Reading configuration from URL file:.../classes/log4j.properties","title":"[source, scala]"},{"location":"time-travel/","text":"Time Travel \u00b6 Delta Lake supports time travelling which is loading a Delta table at a given version or timestamp (defined by path , versionAsOf or timestampAsOf options). Time travel is described using DeltaTimeTravelSpec .","title":"Time Travel"},{"location":"time-travel/#time-travel","text":"Delta Lake supports time travelling which is loading a Delta table at a given version or timestamp (defined by path , versionAsOf or timestampAsOf options). Time travel is described using DeltaTimeTravelSpec .","title":"Time Travel"},{"location":"commands/AlterDeltaTableCommand/","text":"AlterDeltaTableCommand \u00b6 AlterDeltaTableCommand is an extension of the DeltaCommand abstraction for Delta commands that alter a DeltaTableV2 . Contract \u00b6 table \u00b6 table : DeltaTableV2 DeltaTableV2 Used when AlterDeltaTableCommand is requested to startTransaction Implementations \u00b6 AlterTableAddColumnsDeltaCommand AlterTableChangeColumnDeltaCommand AlterTableReplaceColumnsDeltaCommand AlterTableSetLocationDeltaCommand AlterTableSetPropertiesDeltaCommand AlterTableUnsetPropertiesDeltaCommand startTransaction \u00b6 startTransaction () : OptimisticTransaction startTransaction simply requests the DeltaTableV2 for the DeltaLog that in turn is requested to startTransaction .","title":"AlterDeltaTableCommand"},{"location":"commands/AlterDeltaTableCommand/#alterdeltatablecommand","text":"AlterDeltaTableCommand is an extension of the DeltaCommand abstraction for Delta commands that alter a DeltaTableV2 .","title":"AlterDeltaTableCommand"},{"location":"commands/AlterDeltaTableCommand/#contract","text":"","title":"Contract"},{"location":"commands/AlterDeltaTableCommand/#table","text":"table : DeltaTableV2 DeltaTableV2 Used when AlterDeltaTableCommand is requested to startTransaction","title":" table"},{"location":"commands/AlterDeltaTableCommand/#implementations","text":"AlterTableAddColumnsDeltaCommand AlterTableChangeColumnDeltaCommand AlterTableReplaceColumnsDeltaCommand AlterTableSetLocationDeltaCommand AlterTableSetPropertiesDeltaCommand AlterTableUnsetPropertiesDeltaCommand","title":"Implementations"},{"location":"commands/AlterDeltaTableCommand/#starttransaction","text":"startTransaction () : OptimisticTransaction startTransaction simply requests the DeltaTableV2 for the DeltaLog that in turn is requested to startTransaction .","title":" startTransaction"},{"location":"commands/AlterTableAddColumnsDeltaCommand/","text":"AlterTableAddColumnsDeltaCommand \u00b6 AlterTableAddColumnsDeltaCommand is...FIXME","title":"AlterTableAddColumnsDeltaCommand"},{"location":"commands/AlterTableAddColumnsDeltaCommand/#altertableaddcolumnsdeltacommand","text":"AlterTableAddColumnsDeltaCommand is...FIXME","title":"AlterTableAddColumnsDeltaCommand"},{"location":"commands/AlterTableChangeColumnDeltaCommand/","text":"AlterTableChangeColumnDeltaCommand \u00b6 AlterTableChangeColumnDeltaCommand is...FIXME","title":"AlterTableChangeColumnDeltaCommand"},{"location":"commands/AlterTableChangeColumnDeltaCommand/#altertablechangecolumndeltacommand","text":"AlterTableChangeColumnDeltaCommand is...FIXME","title":"AlterTableChangeColumnDeltaCommand"},{"location":"commands/AlterTableReplaceColumnsDeltaCommand/","text":"AlterTableReplaceColumnsDeltaCommand \u00b6 AlterTableReplaceColumnsDeltaCommand is...FIXME","title":"AlterTableReplaceColumnsDeltaCommand"},{"location":"commands/AlterTableReplaceColumnsDeltaCommand/#altertablereplacecolumnsdeltacommand","text":"AlterTableReplaceColumnsDeltaCommand is...FIXME","title":"AlterTableReplaceColumnsDeltaCommand"},{"location":"commands/AlterTableSetLocationDeltaCommand/","text":"AlterTableSetLocationDeltaCommand \u00b6 AlterTableSetLocationDeltaCommand is...FIXME","title":"AlterTableSetLocationDeltaCommand"},{"location":"commands/AlterTableSetLocationDeltaCommand/#altertablesetlocationdeltacommand","text":"AlterTableSetLocationDeltaCommand is...FIXME","title":"AlterTableSetLocationDeltaCommand"},{"location":"commands/AlterTableSetPropertiesDeltaCommand/","text":"AlterTableSetPropertiesDeltaCommand \u00b6 AlterTableSetPropertiesDeltaCommand is...FIXME","title":"AlterTableSetPropertiesDeltaCommand"},{"location":"commands/AlterTableSetPropertiesDeltaCommand/#altertablesetpropertiesdeltacommand","text":"AlterTableSetPropertiesDeltaCommand is...FIXME","title":"AlterTableSetPropertiesDeltaCommand"},{"location":"commands/AlterTableUnsetPropertiesDeltaCommand/","text":"AlterTableUnsetPropertiesDeltaCommand \u00b6 AlterTableUnsetPropertiesDeltaCommand is...FIXME","title":"AlterTableUnsetPropertiesDeltaCommand"},{"location":"commands/AlterTableUnsetPropertiesDeltaCommand/#altertableunsetpropertiesdeltacommand","text":"AlterTableUnsetPropertiesDeltaCommand is...FIXME","title":"AlterTableUnsetPropertiesDeltaCommand"},{"location":"commands/ConvertToDeltaCommand/","text":"ConvertToDeltaCommand \u00b6 ConvertToDeltaCommand is a DeltaCommand that converts a parquet table into delta format ( imports it into Delta). ConvertToDeltaCommand is a Spark SQL RunnableCommand (and executed eagerly on the driver for side-effects). ConvertToDeltaCommand requires that the partition schema matches the partitions of the parquet table ( or an AnalysisException is thrown ) ConvertToDeltaCommandBase is the base of ConvertToDeltaCommand -like commands with the only known implementation being ConvertToDeltaCommand itself. Creating Instance \u00b6 ConvertToDeltaCommand takes the following to be created: Parquet table ( TableIdentifier ) Partition schema ( Option[StructType] ) Delta Path ( Option[String] ) ConvertToDeltaCommand is created when: CONVERT TO DELTA statement is used (and DeltaSqlAstBuilder is requested to visitConvert ) DeltaTable.convertToDelta utility is used (and DeltaConvert utility is used to executeConvert ) Executing Command \u00b6 run ( spark : SparkSession ) : Seq [ Row ] run is part of the RunnableCommand contract. run < > from the < > (with the given SparkSession ). run makes sure that the (data source) provider (the database part of the < >) is either delta or parquet . For all other data source providers, run throws an AnalysisException : CONVERT TO DELTA only supports parquet tables, but you are trying to convert a [sourceName] source: [ident] For delta data source provider, run simply prints out the following message to standard output and returns. The table you are trying to convert is already a delta table For parquet data source provider, run uses DeltaLog utility to < >. run then requests DeltaLog to < > and < >. In the end, run < >. In case the < > of the new transaction is greater than -1 , run simply prints out the following message to standard output and returns. The table you are trying to convert is already a delta table Internal Helper Methods \u00b6 performConvert Method \u00b6 performConvert ( spark : SparkSession , txn : OptimisticTransaction , convertProperties : ConvertTarget ) : Seq [ Row ] performConvert makes sure that the directory exists (from the given ConvertProperties which is the table part of the < > of the command). performConvert requests the OptimisticTransaction for the < > that is then requested to < >. performConvert < > in the directory and leaves only files (by filtering out directories using WHERE clause). NOTE: performConvert uses Dataset API to build a distributed computation to query files. [[performConvert-cache]] performConvert caches the Dataset of file names. [[performConvert-schemaBatchSize]] performConvert uses < > configuration property for the number of files per batch for schema inference. performConvert < > for every batch of files and then < >. performConvert < > using the inferred table schema and the < > (if specified). performConvert creates a new < > using the table schema and the < > (if specified). performConvert requests the OptimisticTransaction to < >. [[performConvert-statsBatchSize]] performConvert uses < > configuration property for the number of files per batch for stats collection. performConvert < > (in the < > of the < > of the OptimisticTransaction ) for every file in a batch. [[performConvert-streamWrite]][[performConvert-unpersist]] In the end, performConvert < > (with the OptimisticTransaction , the AddFile s, and Operation.md#Convert[Convert] operation) and unpersists the Dataset of file names. streamWrite Method \u00b6 streamWrite ( spark : SparkSession , txn : OptimisticTransaction , addFiles : Iterator [ AddFile ], op : DeltaOperations.Operation , numFiles : Long ) : Long streamWrite ...FIXME createAddFile Method \u00b6 createAddFile ( file : SerializableFileStatus , basePath : Path , fs : FileSystem , conf : SQLConf ) : AddFile createAddFile creates an AddFile action. Internally, createAddFile ...FIXME createAddFile throws an AnalysisException if the number of fields in the given < > does not match the number of partitions found (at partition discovery phase): Expecting [size] partition column(s): [expectedCols], but found [size] partition column(s): [parsedCols] from parsing the file name: [path] mergeSchemasInParallel Method \u00b6 mergeSchemasInParallel ( sparkSession : SparkSession , filesToTouch : Seq [ FileStatus ], serializedConf : SerializableConfiguration ) : Option [ StructType ] mergeSchemasInParallel ...FIXME constructTableSchema Method \u00b6 constructTableSchema ( spark : SparkSession , dataSchema : StructType , partitionFields : Seq [ StructField ]) : StructType constructTableSchema ...FIXME","title":"ConvertToDeltaCommand"},{"location":"commands/ConvertToDeltaCommand/#converttodeltacommand","text":"ConvertToDeltaCommand is a DeltaCommand that converts a parquet table into delta format ( imports it into Delta). ConvertToDeltaCommand is a Spark SQL RunnableCommand (and executed eagerly on the driver for side-effects). ConvertToDeltaCommand requires that the partition schema matches the partitions of the parquet table ( or an AnalysisException is thrown ) ConvertToDeltaCommandBase is the base of ConvertToDeltaCommand -like commands with the only known implementation being ConvertToDeltaCommand itself.","title":"ConvertToDeltaCommand"},{"location":"commands/ConvertToDeltaCommand/#creating-instance","text":"ConvertToDeltaCommand takes the following to be created: Parquet table ( TableIdentifier ) Partition schema ( Option[StructType] ) Delta Path ( Option[String] ) ConvertToDeltaCommand is created when: CONVERT TO DELTA statement is used (and DeltaSqlAstBuilder is requested to visitConvert ) DeltaTable.convertToDelta utility is used (and DeltaConvert utility is used to executeConvert )","title":"Creating Instance"},{"location":"commands/ConvertToDeltaCommand/#executing-command","text":"run ( spark : SparkSession ) : Seq [ Row ] run is part of the RunnableCommand contract. run < > from the < > (with the given SparkSession ). run makes sure that the (data source) provider (the database part of the < >) is either delta or parquet . For all other data source providers, run throws an AnalysisException : CONVERT TO DELTA only supports parquet tables, but you are trying to convert a [sourceName] source: [ident] For delta data source provider, run simply prints out the following message to standard output and returns. The table you are trying to convert is already a delta table For parquet data source provider, run uses DeltaLog utility to < >. run then requests DeltaLog to < > and < >. In the end, run < >. In case the < > of the new transaction is greater than -1 , run simply prints out the following message to standard output and returns. The table you are trying to convert is already a delta table","title":" Executing Command"},{"location":"commands/ConvertToDeltaCommand/#internal-helper-methods","text":"","title":"Internal Helper Methods"},{"location":"commands/ConvertToDeltaCommand/#performconvert-method","text":"performConvert ( spark : SparkSession , txn : OptimisticTransaction , convertProperties : ConvertTarget ) : Seq [ Row ] performConvert makes sure that the directory exists (from the given ConvertProperties which is the table part of the < > of the command). performConvert requests the OptimisticTransaction for the < > that is then requested to < >. performConvert < > in the directory and leaves only files (by filtering out directories using WHERE clause). NOTE: performConvert uses Dataset API to build a distributed computation to query files. [[performConvert-cache]] performConvert caches the Dataset of file names. [[performConvert-schemaBatchSize]] performConvert uses < > configuration property for the number of files per batch for schema inference. performConvert < > for every batch of files and then < >. performConvert < > using the inferred table schema and the < > (if specified). performConvert creates a new < > using the table schema and the < > (if specified). performConvert requests the OptimisticTransaction to < >. [[performConvert-statsBatchSize]] performConvert uses < > configuration property for the number of files per batch for stats collection. performConvert < > (in the < > of the < > of the OptimisticTransaction ) for every file in a batch. [[performConvert-streamWrite]][[performConvert-unpersist]] In the end, performConvert < > (with the OptimisticTransaction , the AddFile s, and Operation.md#Convert[Convert] operation) and unpersists the Dataset of file names.","title":" performConvert Method"},{"location":"commands/ConvertToDeltaCommand/#streamwrite-method","text":"streamWrite ( spark : SparkSession , txn : OptimisticTransaction , addFiles : Iterator [ AddFile ], op : DeltaOperations.Operation , numFiles : Long ) : Long streamWrite ...FIXME","title":" streamWrite Method"},{"location":"commands/ConvertToDeltaCommand/#createaddfile-method","text":"createAddFile ( file : SerializableFileStatus , basePath : Path , fs : FileSystem , conf : SQLConf ) : AddFile createAddFile creates an AddFile action. Internally, createAddFile ...FIXME createAddFile throws an AnalysisException if the number of fields in the given < > does not match the number of partitions found (at partition discovery phase): Expecting [size] partition column(s): [expectedCols], but found [size] partition column(s): [parsedCols] from parsing the file name: [path]","title":" createAddFile Method"},{"location":"commands/ConvertToDeltaCommand/#mergeschemasinparallel-method","text":"mergeSchemasInParallel ( sparkSession : SparkSession , filesToTouch : Seq [ FileStatus ], serializedConf : SerializableConfiguration ) : Option [ StructType ] mergeSchemasInParallel ...FIXME","title":" mergeSchemasInParallel Method"},{"location":"commands/ConvertToDeltaCommand/#constructtableschema-method","text":"constructTableSchema ( spark : SparkSession , dataSchema : StructType , partitionFields : Seq [ StructField ]) : StructType constructTableSchema ...FIXME","title":" constructTableSchema Method"},{"location":"commands/CreateDeltaTableCommand/","text":"CreateDeltaTableCommand \u00b6 CreateDeltaTableCommand is a Spark SQL RunnableCommand (and executed eagerly on the driver for side-effects). Creating Instance \u00b6 CreateDeltaTableCommand takes the following to be created: CatalogTable ( Spark SQL ) Existing CatalogTable (if available) SaveMode Optional Data Query ( LogicalPlan ) CreationMode (default: TableCreationModes.Create ) tableByPath flag (default: false ) CreateDeltaTableCommand is created when DeltaCatalog is requested to create a Delta table . Executing Command \u00b6 run ( sparkSession : SparkSession ) : Seq [ Row ] run creates a DeltaLog (for the given table based on a table location) and a DeltaOptions . run starts a transaction (on the DeltaLog ). run branches off based on the optional data query . For data query defined, run creates a WriteIntoDelta and requests it to write . Otherwise, run creates an empty table. Note run does a bit more, but I don't think it's of much interest. run commits the transaction . In the end, run updateCatalog . run is part of the RunnableCommand abstraction. Updating Catalog \u00b6 updateCatalog ( spark : SparkSession , table : CatalogTable ) : Unit updateCatalog uses the given SparkSession to access SessionCatalog to createTable or alterTable when the tableByPath flag is off. Otherwise, updateCatalog does nothing.","title":"CreateDeltaTableCommand"},{"location":"commands/CreateDeltaTableCommand/#createdeltatablecommand","text":"CreateDeltaTableCommand is a Spark SQL RunnableCommand (and executed eagerly on the driver for side-effects).","title":"CreateDeltaTableCommand"},{"location":"commands/CreateDeltaTableCommand/#creating-instance","text":"CreateDeltaTableCommand takes the following to be created: CatalogTable ( Spark SQL ) Existing CatalogTable (if available) SaveMode Optional Data Query ( LogicalPlan ) CreationMode (default: TableCreationModes.Create ) tableByPath flag (default: false ) CreateDeltaTableCommand is created when DeltaCatalog is requested to create a Delta table .","title":"Creating Instance"},{"location":"commands/CreateDeltaTableCommand/#executing-command","text":"run ( sparkSession : SparkSession ) : Seq [ Row ] run creates a DeltaLog (for the given table based on a table location) and a DeltaOptions . run starts a transaction (on the DeltaLog ). run branches off based on the optional data query . For data query defined, run creates a WriteIntoDelta and requests it to write . Otherwise, run creates an empty table. Note run does a bit more, but I don't think it's of much interest. run commits the transaction . In the end, run updateCatalog . run is part of the RunnableCommand abstraction.","title":" Executing Command"},{"location":"commands/CreateDeltaTableCommand/#updating-catalog","text":"updateCatalog ( spark : SparkSession , table : CatalogTable ) : Unit updateCatalog uses the given SparkSession to access SessionCatalog to createTable or alterTable when the tableByPath flag is off. Otherwise, updateCatalog does nothing.","title":" Updating Catalog"},{"location":"commands/DeleteCommand/","text":"DeleteCommand \u00b6 DeleteCommand is a < > that < >. DeleteCommand is < > (using < > factory utility) and < > when < > operator is used (indirectly through DeltaTableOperations when requested to < >). == [[creating-instance]] Creating DeleteCommand Instance DeleteCommand takes the following to be created: [[tahoeFileIndex]] TahoeFileIndex [[target]] Target LogicalPlan [[condition]] Optional Catalyst expression == [[apply]] Creating DeleteCommand Instance -- apply Factory Utility [source, scala] \u00b6 apply(delete: Delete): DeleteCommand \u00b6 apply ...FIXME NOTE: apply is used when...FIXME == [[run]] Running Command -- run Method [source, scala] \u00b6 run(sparkSession: SparkSession): Seq[Row] \u00b6 NOTE: run is part of the RunnableCommand contract to...FIXME. run requests the < > for the < >. run requests the DeltaLog to < > for < >. In the end, run re-caches all cached plans (incl. this relation itself) by requesting the CacheManager to recache the < >. == [[performDelete]] performDelete Internal Method [source, scala] \u00b6 performDelete( sparkSession: SparkSession, deltaLog: DeltaLog, txn: OptimisticTransaction): Unit performDelete ...FIXME NOTE: performDelete is used exclusively when DeleteCommand is requested to < >.","title":"DeleteCommand"},{"location":"commands/DeleteCommand/#deletecommand","text":"DeleteCommand is a < > that < >. DeleteCommand is < > (using < > factory utility) and < > when < > operator is used (indirectly through DeltaTableOperations when requested to < >). == [[creating-instance]] Creating DeleteCommand Instance DeleteCommand takes the following to be created: [[tahoeFileIndex]] TahoeFileIndex [[target]] Target LogicalPlan [[condition]] Optional Catalyst expression == [[apply]] Creating DeleteCommand Instance -- apply Factory Utility","title":"DeleteCommand"},{"location":"commands/DeleteCommand/#source-scala","text":"","title":"[source, scala]"},{"location":"commands/DeleteCommand/#applydelete-delete-deletecommand","text":"apply ...FIXME NOTE: apply is used when...FIXME == [[run]] Running Command -- run Method","title":"apply(delete: Delete): DeleteCommand"},{"location":"commands/DeleteCommand/#source-scala_1","text":"","title":"[source, scala]"},{"location":"commands/DeleteCommand/#runsparksession-sparksession-seqrow","text":"NOTE: run is part of the RunnableCommand contract to...FIXME. run requests the < > for the < >. run requests the DeltaLog to < > for < >. In the end, run re-caches all cached plans (incl. this relation itself) by requesting the CacheManager to recache the < >. == [[performDelete]] performDelete Internal Method","title":"run(sparkSession: SparkSession): Seq[Row]"},{"location":"commands/DeleteCommand/#source-scala_2","text":"performDelete( sparkSession: SparkSession, deltaLog: DeltaLog, txn: OptimisticTransaction): Unit performDelete ...FIXME NOTE: performDelete is used exclusively when DeleteCommand is requested to < >.","title":"[source, scala]"},{"location":"commands/DeltaCommand/","text":"DeltaCommand \u00b6 DeltaCommand is a marker interface for commands to work with data in delta tables. Implementations \u00b6 AlterDeltaTableCommand ConvertToDeltaCommand DeleteCommand MergeIntoCommand UpdateCommand VacuumCommandImpl WriteIntoDelta parsePartitionPredicates Method \u00b6 parsePartitionPredicates ( spark : SparkSession , predicate : String ) : Seq [ Expression ] parsePartitionPredicates ...FIXME parsePartitionPredicates is used when...FIXME verifyPartitionPredicates Method \u00b6 verifyPartitionPredicates ( spark : SparkSession , partitionColumns : Seq [ String ], predicates : Seq [ Expression ]) : Unit verifyPartitionPredicates ...FIXME verifyPartitionPredicates is used when...FIXME generateCandidateFileMap Method \u00b6 generateCandidateFileMap ( basePath : Path , candidateFiles : Seq [ AddFile ]) : Map [ String , AddFile ] generateCandidateFileMap ...FIXME generateCandidateFileMap is used when...FIXME removeFilesFromPaths Method \u00b6 removeFilesFromPaths ( deltaLog : DeltaLog , nameToAddFileMap : Map [ String , AddFile ], filesToRewrite : Seq [ String ], operationTimestamp : Long ) : Seq [ RemoveFile ] removeFilesFromPaths ...FIXME removeFilesFromPaths is used when DeleteCommand and UpdateCommand commands are executed. Creating HadoopFsRelation (with TahoeBatchFileIndex) \u00b6 buildBaseRelation ( spark : SparkSession , txn : OptimisticTransaction , actionType : String , rootPath : Path , inputLeafFiles : Seq [ String ], nameToAddFileMap : Map [ String , AddFile ]) : HadoopFsRelation buildBaseRelation converts the given inputLeafFiles to AddFiles (with the given rootPath and nameToAddFileMap ). buildBaseRelation creates a TahoeBatchFileIndex for the actionType , the AddFiles and the rootPath . In the end, buildBaseRelation creates a HadoopFsRelation with the TahoeBatchFileIndex (and the other properties based on the metadata of the given OptimisticTransaction ). Note Learn more on HadoopFsRelation in The Internals of Spark SQL online book. buildBaseRelation is used when DeleteCommand and UpdateCommand commands are executed (with delete and update action types, respectively). getTouchedFile Method \u00b6 getTouchedFile ( basePath : Path , filePath : String , nameToAddFileMap : Map [ String , AddFile ]) : AddFile getTouchedFile ...FIXME getTouchedFile is used when: DeltaCommand is requested to removeFilesFromPaths and create a HadoopFsRelation (for DeleteCommand and UpdateCommand commands) MergeIntoCommand is executed isCatalogTable Method \u00b6 isCatalogTable ( analyzer : Analyzer , tableIdent : TableIdentifier ) : Boolean isCatalogTable ...FIXME isCatalogTable is used when...FIXME isPathIdentifier Method \u00b6 isPathIdentifier ( tableIdent : TableIdentifier ) : Boolean isPathIdentifier ...FIXME isPathIdentifier is used when...FIXME","title":"DeltaCommand"},{"location":"commands/DeltaCommand/#deltacommand","text":"DeltaCommand is a marker interface for commands to work with data in delta tables.","title":"DeltaCommand"},{"location":"commands/DeltaCommand/#implementations","text":"AlterDeltaTableCommand ConvertToDeltaCommand DeleteCommand MergeIntoCommand UpdateCommand VacuumCommandImpl WriteIntoDelta","title":"Implementations"},{"location":"commands/DeltaCommand/#parsepartitionpredicates-method","text":"parsePartitionPredicates ( spark : SparkSession , predicate : String ) : Seq [ Expression ] parsePartitionPredicates ...FIXME parsePartitionPredicates is used when...FIXME","title":" parsePartitionPredicates Method"},{"location":"commands/DeltaCommand/#verifypartitionpredicates-method","text":"verifyPartitionPredicates ( spark : SparkSession , partitionColumns : Seq [ String ], predicates : Seq [ Expression ]) : Unit verifyPartitionPredicates ...FIXME verifyPartitionPredicates is used when...FIXME","title":" verifyPartitionPredicates Method"},{"location":"commands/DeltaCommand/#generatecandidatefilemap-method","text":"generateCandidateFileMap ( basePath : Path , candidateFiles : Seq [ AddFile ]) : Map [ String , AddFile ] generateCandidateFileMap ...FIXME generateCandidateFileMap is used when...FIXME","title":" generateCandidateFileMap Method"},{"location":"commands/DeltaCommand/#removefilesfrompaths-method","text":"removeFilesFromPaths ( deltaLog : DeltaLog , nameToAddFileMap : Map [ String , AddFile ], filesToRewrite : Seq [ String ], operationTimestamp : Long ) : Seq [ RemoveFile ] removeFilesFromPaths ...FIXME removeFilesFromPaths is used when DeleteCommand and UpdateCommand commands are executed.","title":" removeFilesFromPaths Method"},{"location":"commands/DeltaCommand/#creating-hadoopfsrelation-with-tahoebatchfileindex","text":"buildBaseRelation ( spark : SparkSession , txn : OptimisticTransaction , actionType : String , rootPath : Path , inputLeafFiles : Seq [ String ], nameToAddFileMap : Map [ String , AddFile ]) : HadoopFsRelation buildBaseRelation converts the given inputLeafFiles to AddFiles (with the given rootPath and nameToAddFileMap ). buildBaseRelation creates a TahoeBatchFileIndex for the actionType , the AddFiles and the rootPath . In the end, buildBaseRelation creates a HadoopFsRelation with the TahoeBatchFileIndex (and the other properties based on the metadata of the given OptimisticTransaction ). Note Learn more on HadoopFsRelation in The Internals of Spark SQL online book. buildBaseRelation is used when DeleteCommand and UpdateCommand commands are executed (with delete and update action types, respectively).","title":" Creating HadoopFsRelation (with TahoeBatchFileIndex)"},{"location":"commands/DeltaCommand/#gettouchedfile-method","text":"getTouchedFile ( basePath : Path , filePath : String , nameToAddFileMap : Map [ String , AddFile ]) : AddFile getTouchedFile ...FIXME getTouchedFile is used when: DeltaCommand is requested to removeFilesFromPaths and create a HadoopFsRelation (for DeleteCommand and UpdateCommand commands) MergeIntoCommand is executed","title":" getTouchedFile Method"},{"location":"commands/DeltaCommand/#iscatalogtable-method","text":"isCatalogTable ( analyzer : Analyzer , tableIdent : TableIdentifier ) : Boolean isCatalogTable ...FIXME isCatalogTable is used when...FIXME","title":" isCatalogTable Method"},{"location":"commands/DeltaCommand/#ispathidentifier-method","text":"isPathIdentifier ( tableIdent : TableIdentifier ) : Boolean isPathIdentifier ...FIXME isPathIdentifier is used when...FIXME","title":" isPathIdentifier Method"},{"location":"commands/DeltaGenerateCommand/","text":"= DeltaGenerateCommand -- Executing Generation Functions On Delta Tables DeltaGenerateCommand is a concrete < > (and so can < >) that can < >. [[symlink_format_manifest]] DeltaGenerateCommand supports symlink_format_manifest only for the < >. [source,text] \u00b6 val generateQ = \"\"\"GENERATE symlink_format_manifest for table delta. /tmp/delta/t1 \"\"\" scala> sql(generateQ).foreach(_ => ()) [[modeNameToGenerationFunc]] DeltaGenerateCommand uses a lookup table for generation functions per mode: Uses < > for the only-supported < > DeltaGenerateCommand is < > when: DeltaSqlAstBuilder is requested to < > < > operator is used (that < >) == [[creating-instance]] Creating DeltaGenerateCommand Instance DeltaGenerateCommand takes the following to be created: [[modeName]] Mode (< > is the only supported mode) [[tableId]] Delta table ( TableIdentifier ) == [[run]] Running Command -- run Method [source, scala] \u00b6 run(sparkSession: SparkSession): Seq[Row] \u00b6 NOTE: run is part of the RunnableCommand contract to...FIXME. run < > for the < > (from the < >). run finds the generate function for the mode (in the < > registry) and applies ( executes ) it to the DeltaLog . run throws an AnalysisException when the < > of the < > of the DeltaLog is negative (less than 0 ): Delta table not found at [tablePath]. run throws an IllegalArgumentException for unsupported < >: Specified mode '[modeName]' is not supported. Supported modes are: symlink_format_manifest","title":"DeltaGenerateCommand"},{"location":"commands/DeltaGenerateCommand/#sourcetext","text":"val generateQ = \"\"\"GENERATE symlink_format_manifest for table delta. /tmp/delta/t1 \"\"\" scala> sql(generateQ).foreach(_ => ()) [[modeNameToGenerationFunc]] DeltaGenerateCommand uses a lookup table for generation functions per mode: Uses < > for the only-supported < > DeltaGenerateCommand is < > when: DeltaSqlAstBuilder is requested to < > < > operator is used (that < >) == [[creating-instance]] Creating DeltaGenerateCommand Instance DeltaGenerateCommand takes the following to be created: [[modeName]] Mode (< > is the only supported mode) [[tableId]] Delta table ( TableIdentifier ) == [[run]] Running Command -- run Method","title":"[source,text]"},{"location":"commands/DeltaGenerateCommand/#source-scala","text":"","title":"[source, scala]"},{"location":"commands/DeltaGenerateCommand/#runsparksession-sparksession-seqrow","text":"NOTE: run is part of the RunnableCommand contract to...FIXME. run < > for the < > (from the < >). run finds the generate function for the mode (in the < > registry) and applies ( executes ) it to the DeltaLog . run throws an AnalysisException when the < > of the < > of the DeltaLog is negative (less than 0 ): Delta table not found at [tablePath]. run throws an IllegalArgumentException for unsupported < >: Specified mode '[modeName]' is not supported. Supported modes are: symlink_format_manifest","title":"run(sparkSession: SparkSession): Seq[Row]"},{"location":"commands/DeltaMergeBuilder/","text":"DeltaMergeBuilder \u00b6 DeltaMergeBuilder is a builder interface to describe how to merge data from a source DataFrame into the target delta table (using whenMatched and whenNotMatched conditions). DeltaMergeBuilder is created using DeltaTable.merge operator. In the end, DeltaMergeBuilder is supposed to be executed to take action. DeltaMergeBuilder creates a DeltaMergeInto logical command that is resolved to a MergeIntoCommand runnable logical command (using PreprocessTableMerge logical resolution rule). Creating Instance \u00b6 DeltaMergeBuilder takes the following to be created: Target DeltaTable Source DataFrame Condition Column When Clauses DeltaMergeBuilder is created using DeltaTable.merge operator. Operators \u00b6 whenMatched \u00b6 whenMatched () : DeltaMergeMatchedActionBuilder whenMatched ( condition : Column ) : DeltaMergeMatchedActionBuilder whenMatched ( condition : String ) : DeltaMergeMatchedActionBuilder Creates a DeltaMergeMatchedActionBuilder (for the DeltaMergeBuilder and a condition) whenNotMatched \u00b6 whenNotMatched () : DeltaMergeNotMatchedActionBuilder whenNotMatched ( condition : Column ) : DeltaMergeNotMatchedActionBuilder whenNotMatched ( condition : String ) : DeltaMergeNotMatchedActionBuilder Creates a DeltaMergeNotMatchedActionBuilder (for the DeltaMergeBuilder and a condition) Executing Merge \u00b6 execute () : Unit execute creates a merge plan (that is DeltaMergeInto logical command) and resolves column references . execute runs PreprocessTableMerge logical resolution rule on the DeltaMergeInto logical command (that gives MergeIntoCommand runnable logical command). In the end, execute executes the MergeIntoCommand logical command. Creating Logical Plan for Merge \u00b6 mergePlan : DeltaMergeInto mergePlan creates a DeltaMergeInto logical command. mergePlan is used when DeltaMergeBuilder is requested to execute . Creating DeltaMergeBuilder \u00b6 apply ( targetTable : DeltaTable , source : DataFrame , onCondition : Column ) : DeltaMergeBuilder apply utility creates a new DeltaMergeBuilder for the given parameters and no DeltaMergeIntoClauses . apply is used for DeltaTable.merge operator. Adding DeltaMergeIntoClause \u00b6 withClause ( clause : DeltaMergeIntoClause ) : DeltaMergeBuilder withClause creates a new DeltaMergeBuilder (based on the existing properties, e.g. the DeltaTable ) with the given DeltaMergeIntoClause added to the existing DeltaMergeIntoClauses (to create a more refined DeltaMergeBuilder ). withClause is used when: DeltaMergeMatchedActionBuilder is requested to updateAll , delete and addUpdateClause DeltaMergeNotMatchedActionBuilder is requested to insertAll and addInsertClause","title":"DeltaMergeBuilder"},{"location":"commands/DeltaMergeBuilder/#deltamergebuilder","text":"DeltaMergeBuilder is a builder interface to describe how to merge data from a source DataFrame into the target delta table (using whenMatched and whenNotMatched conditions). DeltaMergeBuilder is created using DeltaTable.merge operator. In the end, DeltaMergeBuilder is supposed to be executed to take action. DeltaMergeBuilder creates a DeltaMergeInto logical command that is resolved to a MergeIntoCommand runnable logical command (using PreprocessTableMerge logical resolution rule).","title":"DeltaMergeBuilder"},{"location":"commands/DeltaMergeBuilder/#creating-instance","text":"DeltaMergeBuilder takes the following to be created: Target DeltaTable Source DataFrame Condition Column When Clauses DeltaMergeBuilder is created using DeltaTable.merge operator.","title":"Creating Instance"},{"location":"commands/DeltaMergeBuilder/#operators","text":"","title":"Operators"},{"location":"commands/DeltaMergeBuilder/#whenmatched","text":"whenMatched () : DeltaMergeMatchedActionBuilder whenMatched ( condition : Column ) : DeltaMergeMatchedActionBuilder whenMatched ( condition : String ) : DeltaMergeMatchedActionBuilder Creates a DeltaMergeMatchedActionBuilder (for the DeltaMergeBuilder and a condition)","title":" whenMatched"},{"location":"commands/DeltaMergeBuilder/#whennotmatched","text":"whenNotMatched () : DeltaMergeNotMatchedActionBuilder whenNotMatched ( condition : Column ) : DeltaMergeNotMatchedActionBuilder whenNotMatched ( condition : String ) : DeltaMergeNotMatchedActionBuilder Creates a DeltaMergeNotMatchedActionBuilder (for the DeltaMergeBuilder and a condition)","title":" whenNotMatched"},{"location":"commands/DeltaMergeBuilder/#executing-merge","text":"execute () : Unit execute creates a merge plan (that is DeltaMergeInto logical command) and resolves column references . execute runs PreprocessTableMerge logical resolution rule on the DeltaMergeInto logical command (that gives MergeIntoCommand runnable logical command). In the end, execute executes the MergeIntoCommand logical command.","title":" Executing Merge"},{"location":"commands/DeltaMergeBuilder/#creating-logical-plan-for-merge","text":"mergePlan : DeltaMergeInto mergePlan creates a DeltaMergeInto logical command. mergePlan is used when DeltaMergeBuilder is requested to execute .","title":" Creating Logical Plan for Merge"},{"location":"commands/DeltaMergeBuilder/#creating-deltamergebuilder","text":"apply ( targetTable : DeltaTable , source : DataFrame , onCondition : Column ) : DeltaMergeBuilder apply utility creates a new DeltaMergeBuilder for the given parameters and no DeltaMergeIntoClauses . apply is used for DeltaTable.merge operator.","title":" Creating DeltaMergeBuilder"},{"location":"commands/DeltaMergeBuilder/#adding-deltamergeintoclause","text":"withClause ( clause : DeltaMergeIntoClause ) : DeltaMergeBuilder withClause creates a new DeltaMergeBuilder (based on the existing properties, e.g. the DeltaTable ) with the given DeltaMergeIntoClause added to the existing DeltaMergeIntoClauses (to create a more refined DeltaMergeBuilder ). withClause is used when: DeltaMergeMatchedActionBuilder is requested to updateAll , delete and addUpdateClause DeltaMergeNotMatchedActionBuilder is requested to insertAll and addInsertClause","title":" Adding DeltaMergeIntoClause"},{"location":"commands/DeltaMergeInto/","text":"DeltaMergeInto Logical Command \u00b6 DeltaMergeInto is a logical command (Spark SQL's Command ). Creating Instance \u00b6 DeltaMergeInto takes the following to be created: Target LogicalPlan Source LogicalPlan Condition Expression Matched Clauses ( Seq[DeltaMergeIntoMatchedClause] ) Optional Non-Matched Clause ( Option[DeltaMergeIntoInsertClause] ) Optional Migrated Schema (default: undefined ) DeltaMergeInto is created (using apply and resolveReferences utilities) when: DeltaMergeBuilder is requested to execute DeltaAnalysis logical resolution rule is executed Utilities \u00b6 apply \u00b6 apply ( target : LogicalPlan , source : LogicalPlan , condition : Expression , whenClauses : Seq [ DeltaMergeIntoClause ]) : DeltaMergeInto apply ...FIXME apply is used when: DeltaMergeBuilder is requested to execute (when mergePlan ) DeltaAnalysis logical resolution rule is executed (and resolves MergeIntoTable logical command) resolveReferences \u00b6 resolveReferences ( merge : DeltaMergeInto , conf : SQLConf )( resolveExpr : ( Expression , LogicalPlan ) => Expression ) : DeltaMergeInto resolveReferences ...FIXME resolveReferences is used when: DeltaMergeBuilder is requested to execute DeltaAnalysis logical resolution rule is executed (and resolves MergeIntoTable logical command)","title":"DeltaMergeInto"},{"location":"commands/DeltaMergeInto/#deltamergeinto-logical-command","text":"DeltaMergeInto is a logical command (Spark SQL's Command ).","title":"DeltaMergeInto Logical Command"},{"location":"commands/DeltaMergeInto/#creating-instance","text":"DeltaMergeInto takes the following to be created: Target LogicalPlan Source LogicalPlan Condition Expression Matched Clauses ( Seq[DeltaMergeIntoMatchedClause] ) Optional Non-Matched Clause ( Option[DeltaMergeIntoInsertClause] ) Optional Migrated Schema (default: undefined ) DeltaMergeInto is created (using apply and resolveReferences utilities) when: DeltaMergeBuilder is requested to execute DeltaAnalysis logical resolution rule is executed","title":"Creating Instance"},{"location":"commands/DeltaMergeInto/#utilities","text":"","title":"Utilities"},{"location":"commands/DeltaMergeInto/#apply","text":"apply ( target : LogicalPlan , source : LogicalPlan , condition : Expression , whenClauses : Seq [ DeltaMergeIntoClause ]) : DeltaMergeInto apply ...FIXME apply is used when: DeltaMergeBuilder is requested to execute (when mergePlan ) DeltaAnalysis logical resolution rule is executed (and resolves MergeIntoTable logical command)","title":" apply"},{"location":"commands/DeltaMergeInto/#resolvereferences","text":"resolveReferences ( merge : DeltaMergeInto , conf : SQLConf )( resolveExpr : ( Expression , LogicalPlan ) => Expression ) : DeltaMergeInto resolveReferences ...FIXME resolveReferences is used when: DeltaMergeBuilder is requested to execute DeltaAnalysis logical resolution rule is executed (and resolves MergeIntoTable logical command)","title":" resolveReferences"},{"location":"commands/DeltaMergeIntoClause/","text":"DeltaMergeIntoClause \u00b6 DeltaMergeIntoClause is...FIXME","title":"DeltaMergeIntoClause"},{"location":"commands/DeltaMergeIntoClause/#deltamergeintoclause","text":"DeltaMergeIntoClause is...FIXME","title":"DeltaMergeIntoClause"},{"location":"commands/DeltaMergeMatchedActionBuilder/","text":"DeltaMergeMatchedActionBuilder \u00b6 DeltaMergeMatchedActionBuilder is a builder interface for DeltaMergeBuilder.whenMatched operator. Creating Instance \u00b6 DeltaMergeMatchedActionBuilder takes the following to be created: DeltaMergeBuilder Optional match condition DeltaMergeMatchedActionBuilder is created when DeltaMergeBuilder is requested to whenMatched (using apply factory method). Operators \u00b6 delete \u00b6 delete () : DeltaMergeBuilder Adds a DeltaMergeIntoDeleteClause (with the matchCondition ) to the DeltaMergeBuilder . update \u00b6 update ( set : Map [ String , Column ]) : DeltaMergeBuilder updateAll \u00b6 updateAll () : DeltaMergeBuilder updateExpr \u00b6 updateExpr ( set : Map [ String , String ]) : DeltaMergeBuilder Creating DeltaMergeMatchedActionBuilder \u00b6 apply ( mergeBuilder : DeltaMergeBuilder , matchCondition : Option [ Column ]) : DeltaMergeMatchedActionBuilder apply creates a DeltaMergeMatchedActionBuilder (for the given parameters). apply is used when DeltaMergeBuilder is requested to whenMatched .","title":"DeltaMergeMatchedActionBuilder"},{"location":"commands/DeltaMergeMatchedActionBuilder/#deltamergematchedactionbuilder","text":"DeltaMergeMatchedActionBuilder is a builder interface for DeltaMergeBuilder.whenMatched operator.","title":"DeltaMergeMatchedActionBuilder"},{"location":"commands/DeltaMergeMatchedActionBuilder/#creating-instance","text":"DeltaMergeMatchedActionBuilder takes the following to be created: DeltaMergeBuilder Optional match condition DeltaMergeMatchedActionBuilder is created when DeltaMergeBuilder is requested to whenMatched (using apply factory method).","title":"Creating Instance"},{"location":"commands/DeltaMergeMatchedActionBuilder/#operators","text":"","title":"Operators"},{"location":"commands/DeltaMergeMatchedActionBuilder/#delete","text":"delete () : DeltaMergeBuilder Adds a DeltaMergeIntoDeleteClause (with the matchCondition ) to the DeltaMergeBuilder .","title":" delete"},{"location":"commands/DeltaMergeMatchedActionBuilder/#update","text":"update ( set : Map [ String , Column ]) : DeltaMergeBuilder","title":" update"},{"location":"commands/DeltaMergeMatchedActionBuilder/#updateall","text":"updateAll () : DeltaMergeBuilder","title":" updateAll"},{"location":"commands/DeltaMergeMatchedActionBuilder/#updateexpr","text":"updateExpr ( set : Map [ String , String ]) : DeltaMergeBuilder","title":" updateExpr"},{"location":"commands/DeltaMergeMatchedActionBuilder/#creating-deltamergematchedactionbuilder","text":"apply ( mergeBuilder : DeltaMergeBuilder , matchCondition : Option [ Column ]) : DeltaMergeMatchedActionBuilder apply creates a DeltaMergeMatchedActionBuilder (for the given parameters). apply is used when DeltaMergeBuilder is requested to whenMatched .","title":" Creating DeltaMergeMatchedActionBuilder"},{"location":"commands/DeltaMergeNotMatchedActionBuilder/","text":"DeltaMergeNotMatchedActionBuilder \u00b6 DeltaMergeNotMatchedActionBuilder is...FIXME","title":"DeltaMergeNotMatchedActionBuilder"},{"location":"commands/DeltaMergeNotMatchedActionBuilder/#deltamergenotmatchedactionbuilder","text":"DeltaMergeNotMatchedActionBuilder is...FIXME","title":"DeltaMergeNotMatchedActionBuilder"},{"location":"commands/DescribeDeltaDetailCommand/","text":"= DescribeDeltaDetailCommand (And DescribeDeltaDetailCommandBase) DescribeDeltaDetailCommand represents DESCRIBE DETAIL SQL command at execution (and is < > when DeltaSqlAstBuilder is requested to < >). Like DESCRIBE DETAIL SQL command, DescribeDeltaDetailCommand accepts either a < > or a < > (e.g. '/tmp/delta/t1' or ++delta. /tmp/delta/t1 ++ ) (DESC | DESCRIBE) DETAIL (path | table) [[demo]] .DESCRIBE DETAIL SQL Command's Demo val q = sql(\"DESCRIBE DETAIL '/tmp/delta/users'\") scala> q.show +------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+ |format| id|name|description| location| createdAt| lastModified|partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion| +------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+ | delta|3799b291-dbfa-4f8...|null| null|file:/tmp/delta/u...|2020-01-06 17:08:...|2020-01-06 17:12:28| [city, country]| 4| 2581| []| 1| 2| +------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+ [[implementations]] DescribeDeltaDetailCommand is the default and only known < > in Delta Lake. [[DescribeDeltaDetailCommandBase]] DescribeDeltaDetailCommandBase is an extension of the RunnableCommand contract (from Spark SQL) for < >. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-RunnableCommand.html[RunnableCommand ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. == [[creating-instance]] Creating DescribeDeltaDetailCommand Instance DescribeDeltaDetailCommand takes the following to be created: [[path]] Table path [[tableIdentifier]] Table identifier (e.g. t1 or ++delta. /tmp/delta/t1 ++ ) == [[run]] Running Command -- run Method [source, scala] \u00b6 run(sparkSession: SparkSession): Seq[Row] \u00b6 NOTE: run is part of the RunnableCommand contract to...FIXME. run ...FIXME","title":"DescribeDeltaDetailCommand"},{"location":"commands/DescribeDeltaDetailCommand/#source-scala","text":"","title":"[source, scala]"},{"location":"commands/DescribeDeltaDetailCommand/#runsparksession-sparksession-seqrow","text":"NOTE: run is part of the RunnableCommand contract to...FIXME. run ...FIXME","title":"run(sparkSession: SparkSession): Seq[Row]"},{"location":"commands/DescribeDeltaHistoryCommand/","text":"= DescribeDeltaHistoryCommand DescribeDeltaHistoryCommand is...FIXME","title":"DescribeDeltaHistoryCommand"},{"location":"commands/JoinedRowProcessor/","text":"JoinedRowProcessor \u00b6 JoinedRowProcessor is...FIXME Creating Instance \u00b6 JoinedRowProcessor takes the following to be created: targetRowHasNoMatch Expression sourceRowHasNoMatch Expression Optional matchedCondition1 Expression Optional matchedOutput1 Expressions Optional matchedCondition2 Expression Optional matchedOutput2 Expressions Optional notMatchedCondition Expression Optional notMatchedOutput Expressions Optional noopCopyOutput Expression deleteRowOutput Expressions joinedAttributes Attributes joinedRowEncoder ExpressionEncoder outputRowEncoder ExpressionEncoder JoinedRowProcessor is created when MergeIntoCommand is requested to writeAllChanges . Processing Partition \u00b6 processPartition ( rowIterator : Iterator [ Row ]) : Iterator [ Row ] processPartition ...FIXME processPartition is used when MergeIntoCommand is requested to writeAllChanges .","title":"JoinedRowProcessor"},{"location":"commands/JoinedRowProcessor/#joinedrowprocessor","text":"JoinedRowProcessor is...FIXME","title":"JoinedRowProcessor"},{"location":"commands/JoinedRowProcessor/#creating-instance","text":"JoinedRowProcessor takes the following to be created: targetRowHasNoMatch Expression sourceRowHasNoMatch Expression Optional matchedCondition1 Expression Optional matchedOutput1 Expressions Optional matchedCondition2 Expression Optional matchedOutput2 Expressions Optional notMatchedCondition Expression Optional notMatchedOutput Expressions Optional noopCopyOutput Expression deleteRowOutput Expressions joinedAttributes Attributes joinedRowEncoder ExpressionEncoder outputRowEncoder ExpressionEncoder JoinedRowProcessor is created when MergeIntoCommand is requested to writeAllChanges .","title":"Creating Instance"},{"location":"commands/JoinedRowProcessor/#processing-partition","text":"processPartition ( rowIterator : Iterator [ Row ]) : Iterator [ Row ] processPartition ...FIXME processPartition is used when MergeIntoCommand is requested to writeAllChanges .","title":" Processing Partition"},{"location":"commands/MergeIntoCommand/","text":"MergeIntoCommand \u00b6 MergeIntoCommand is a DeltaCommand that represents a DeltaMergeInto logical command at execution time. MergeIntoCommand is a logical command (Spark SQL's RunnableCommand ). Tip Learn more on the internals of MergeIntoCommand in Demo: Merge Operation . Performance Metrics \u00b6 Name web UI numSourceRows number of source rows numTargetRowsCopied number of target rows rewritten unmodified numTargetRowsInserted number of inserted rows numTargetRowsUpdated number of updated rows numTargetRowsDeleted number of deleted rows numTargetFilesBeforeSkipping number of target files before skipping numTargetFilesAfterSkipping number of target files after skipping numTargetFilesRemoved number of files removed to target numTargetFilesAdded number of files added to target number of target rows rewritten unmodified \u00b6 numTargetRowsCopied performance metric (like the other metrics ) is turned into a non-deterministic user-defined function (UDF). numTargetRowsCopied becomes incrNoopCountExpr UDF. incrNoopCountExpr UDF is resolved on a joined plan and used to create a JoinedRowProcessor for processing partitions of the joined plan Dataset . Creating Instance \u00b6 MergeIntoCommand takes the following to be created: Source Data Target Data ( LogicalPlan ) TahoeFileIndex Condition Expression Matched Clauses ( Seq[DeltaMergeIntoMatchedClause] ) Optional Non-Matched Clause ( Option[DeltaMergeIntoInsertClause] ) Migrated Schema MergeIntoCommand is created when PreprocessTableMerge logical resolution rule is executed (on a DeltaMergeInto logical command). Source Data (to Merge From) \u00b6 When created , MergeIntoCommand is given a LogicalPlan for the source data to merge from (referred to internally as source ). The source LogicalPlan is used twice: Firstly, in one of the following: An inner join (in findTouchedFiles ) that is count in web UI A leftanti join (in writeInsertsOnlyWhenNoMatchedClauses ) Secondly, in rightOuter or fullOuter join (in writeAllChanges ) Tip Enable DEBUG logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger to see the inner-workings of writeAllChanges . Target DeltaLog \u00b6 targetDeltaLog : DeltaLog targetDeltaLog is the DeltaLog of the TahoeFileIndex . targetDeltaLog is used for the following: Start a new transaction when executed To access the Data Path when finding files to rewrite Lazy Value targetDeltaLog is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards. Executing Command \u00b6 run ( spark : SparkSession ) : Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run requests the target DeltaLog to start a new transaction . With spark.databricks.delta.schema.autoMerge.enabled configuration property enabled, run updates the metadata (of the transaction). run determines Delta actions ( RemoveFile s and AddFile s). Describe deltaActions part With spark.databricks.delta.history.metricsEnabled configuration property enabled, run requests the current transaction to register SQL metrics for the Delta operation . run requests the current transaction to commit (with the Delta actions and Merge operation). run records the Delta event. run posts a SparkListenerDriverAccumUpdates Spark event (with the metrics). In the end, run requests the CacheManager to recacheByPlan . Finding Files to Rewrite \u00b6 findTouchedFiles ( spark : SparkSession , deltaTxn : OptimisticTransaction ) : Seq [ AddFile ] Important findTouchedFiles is such a fine piece of art ( a gem ). It uses a custom accumulator, a UDF (to use this accumulator to record touched file names) and input_file_name() standard function for the names of the files read. It is always worth keeping in mind that Delta Lake uses files for data storage and that is why input_file_name() standard function works. It would not work for non-file-based data sources. Example 1: Understanding the Internals of findTouchedFiles The following query writes out a 10-element dataset using the default parquet data source to /tmp/parquet directory: val target = \"/tmp/parquet\" spark . range ( 10 ). write . save ( target ) The number of parquet part files varies based on the number of partitions (CPU cores). The following query loads the parquet dataset back alongside input_file_name() standard function to mimic findTouchedFiles 's behaviour. val FILE_NAME_COL = \"_file_name_\" val dataFiles = spark . read . parquet ( target ). withColumn ( FILE_NAME_COL , input_file_name ()) scala> dataFiles.show(truncate = false) +---+---------------------------------------------------------------------------------------+ |id |_file_name_ | +---+---------------------------------------------------------------------------------------+ |4 |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |0 |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |3 |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |6 |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |1 |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |8 |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |2 |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |7 |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |5 |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |9 |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| +---+---------------------------------------------------------------------------------------+ As you may have thought, not all part files have got data and so they are not included in the dataset. That is when findTouchedFiles uses groupBy operator and count action to calculate match frequency. val counts = dataFiles.groupBy(FILE_NAME_COL).count() scala> counts.show(truncate = false) +---------------------------------------------------------------------------------------+-----+ |_file_name_ |count| +---------------------------------------------------------------------------------------+-----+ |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | +---------------------------------------------------------------------------------------+-----+ Let's load all the part files in the /tmp/parquet directory and find which file(s) have no data. import scala.sys.process._ val cmd = ( s\"ls $target \" #| \"grep .parquet\" ). lineStream val allFiles = cmd . toArray . toSeq . toDF ( FILE_NAME_COL ) . select ( concat ( lit ( s\"file:// $target /\" ), col ( FILE_NAME_COL )) as FILE_NAME_COL ) val joinType = \"left_anti\" // MergeIntoCommand uses inner as it wants data file val noDataFiles = allFiles . join ( dataFiles , Seq ( FILE_NAME_COL ), joinType ) Mind that the data vs non-data datasets could be different, but that should not \"interfere\" with the main reasoning flow. scala> noDataFiles.show(truncate = false) +---------------------------------------------------------------------------------------+ |_file_name_ | +---------------------------------------------------------------------------------------+ |file:///tmp/parquet/part-00000-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| +---------------------------------------------------------------------------------------+ findTouchedFiles registers an accumulator to collect all the distinct files that need to be rewritten ( touched files ). Note The name of the accumulator is internal.metrics.MergeIntoDelta.touchedFiles and internal.metrics part is supposed to hide it from web UI as potentially large (set of file names to be rewritten). findTouchedFiles defines a nondeterministic UDF that adds the file names to the accumulator ( recordTouchedFileName ). findTouchedFiles splits conjunctive predicates ( And binary expressions) in the condition expression and collects the predicates that use the target 's columns ( targetOnlyPredicates ). findTouchedFiles requests the given OptimisticTransaction for the files that match the target-only predicates (and creates a dataSkippedFiles collection of AddFile s). Note This step looks similar to filter predicate pushdown . findTouchedFiles creates one DataFrame for the source data (using Dataset.ofRows utility). Tip Learn more about Dataset.ofRows utility in The Internals of Spark SQL online book. findTouchedFiles builds a logical query plan for the files (matching the predicates) and creates another DataFrame for the target data. findTouchedFiles adds two columns to the target dataframe: _row_id_ for monotonically_increasing_id() standard function _file_name_ for input_file_name() standard function findTouchedFiles creates (a DataFrame that is) an INNER JOIN of the source and target DataFrame s using the condition expression. findTouchedFiles takes the joined dataframe and selects _row_id_ column and the recordTouchedFileName UDF on the _file_name_ column as one . The DataFrame is internally known as collectTouchedFiles . findTouchedFiles uses groupBy operator on _row_id_ to calculate a sum of all the values in the one column (as count column) in the two-column collectTouchedFiles dataset. The DataFrame is internally known as matchedRowCounts . Note No Spark job has been submitted yet. findTouchedFiles is still in \"query preparation\" mode. findTouchedFiles uses filter on the count column (in the matchedRowCounts dataset) with values greater than 1 . If there are any, findTouchedFiles throws an UnsupportedOperationException exception: Cannot perform MERGE as multiple source rows matched and attempted to update the same target row in the Delta table. By SQL semantics of merge, when multiple source rows match on the same target row, the update operation is ambiguous as it is unclear which source should be used to update the matching target row. You can preprocess the source table to eliminate the possibility of multiple matches. Note Since findTouchedFiles uses count action there should be a Spark SQL query reported (and possibly Spark jobs) in web UI. findTouchedFiles requests the touchedFilesAccum accumulator for the touched file names. Example 2: Understanding the Internals of findTouchedFiles val TOUCHED_FILES_ACCUM_NAME = \"MergeIntoDelta.touchedFiles\" val touchedFilesAccum = spark . sparkContext . collectionAccumulator [ String ]( TOUCHED_FILES_ACCUM_NAME ) val recordTouchedFileName = udf { ( fileName : String ) => { touchedFilesAccum . add ( fileName ) 1 }}. asNondeterministic () val target = \"/tmp/parquet\" spark . range ( 10 ). write . save ( target ) val FILE_NAME_COL = \"_file_name_\" val dataFiles = spark . read . parquet ( target ). withColumn ( FILE_NAME_COL , input_file_name ()) val collectTouchedFiles = dataFiles . select ( col ( FILE_NAME_COL ), recordTouchedFileName ( col ( FILE_NAME_COL )). as ( \"one\" )) scala> collectTouchedFiles.show(truncate = false) +---------------------------------------------------------------------------------------+---+ |_file_name_ |one| +---------------------------------------------------------------------------------------+---+ |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | +---------------------------------------------------------------------------------------+---+ import scala.collection.JavaConverters._ val touchedFileNames = touchedFilesAccum . value . asScala . toSeq Use the Stages tab in web UI to review the accumulator values. findTouchedFiles prints out the following TRACE message to the logs: findTouchedFiles: matched files: [touchedFileNames] findTouchedFiles generateCandidateFileMap for the files that match the target-only predicates . findTouchedFiles getTouchedFile for every touched file name. findTouchedFiles updates the following performance metrics: numTargetFilesBeforeSkipping and adds the numOfFiles of the Snapshot of the given OptimisticTransaction numTargetFilesAfterSkipping and adds the number of the files that match the target-only predicates numTargetFilesRemoved and adds the number of the touched files In the end, findTouchedFiles gives the touched files (as AddFile s). Writing All Changes \u00b6 writeAllChanges ( spark : SparkSession , deltaTxn : OptimisticTransaction , filesToRewrite : Seq [ AddFile ]) : Seq [ AddFile ] writeAllChanges builds the target output columns (possibly with some null s for the target columns that are not in the current schema). writeAllChanges builds a target logical query plan for the AddFiles . writeAllChanges determines a join type to use ( rightOuter or fullOuter ). writeAllChanges prints out the following DEBUG message to the logs: writeAllChanges using [joinType] join: source.output: [outputSet] target.output: [outputSet] condition: [condition] newTarget.output: [outputSet] writeAllChanges creates a joinedDF DataFrame that is a join of the DataFrames for the source and the new target logical plans with the given join condition and the join type . writeAllChanges creates a JoinedRowProcessor that is then used to map over partitions of the joined DataFrame . writeAllChanges prints out the following DEBUG message to the logs: writeAllChanges: join output plan: [outputDF.queryExecution] writeAllChanges requests the input OptimisticTransaction to writeFiles (possibly repartitioning by the partition columns if table is partitioned and spark.databricks.delta.merge.repartitionBeforeWrite.enabled configuration property is enabled). writeAllChanges is used when MergeIntoCommand is requested to run . Building Target Logical Query Plan for AddFiles \u00b6 buildTargetPlanWithFiles ( deltaTxn : OptimisticTransaction , files : Seq [ AddFile ]) : LogicalPlan buildTargetPlanWithFiles creates a DataFrame to represent the given AddFile s to access the analyzed logical query plan. buildTargetPlanWithFiles requests the given OptimisticTransaction for the DeltaLog to create a DataFrame (for the Snapshot and the given AddFile s). In the end, buildTargetPlanWithFiles creates a Project logical operator with Alias expressions so the output columns of the analyzed logical query plan (of the DataFrame of the AddFiles ) reference the target's output columns (by name). Note The output columns of the target delta table are associated with a OptimisticTransaction as the Metadata . deltaTxn . metadata . schema writeInsertsOnlyWhenNoMatchedClauses \u00b6 writeInsertsOnlyWhenNoMatchedClauses ( spark : SparkSession , deltaTxn : OptimisticTransaction ) : Seq [ AddFile ] writeInsertsOnlyWhenNoMatchedClauses ...FIXME Exceptions \u00b6 run throws an AnalysisException when the target schema is different than the delta table's (has changed after analysis phase): The schema of your Delta table has changed in an incompatible way since your DataFrame or DeltaTable object was created. Please redefine your DataFrame or DeltaTable object. Changes: [schemaDiff] This check can be turned off by setting the session configuration key spark.databricks.delta.checkLatestSchemaOnRead to false. Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.commands.MergeIntoCommand=ALL Refer to Logging .","title":"MergeIntoCommand"},{"location":"commands/MergeIntoCommand/#mergeintocommand","text":"MergeIntoCommand is a DeltaCommand that represents a DeltaMergeInto logical command at execution time. MergeIntoCommand is a logical command (Spark SQL's RunnableCommand ). Tip Learn more on the internals of MergeIntoCommand in Demo: Merge Operation .","title":"MergeIntoCommand"},{"location":"commands/MergeIntoCommand/#performance-metrics","text":"Name web UI numSourceRows number of source rows numTargetRowsCopied number of target rows rewritten unmodified numTargetRowsInserted number of inserted rows numTargetRowsUpdated number of updated rows numTargetRowsDeleted number of deleted rows numTargetFilesBeforeSkipping number of target files before skipping numTargetFilesAfterSkipping number of target files after skipping numTargetFilesRemoved number of files removed to target numTargetFilesAdded number of files added to target","title":"Performance Metrics"},{"location":"commands/MergeIntoCommand/#number-of-target-rows-rewritten-unmodified","text":"numTargetRowsCopied performance metric (like the other metrics ) is turned into a non-deterministic user-defined function (UDF). numTargetRowsCopied becomes incrNoopCountExpr UDF. incrNoopCountExpr UDF is resolved on a joined plan and used to create a JoinedRowProcessor for processing partitions of the joined plan Dataset .","title":" number of target rows rewritten unmodified"},{"location":"commands/MergeIntoCommand/#creating-instance","text":"MergeIntoCommand takes the following to be created: Source Data Target Data ( LogicalPlan ) TahoeFileIndex Condition Expression Matched Clauses ( Seq[DeltaMergeIntoMatchedClause] ) Optional Non-Matched Clause ( Option[DeltaMergeIntoInsertClause] ) Migrated Schema MergeIntoCommand is created when PreprocessTableMerge logical resolution rule is executed (on a DeltaMergeInto logical command).","title":"Creating Instance"},{"location":"commands/MergeIntoCommand/#source-data-to-merge-from","text":"When created , MergeIntoCommand is given a LogicalPlan for the source data to merge from (referred to internally as source ). The source LogicalPlan is used twice: Firstly, in one of the following: An inner join (in findTouchedFiles ) that is count in web UI A leftanti join (in writeInsertsOnlyWhenNoMatchedClauses ) Secondly, in rightOuter or fullOuter join (in writeAllChanges ) Tip Enable DEBUG logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger to see the inner-workings of writeAllChanges .","title":" Source Data (to Merge From)"},{"location":"commands/MergeIntoCommand/#target-deltalog","text":"targetDeltaLog : DeltaLog targetDeltaLog is the DeltaLog of the TahoeFileIndex . targetDeltaLog is used for the following: Start a new transaction when executed To access the Data Path when finding files to rewrite Lazy Value targetDeltaLog is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards.","title":" Target DeltaLog"},{"location":"commands/MergeIntoCommand/#executing-command","text":"run ( spark : SparkSession ) : Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run requests the target DeltaLog to start a new transaction . With spark.databricks.delta.schema.autoMerge.enabled configuration property enabled, run updates the metadata (of the transaction). run determines Delta actions ( RemoveFile s and AddFile s). Describe deltaActions part With spark.databricks.delta.history.metricsEnabled configuration property enabled, run requests the current transaction to register SQL metrics for the Delta operation . run requests the current transaction to commit (with the Delta actions and Merge operation). run records the Delta event. run posts a SparkListenerDriverAccumUpdates Spark event (with the metrics). In the end, run requests the CacheManager to recacheByPlan .","title":" Executing Command"},{"location":"commands/MergeIntoCommand/#finding-files-to-rewrite","text":"findTouchedFiles ( spark : SparkSession , deltaTxn : OptimisticTransaction ) : Seq [ AddFile ] Important findTouchedFiles is such a fine piece of art ( a gem ). It uses a custom accumulator, a UDF (to use this accumulator to record touched file names) and input_file_name() standard function for the names of the files read. It is always worth keeping in mind that Delta Lake uses files for data storage and that is why input_file_name() standard function works. It would not work for non-file-based data sources. Example 1: Understanding the Internals of findTouchedFiles The following query writes out a 10-element dataset using the default parquet data source to /tmp/parquet directory: val target = \"/tmp/parquet\" spark . range ( 10 ). write . save ( target ) The number of parquet part files varies based on the number of partitions (CPU cores). The following query loads the parquet dataset back alongside input_file_name() standard function to mimic findTouchedFiles 's behaviour. val FILE_NAME_COL = \"_file_name_\" val dataFiles = spark . read . parquet ( target ). withColumn ( FILE_NAME_COL , input_file_name ()) scala> dataFiles.show(truncate = false) +---+---------------------------------------------------------------------------------------+ |id |_file_name_ | +---+---------------------------------------------------------------------------------------+ |4 |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |0 |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |3 |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |6 |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |1 |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |8 |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |2 |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |7 |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |5 |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |9 |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| +---+---------------------------------------------------------------------------------------+ As you may have thought, not all part files have got data and so they are not included in the dataset. That is when findTouchedFiles uses groupBy operator and count action to calculate match frequency. val counts = dataFiles.groupBy(FILE_NAME_COL).count() scala> counts.show(truncate = false) +---------------------------------------------------------------------------------------+-----+ |_file_name_ |count| +---------------------------------------------------------------------------------------+-----+ |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | +---------------------------------------------------------------------------------------+-----+ Let's load all the part files in the /tmp/parquet directory and find which file(s) have no data. import scala.sys.process._ val cmd = ( s\"ls $target \" #| \"grep .parquet\" ). lineStream val allFiles = cmd . toArray . toSeq . toDF ( FILE_NAME_COL ) . select ( concat ( lit ( s\"file:// $target /\" ), col ( FILE_NAME_COL )) as FILE_NAME_COL ) val joinType = \"left_anti\" // MergeIntoCommand uses inner as it wants data file val noDataFiles = allFiles . join ( dataFiles , Seq ( FILE_NAME_COL ), joinType ) Mind that the data vs non-data datasets could be different, but that should not \"interfere\" with the main reasoning flow. scala> noDataFiles.show(truncate = false) +---------------------------------------------------------------------------------------+ |_file_name_ | +---------------------------------------------------------------------------------------+ |file:///tmp/parquet/part-00000-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| +---------------------------------------------------------------------------------------+ findTouchedFiles registers an accumulator to collect all the distinct files that need to be rewritten ( touched files ). Note The name of the accumulator is internal.metrics.MergeIntoDelta.touchedFiles and internal.metrics part is supposed to hide it from web UI as potentially large (set of file names to be rewritten). findTouchedFiles defines a nondeterministic UDF that adds the file names to the accumulator ( recordTouchedFileName ). findTouchedFiles splits conjunctive predicates ( And binary expressions) in the condition expression and collects the predicates that use the target 's columns ( targetOnlyPredicates ). findTouchedFiles requests the given OptimisticTransaction for the files that match the target-only predicates (and creates a dataSkippedFiles collection of AddFile s). Note This step looks similar to filter predicate pushdown . findTouchedFiles creates one DataFrame for the source data (using Dataset.ofRows utility). Tip Learn more about Dataset.ofRows utility in The Internals of Spark SQL online book. findTouchedFiles builds a logical query plan for the files (matching the predicates) and creates another DataFrame for the target data. findTouchedFiles adds two columns to the target dataframe: _row_id_ for monotonically_increasing_id() standard function _file_name_ for input_file_name() standard function findTouchedFiles creates (a DataFrame that is) an INNER JOIN of the source and target DataFrame s using the condition expression. findTouchedFiles takes the joined dataframe and selects _row_id_ column and the recordTouchedFileName UDF on the _file_name_ column as one . The DataFrame is internally known as collectTouchedFiles . findTouchedFiles uses groupBy operator on _row_id_ to calculate a sum of all the values in the one column (as count column) in the two-column collectTouchedFiles dataset. The DataFrame is internally known as matchedRowCounts . Note No Spark job has been submitted yet. findTouchedFiles is still in \"query preparation\" mode. findTouchedFiles uses filter on the count column (in the matchedRowCounts dataset) with values greater than 1 . If there are any, findTouchedFiles throws an UnsupportedOperationException exception: Cannot perform MERGE as multiple source rows matched and attempted to update the same target row in the Delta table. By SQL semantics of merge, when multiple source rows match on the same target row, the update operation is ambiguous as it is unclear which source should be used to update the matching target row. You can preprocess the source table to eliminate the possibility of multiple matches. Note Since findTouchedFiles uses count action there should be a Spark SQL query reported (and possibly Spark jobs) in web UI. findTouchedFiles requests the touchedFilesAccum accumulator for the touched file names. Example 2: Understanding the Internals of findTouchedFiles val TOUCHED_FILES_ACCUM_NAME = \"MergeIntoDelta.touchedFiles\" val touchedFilesAccum = spark . sparkContext . collectionAccumulator [ String ]( TOUCHED_FILES_ACCUM_NAME ) val recordTouchedFileName = udf { ( fileName : String ) => { touchedFilesAccum . add ( fileName ) 1 }}. asNondeterministic () val target = \"/tmp/parquet\" spark . range ( 10 ). write . save ( target ) val FILE_NAME_COL = \"_file_name_\" val dataFiles = spark . read . parquet ( target ). withColumn ( FILE_NAME_COL , input_file_name ()) val collectTouchedFiles = dataFiles . select ( col ( FILE_NAME_COL ), recordTouchedFileName ( col ( FILE_NAME_COL )). as ( \"one\" )) scala> collectTouchedFiles.show(truncate = false) +---------------------------------------------------------------------------------------+---+ |_file_name_ |one| +---------------------------------------------------------------------------------------+---+ |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | +---------------------------------------------------------------------------------------+---+ import scala.collection.JavaConverters._ val touchedFileNames = touchedFilesAccum . value . asScala . toSeq Use the Stages tab in web UI to review the accumulator values. findTouchedFiles prints out the following TRACE message to the logs: findTouchedFiles: matched files: [touchedFileNames] findTouchedFiles generateCandidateFileMap for the files that match the target-only predicates . findTouchedFiles getTouchedFile for every touched file name. findTouchedFiles updates the following performance metrics: numTargetFilesBeforeSkipping and adds the numOfFiles of the Snapshot of the given OptimisticTransaction numTargetFilesAfterSkipping and adds the number of the files that match the target-only predicates numTargetFilesRemoved and adds the number of the touched files In the end, findTouchedFiles gives the touched files (as AddFile s).","title":" Finding Files to Rewrite"},{"location":"commands/MergeIntoCommand/#writing-all-changes","text":"writeAllChanges ( spark : SparkSession , deltaTxn : OptimisticTransaction , filesToRewrite : Seq [ AddFile ]) : Seq [ AddFile ] writeAllChanges builds the target output columns (possibly with some null s for the target columns that are not in the current schema). writeAllChanges builds a target logical query plan for the AddFiles . writeAllChanges determines a join type to use ( rightOuter or fullOuter ). writeAllChanges prints out the following DEBUG message to the logs: writeAllChanges using [joinType] join: source.output: [outputSet] target.output: [outputSet] condition: [condition] newTarget.output: [outputSet] writeAllChanges creates a joinedDF DataFrame that is a join of the DataFrames for the source and the new target logical plans with the given join condition and the join type . writeAllChanges creates a JoinedRowProcessor that is then used to map over partitions of the joined DataFrame . writeAllChanges prints out the following DEBUG message to the logs: writeAllChanges: join output plan: [outputDF.queryExecution] writeAllChanges requests the input OptimisticTransaction to writeFiles (possibly repartitioning by the partition columns if table is partitioned and spark.databricks.delta.merge.repartitionBeforeWrite.enabled configuration property is enabled). writeAllChanges is used when MergeIntoCommand is requested to run .","title":" Writing All Changes"},{"location":"commands/MergeIntoCommand/#building-target-logical-query-plan-for-addfiles","text":"buildTargetPlanWithFiles ( deltaTxn : OptimisticTransaction , files : Seq [ AddFile ]) : LogicalPlan buildTargetPlanWithFiles creates a DataFrame to represent the given AddFile s to access the analyzed logical query plan. buildTargetPlanWithFiles requests the given OptimisticTransaction for the DeltaLog to create a DataFrame (for the Snapshot and the given AddFile s). In the end, buildTargetPlanWithFiles creates a Project logical operator with Alias expressions so the output columns of the analyzed logical query plan (of the DataFrame of the AddFiles ) reference the target's output columns (by name). Note The output columns of the target delta table are associated with a OptimisticTransaction as the Metadata . deltaTxn . metadata . schema","title":" Building Target Logical Query Plan for AddFiles"},{"location":"commands/MergeIntoCommand/#writeinsertsonlywhennomatchedclauses","text":"writeInsertsOnlyWhenNoMatchedClauses ( spark : SparkSession , deltaTxn : OptimisticTransaction ) : Seq [ AddFile ] writeInsertsOnlyWhenNoMatchedClauses ...FIXME","title":" writeInsertsOnlyWhenNoMatchedClauses"},{"location":"commands/MergeIntoCommand/#exceptions","text":"run throws an AnalysisException when the target schema is different than the delta table's (has changed after analysis phase): The schema of your Delta table has changed in an incompatible way since your DataFrame or DeltaTable object was created. Please redefine your DataFrame or DeltaTable object. Changes: [schemaDiff] This check can be turned off by setting the session configuration key spark.databricks.delta.checkLatestSchemaOnRead to false.","title":" Exceptions"},{"location":"commands/MergeIntoCommand/#logging","text":"Enable ALL logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.commands.MergeIntoCommand=ALL Refer to Logging .","title":"Logging"},{"location":"commands/UpdateCommand/","text":"UpdateCommand \u00b6 UpdateCommand is...FIXME == [[run]] Running Command -- run Method [source, scala] \u00b6 run(sparkSession: SparkSession): Seq[Row] \u00b6 NOTE: run is part of the RunnableCommand contract to...FIXME. run ...FIXME == [[performUpdate]] performUpdate Internal Method [source, scala] \u00b6 performUpdate( sparkSession: SparkSession, deltaLog: DeltaLog, txn: OptimisticTransaction): Unit performUpdate ...FIXME NOTE: performUpdate is used exclusively when UpdateCommand is requested to < >. == [[rewriteFiles]] rewriteFiles Internal Method [source, scala] \u00b6 rewriteFiles( spark: SparkSession, txn: OptimisticTransaction, rootPath: Path, inputLeafFiles: Seq[String], nameToAddFileMap: Map[String, AddFile], condition: Expression): Seq[AddFile] rewriteFiles ...FIXME NOTE: rewriteFiles is used exclusively when UpdateCommand is requested to < >.","title":"UpdateCommand"},{"location":"commands/UpdateCommand/#updatecommand","text":"UpdateCommand is...FIXME == [[run]] Running Command -- run Method","title":"UpdateCommand"},{"location":"commands/UpdateCommand/#source-scala","text":"","title":"[source, scala]"},{"location":"commands/UpdateCommand/#runsparksession-sparksession-seqrow","text":"NOTE: run is part of the RunnableCommand contract to...FIXME. run ...FIXME == [[performUpdate]] performUpdate Internal Method","title":"run(sparkSession: SparkSession): Seq[Row]"},{"location":"commands/UpdateCommand/#source-scala_1","text":"performUpdate( sparkSession: SparkSession, deltaLog: DeltaLog, txn: OptimisticTransaction): Unit performUpdate ...FIXME NOTE: performUpdate is used exclusively when UpdateCommand is requested to < >. == [[rewriteFiles]] rewriteFiles Internal Method","title":"[source, scala]"},{"location":"commands/UpdateCommand/#source-scala_2","text":"rewriteFiles( spark: SparkSession, txn: OptimisticTransaction, rootPath: Path, inputLeafFiles: Seq[String], nameToAddFileMap: Map[String, AddFile], condition: Expression): Seq[AddFile] rewriteFiles ...FIXME NOTE: rewriteFiles is used exclusively when UpdateCommand is requested to < >.","title":"[source, scala]"},{"location":"commands/VacuumCommand/","text":"VacuumCommand Utility \u2014 Garbage Collecting Delta Table \u00b6 VacuumCommand is a concrete VacuumCommandImpl for gc . Garbage Collecting Of Delta Table \u00b6 gc ( spark : SparkSession , deltaLog : DeltaLog , dryRun : Boolean = true , retentionHours : Option [ Double ] = None , clock : Clock = new SystemClock ) : DataFrame gc requests the given DeltaLog to < > (and give the latest < > of the delta table). [[gc-deleteBeforeTimestamp]] gc...FIXME (deleteBeforeTimestamp) gc prints out the following INFO message to the logs: Starting garbage collection (dryRun = [dryRun]) of untracked files older than [deleteBeforeTimestamp] in [path] [[gc-validFiles]] gc requests the Snapshot for the < > and defines a function for every action (in a partition) that does the following: . FIXME gc converts the mapped state dataset (of actions) into a DataFrame with a single path column. [[gc-allFilesAndDirs]] gc...FIXME gc caches the < > dataset. gc prints out the following INFO message to the logs: Deleting untracked files and empty directories in [path] gc...FIXME gc prints out the following message to standard output: Deleted [filesDeleted] files and directories in a total of [dirCounts] directories. gc...FIXME In the end, gc unpersists the < > dataset. [NOTE] \u00b6 gc is used when: DeltaTableOperations is requested to < > (for < > operator) * < > is executed (for delta-sql.md#VACUUM[VACUUM] SQL command) \u00b6 == [[checkRetentionPeriodSafety]] checkRetentionPeriodSafety Method [source, scala] \u00b6 checkRetentionPeriodSafety( spark: SparkSession, retentionMs: Option[Long], configuredRetention: Long): Unit checkRetentionPeriodSafety ...FIXME NOTE: checkRetentionPeriodSafety is used exclusively when VacuumCommand utility is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.sql.delta.commands.VacuumCommand logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.sql.delta.commands.VacuumCommand=ALL \u00b6 Refer to Logging .","title":"VacuumCommand"},{"location":"commands/VacuumCommand/#vacuumcommand-utility-garbage-collecting-delta-table","text":"VacuumCommand is a concrete VacuumCommandImpl for gc .","title":"VacuumCommand Utility &mdash; Garbage Collecting Delta Table"},{"location":"commands/VacuumCommand/#garbage-collecting-of-delta-table","text":"gc ( spark : SparkSession , deltaLog : DeltaLog , dryRun : Boolean = true , retentionHours : Option [ Double ] = None , clock : Clock = new SystemClock ) : DataFrame gc requests the given DeltaLog to < > (and give the latest < > of the delta table). [[gc-deleteBeforeTimestamp]] gc...FIXME (deleteBeforeTimestamp) gc prints out the following INFO message to the logs: Starting garbage collection (dryRun = [dryRun]) of untracked files older than [deleteBeforeTimestamp] in [path] [[gc-validFiles]] gc requests the Snapshot for the < > and defines a function for every action (in a partition) that does the following: . FIXME gc converts the mapped state dataset (of actions) into a DataFrame with a single path column. [[gc-allFilesAndDirs]] gc...FIXME gc caches the < > dataset. gc prints out the following INFO message to the logs: Deleting untracked files and empty directories in [path] gc...FIXME gc prints out the following message to standard output: Deleted [filesDeleted] files and directories in a total of [dirCounts] directories. gc...FIXME In the end, gc unpersists the < > dataset.","title":" Garbage Collecting Of Delta Table"},{"location":"commands/VacuumCommand/#note","text":"gc is used when: DeltaTableOperations is requested to < > (for < > operator)","title":"[NOTE]"},{"location":"commands/VacuumCommand/#is-executed-for-delta-sqlmdvacuumvacuum-sql-command","text":"== [[checkRetentionPeriodSafety]] checkRetentionPeriodSafety Method","title":"* &lt;&gt; is executed (for delta-sql.md#VACUUM[VACUUM] SQL command)"},{"location":"commands/VacuumCommand/#source-scala","text":"checkRetentionPeriodSafety( spark: SparkSession, retentionMs: Option[Long], configuredRetention: Long): Unit checkRetentionPeriodSafety ...FIXME NOTE: checkRetentionPeriodSafety is used exclusively when VacuumCommand utility is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.sql.delta.commands.VacuumCommand logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"commands/VacuumCommand/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"commands/VacuumCommand/#log4jloggerorgapachesparksqldeltacommandsvacuumcommandall","text":"Refer to Logging .","title":"log4j.logger.org.apache.spark.sql.delta.commands.VacuumCommand=ALL"},{"location":"commands/VacuumCommandImpl/","text":"VacuumCommandImpl \u00b6 VacuumCommandImpl is...FIXME == [[delete]] delete Method [source,scala] \u00b6 delete( diff: Dataset[String], fs: FileSystem): Long delete...FIXME delete is used when VacuumCommand utility is requested to VacuumCommand.md#gc[gc]","title":"VacuumCommandImpl"},{"location":"commands/VacuumCommandImpl/#vacuumcommandimpl","text":"VacuumCommandImpl is...FIXME == [[delete]] delete Method","title":"VacuumCommandImpl"},{"location":"commands/VacuumCommandImpl/#sourcescala","text":"delete( diff: Dataset[String], fs: FileSystem): Long delete...FIXME delete is used when VacuumCommand utility is requested to VacuumCommand.md#gc[gc]","title":"[source,scala]"},{"location":"commands/VacuumTableCommand/","text":"= VacuumTableCommand VacuumTableCommand is a logical command ( RunnableCommand ) for delta-sql.md#VACUUM[VACUUM] SQL command. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-RunnableCommand.html[RunnableCommand ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. VacuumTableCommand is < > exclusively when DeltaSqlAstBuilder is requested to < >. VacuumTableCommand < > that either the < > or the < > is defined and it is the root directory of a delta table. Partition directories are not supported. [[output]] The output of VacuumTableCommand is a single path column (of type StringType ). == [[creating-instance]] Creating VacuumTableCommand Instance VacuumTableCommand takes the following to be created: [[path]] Path (optional) [[table]] TableIdentifier (optional) [[horizonHours]] Optional horizonHours [[dryRun]] dryRun flag == [[run]] Running Command -- run Method [source, scala] \u00b6 run(sparkSession: SparkSession): Seq[Row] \u00b6 NOTE: run is part of the RunnableCommand contract to...FIXME. run takes the path to vacuum (i.e. either the < > or the < >) and < >. run < > for the delta table and executes < > utility (passing in the DeltaLog instance, the < > and the < > options). run throws an AnalysisException when executed for a non-root directory of a delta table: Please provide the base path ([baseDeltaPath]) when Vacuuming Delta tables. Vacuuming specific partitions is currently not supported. run throws an AnalysisException when executed for a DeltaLog with the snapshot version being -1 : [deltaTableIdentifier] is not a Delta table. VACUUM is only supported for Delta tables.","title":"VacuumTableCommand"},{"location":"commands/VacuumTableCommand/#source-scala","text":"","title":"[source, scala]"},{"location":"commands/VacuumTableCommand/#runsparksession-sparksession-seqrow","text":"NOTE: run is part of the RunnableCommand contract to...FIXME. run takes the path to vacuum (i.e. either the < > or the < >) and < >. run < > for the delta table and executes < > utility (passing in the DeltaLog instance, the < > and the < > options). run throws an AnalysisException when executed for a non-root directory of a delta table: Please provide the base path ([baseDeltaPath]) when Vacuuming Delta tables. Vacuuming specific partitions is currently not supported. run throws an AnalysisException when executed for a DeltaLog with the snapshot version being -1 : [deltaTableIdentifier] is not a Delta table. VACUUM is only supported for Delta tables.","title":"run(sparkSession: SparkSession): Seq[Row]"},{"location":"commands/WriteIntoDelta/","text":"WriteIntoDelta Command \u00b6 WriteIntoDelta is a < > that can write < > transactionally into a < >. [[demo]] .Demo [source, scala] import org.apache.spark.sql.delta.commands.WriteIntoDelta import org.apache.spark.sql.delta.DeltaLog import org.apache.spark.sql.SaveMode import org.apache.spark.sql.delta.DeltaOptions val tableName = \"/tmp/delta/t1\" val data = spark.range(5).toDF val writeCmd = WriteIntoDelta( deltaLog = DeltaLog.forTable(spark, tableName), mode = SaveMode.Overwrite, options = new DeltaOptions(Map.empty[String, String], spark.sessionState.conf), partitionColumns = Seq.empty[String], configuration = Map.empty[String, String], data) // Review web UI @ http://localhost:4040 writeCmd.run(spark) \u00b6 [[ImplicitMetadataOperation]] WriteIntoDelta is an < > of a < >. [[RunnableCommand]] WriteIntoDelta is a logical command ( RunnableCommand ). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-RunnableCommand.html[RunnableCommand ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. WriteIntoDelta is < > when: DeltaLog is requested to < > (when DeltaDataSource is requested to create a relation as a < > or a < >) DeltaDataSource is requested to < > (as a < >) == [[creating-instance]] Creating WriteIntoDelta Instance WriteIntoDelta takes the following to be created: [[deltaLog]] < > [[mode]] SaveMode [[options]] < > [[partitionColumns]] Names of the partition columns ( Seq[String] ) [[configuration]] Configuration ( Map[String, String] ) [[data]] Data ( DataFrame ) == [[run]] Running Command -- run Method [source, scala] \u00b6 run( sparkSession: SparkSession): Seq[Row] NOTE: run is part of the RunnableCommand contract to run a command. run requests the < > to < >. run < > and requests the OptimisticTransaction to < >. == [[write]] write Method [source, scala] \u00b6 write( txn: OptimisticTransaction, sparkSession: SparkSession): Seq[Action] write checks out whether the write operation is to a delta table that already exists. If so (i.e. the < > of the transaction is above -1 ), write branches per the < >: For ErrorIfExists , write throws an AnalysisException : + [path] already exists. For Ignore , write does nothing For Overwrite , write requests the < > to < > write < >. write ...FIXME NOTE: write is used exclusively when WriteIntoDelta is requested to < >.","title":"WriteIntoDelta"},{"location":"commands/WriteIntoDelta/#writeintodelta-command","text":"WriteIntoDelta is a < > that can write < > transactionally into a < >. [[demo]] .Demo [source, scala] import org.apache.spark.sql.delta.commands.WriteIntoDelta import org.apache.spark.sql.delta.DeltaLog import org.apache.spark.sql.SaveMode import org.apache.spark.sql.delta.DeltaOptions val tableName = \"/tmp/delta/t1\" val data = spark.range(5).toDF val writeCmd = WriteIntoDelta( deltaLog = DeltaLog.forTable(spark, tableName), mode = SaveMode.Overwrite, options = new DeltaOptions(Map.empty[String, String], spark.sessionState.conf), partitionColumns = Seq.empty[String], configuration = Map.empty[String, String], data) // Review web UI @ http://localhost:4040","title":"WriteIntoDelta Command"},{"location":"commands/WriteIntoDelta/#writecmdrunspark","text":"[[ImplicitMetadataOperation]] WriteIntoDelta is an < > of a < >. [[RunnableCommand]] WriteIntoDelta is a logical command ( RunnableCommand ). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-RunnableCommand.html[RunnableCommand ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. WriteIntoDelta is < > when: DeltaLog is requested to < > (when DeltaDataSource is requested to create a relation as a < > or a < >) DeltaDataSource is requested to < > (as a < >) == [[creating-instance]] Creating WriteIntoDelta Instance WriteIntoDelta takes the following to be created: [[deltaLog]] < > [[mode]] SaveMode [[options]] < > [[partitionColumns]] Names of the partition columns ( Seq[String] ) [[configuration]] Configuration ( Map[String, String] ) [[data]] Data ( DataFrame ) == [[run]] Running Command -- run Method","title":"writeCmd.run(spark)"},{"location":"commands/WriteIntoDelta/#source-scala","text":"run( sparkSession: SparkSession): Seq[Row] NOTE: run is part of the RunnableCommand contract to run a command. run requests the < > to < >. run < > and requests the OptimisticTransaction to < >. == [[write]] write Method","title":"[source, scala]"},{"location":"commands/WriteIntoDelta/#source-scala_1","text":"write( txn: OptimisticTransaction, sparkSession: SparkSession): Seq[Action] write checks out whether the write operation is to a delta table that already exists. If so (i.e. the < > of the transaction is above -1 ), write branches per the < >: For ErrorIfExists , write throws an AnalysisException : + [path] already exists. For Ignore , write does nothing For Overwrite , write requests the < > to < > write < >. write ...FIXME NOTE: write is used exclusively when WriteIntoDelta is requested to < >.","title":"[source, scala]"},{"location":"commands/vacuum/","text":"Vacuum Command \u00b6 Vacuum command does...FIXME (see < >) Vacuum command can be executed as delta-sql.md#VACUUM[VACUUM] SQL command or < > operator. scala> sql(\"VACUUM delta.`/tmp/delta/t1`\").show Deleted 0 files and directories in a total of 2 directories. +------------------+ | path| +------------------+ |file:/tmp/delta/t1| +------------------+","title":"Vacuum"},{"location":"commands/vacuum/#vacuum-command","text":"Vacuum command does...FIXME (see < >) Vacuum command can be executed as delta-sql.md#VACUUM[VACUUM] SQL command or < > operator. scala> sql(\"VACUUM delta.`/tmp/delta/t1`\").show Deleted 0 files and directories in a total of 2 directories. +------------------+ | path| +------------------+ |file:/tmp/delta/t1| +------------------+","title":"Vacuum Command"},{"location":"contenders/","text":"Contenders \u00b6 As it happens in the open source software world, Delta Lake is not alone in the area of Data Lakes on top of Apache Spark. The following is a list of some other open source projects that seems to compete or cover the same use cases. Apache Iceberg \u00b6 Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to Presto and Spark that use a high-performance format that works just like a SQL table. Videos \u00b6 ACID ORC, Iceberg, and Delta Lake\u2014An Overview of Table Formats for Large Scale Storage and Analytics Introducing Iceberg Tables designed for object stores Introducing Apache Iceberg: Tables Designed for Object Stores Iceberg: a fast table format for S3 Apache Hudi \u00b6 Apache Hudi ingests and manages storage of large analytical datasets over DFS (HDFS or cloud stores) and provides three logical views for query access. Videos \u00b6 Hoodie: An Open Source Incremental Processing Framework From Uber Powering Uber's global network analytics pipelines in real-time with Apache Hudi","title":"Contenders"},{"location":"contenders/#contenders","text":"As it happens in the open source software world, Delta Lake is not alone in the area of Data Lakes on top of Apache Spark. The following is a list of some other open source projects that seems to compete or cover the same use cases.","title":"Contenders"},{"location":"contenders/#apache-iceberg","text":"Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to Presto and Spark that use a high-performance format that works just like a SQL table.","title":"Apache Iceberg"},{"location":"contenders/#videos","text":"ACID ORC, Iceberg, and Delta Lake\u2014An Overview of Table Formats for Large Scale Storage and Analytics Introducing Iceberg Tables designed for object stores Introducing Apache Iceberg: Tables Designed for Object Stores Iceberg: a fast table format for S3","title":"Videos"},{"location":"contenders/#apache-hudi","text":"Apache Hudi ingests and manages storage of large analytical datasets over DFS (HDFS or cloud stores) and provides three logical views for query access.","title":"Apache Hudi"},{"location":"contenders/#videos_1","text":"Hoodie: An Open Source Incremental Processing Framework From Uber Powering Uber's global network analytics pipelines in real-time with Apache Hudi","title":"Videos"},{"location":"demo/","text":"Demos \u00b6 The following demos are available: Merge Operation Converting Parquet Dataset Into Delta Format Stream Processing of Delta Table Using Delta Lake as Streaming Sink in Structured Streaming Debugging Delta Lake Using IntelliJ IDEA Observing Transaction Retries DeltaTable, DeltaLog And Snapshots Schema Evolution User Metadata for Labelling Commits","title":"Welcome"},{"location":"demo/#demos","text":"The following demos are available: Merge Operation Converting Parquet Dataset Into Delta Format Stream Processing of Delta Table Using Delta Lake as Streaming Sink in Structured Streaming Debugging Delta Lake Using IntelliJ IDEA Observing Transaction Retries DeltaTable, DeltaLog And Snapshots Schema Evolution User Metadata for Labelling Commits","title":"Demos"},{"location":"demo/Converting-Parquet-Dataset-Into-Delta-Format/","text":"Demo: Converting Parquet Dataset Into Delta Format \u00b6 /* spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ assert(spark.version.matches(\"2.4.[2-6]\"), \"Delta Lake supports Spark 2.4.2+\") import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val deltaLake = \"/tmp/delta\" // Create parquet table val users = s\"$deltaLake/users\" import spark.implicits._ val data = Seq( (0L, \"Agata\", \"Warsaw\", \"Poland\"), (1L, \"Jacek\", \"Warsaw\", \"Poland\"), (2L, \"Bartosz\", \"Paris\", \"France\") ).toDF(\"id\", \"name\", \"city\", \"country\") data .write .format(\"parquet\") .partitionBy(\"city\", \"country\") .mode(\"overwrite\") .save(users) // TIP: Use git to version the users directory // to track the changes for import // CONVERT TO DELTA only supports parquet tables // TableIdentifier should be parquet.`users` // Use TableIdentifier to refer to the parquet table // The path itself would work too val tableId = s\"parquet.`$users`\" val partitionSchema = \"city STRING, country STRING\" // Import users table into Delta Lake // Well, convert the parquet table into delta table // Use web UI to monitor execution, e.g. http://localhost:4040 import io.delta.tables.DeltaTable val dt = DeltaTable.convertToDelta(spark, tableId, partitionSchema) assert(dt.isInstanceOf[DeltaTable]) // users table is now in delta format assert(DeltaTable.isDeltaTable(users))","title":"Converting Parquet Dataset Into Delta Format"},{"location":"demo/Converting-Parquet-Dataset-Into-Delta-Format/#demo-converting-parquet-dataset-into-delta-format","text":"/* spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ assert(spark.version.matches(\"2.4.[2-6]\"), \"Delta Lake supports Spark 2.4.2+\") import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val deltaLake = \"/tmp/delta\" // Create parquet table val users = s\"$deltaLake/users\" import spark.implicits._ val data = Seq( (0L, \"Agata\", \"Warsaw\", \"Poland\"), (1L, \"Jacek\", \"Warsaw\", \"Poland\"), (2L, \"Bartosz\", \"Paris\", \"France\") ).toDF(\"id\", \"name\", \"city\", \"country\") data .write .format(\"parquet\") .partitionBy(\"city\", \"country\") .mode(\"overwrite\") .save(users) // TIP: Use git to version the users directory // to track the changes for import // CONVERT TO DELTA only supports parquet tables // TableIdentifier should be parquet.`users` // Use TableIdentifier to refer to the parquet table // The path itself would work too val tableId = s\"parquet.`$users`\" val partitionSchema = \"city STRING, country STRING\" // Import users table into Delta Lake // Well, convert the parquet table into delta table // Use web UI to monitor execution, e.g. http://localhost:4040 import io.delta.tables.DeltaTable val dt = DeltaTable.convertToDelta(spark, tableId, partitionSchema) assert(dt.isInstanceOf[DeltaTable]) // users table is now in delta format assert(DeltaTable.isDeltaTable(users))","title":"Demo: Converting Parquet Dataset Into Delta Format"},{"location":"demo/Debugging-Delta-Lake-Using-IntelliJ-IDEA/","text":"Demo: Debugging Delta Lake Using IntelliJ IDEA \u00b6 Import Delta Lake's sources to IntelliJ IDEA. Configure a new Remote debug configuration in IntelliJ IDEA (e.g. Run > Debug > Edit Configurations...) and simply give it a name and save. Tip Use Option+Ctrl+D to access Debug menu on mac OS. Run spark-shell as follows to enable remote JVM for debugging. export SPARK_SUBMIT_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\ --conf spark.databricks.delta.snapshotPartitions=1","title":"Debugging Delta Lake Using IntelliJ IDEA"},{"location":"demo/Debugging-Delta-Lake-Using-IntelliJ-IDEA/#demo-debugging-delta-lake-using-intellij-idea","text":"Import Delta Lake's sources to IntelliJ IDEA. Configure a new Remote debug configuration in IntelliJ IDEA (e.g. Run > Debug > Edit Configurations...) and simply give it a name and save. Tip Use Option+Ctrl+D to access Debug menu on mac OS. Run spark-shell as follows to enable remote JVM for debugging. export SPARK_SUBMIT_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\ --conf spark.databricks.delta.snapshotPartitions=1","title":"Demo: Debugging Delta Lake Using IntelliJ IDEA"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/","text":"Demo: DeltaTable, DeltaLog And Snapshots \u00b6 Create Delta Table \u00b6 import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val name = \"users\" sql(s\"DROP TABLE IF EXISTS $name\") sql(s\"\"\" | CREATE TABLE $name (id bigint, name string, city string, country string) | USING delta \"\"\".stripMargin) Access Transaction Log (DeltaLog) \u00b6 import org.apache.spark.sql.catalyst.TableIdentifier val tid = TableIdentifier(name) import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, tid) Update the state of the delta table to the most recent version. val snapshot = deltaLog.update() assert(snapshot.version == 0) val state = snapshot.state scala> :type state org.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.SingleAction] Review the cached RDD for the state snapshot in the Storage tab of the web UI (e.g. http://localhost:4040/storage/ ). The \"version\" part of Delta Table State name of the cached RDD should match the version of the snapshot // Show the changes (actions) scala> snapshot.state.show +----+----+------+--------------------+--------+----------+ | txn| add|remove| metaData|protocol|commitInfo| +----+----+------+--------------------+--------+----------+ |null|null| null| null| [1, 2]| null| |null|null| null|[5156c9e3-9668-43...| null| null| +----+----+------+--------------------+--------+----------+ DeltaTable as DataFrame \u00b6 import io.delta.tables.DeltaTable val dt = DeltaTable.forName(name) scala> dt.history.select('version, 'operation, 'operationParameters, 'operationMetrics).show(truncate = false) +-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+ |version| timestamp|userId|userName| operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics|userMetadata| +-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+ | 0|2020-09-29 10:31:30| null| null|CREATE TABLE|[isManaged -> tru...|null| null| null| null| null| true| []| null| +-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+ val users = dt.toDF scala> users.show +---+----+----+-------+ | id|name|city|country| +---+----+----+-------+ +---+----+----+-------+ Add new users \u00b6 val newUsers = Seq( (0L, \"Agata\", \"Warsaw\", \"Poland\"), (1L, \"Bartosz\", \"Paris\", \"France\") ).toDF(\"id\", \"name\", \"city\", \"country\") scala> newUsers.show +---+-------+------+-------+ | id| name| city|country| +---+-------+------+-------+ | 0| Agata|Warsaw| Poland| | 1|Bartosz| Paris| France| +---+-------+------+-------+ newUsers.write.format(\"delta\").mode(\"append\").saveAsTable(name) assert(deltaLog.snapshot.version == 1) Review the cached RDD for the state snapshot in the Storage tab of the web UI (e.g. http://localhost:4040/storage/ ). The \"version\" part of Delta Table State name of the cached RDD should match the version of the snapshot scala> users.show +---+-------+------+-------+ | id| name| city|country| +---+-------+------+-------+ | 1|Bartosz| Paris| France| | 0| Agata|Warsaw| Poland| +---+-------+------+-------+ scala> dt.history.select('version, 'operation, 'operationParameters, 'operationMetrics).show(truncate = false) +-------+------------+------------------------------------------------------------------------+-----------------------------------------------------------+ |version|operation |operationParameters |operationMetrics | +-------+------------+------------------------------------------------------------------------+-----------------------------------------------------------+ |1 |WRITE |[mode -> Append, partitionBy -> []] |[numFiles -> 2, numOutputBytes -> 2299, numOutputRows -> 2]| |0 |CREATE TABLE|[isManaged -> true, description ->, partitionBy -> [], properties -> {}]|[] | +-------+------------+------------------------------------------------------------------------+-----------------------------------------------------------+","title":"DeltaTable, DeltaLog And Snapshots"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#demo-deltatable-deltalog-and-snapshots","text":"","title":"Demo: DeltaTable, DeltaLog And Snapshots"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#create-delta-table","text":"import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val name = \"users\" sql(s\"DROP TABLE IF EXISTS $name\") sql(s\"\"\" | CREATE TABLE $name (id bigint, name string, city string, country string) | USING delta \"\"\".stripMargin)","title":"Create Delta Table"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#access-transaction-log-deltalog","text":"import org.apache.spark.sql.catalyst.TableIdentifier val tid = TableIdentifier(name) import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, tid) Update the state of the delta table to the most recent version. val snapshot = deltaLog.update() assert(snapshot.version == 0) val state = snapshot.state scala> :type state org.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.SingleAction] Review the cached RDD for the state snapshot in the Storage tab of the web UI (e.g. http://localhost:4040/storage/ ). The \"version\" part of Delta Table State name of the cached RDD should match the version of the snapshot // Show the changes (actions) scala> snapshot.state.show +----+----+------+--------------------+--------+----------+ | txn| add|remove| metaData|protocol|commitInfo| +----+----+------+--------------------+--------+----------+ |null|null| null| null| [1, 2]| null| |null|null| null|[5156c9e3-9668-43...| null| null| +----+----+------+--------------------+--------+----------+","title":"Access Transaction Log (DeltaLog)"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#deltatable-as-dataframe","text":"import io.delta.tables.DeltaTable val dt = DeltaTable.forName(name) scala> dt.history.select('version, 'operation, 'operationParameters, 'operationMetrics).show(truncate = false) +-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+ |version| timestamp|userId|userName| operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics|userMetadata| +-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+ | 0|2020-09-29 10:31:30| null| null|CREATE TABLE|[isManaged -> tru...|null| null| null| null| null| true| []| null| +-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+ val users = dt.toDF scala> users.show +---+----+----+-------+ | id|name|city|country| +---+----+----+-------+ +---+----+----+-------+","title":"DeltaTable as DataFrame"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#add-new-users","text":"val newUsers = Seq( (0L, \"Agata\", \"Warsaw\", \"Poland\"), (1L, \"Bartosz\", \"Paris\", \"France\") ).toDF(\"id\", \"name\", \"city\", \"country\") scala> newUsers.show +---+-------+------+-------+ | id| name| city|country| +---+-------+------+-------+ | 0| Agata|Warsaw| Poland| | 1|Bartosz| Paris| France| +---+-------+------+-------+ newUsers.write.format(\"delta\").mode(\"append\").saveAsTable(name) assert(deltaLog.snapshot.version == 1) Review the cached RDD for the state snapshot in the Storage tab of the web UI (e.g. http://localhost:4040/storage/ ). The \"version\" part of Delta Table State name of the cached RDD should match the version of the snapshot scala> users.show +---+-------+------+-------+ | id| name| city|country| +---+-------+------+-------+ | 1|Bartosz| Paris| France| | 0| Agata|Warsaw| Poland| +---+-------+------+-------+ scala> dt.history.select('version, 'operation, 'operationParameters, 'operationMetrics).show(truncate = false) +-------+------------+------------------------------------------------------------------------+-----------------------------------------------------------+ |version|operation |operationParameters |operationMetrics | +-------+------------+------------------------------------------------------------------------+-----------------------------------------------------------+ |1 |WRITE |[mode -> Append, partitionBy -> []] |[numFiles -> 2, numOutputBytes -> 2299, numOutputRows -> 2]| |0 |CREATE TABLE|[isManaged -> true, description ->, partitionBy -> [], properties -> {}]|[] | +-------+------------+------------------------------------------------------------------------+-----------------------------------------------------------+","title":"Add new users"},{"location":"demo/Observing-Transaction-Retries/","text":"Demo: Observing Transaction Retries \u00b6 Enable ALL logging level for org.apache.spark.sql.delta.OptimisticTransaction logger. You'll be looking for the following DEBUG message in the logs: Attempting to commit version [version] with 13 actions with Serializable isolation level Start with < > and place the following line breakpoints in OptimisticTransactionImpl : . In OptimisticTransactionImpl.doCommit when a transaction is about to deltaLog.store.write (line 388) . In OptimisticTransactionImpl.doCommit when a transaction is about to checkAndRetry after a FileAlreadyExistsException (line 433) . In OptimisticTransactionImpl.checkAndRetry when a transaction calculates nextAttemptVersion (line 453) In order to interfere with a transaction about to be committed, you will use ROOT:WriteIntoDelta.md[WriteIntoDelta] action (it is simple and does the work). Run the command (copy and paste the ROOT:WriteIntoDelta.md#demo[demo code] to spark-shell using paste mode). You should see the following messages in the logs: scala> writeCmd.run(spark) DeltaLog: DELTA: Updating the Delta table's state OptimisticTransaction: Attempting to commit version 6 with 13 actions with Serializable isolation level That's when you \"commit\" another transaction (to simulate two competing transactional writes). Simply create a delta file for the transaction. The commit version in the message above is 6 so the name of the delta file should be 00000000000000000006.json : $ touch /tmp/delta/t1/_delta_log/00000000000000000006.json F9 in IntelliJ IDEA to resume the WriteIntoDelta command. It should stop at checkAndRetry due to FileAlreadyExistsException . Press F9 twice to resume. You should see the following messages in the logs: OptimisticTransaction: No logical conflicts with deltas [6, 7), retrying. OptimisticTransaction: Attempting to commit version 7 with 13 actions with Serializable isolation level Rinse and repeat. You know the drill already. Happy debugging!","title":"Observing Transaction Retries"},{"location":"demo/Observing-Transaction-Retries/#demo-observing-transaction-retries","text":"Enable ALL logging level for org.apache.spark.sql.delta.OptimisticTransaction logger. You'll be looking for the following DEBUG message in the logs: Attempting to commit version [version] with 13 actions with Serializable isolation level Start with < > and place the following line breakpoints in OptimisticTransactionImpl : . In OptimisticTransactionImpl.doCommit when a transaction is about to deltaLog.store.write (line 388) . In OptimisticTransactionImpl.doCommit when a transaction is about to checkAndRetry after a FileAlreadyExistsException (line 433) . In OptimisticTransactionImpl.checkAndRetry when a transaction calculates nextAttemptVersion (line 453) In order to interfere with a transaction about to be committed, you will use ROOT:WriteIntoDelta.md[WriteIntoDelta] action (it is simple and does the work). Run the command (copy and paste the ROOT:WriteIntoDelta.md#demo[demo code] to spark-shell using paste mode). You should see the following messages in the logs: scala> writeCmd.run(spark) DeltaLog: DELTA: Updating the Delta table's state OptimisticTransaction: Attempting to commit version 6 with 13 actions with Serializable isolation level That's when you \"commit\" another transaction (to simulate two competing transactional writes). Simply create a delta file for the transaction. The commit version in the message above is 6 so the name of the delta file should be 00000000000000000006.json : $ touch /tmp/delta/t1/_delta_log/00000000000000000006.json F9 in IntelliJ IDEA to resume the WriteIntoDelta command. It should stop at checkAndRetry due to FileAlreadyExistsException . Press F9 twice to resume. You should see the following messages in the logs: OptimisticTransaction: No logical conflicts with deltas [6, 7), retrying. OptimisticTransaction: Attempting to commit version 7 with 13 actions with Serializable isolation level Rinse and repeat. You know the drill already. Happy debugging!","title":"Demo: Observing Transaction Retries"},{"location":"demo/Using-Delta-Lake-as-Streaming-Sink-in-Structured-Streaming/","text":"Demo: Using Delta Lake as Streaming Sink in Structured Streaming \u00b6 assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession]) assert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\") // Input data \"format\" case class User(id: Long, name: String, city: String) // Any streaming data source would work // Using memory data source // Gives control over the input stream implicit val ctx = spark.sqlContext import org.apache.spark.sql.execution.streaming.MemoryStream val usersIn = MemoryStream[User] val users = usersIn.toDF val deltaLake = \"/tmp/delta-lake\" val checkpointLocation = \"/tmp/delta-checkpointLocation\" val path = s\"$deltaLake/users\" val partitionBy = \"city\" // The streaming query that writes out to Delta Lake val sq = users .writeStream .format(\"delta\") .option(\"checkpointLocation\", checkpointLocation) .option(\"path\", path) .partitionBy(partitionBy) .start // TIP: You could use git to version the path directory // and track the changes of every micro-batch // TIP: Use web UI to monitor execution, e.g. http://localhost:4040 // FIXME: Use DESCRIBE HISTORY every micro-batch val batch1 = Seq( User(0, \"Agata\", \"Warsaw\"), User(1, \"Jacek\", \"Warsaw\")) val offset = usersIn.addData(batch1) sq.processAllAvailable() val history = s\"DESCRIBE HISTORY delta.`$path`\" val clmns = Seq($\"version\", $\"timestamp\", $\"operation\", $\"operationParameters\", $\"isBlindAppend\") val h = sql(history).select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ val batch2 = Seq( User(2, \"Bartek\", \"Paris\"), User(3, \"Jacek\", \"Paris\")) val offset = usersIn.addData(batch2) sq.processAllAvailable() // You have to execute the history SQL command again // It materializes immediately with whatever data is available at the time val h = sql(history).select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | |1 |2019-12-06 10:13:27|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 1]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ val batch3 = Seq( User(4, \"Gorazd\", \"Ljubljana\")) val offset = usersIn.addData(batch3) sq.processAllAvailable() // Let's use DeltaTable API instead import io.delta.tables.DeltaTable val usersDT = DeltaTable.forPath(path) val h = usersDT.history.select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | |1 |2019-12-06 10:13:27|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 1]|true | |2 |2019-12-06 10:20:56|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 2]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+","title":"Using Delta Lake as Streaming Sink in Structured Streaming"},{"location":"demo/Using-Delta-Lake-as-Streaming-Sink-in-Structured-Streaming/#demo-using-delta-lake-as-streaming-sink-in-structured-streaming","text":"assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession]) assert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\") // Input data \"format\" case class User(id: Long, name: String, city: String) // Any streaming data source would work // Using memory data source // Gives control over the input stream implicit val ctx = spark.sqlContext import org.apache.spark.sql.execution.streaming.MemoryStream val usersIn = MemoryStream[User] val users = usersIn.toDF val deltaLake = \"/tmp/delta-lake\" val checkpointLocation = \"/tmp/delta-checkpointLocation\" val path = s\"$deltaLake/users\" val partitionBy = \"city\" // The streaming query that writes out to Delta Lake val sq = users .writeStream .format(\"delta\") .option(\"checkpointLocation\", checkpointLocation) .option(\"path\", path) .partitionBy(partitionBy) .start // TIP: You could use git to version the path directory // and track the changes of every micro-batch // TIP: Use web UI to monitor execution, e.g. http://localhost:4040 // FIXME: Use DESCRIBE HISTORY every micro-batch val batch1 = Seq( User(0, \"Agata\", \"Warsaw\"), User(1, \"Jacek\", \"Warsaw\")) val offset = usersIn.addData(batch1) sq.processAllAvailable() val history = s\"DESCRIBE HISTORY delta.`$path`\" val clmns = Seq($\"version\", $\"timestamp\", $\"operation\", $\"operationParameters\", $\"isBlindAppend\") val h = sql(history).select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ val batch2 = Seq( User(2, \"Bartek\", \"Paris\"), User(3, \"Jacek\", \"Paris\")) val offset = usersIn.addData(batch2) sq.processAllAvailable() // You have to execute the history SQL command again // It materializes immediately with whatever data is available at the time val h = sql(history).select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | |1 |2019-12-06 10:13:27|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 1]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ val batch3 = Seq( User(4, \"Gorazd\", \"Ljubljana\")) val offset = usersIn.addData(batch3) sq.processAllAvailable() // Let's use DeltaTable API instead import io.delta.tables.DeltaTable val usersDT = DeltaTable.forPath(path) val h = usersDT.history.select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | |1 |2019-12-06 10:13:27|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 1]|true | |2 |2019-12-06 10:20:56|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 2]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+","title":"Demo: Using Delta Lake as Streaming Sink in Structured Streaming"},{"location":"demo/merge-operation/","text":"Demo: Merge Operation \u00b6 This demo shows DeltaTable.merge operation (and the underlying MergeIntoCommand ) in action. Tip Enable ALL logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger as described in Logging . Create Delta Table (Target Data) \u00b6 val path = \"/tmp/delta/demo\" val data = spark . range ( 5 ) data . write . format ( \"delta\" ). mode ( \"overwrite\" ). save ( path ) import io.delta.tables.DeltaTable val target = DeltaTable . forPath ( path ) assert ( target . isInstanceOf [ io.delta.tables.DeltaTable ]) assert ( target . history . count == 1 , \"There must be version 0 only\" ) Source Data \u00b6 case class Person ( id : Long , name : String ) val source = Seq ( Person ( 0 , \"Zero\" ), Person ( 1 , \"One\" )). toDF Note the difference in schemas scala> target.toDF.printSchema root |-- id: long (nullable = true) scala> source.printSchema root |-- id: long (nullable = false) |-- name: string (nullable = true) Merge with Schema Evolution \u00b6 Not only do we update the matching rows, but also update the schema (schema evolution) val mergeBuilder = target . as ( \"to\" ) . merge ( source = source . as ( \"from\" ), condition = $ \"to.id\" === $ \"from.id\" ) assert(mergeBuilder.isInstanceOf[io.delta.tables.DeltaMergeBuilder]) scala> mergeBuilder.execute org.apache.spark.sql.AnalysisException: There must be at least one WHEN clause in a MERGE query; at org.apache.spark.sql.catalyst.plans.logical.DeltaMergeInto$.apply(deltaMerge.scala:217) at io.delta.tables.DeltaMergeBuilder.mergePlan(DeltaMergeBuilder.scala:255) at io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:228) at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:60) at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:48) at io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:121) at io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:225) ... 47 elided val mergeMatchedBuilder = mergeBuilder . whenMatched () assert ( mergeMatchedBuilder . isInstanceOf [ io.delta.tables.DeltaMergeMatchedActionBuilder ]) val mergeBuilderDeleteMatched = mergeMatchedBuilder . delete () assert ( mergeBuilderDeleteMatched . isInstanceOf [ io.delta.tables.DeltaMergeBuilder ]) mergeBuilderDeleteMatched . execute () assert ( target . history . count == 2 , \"There must be two versions only\" )","title":"Merge Operation"},{"location":"demo/merge-operation/#demo-merge-operation","text":"This demo shows DeltaTable.merge operation (and the underlying MergeIntoCommand ) in action. Tip Enable ALL logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger as described in Logging .","title":"Demo: Merge Operation"},{"location":"demo/merge-operation/#create-delta-table-target-data","text":"val path = \"/tmp/delta/demo\" val data = spark . range ( 5 ) data . write . format ( \"delta\" ). mode ( \"overwrite\" ). save ( path ) import io.delta.tables.DeltaTable val target = DeltaTable . forPath ( path ) assert ( target . isInstanceOf [ io.delta.tables.DeltaTable ]) assert ( target . history . count == 1 , \"There must be version 0 only\" )","title":"Create Delta Table (Target Data)"},{"location":"demo/merge-operation/#source-data","text":"case class Person ( id : Long , name : String ) val source = Seq ( Person ( 0 , \"Zero\" ), Person ( 1 , \"One\" )). toDF Note the difference in schemas scala> target.toDF.printSchema root |-- id: long (nullable = true) scala> source.printSchema root |-- id: long (nullable = false) |-- name: string (nullable = true)","title":"Source Data"},{"location":"demo/merge-operation/#merge-with-schema-evolution","text":"Not only do we update the matching rows, but also update the schema (schema evolution) val mergeBuilder = target . as ( \"to\" ) . merge ( source = source . as ( \"from\" ), condition = $ \"to.id\" === $ \"from.id\" ) assert(mergeBuilder.isInstanceOf[io.delta.tables.DeltaMergeBuilder]) scala> mergeBuilder.execute org.apache.spark.sql.AnalysisException: There must be at least one WHEN clause in a MERGE query; at org.apache.spark.sql.catalyst.plans.logical.DeltaMergeInto$.apply(deltaMerge.scala:217) at io.delta.tables.DeltaMergeBuilder.mergePlan(DeltaMergeBuilder.scala:255) at io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:228) at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:60) at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:48) at io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:121) at io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:225) ... 47 elided val mergeMatchedBuilder = mergeBuilder . whenMatched () assert ( mergeMatchedBuilder . isInstanceOf [ io.delta.tables.DeltaMergeMatchedActionBuilder ]) val mergeBuilderDeleteMatched = mergeMatchedBuilder . delete () assert ( mergeBuilderDeleteMatched . isInstanceOf [ io.delta.tables.DeltaMergeBuilder ]) mergeBuilderDeleteMatched . execute () assert ( target . history . count == 2 , \"There must be two versions only\" )","title":"Merge with Schema Evolution"},{"location":"demo/schema-evolution/","text":"Demo: Schema Evolution \u00b6 /* spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ case class PersonV1(id: Long, name: String) import org.apache.spark.sql.Encoders val schemaV1 = Encoders.product[PersonV1].schema scala> schemaV1.printTreeString root |-- id: long (nullable = false) |-- name: string (nullable = true) val dataPath = \"/tmp/delta/people\" // Write data Seq(PersonV1(0, \"Zero\"), PersonV1(1, \"One\")) .toDF .write .format(\"delta\") .mode(\"overwrite\") .option(\"overwriteSchema\", \"true\") .save(dataPath) // Committed delta #0 to file:/tmp/delta/people/_delta_log import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, dataPath) assert(deltaLog.snapshot.version == 0) scala> deltaLog.snapshot.dataSchema.printTreeString root |-- id: long (nullable = true) |-- name: string (nullable = true) import io.delta.tables.DeltaTable val dt = DeltaTable.forPath(dataPath) scala> dt.toDF.show +---+----+ | id|name| +---+----+ | 0|Zero| | 1| One| +---+----+ val main = dt.as(\"main\") case class PersonV2(id: Long, name: String, newField: Boolean) val schemaV2 = Encoders.product[PersonV2].schema scala> schemaV2.printTreeString root |-- id: long (nullable = false) |-- name: string (nullable = true) |-- newField: boolean (nullable = false) val updates = Seq( PersonV2(0, \"ZERO\", newField = true), PersonV2(2, \"TWO\", newField = false)).toDF // Merge two datasets and create a new version // Schema evolution in play main.merge( source = updates.as(\"updates\"), condition = $\"main.id\" === $\"updates.id\") .whenMatched.updateAll .execute val latestPeople = spark .read .format(\"delta\") .load(dataPath) scala> latestPeople.show +---+----+ | id|name| +---+----+ | 0|ZERO| | 1| One| +---+----+","title":"Schema Evolution"},{"location":"demo/schema-evolution/#demo-schema-evolution","text":"/* spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ case class PersonV1(id: Long, name: String) import org.apache.spark.sql.Encoders val schemaV1 = Encoders.product[PersonV1].schema scala> schemaV1.printTreeString root |-- id: long (nullable = false) |-- name: string (nullable = true) val dataPath = \"/tmp/delta/people\" // Write data Seq(PersonV1(0, \"Zero\"), PersonV1(1, \"One\")) .toDF .write .format(\"delta\") .mode(\"overwrite\") .option(\"overwriteSchema\", \"true\") .save(dataPath) // Committed delta #0 to file:/tmp/delta/people/_delta_log import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, dataPath) assert(deltaLog.snapshot.version == 0) scala> deltaLog.snapshot.dataSchema.printTreeString root |-- id: long (nullable = true) |-- name: string (nullable = true) import io.delta.tables.DeltaTable val dt = DeltaTable.forPath(dataPath) scala> dt.toDF.show +---+----+ | id|name| +---+----+ | 0|Zero| | 1| One| +---+----+ val main = dt.as(\"main\") case class PersonV2(id: Long, name: String, newField: Boolean) val schemaV2 = Encoders.product[PersonV2].schema scala> schemaV2.printTreeString root |-- id: long (nullable = false) |-- name: string (nullable = true) |-- newField: boolean (nullable = false) val updates = Seq( PersonV2(0, \"ZERO\", newField = true), PersonV2(2, \"TWO\", newField = false)).toDF // Merge two datasets and create a new version // Schema evolution in play main.merge( source = updates.as(\"updates\"), condition = $\"main.id\" === $\"updates.id\") .whenMatched.updateAll .execute val latestPeople = spark .read .format(\"delta\") .load(dataPath) scala> latestPeople.show +---+----+ | id|name| +---+----+ | 0|ZERO| | 1| One| +---+----+","title":"Demo: Schema Evolution"},{"location":"demo/stream-processing-of-delta-table/","text":"Demo: Stream Processing of Delta Table \u00b6 /* spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ assert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\") import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val deltaTableDir = \"/tmp/delta/users\" val checkpointLocation = \"/tmp/checkpointLocation\" // Initialize the delta table // - No data // - Schema only case class Person(id: Long, name: String, city: String) spark.emptyDataset[Person].write.format(\"delta\").save(deltaTableDir) import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"delta\") .option(\"maxFilesPerTrigger\", 1) // Maximum number of files to scan per micro-batch .load(deltaTableDir) .writeStream .format(\"console\") .option(\"truncate\", false) .option(\"checkpointLocation\", checkpointLocation) .trigger(Trigger.ProcessingTime(10.seconds)) // Useful for debugging .start // The streaming query over delta table // should display the 0th version as Batch 0 ------------------------------------------- Batch: 0 ------------------------------------------- +---+----+----+ |id |name|city| +---+----+----+ +---+----+----+ // Let's write to the delta table val users = Seq( Person(0, \"Jacek\", \"Warsaw\"), Person(1, \"Agata\", \"Warsaw\"), Person(2, \"Jacek\", \"Paris\"), Person(3, \"Domas\", \"Vilnius\")).toDF // More partitions are more file added // And per maxFilesPerTrigger as 1 file addition per micro-batch // You should see more micro-batches scala> println(users.rdd.getNumPartitions) 4 // Change the default SaveMode.ErrorIfExists to more meaningful save mode import org.apache.spark.sql.SaveMode users .write .format(\"delta\") .mode(SaveMode.Append) // Appending rows .save(deltaTableDir) // Immediately after the above write finishes // New batches should be printed out to the console // Per the number of partitions of users dataset // And per maxFilesPerTrigger as 1 file addition // You should see as many micro-batches as files ------------------------------------------- Batch: 1 ------------------------------------------- +---+-----+------+ |id |name |city | +---+-----+------+ |0 |Jacek|Warsaw| +---+-----+------+ ------------------------------------------- Batch: 2 ------------------------------------------- +---+-----+------+ |id |name |city | +---+-----+------+ |1 |Agata|Warsaw| +---+-----+------+ ------------------------------------------- Batch: 3 ------------------------------------------- +---+-----+-----+ |id |name |city | +---+-----+-----+ |2 |Jacek|Paris| +---+-----+-----+ ------------------------------------------- Batch: 4 ------------------------------------------- +---+-----+-------+ |id |name |city | +---+-----+-------+ |3 |Domas|Vilnius| +---+-----+-------+","title":"Stream Processing of Delta Table"},{"location":"demo/stream-processing-of-delta-table/#demo-stream-processing-of-delta-table","text":"/* spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ assert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\") import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val deltaTableDir = \"/tmp/delta/users\" val checkpointLocation = \"/tmp/checkpointLocation\" // Initialize the delta table // - No data // - Schema only case class Person(id: Long, name: String, city: String) spark.emptyDataset[Person].write.format(\"delta\").save(deltaTableDir) import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"delta\") .option(\"maxFilesPerTrigger\", 1) // Maximum number of files to scan per micro-batch .load(deltaTableDir) .writeStream .format(\"console\") .option(\"truncate\", false) .option(\"checkpointLocation\", checkpointLocation) .trigger(Trigger.ProcessingTime(10.seconds)) // Useful for debugging .start // The streaming query over delta table // should display the 0th version as Batch 0 ------------------------------------------- Batch: 0 ------------------------------------------- +---+----+----+ |id |name|city| +---+----+----+ +---+----+----+ // Let's write to the delta table val users = Seq( Person(0, \"Jacek\", \"Warsaw\"), Person(1, \"Agata\", \"Warsaw\"), Person(2, \"Jacek\", \"Paris\"), Person(3, \"Domas\", \"Vilnius\")).toDF // More partitions are more file added // And per maxFilesPerTrigger as 1 file addition per micro-batch // You should see more micro-batches scala> println(users.rdd.getNumPartitions) 4 // Change the default SaveMode.ErrorIfExists to more meaningful save mode import org.apache.spark.sql.SaveMode users .write .format(\"delta\") .mode(SaveMode.Append) // Appending rows .save(deltaTableDir) // Immediately after the above write finishes // New batches should be printed out to the console // Per the number of partitions of users dataset // And per maxFilesPerTrigger as 1 file addition // You should see as many micro-batches as files ------------------------------------------- Batch: 1 ------------------------------------------- +---+-----+------+ |id |name |city | +---+-----+------+ |0 |Jacek|Warsaw| +---+-----+------+ ------------------------------------------- Batch: 2 ------------------------------------------- +---+-----+------+ |id |name |city | +---+-----+------+ |1 |Agata|Warsaw| +---+-----+------+ ------------------------------------------- Batch: 3 ------------------------------------------- +---+-----+-----+ |id |name |city | +---+-----+-----+ |2 |Jacek|Paris| +---+-----+-----+ ------------------------------------------- Batch: 4 ------------------------------------------- +---+-----+-------+ |id |name |city | +---+-----+-------+ |3 |Domas|Vilnius| +---+-----+-------+","title":"Demo: Stream Processing of Delta Table"},{"location":"demo/user-metadata-for-labelling-commits/","text":"Demo: User Metadata for Labelling Commits \u00b6 The demo shows how to differentiate commits of a write batch query using userMetadata option. Tip A fine example could be for distinguishing between two or more separate streaming write queries. Creating Delta Table \u00b6 val tableName = \"/tmp/delta-demo-userMetadata\" spark.range(5) .write .format(\"delta\") .save(tableName) Describing History \u00b6 import io.delta.tables.DeltaTable val d = DeltaTable . forPath ( tableName ) We are interested in a subset of the available history metadata. d . history . select ( 'version , 'operation , 'operationParameters , 'userMetadata ) . show ( truncate = false ) +-------+---------+------------------------------------------+------------+ |version|operation|operationParameters |userMetadata| +-------+---------+------------------------------------------+------------+ |0 |WRITE |[mode -> ErrorIfExists, partitionBy -> []]|null | +-------+---------+------------------------------------------+------------+ Appending Data \u00b6 In this step, you're going to append new data to the existing Delta table. You're going to use userMetadata option for a custom user-defined historical marker (e.g. to know when this extra append happended in the life of the Delta table). val userMetadata = \"two more rows for demo\" Since you're appending new rows, it is required to use Append mode. import org.apache.spark.sql.SaveMode.Append The whole append write is as follows: spark . range ( start = 5 , end = 7 ) . write . format ( \"delta\" ) . option ( \"userMetadata\" , userMetadata ) . mode ( Append ) . save ( tableName ) That write query creates another version of the Delta table. Listing Versions with userMetadata \u00b6 For the sake of the demo, you are going to show the versions of the Delta table with userMetadata defined. d . history . select ( 'version , 'operation , 'operationParameters , 'userMetadata ) . where ( 'userMetadata . isNotNull ) . show ( truncate = false ) +-------+---------+-----------------------------------+----------------------+ |version|operation|operationParameters |userMetadata | +-------+---------+-----------------------------------+----------------------+ |1 |WRITE |[mode -> Append, partitionBy -> []]|two more rows for demo| +-------+---------+-----------------------------------+----------------------+","title":"User Metadata for Labelling Commits"},{"location":"demo/user-metadata-for-labelling-commits/#demo-user-metadata-for-labelling-commits","text":"The demo shows how to differentiate commits of a write batch query using userMetadata option. Tip A fine example could be for distinguishing between two or more separate streaming write queries.","title":"Demo: User Metadata for Labelling Commits"},{"location":"demo/user-metadata-for-labelling-commits/#creating-delta-table","text":"val tableName = \"/tmp/delta-demo-userMetadata\" spark.range(5) .write .format(\"delta\") .save(tableName)","title":"Creating Delta Table"},{"location":"demo/user-metadata-for-labelling-commits/#describing-history","text":"import io.delta.tables.DeltaTable val d = DeltaTable . forPath ( tableName ) We are interested in a subset of the available history metadata. d . history . select ( 'version , 'operation , 'operationParameters , 'userMetadata ) . show ( truncate = false ) +-------+---------+------------------------------------------+------------+ |version|operation|operationParameters |userMetadata| +-------+---------+------------------------------------------+------------+ |0 |WRITE |[mode -> ErrorIfExists, partitionBy -> []]|null | +-------+---------+------------------------------------------+------------+","title":"Describing History"},{"location":"demo/user-metadata-for-labelling-commits/#appending-data","text":"In this step, you're going to append new data to the existing Delta table. You're going to use userMetadata option for a custom user-defined historical marker (e.g. to know when this extra append happended in the life of the Delta table). val userMetadata = \"two more rows for demo\" Since you're appending new rows, it is required to use Append mode. import org.apache.spark.sql.SaveMode.Append The whole append write is as follows: spark . range ( start = 5 , end = 7 ) . write . format ( \"delta\" ) . option ( \"userMetadata\" , userMetadata ) . mode ( Append ) . save ( tableName ) That write query creates another version of the Delta table.","title":"Appending Data"},{"location":"demo/user-metadata-for-labelling-commits/#listing-versions-with-usermetadata","text":"For the sake of the demo, you are going to show the versions of the Delta table with userMetadata defined. d . history . select ( 'version , 'operation , 'operationParameters , 'userMetadata ) . where ( 'userMetadata . isNotNull ) . show ( truncate = false ) +-------+---------+-----------------------------------+----------------------+ |version|operation|operationParameters |userMetadata | +-------+---------+-----------------------------------+----------------------+ |1 |WRITE |[mode -> Append, partitionBy -> []]|two more rows for demo| +-------+---------+-----------------------------------+----------------------+","title":"Listing Versions with userMetadata"},{"location":"sql/","text":"Delta SQL \u00b6 Delta Lake registers custom SQL statements (using DeltaSparkSessionExtension to inject DeltaSqlParser with DeltaSqlAstBuilder ). The SQL statements support table of the format delta.`path` (with backticks), e.g. delta.`/tmp/delta/t1` while path is between single quotes, e.g. '/tmp/delta/t1' . The SQL statements can also refer to a table registered in a metastore. Note SQL grammar is described using ANTLR in DeltaSqlBase.g4 . CONVERT TO DELTA \u00b6 CONVERT TO DELTA table (PARTITIONED BY '(' colTypeList ')')? Runs a ConvertToDeltaCommand DESCRIBE DETAIL \u00b6 (DESC | DESCRIBE) DETAIL (path | table) Runs a DescribeDeltaDetailCommand DESCRIBE HISTORY \u00b6 (DESC | DESCRIBE) HISTORY (path | table) (LIMIT limit)? Runs a DescribeDeltaHistoryCommand GENERATE \u00b6 GENERATE modeName FOR TABLE table Runs a DeltaGenerateCommand VACUUM \u00b6 VACUUM (path | table) (RETAIN number HOURS)? (DRY RUN)? Runs a VacuumTableCommand","title":"Delta SQL"},{"location":"sql/#delta-sql","text":"Delta Lake registers custom SQL statements (using DeltaSparkSessionExtension to inject DeltaSqlParser with DeltaSqlAstBuilder ). The SQL statements support table of the format delta.`path` (with backticks), e.g. delta.`/tmp/delta/t1` while path is between single quotes, e.g. '/tmp/delta/t1' . The SQL statements can also refer to a table registered in a metastore. Note SQL grammar is described using ANTLR in DeltaSqlBase.g4 .","title":"Delta SQL"},{"location":"sql/#convert-to-delta","text":"CONVERT TO DELTA table (PARTITIONED BY '(' colTypeList ')')? Runs a ConvertToDeltaCommand","title":" CONVERT TO DELTA"},{"location":"sql/#describe-detail","text":"(DESC | DESCRIBE) DETAIL (path | table) Runs a DescribeDeltaDetailCommand","title":" DESCRIBE DETAIL"},{"location":"sql/#describe-history","text":"(DESC | DESCRIBE) HISTORY (path | table) (LIMIT limit)? Runs a DescribeDeltaHistoryCommand","title":" DESCRIBE HISTORY"},{"location":"sql/#generate","text":"GENERATE modeName FOR TABLE table Runs a DeltaGenerateCommand","title":" GENERATE"},{"location":"sql/#vacuum","text":"VACUUM (path | table) (RETAIN number HOURS)? (DRY RUN)? Runs a VacuumTableCommand","title":" VACUUM"},{"location":"sql/DeltaSqlAstBuilder/","text":"DeltaSqlAstBuilder \u00b6 DeltaSqlAstBuilder is a command builder for the Delta SQL statements (described in DeltaSqlBase.g4 ANTLR grammar). DeltaSqlParser is used by DeltaSqlParser . SQL Statement Logical Command CONVERT TO DELTA ConvertToDeltaCommand DESCRIBE DETAIL DescribeDeltaDetailCommand DESCRIBE HISTORY DescribeDeltaHistoryCommand GENERATE DeltaGenerateCommand VACUUM VacuumTableCommand","title":"DeltaSqlAstBuilder"},{"location":"sql/DeltaSqlAstBuilder/#deltasqlastbuilder","text":"DeltaSqlAstBuilder is a command builder for the Delta SQL statements (described in DeltaSqlBase.g4 ANTLR grammar). DeltaSqlParser is used by DeltaSqlParser . SQL Statement Logical Command CONVERT TO DELTA ConvertToDeltaCommand DESCRIBE DETAIL DescribeDeltaDetailCommand DESCRIBE HISTORY DescribeDeltaHistoryCommand GENERATE DeltaGenerateCommand VACUUM VacuumTableCommand","title":"DeltaSqlAstBuilder"},{"location":"sql/DeltaSqlParser/","text":"DeltaSqlParser \u00b6 DeltaSqlParser is a SQL parser (Spark SQL's ParserInterface ) for Delta SQL . DeltaSqlParser is registered in a Spark SQL application using DeltaSparkSessionExtension . Creating Instance \u00b6 DeltaSqlParser takes the following to be created: ParserInterface (to fall back to for unsupported SQL) DeltaSqlParser is created when DeltaSparkSessionExtension is requested to register Delta SQL support . DeltaSqlAstBuilder \u00b6 DeltaSqlParser uses DeltaSqlAstBuilder to convert SQL statements to their runtime representation (as a LogicalPlan ). In case an AST could not be converted to a LogicalPlan , DeltaSqlAstBuilder requests the delegate ParserInterface to parse it.","title":"DeltaSqlParser"},{"location":"sql/DeltaSqlParser/#deltasqlparser","text":"DeltaSqlParser is a SQL parser (Spark SQL's ParserInterface ) for Delta SQL . DeltaSqlParser is registered in a Spark SQL application using DeltaSparkSessionExtension .","title":"DeltaSqlParser"},{"location":"sql/DeltaSqlParser/#creating-instance","text":"DeltaSqlParser takes the following to be created: ParserInterface (to fall back to for unsupported SQL) DeltaSqlParser is created when DeltaSparkSessionExtension is requested to register Delta SQL support .","title":"Creating Instance"},{"location":"sql/DeltaSqlParser/#deltasqlastbuilder","text":"DeltaSqlParser uses DeltaSqlAstBuilder to convert SQL statements to their runtime representation (as a LogicalPlan ). In case an AST could not be converted to a LogicalPlan , DeltaSqlAstBuilder requests the delegate ParserInterface to parse it.","title":" DeltaSqlAstBuilder"}]}