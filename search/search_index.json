{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Delta Lake 1.0.0-SNAPSHOT \u00b6 Welcome to The Internals of Delta Lake online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Delta Lake as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's take a deep dive into Delta Lake \ud83d\udd25","title":"Home"},{"location":"#the-internals-of-delta-lake-100-snapshot","text":"Welcome to The Internals of Delta Lake online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Delta Lake as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's take a deep dive into Delta Lake \ud83d\udd25","title":"The Internals of Delta Lake 1.0.0-SNAPSHOT"},{"location":"Action/","text":"Action \u00b6 Action is an abstraction of operations that change (the state of) a delta table. Contract \u00b6 Serializing to JSON \u00b6 json : String Serializes ( converts ) the (wrapped) action to JSON format json uses Jackson library (with jackson-module-scala ) as the JSON processor. Used when: OptimisticTransactionImpl is requested to doCommit DeltaCommand is requested to commitLarge Wrapping Up as SingleAction \u00b6 wrap : SingleAction Wraps the action into a SingleAction for serialization Used when: Snapshot is requested to stateReconstruction Action is requested to serialize to JSON format Implementations \u00b6 CommitInfo FileAction Metadata Protocol SetTransaction Sealed Trait Action is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file). Log Schema \u00b6 logSchema : StructType logSchema is the schema ( Spark SQL ) of SingleAction s for Snapshot to convert a DeltaLogFileIndex to a LogicalRelation and emptyActions . import org . apache . spark . sql . delta . actions . Action . logSchema logSchema . printTreeString root |-- txn: struct (nullable = true) | |-- appId: string (nullable = true) | |-- version: long (nullable = false) | |-- lastUpdated: long (nullable = true) |-- add: struct (nullable = true) | |-- path: string (nullable = true) | |-- partitionValues: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- size: long (nullable = false) | |-- modificationTime: long (nullable = false) | |-- dataChange: boolean (nullable = false) | |-- stats: string (nullable = true) | |-- tags: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) |-- remove: struct (nullable = true) | |-- path: string (nullable = true) | |-- deletionTimestamp: long (nullable = true) | |-- dataChange: boolean (nullable = false) | |-- extendedFileMetadata: boolean (nullable = false) | |-- partitionValues: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- size: long (nullable = false) | |-- tags: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) |-- metaData: struct (nullable = true) | |-- id: string (nullable = true) | |-- name: string (nullable = true) | |-- description: string (nullable = true) | |-- format: struct (nullable = true) | | |-- provider: string (nullable = true) | | |-- options: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) | |-- schemaString: string (nullable = true) | |-- partitionColumns: array (nullable = true) | | |-- element: string (containsNull = true) | |-- configuration: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- createdTime: long (nullable = true) |-- protocol: struct (nullable = true) | |-- minReaderVersion: integer (nullable = false) | |-- minWriterVersion: integer (nullable = false) |-- cdc: struct (nullable = true) | |-- path: string (nullable = true) | |-- partitionValues: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- size: long (nullable = false) | |-- tags: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) |-- commitInfo: struct (nullable = true) | |-- version: long (nullable = true) | |-- timestamp: timestamp (nullable = true) | |-- userId: string (nullable = true) | |-- userName: string (nullable = true) | |-- operation: string (nullable = true) | |-- operationParameters: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- job: struct (nullable = true) | | |-- jobId: string (nullable = true) | | |-- jobName: string (nullable = true) | | |-- runId: string (nullable = true) | | |-- jobOwnerId: string (nullable = true) | | |-- triggerType: string (nullable = true) | |-- notebook: struct (nullable = true) | | |-- notebookId: string (nullable = true) | |-- clusterId: string (nullable = true) | |-- readVersion: long (nullable = true) | |-- isolationLevel: string (nullable = true) | |-- isBlindAppend: boolean (nullable = true) | |-- operationMetrics: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- userMetadata: string (nullable = true) Deserializing Action (from JSON) \u00b6 fromJson ( json : String ): Action fromJson utility...FIXME fromJson is used when: DeltaHistoryManager is requested for CommitInfo of the given delta file DeltaLog is requested for the changes of the given delta version and later OptimisticTransactionImpl is requested to checkForConflicts DeltaCommand is requested to commitLarge","title":"Action"},{"location":"Action/#action","text":"Action is an abstraction of operations that change (the state of) a delta table.","title":"Action"},{"location":"Action/#contract","text":"","title":"Contract"},{"location":"Action/#serializing-to-json","text":"json : String Serializes ( converts ) the (wrapped) action to JSON format json uses Jackson library (with jackson-module-scala ) as the JSON processor. Used when: OptimisticTransactionImpl is requested to doCommit DeltaCommand is requested to commitLarge","title":" Serializing to JSON"},{"location":"Action/#wrapping-up-as-singleaction","text":"wrap : SingleAction Wraps the action into a SingleAction for serialization Used when: Snapshot is requested to stateReconstruction Action is requested to serialize to JSON format","title":" Wrapping Up as SingleAction"},{"location":"Action/#implementations","text":"CommitInfo FileAction Metadata Protocol SetTransaction Sealed Trait Action is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).","title":"Implementations"},{"location":"Action/#log-schema","text":"logSchema : StructType logSchema is the schema ( Spark SQL ) of SingleAction s for Snapshot to convert a DeltaLogFileIndex to a LogicalRelation and emptyActions . import org . apache . spark . sql . delta . actions . Action . logSchema logSchema . printTreeString root |-- txn: struct (nullable = true) | |-- appId: string (nullable = true) | |-- version: long (nullable = false) | |-- lastUpdated: long (nullable = true) |-- add: struct (nullable = true) | |-- path: string (nullable = true) | |-- partitionValues: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- size: long (nullable = false) | |-- modificationTime: long (nullable = false) | |-- dataChange: boolean (nullable = false) | |-- stats: string (nullable = true) | |-- tags: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) |-- remove: struct (nullable = true) | |-- path: string (nullable = true) | |-- deletionTimestamp: long (nullable = true) | |-- dataChange: boolean (nullable = false) | |-- extendedFileMetadata: boolean (nullable = false) | |-- partitionValues: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- size: long (nullable = false) | |-- tags: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) |-- metaData: struct (nullable = true) | |-- id: string (nullable = true) | |-- name: string (nullable = true) | |-- description: string (nullable = true) | |-- format: struct (nullable = true) | | |-- provider: string (nullable = true) | | |-- options: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) | |-- schemaString: string (nullable = true) | |-- partitionColumns: array (nullable = true) | | |-- element: string (containsNull = true) | |-- configuration: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- createdTime: long (nullable = true) |-- protocol: struct (nullable = true) | |-- minReaderVersion: integer (nullable = false) | |-- minWriterVersion: integer (nullable = false) |-- cdc: struct (nullable = true) | |-- path: string (nullable = true) | |-- partitionValues: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- size: long (nullable = false) | |-- tags: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) |-- commitInfo: struct (nullable = true) | |-- version: long (nullable = true) | |-- timestamp: timestamp (nullable = true) | |-- userId: string (nullable = true) | |-- userName: string (nullable = true) | |-- operation: string (nullable = true) | |-- operationParameters: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- job: struct (nullable = true) | | |-- jobId: string (nullable = true) | | |-- jobName: string (nullable = true) | | |-- runId: string (nullable = true) | | |-- jobOwnerId: string (nullable = true) | | |-- triggerType: string (nullable = true) | |-- notebook: struct (nullable = true) | | |-- notebookId: string (nullable = true) | |-- clusterId: string (nullable = true) | |-- readVersion: long (nullable = true) | |-- isolationLevel: string (nullable = true) | |-- isBlindAppend: boolean (nullable = true) | |-- operationMetrics: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- userMetadata: string (nullable = true)","title":" Log Schema"},{"location":"Action/#deserializing-action-from-json","text":"fromJson ( json : String ): Action fromJson utility...FIXME fromJson is used when: DeltaHistoryManager is requested for CommitInfo of the given delta file DeltaLog is requested for the changes of the given delta version and later OptimisticTransactionImpl is requested to checkForConflicts DeltaCommand is requested to commitLarge","title":" Deserializing Action (from JSON)"},{"location":"ActiveOptimisticTransactionRule/","text":"ActiveOptimisticTransactionRule Logical Optimization Rule \u00b6 ActiveOptimisticTransactionRule is a logical optimization rule ( Spark SQL ). Creating Instance \u00b6 ActiveOptimisticTransactionRule takes the following to be created: SparkSession ( Spark SQL ) ActiveOptimisticTransactionRule is created when: DeltaSparkSessionExtension is requested to inject extensions Executing Rule \u00b6 apply ( plan : LogicalPlan ): LogicalPlan apply is part of the Rule ( Spark SQL ) abstraction. apply ...FIXME","title":"ActiveOptimisticTransactionRule"},{"location":"ActiveOptimisticTransactionRule/#activeoptimistictransactionrule-logical-optimization-rule","text":"ActiveOptimisticTransactionRule is a logical optimization rule ( Spark SQL ).","title":"ActiveOptimisticTransactionRule Logical Optimization Rule"},{"location":"ActiveOptimisticTransactionRule/#creating-instance","text":"ActiveOptimisticTransactionRule takes the following to be created: SparkSession ( Spark SQL ) ActiveOptimisticTransactionRule is created when: DeltaSparkSessionExtension is requested to inject extensions","title":"Creating Instance"},{"location":"ActiveOptimisticTransactionRule/#executing-rule","text":"apply ( plan : LogicalPlan ): LogicalPlan apply is part of the Rule ( Spark SQL ) abstraction. apply ...FIXME","title":" Executing Rule"},{"location":"AddCDCFile/","text":"AddCDCFile \u00b6 AddCDCFile is a FileAction . Creating Instance \u00b6 AddCDCFile takes the following to be created: Path Partition values ( Map[String, String] ) Size (in bytes) Tags (default: null ) AddCDCFile does not seem to be created ever. dataChange \u00b6 dataChange : Boolean dataChange is part of the FileAction abstraction. dataChange is always turned off ( false ). Converting to SingleAction \u00b6 wrap : SingleAction wrap is part of the Action abstraction. wrap creates a new SingleAction with the cdc field set to this AddCDCFile .","title":"AddCDCFile"},{"location":"AddCDCFile/#addcdcfile","text":"AddCDCFile is a FileAction .","title":"AddCDCFile"},{"location":"AddCDCFile/#creating-instance","text":"AddCDCFile takes the following to be created: Path Partition values ( Map[String, String] ) Size (in bytes) Tags (default: null ) AddCDCFile does not seem to be created ever.","title":"Creating Instance"},{"location":"AddCDCFile/#datachange","text":"dataChange : Boolean dataChange is part of the FileAction abstraction. dataChange is always turned off ( false ).","title":" dataChange"},{"location":"AddCDCFile/#converting-to-singleaction","text":"wrap : SingleAction wrap is part of the Action abstraction. wrap creates a new SingleAction with the cdc field set to this AddCDCFile .","title":" Converting to SingleAction"},{"location":"AddFile/","text":"AddFile \u00b6 AddFile is a FileAction that represents an action of adding a file to a delta table. Creating Instance \u00b6 AddFile takes the following to be created: Path Partition values ( Map[String, String] ) Size (in bytes) Modification time dataChange flag Stats (default: null ) Tags ( Map[String, String] ) (default: null ) AddFile is created when: ConvertToDeltaCommand is executed (for every data file to import ) DelayedCommitProtocol is requested to commit a task (after successful write) (for optimistic transactional writers ) Converting to SingleAction \u00b6 wrap : SingleAction wrap is part of the Action abstraction. wrap creates a new SingleAction with the add field set to this AddFile . Converting to RemoveFile with Defaults \u00b6 remove : RemoveFile remove creates a RemoveFile for the path (with the current time and dataChange flag enabled). remove is used when: MergeIntoCommand is executed WriteIntoDelta is requested to write (with Overwrite mode) DeltaSink is requested to add a streaming micro-batch (with Complete output mode) Converting to RemoveFile \u00b6 removeWithTimestamp ( timestamp : Long = System . currentTimeMillis (), dataChange : Boolean = true ): RemoveFile remove creates a new RemoveFile action for the path with the given timestamp and dataChange flag. removeWithTimestamp is used when: AddFile is requested to create a RemoveFile action with the defaults CreateDeltaTableCommand , DeleteCommand and UpdateCommand commands are executed DeltaCommand is requested to removeFilesFromPaths","title":"AddFile"},{"location":"AddFile/#addfile","text":"AddFile is a FileAction that represents an action of adding a file to a delta table.","title":"AddFile"},{"location":"AddFile/#creating-instance","text":"AddFile takes the following to be created: Path Partition values ( Map[String, String] ) Size (in bytes) Modification time dataChange flag Stats (default: null ) Tags ( Map[String, String] ) (default: null ) AddFile is created when: ConvertToDeltaCommand is executed (for every data file to import ) DelayedCommitProtocol is requested to commit a task (after successful write) (for optimistic transactional writers )","title":"Creating Instance"},{"location":"AddFile/#converting-to-singleaction","text":"wrap : SingleAction wrap is part of the Action abstraction. wrap creates a new SingleAction with the add field set to this AddFile .","title":" Converting to SingleAction"},{"location":"AddFile/#converting-to-removefile-with-defaults","text":"remove : RemoveFile remove creates a RemoveFile for the path (with the current time and dataChange flag enabled). remove is used when: MergeIntoCommand is executed WriteIntoDelta is requested to write (with Overwrite mode) DeltaSink is requested to add a streaming micro-batch (with Complete output mode)","title":" Converting to RemoveFile with Defaults"},{"location":"AddFile/#converting-to-removefile","text":"removeWithTimestamp ( timestamp : Long = System . currentTimeMillis (), dataChange : Boolean = true ): RemoveFile remove creates a new RemoveFile action for the path with the given timestamp and dataChange flag. removeWithTimestamp is used when: AddFile is requested to create a RemoveFile action with the defaults CreateDeltaTableCommand , DeleteCommand and UpdateCommand commands are executed DeltaCommand is requested to removeFilesFromPaths","title":" Converting to RemoveFile"},{"location":"AdmissionLimits/","text":"AdmissionLimits \u00b6 AdmissionLimits is used by DeltaSource to control how much data should be processed by a single micro-batch . Creating Instance \u00b6 AdmissionLimits takes the following to be created: Maximum Number of Files (based on maxFilesPerTrigger option) Maximum Bytes (based on maxBytesPerTrigger option) AdmissionLimits is created when: DeltaSource is requested to getChangesWithRateLimit , getStartingOffset , getDefaultReadLimit Converting ReadLimit to AdmissionLimits \u00b6 apply ( limit : ReadLimit ): Option [ AdmissionLimits ] apply creates an AdmissionLimits for the given ReadLimit (Spark Structured Streaming). ReadLimit AdmissionLimits ReadAllAvailable None ReadMaxFiles Maximum Number of Files ReadMaxBytes Maximum Bytes CompositeLimit Maximum Number of Files and Maximum Bytes apply throws an UnsupportedOperationException for unknown ReadLimit s: Unknown ReadLimit: [limit] apply is used when: DeltaSource is requested for the latest available offset Admitting AddFile \u00b6 admit ( add : Option [ AddFile ]): Boolean admit ...FIXME admit is used when: DeltaSource is requested to getChangesWithRateLimit","title":"AdmissionLimits"},{"location":"AdmissionLimits/#admissionlimits","text":"AdmissionLimits is used by DeltaSource to control how much data should be processed by a single micro-batch .","title":"AdmissionLimits"},{"location":"AdmissionLimits/#creating-instance","text":"AdmissionLimits takes the following to be created: Maximum Number of Files (based on maxFilesPerTrigger option) Maximum Bytes (based on maxBytesPerTrigger option) AdmissionLimits is created when: DeltaSource is requested to getChangesWithRateLimit , getStartingOffset , getDefaultReadLimit","title":"Creating Instance"},{"location":"AdmissionLimits/#converting-readlimit-to-admissionlimits","text":"apply ( limit : ReadLimit ): Option [ AdmissionLimits ] apply creates an AdmissionLimits for the given ReadLimit (Spark Structured Streaming). ReadLimit AdmissionLimits ReadAllAvailable None ReadMaxFiles Maximum Number of Files ReadMaxBytes Maximum Bytes CompositeLimit Maximum Number of Files and Maximum Bytes apply throws an UnsupportedOperationException for unknown ReadLimit s: Unknown ReadLimit: [limit] apply is used when: DeltaSource is requested for the latest available offset","title":" Converting ReadLimit to AdmissionLimits"},{"location":"AdmissionLimits/#admitting-addfile","text":"admit ( add : Option [ AddFile ]): Boolean admit ...FIXME admit is used when: DeltaSource is requested to getChangesWithRateLimit","title":" Admitting AddFile"},{"location":"AlterTableAddConstraintStatement/","text":"AlterTableAddConstraintStatement \u00b6 AlterTableAddConstraintStatement is a ParsedStatement ( Spark SQL ) for ALTER TABLE ADD CONSTRAINT SQL statement. Creating Instance \u00b6 AlterTableAddConstraintStatement takes the following to be created: Table Name Constraint Name Expression Analysis Phase \u00b6 AlterTableAddConstraintStatement is resolved by DeltaAnalysis logical resolution rule.","title":"AlterTableAddConstraintStatement"},{"location":"AlterTableAddConstraintStatement/#altertableaddconstraintstatement","text":"AlterTableAddConstraintStatement is a ParsedStatement ( Spark SQL ) for ALTER TABLE ADD CONSTRAINT SQL statement.","title":"AlterTableAddConstraintStatement"},{"location":"AlterTableAddConstraintStatement/#creating-instance","text":"AlterTableAddConstraintStatement takes the following to be created: Table Name Constraint Name Expression","title":"Creating Instance"},{"location":"AlterTableAddConstraintStatement/#analysis-phase","text":"AlterTableAddConstraintStatement is resolved by DeltaAnalysis logical resolution rule.","title":"Analysis Phase"},{"location":"AlterTableDropConstraintStatement/","text":"AlterTableDropConstraintStatement \u00b6 AlterTableDropConstraintStatement is...FIXME","title":"AlterTableDropConstraintStatement"},{"location":"AlterTableDropConstraintStatement/#altertabledropconstraintstatement","text":"AlterTableDropConstraintStatement is...FIXME","title":"AlterTableDropConstraintStatement"},{"location":"CachedDS/","text":"CachedDS \u2014 Cached Delta State \u00b6 CachedDS is used when StateCache is requested to cacheDS . When created, CachedDS immediately initializes the cachedDs internal registry that requests the Dataset to generate a RDD[InternalRow] and associates the RDD with the given name : Delta Table State for Snapshot Delta Source Snapshot for DeltaSourceSnapshot The RDD is marked to be persisted using StorageLevel.MEMORY_AND_DISK_SER storage level. Note CachedDS is an internal class of StateCache and has access to its internals. Creating Instance \u00b6 CachedDS takes the following to be created: Dataset[A] Name CachedDS is created when StateCache is requested to cacheDS . getDS Method \u00b6 getDS : Dataset [ A ] getDS ...FIXME getDS is used when: Snapshot is requested to state DeltaSourceSnapshot is requested to initialFiles","title":"CachedDS"},{"location":"CachedDS/#cachedds-cached-delta-state","text":"CachedDS is used when StateCache is requested to cacheDS . When created, CachedDS immediately initializes the cachedDs internal registry that requests the Dataset to generate a RDD[InternalRow] and associates the RDD with the given name : Delta Table State for Snapshot Delta Source Snapshot for DeltaSourceSnapshot The RDD is marked to be persisted using StorageLevel.MEMORY_AND_DISK_SER storage level. Note CachedDS is an internal class of StateCache and has access to its internals.","title":"CachedDS &mdash; Cached Delta State"},{"location":"CachedDS/#creating-instance","text":"CachedDS takes the following to be created: Dataset[A] Name CachedDS is created when StateCache is requested to cacheDS .","title":"Creating Instance"},{"location":"CachedDS/#getds-method","text":"getDS : Dataset [ A ] getDS ...FIXME getDS is used when: Snapshot is requested to state DeltaSourceSnapshot is requested to initialFiles","title":" getDS Method"},{"location":"Checkpoints/","text":"Checkpoints \u00b6 Checkpoints is an abstraction of DeltaLogs that can checkpoint the current state of a delta table . Checkpoints requires to be used with DeltaLog (or subtypes) only. Contract \u00b6 dataPath \u00b6 dataPath : Path Hadoop Path to the data directory of the delta table doLogCleanup \u00b6 doLogCleanup (): Unit Used when: Checkpoints is requested to checkpoint logPath \u00b6 logPath : Path Hadoop Path to the log directory of the delta table Metadata \u00b6 metadata : Metadata Metadata of the delta table snapshot \u00b6 snapshot : Snapshot Snapshot of the delta table store \u00b6 store : LogStore LogStore Implementations \u00b6 DeltaLog _last_checkpoint Metadata File \u00b6 Checkpoints uses _last_checkpoint metadata file (under the log path ) for the following: Writing checkpoint metadata out Loading checkpoint metadata in Checkpointing \u00b6 checkpoint (): Unit checkpoint ( snapshotToCheckpoint : Snapshot ): CheckpointMetaData checkpoint writes a checkpoint of the current state of the delta table ( Snapshot ). That produces a checkpoint metadata with the version, the number of actions and possibly parts (for multi-part checkpoints). checkpoint requests the LogStore to overwrite the _last_checkpoint file with the JSON-encoded checkpoint metadata. In the end, checkpoint cleans up the expired logs (if enabled). checkpoint is used when: OptimisticTransactionImpl is requested to postCommit (based on checkpoint interval table property) ConvertToDelta command is executed (that in the end requests DeltaCommand to updateAndCheckpoint ) Writing Out State Checkpoint \u00b6 writeCheckpoint ( spark : SparkSession , deltaLog : DeltaLog , snapshot : Snapshot ): CheckpointMetaData writeCheckpoint ...FIXME Loading Latest Checkpoint Metadata \u00b6 lastCheckpoint : Option [ CheckpointMetaData ] lastCheckpoint loadMetadataFromFile (allowing for 3 retries). lastCheckpoint is used when: SnapshotManagement is requested to load the latest snapshot MetadataCleanup is requested to listExpiredDeltaLogs loadMetadataFromFile \u00b6 loadMetadataFromFile ( tries : Int ): Option [ CheckpointMetaData ] loadMetadataFromFile loads the _last_checkpoint file (in JSON format) and converts it to CheckpointMetaData (with a version, size and parts). loadMetadataFromFile uses the LogStore to read the _last_checkpoint file. In case the _last_checkpoint file is corrupted, loadMetadataFromFile ...FIXME","title":"Checkpoints"},{"location":"Checkpoints/#checkpoints","text":"Checkpoints is an abstraction of DeltaLogs that can checkpoint the current state of a delta table . Checkpoints requires to be used with DeltaLog (or subtypes) only.","title":"Checkpoints"},{"location":"Checkpoints/#contract","text":"","title":"Contract"},{"location":"Checkpoints/#datapath","text":"dataPath : Path Hadoop Path to the data directory of the delta table","title":" dataPath"},{"location":"Checkpoints/#dologcleanup","text":"doLogCleanup (): Unit Used when: Checkpoints is requested to checkpoint","title":" doLogCleanup"},{"location":"Checkpoints/#logpath","text":"logPath : Path Hadoop Path to the log directory of the delta table","title":" logPath"},{"location":"Checkpoints/#metadata","text":"metadata : Metadata Metadata of the delta table","title":" Metadata"},{"location":"Checkpoints/#snapshot","text":"snapshot : Snapshot Snapshot of the delta table","title":" snapshot"},{"location":"Checkpoints/#store","text":"store : LogStore LogStore","title":" store"},{"location":"Checkpoints/#implementations","text":"DeltaLog","title":"Implementations"},{"location":"Checkpoints/#_last_checkpoint-metadata-file","text":"Checkpoints uses _last_checkpoint metadata file (under the log path ) for the following: Writing checkpoint metadata out Loading checkpoint metadata in","title":" _last_checkpoint Metadata File"},{"location":"Checkpoints/#checkpointing","text":"checkpoint (): Unit checkpoint ( snapshotToCheckpoint : Snapshot ): CheckpointMetaData checkpoint writes a checkpoint of the current state of the delta table ( Snapshot ). That produces a checkpoint metadata with the version, the number of actions and possibly parts (for multi-part checkpoints). checkpoint requests the LogStore to overwrite the _last_checkpoint file with the JSON-encoded checkpoint metadata. In the end, checkpoint cleans up the expired logs (if enabled). checkpoint is used when: OptimisticTransactionImpl is requested to postCommit (based on checkpoint interval table property) ConvertToDelta command is executed (that in the end requests DeltaCommand to updateAndCheckpoint )","title":" Checkpointing"},{"location":"Checkpoints/#writing-out-state-checkpoint","text":"writeCheckpoint ( spark : SparkSession , deltaLog : DeltaLog , snapshot : Snapshot ): CheckpointMetaData writeCheckpoint ...FIXME","title":" Writing Out State Checkpoint"},{"location":"Checkpoints/#loading-latest-checkpoint-metadata","text":"lastCheckpoint : Option [ CheckpointMetaData ] lastCheckpoint loadMetadataFromFile (allowing for 3 retries). lastCheckpoint is used when: SnapshotManagement is requested to load the latest snapshot MetadataCleanup is requested to listExpiredDeltaLogs","title":" Loading Latest Checkpoint Metadata"},{"location":"Checkpoints/#loadmetadatafromfile","text":"loadMetadataFromFile ( tries : Int ): Option [ CheckpointMetaData ] loadMetadataFromFile loads the _last_checkpoint file (in JSON format) and converts it to CheckpointMetaData (with a version, size and parts). loadMetadataFromFile uses the LogStore to read the _last_checkpoint file. In case the _last_checkpoint file is corrupted, loadMetadataFromFile ...FIXME","title":" loadMetadataFromFile"},{"location":"CommitInfo/","text":"CommitInfo \u00b6 CommitInfo is an Action defined by the following properties: Version (optional) Timestamp User ID (optional) User Name (optional) Operation Operation Parameters JobInfo (optional) NotebookInfo (optional) Cluster ID (optional) Read Version (optional) Isolation Level (optional) isBlindAppend flag (optional) Operation Metrics (optional) User metadata (optional) CommitInfo is created (using apply and empty utilities) when: DeltaHistoryManager is requested for version and commit history (for DeltaTable.history operator and DESCRIBE HISTORY SQL command) OptimisticTransactionImpl is requested to commit (with spark.databricks.delta.commitInfo.enabled configuration property enabled) DeltaCommand is requested to commitLarge (for ConvertToDeltaCommand command and FileAlreadyExistsException was thrown) CommitInfo is used as a part of OptimisticTransactionImpl and CommitStats . Blind Append \u00b6 CommitInfo is given isBlindAppend flag (when created) to indicate whether a commit has blindly appended data without caring about existing files. isBlindAppend flag is used while checking for logical conflicts with concurrent updates (at commit ). isBlindAppend flag is always false when DeltaCommand is requested to commitLarge . DeltaHistoryManager \u00b6 CommitInfo can be looked up using DeltaHistoryManager for the following: DESCRIBE HISTORY SQL command DeltaTable.history operation spark.databricks.delta.commitInfo.enabled \u00b6 CommitInfo is added ( logged ) to a delta log only with spark.databricks.delta.commitInfo.enabled configuration property enabled. Creating Empty CommitInfo \u00b6 empty ( version : Option [ Long ] = None ): CommitInfo empty ...FIXME empty is used when: DeltaHistoryManager is requested to getCommitInfo Creating CommitInfo \u00b6 apply ( time : Long , operation : String , operationParameters : Map [ String , String ], commandContext : Map [ String , String ], readVersion : Option [ Long ], isolationLevel : Option [ String ], isBlindAppend : Option [ Boolean ], operationMetrics : Option [ Map [ String , String ]], userMetadata : Option [ String ]): CommitInfo apply creates a CommitInfo (for the given arguments and based on the given commandContext for the user ID, user name, job, notebook, cluster). commandContext argument is always empty, but could be customized using ConvertToDeltaCommandBase . apply is used when: OptimisticTransactionImpl is requested to commit (with spark.databricks.delta.commitInfo.enabled configuration property enabled) DeltaCommand is requested to commitLarge","title":"CommitInfo"},{"location":"CommitInfo/#commitinfo","text":"CommitInfo is an Action defined by the following properties: Version (optional) Timestamp User ID (optional) User Name (optional) Operation Operation Parameters JobInfo (optional) NotebookInfo (optional) Cluster ID (optional) Read Version (optional) Isolation Level (optional) isBlindAppend flag (optional) Operation Metrics (optional) User metadata (optional) CommitInfo is created (using apply and empty utilities) when: DeltaHistoryManager is requested for version and commit history (for DeltaTable.history operator and DESCRIBE HISTORY SQL command) OptimisticTransactionImpl is requested to commit (with spark.databricks.delta.commitInfo.enabled configuration property enabled) DeltaCommand is requested to commitLarge (for ConvertToDeltaCommand command and FileAlreadyExistsException was thrown) CommitInfo is used as a part of OptimisticTransactionImpl and CommitStats .","title":"CommitInfo"},{"location":"CommitInfo/#blind-append","text":"CommitInfo is given isBlindAppend flag (when created) to indicate whether a commit has blindly appended data without caring about existing files. isBlindAppend flag is used while checking for logical conflicts with concurrent updates (at commit ). isBlindAppend flag is always false when DeltaCommand is requested to commitLarge .","title":" Blind Append"},{"location":"CommitInfo/#deltahistorymanager","text":"CommitInfo can be looked up using DeltaHistoryManager for the following: DESCRIBE HISTORY SQL command DeltaTable.history operation","title":"DeltaHistoryManager"},{"location":"CommitInfo/#sparkdatabricksdeltacommitinfoenabled","text":"CommitInfo is added ( logged ) to a delta log only with spark.databricks.delta.commitInfo.enabled configuration property enabled.","title":" spark.databricks.delta.commitInfo.enabled"},{"location":"CommitInfo/#creating-empty-commitinfo","text":"empty ( version : Option [ Long ] = None ): CommitInfo empty ...FIXME empty is used when: DeltaHistoryManager is requested to getCommitInfo","title":" Creating Empty CommitInfo"},{"location":"CommitInfo/#creating-commitinfo","text":"apply ( time : Long , operation : String , operationParameters : Map [ String , String ], commandContext : Map [ String , String ], readVersion : Option [ Long ], isolationLevel : Option [ String ], isBlindAppend : Option [ Boolean ], operationMetrics : Option [ Map [ String , String ]], userMetadata : Option [ String ]): CommitInfo apply creates a CommitInfo (for the given arguments and based on the given commandContext for the user ID, user name, job, notebook, cluster). commandContext argument is always empty, but could be customized using ConvertToDeltaCommandBase . apply is used when: OptimisticTransactionImpl is requested to commit (with spark.databricks.delta.commitInfo.enabled configuration property enabled) DeltaCommand is requested to commitLarge","title":" Creating CommitInfo"},{"location":"DelayedCommitProtocol/","text":"DelayedCommitProtocol \u00b6 DelayedCommitProtocol is a FileCommitProtocol ( Apache Spark ) to write out data to a directory and return the files added . DelayedCommitProtocol is used to model a distributed write that is orchestrated by the Spark driver with the write itself happening on executors. Note FileCommitProtocol allows to track a write job (with a write task per partition) and inform the driver when all the write tasks finished successfully (and were committed ) to consider the write job completed . TaskCommitMessage (Spark Core) allows to \"transfer\" the files added (written out) on the executors to the driver for the optimistic transactional writer . DelayedCommitProtocol is a Serializable . Creating Instance \u00b6 DelayedCommitProtocol takes the following to be created: Job ID Path (to write files to) (optional) Length of Random Prefix DelayedCommitProtocol is created when: TransactionalWrite is requested for a committer (to write a structured query to the directory ) addedFiles \u00b6 addedFiles : ArrayBuffer [( Map [ String , String ], String )] DelayedCommitProtocol uses addedFiles internal registry to track the files added by a Spark write task . addedFiles is used on the executors only. addedFiles is initialized (as an empty collection) when setting up a task . addedFiles is used when: DelayedCommitProtocol is requested to commit a task (on an executor and create a TaskCommitMessage with the files added while a task was writing out a partition of a streaming query) addedStatuses \u00b6 addedStatuses : ArrayBuffer [ AddFile ] DelayedCommitProtocol uses addedStatuses internal registry to track the files that were added by write tasks (on executors) once all they finish successfully and the write job is committed (on a driver). addedStatuses is used on the driver only. addedStatuses is used when: DelayedCommitProtocol is requested to commit a job (on a driver) TransactionalWrite is requested to write out a structured query Setting Up Job \u00b6 setupJob ( jobContext : JobContext ): Unit setupJob is part of the FileCommitProtocol ( Apache Spark ) abstraction. setupJob is a noop. Committing Job \u00b6 commitJob ( jobContext : JobContext , taskCommits : Seq [ TaskCommitMessage ]): Unit commitJob is part of the FileCommitProtocol ( Apache Spark ) abstraction. commitJob adds the AddFile s (from the given taskCommits from every commitTask ) to the addedStatuses internal registry. Aborting Job \u00b6 abortJob ( jobContext : JobContext ): Unit abortJob is part of the FileCommitProtocol ( Apache Spark ) abstraction. abortJob is a noop. Setting Up Task \u00b6 setupTask ( taskContext : TaskAttemptContext ): Unit setupTask is part of the FileCommitProtocol ( Apache Spark ) abstraction. setupTask initializes the addedFiles internal registry to be empty. New Temp File (Relative Path) \u00b6 newTaskTempFile ( taskContext : TaskAttemptContext , dir : Option [ String ], ext : String ): String newTaskTempFile is part of the FileCommitProtocol ( Apache Spark ) abstraction. newTaskTempFile creates a file name for the given TaskAttemptContext and ext . newTaskTempFile tries to parsePartitions with the given dir or falls back to an empty partitionValues . Note The given dir defines a partition directory if the streaming query (and hence the write) is partitioned. newTaskTempFile builds a path (based on the given randomPrefixLength and the dir , or uses the file name directly). Fixme When are the optional dir and the randomPrefixLength defined? newTaskTempFile adds the partition values and the relative path to the addedFiles internal registry. In the end, newTaskTempFile returns the absolute path of the (relative) path in the directory . File Name \u00b6 getFileName ( taskContext : TaskAttemptContext , ext : String , partitionValues : Map [ String , String ]): String getFileName takes the task ID from the given TaskAttemptContext ( Apache Spark ) (for the split part below). getFileName generates a random UUID (for the uuid part below). In the end, getFileName returns a file name of the format: part-[split]-[uuid][ext] New Temp File (Absolute Path) \u00b6 newTaskTempFileAbsPath ( taskContext : TaskAttemptContext , absoluteDir : String , ext : String ): String newTaskTempFileAbsPath is part of the FileCommitProtocol ( Apache Spark ) abstraction. newTaskTempFileAbsPath throws an UnsupportedOperationException : [this] does not support adding files with an absolute path Committing Task \u00b6 commitTask ( taskContext : TaskAttemptContext ): TaskCommitMessage commitTask is part of the FileCommitProtocol ( Apache Spark ) abstraction. commitTask creates a TaskCommitMessage with an AddFile for every file added if there are any. Otherwise, commitTask creates an empty TaskCommitMessage . Note A file is added (to the addedFiles internal registry) when DelayedCommitProtocol is requested for a new file (path) . Aborting Task \u00b6 abortTask ( taskContext : TaskAttemptContext ): Unit abortTask is part of the FileCommitProtocol ( Apache Spark ) abstraction. abortTask is a noop. Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.delta.files.DelayedCommitProtocol logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.files.DelayedCommitProtocol=ALL Refer to Logging .","title":"DelayedCommitProtocol"},{"location":"DelayedCommitProtocol/#delayedcommitprotocol","text":"DelayedCommitProtocol is a FileCommitProtocol ( Apache Spark ) to write out data to a directory and return the files added . DelayedCommitProtocol is used to model a distributed write that is orchestrated by the Spark driver with the write itself happening on executors. Note FileCommitProtocol allows to track a write job (with a write task per partition) and inform the driver when all the write tasks finished successfully (and were committed ) to consider the write job completed . TaskCommitMessage (Spark Core) allows to \"transfer\" the files added (written out) on the executors to the driver for the optimistic transactional writer . DelayedCommitProtocol is a Serializable .","title":"DelayedCommitProtocol"},{"location":"DelayedCommitProtocol/#creating-instance","text":"DelayedCommitProtocol takes the following to be created: Job ID Path (to write files to) (optional) Length of Random Prefix DelayedCommitProtocol is created when: TransactionalWrite is requested for a committer (to write a structured query to the directory )","title":"Creating Instance"},{"location":"DelayedCommitProtocol/#addedfiles","text":"addedFiles : ArrayBuffer [( Map [ String , String ], String )] DelayedCommitProtocol uses addedFiles internal registry to track the files added by a Spark write task . addedFiles is used on the executors only. addedFiles is initialized (as an empty collection) when setting up a task . addedFiles is used when: DelayedCommitProtocol is requested to commit a task (on an executor and create a TaskCommitMessage with the files added while a task was writing out a partition of a streaming query)","title":" addedFiles"},{"location":"DelayedCommitProtocol/#addedstatuses","text":"addedStatuses : ArrayBuffer [ AddFile ] DelayedCommitProtocol uses addedStatuses internal registry to track the files that were added by write tasks (on executors) once all they finish successfully and the write job is committed (on a driver). addedStatuses is used on the driver only. addedStatuses is used when: DelayedCommitProtocol is requested to commit a job (on a driver) TransactionalWrite is requested to write out a structured query","title":" addedStatuses"},{"location":"DelayedCommitProtocol/#setting-up-job","text":"setupJob ( jobContext : JobContext ): Unit setupJob is part of the FileCommitProtocol ( Apache Spark ) abstraction. setupJob is a noop.","title":" Setting Up Job"},{"location":"DelayedCommitProtocol/#committing-job","text":"commitJob ( jobContext : JobContext , taskCommits : Seq [ TaskCommitMessage ]): Unit commitJob is part of the FileCommitProtocol ( Apache Spark ) abstraction. commitJob adds the AddFile s (from the given taskCommits from every commitTask ) to the addedStatuses internal registry.","title":" Committing Job"},{"location":"DelayedCommitProtocol/#aborting-job","text":"abortJob ( jobContext : JobContext ): Unit abortJob is part of the FileCommitProtocol ( Apache Spark ) abstraction. abortJob is a noop.","title":" Aborting Job"},{"location":"DelayedCommitProtocol/#setting-up-task","text":"setupTask ( taskContext : TaskAttemptContext ): Unit setupTask is part of the FileCommitProtocol ( Apache Spark ) abstraction. setupTask initializes the addedFiles internal registry to be empty.","title":" Setting Up Task"},{"location":"DelayedCommitProtocol/#new-temp-file-relative-path","text":"newTaskTempFile ( taskContext : TaskAttemptContext , dir : Option [ String ], ext : String ): String newTaskTempFile is part of the FileCommitProtocol ( Apache Spark ) abstraction. newTaskTempFile creates a file name for the given TaskAttemptContext and ext . newTaskTempFile tries to parsePartitions with the given dir or falls back to an empty partitionValues . Note The given dir defines a partition directory if the streaming query (and hence the write) is partitioned. newTaskTempFile builds a path (based on the given randomPrefixLength and the dir , or uses the file name directly). Fixme When are the optional dir and the randomPrefixLength defined? newTaskTempFile adds the partition values and the relative path to the addedFiles internal registry. In the end, newTaskTempFile returns the absolute path of the (relative) path in the directory .","title":" New Temp File (Relative Path)"},{"location":"DelayedCommitProtocol/#file-name","text":"getFileName ( taskContext : TaskAttemptContext , ext : String , partitionValues : Map [ String , String ]): String getFileName takes the task ID from the given TaskAttemptContext ( Apache Spark ) (for the split part below). getFileName generates a random UUID (for the uuid part below). In the end, getFileName returns a file name of the format: part-[split]-[uuid][ext]","title":" File Name"},{"location":"DelayedCommitProtocol/#new-temp-file-absolute-path","text":"newTaskTempFileAbsPath ( taskContext : TaskAttemptContext , absoluteDir : String , ext : String ): String newTaskTempFileAbsPath is part of the FileCommitProtocol ( Apache Spark ) abstraction. newTaskTempFileAbsPath throws an UnsupportedOperationException : [this] does not support adding files with an absolute path","title":" New Temp File (Absolute Path)"},{"location":"DelayedCommitProtocol/#committing-task","text":"commitTask ( taskContext : TaskAttemptContext ): TaskCommitMessage commitTask is part of the FileCommitProtocol ( Apache Spark ) abstraction. commitTask creates a TaskCommitMessage with an AddFile for every file added if there are any. Otherwise, commitTask creates an empty TaskCommitMessage . Note A file is added (to the addedFiles internal registry) when DelayedCommitProtocol is requested for a new file (path) .","title":" Committing Task"},{"location":"DelayedCommitProtocol/#aborting-task","text":"abortTask ( taskContext : TaskAttemptContext ): Unit abortTask is part of the FileCommitProtocol ( Apache Spark ) abstraction. abortTask is a noop.","title":" Aborting Task"},{"location":"DelayedCommitProtocol/#logging","text":"Enable ALL logging level for org.apache.spark.sql.delta.files.DelayedCommitProtocol logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.files.DelayedCommitProtocol=ALL Refer to Logging .","title":"Logging"},{"location":"DeltaAnalysis/","text":"DeltaAnalysis Logical Resolution Rule \u00b6 DeltaAnalysis is a logical resolution rule ( Spark SQL ). Creating Instance \u00b6 DeltaAnalysis takes the following to be created: SparkSession SQLConf DeltaAnalysis is created when: DeltaSparkSessionExtension is requested to inject Delta extensions Executing Rule \u00b6 apply ( plan : LogicalPlan ): LogicalPlan apply is part of the Rule ( Spark SQL ) abstraction. apply resolves logical operators. AlterTableAddConstraintStatement \u00b6 apply creates an AlterTable ( Spark SQL ) logical command with an AddConstraint table change. AlterTableDropConstraintStatement \u00b6 apply creates an AlterTable ( Spark SQL ) logical command with an DropConstraint table change. AppendDelta \u00b6 DataSourceV2Relation \u00b6 DeleteFromTable \u00b6 DeltaTable \u00b6 MergeIntoTable \u00b6 OverwriteDelta \u00b6 UpdateTable \u00b6","title":"DeltaAnalysis"},{"location":"DeltaAnalysis/#deltaanalysis-logical-resolution-rule","text":"DeltaAnalysis is a logical resolution rule ( Spark SQL ).","title":"DeltaAnalysis Logical Resolution Rule"},{"location":"DeltaAnalysis/#creating-instance","text":"DeltaAnalysis takes the following to be created: SparkSession SQLConf DeltaAnalysis is created when: DeltaSparkSessionExtension is requested to inject Delta extensions","title":"Creating Instance"},{"location":"DeltaAnalysis/#executing-rule","text":"apply ( plan : LogicalPlan ): LogicalPlan apply is part of the Rule ( Spark SQL ) abstraction. apply resolves logical operators.","title":"Executing Rule"},{"location":"DeltaAnalysis/#altertableaddconstraintstatement","text":"apply creates an AlterTable ( Spark SQL ) logical command with an AddConstraint table change.","title":" AlterTableAddConstraintStatement"},{"location":"DeltaAnalysis/#altertabledropconstraintstatement","text":"apply creates an AlterTable ( Spark SQL ) logical command with an DropConstraint table change.","title":" AlterTableDropConstraintStatement"},{"location":"DeltaAnalysis/#appenddelta","text":"","title":" AppendDelta"},{"location":"DeltaAnalysis/#datasourcev2relation","text":"","title":" DataSourceV2Relation"},{"location":"DeltaAnalysis/#deletefromtable","text":"","title":" DeleteFromTable"},{"location":"DeltaAnalysis/#deltatable","text":"","title":" DeltaTable"},{"location":"DeltaAnalysis/#mergeintotable","text":"","title":" MergeIntoTable"},{"location":"DeltaAnalysis/#overwritedelta","text":"","title":" OverwriteDelta"},{"location":"DeltaAnalysis/#updatetable","text":"","title":" UpdateTable"},{"location":"DeltaCatalog/","text":"DeltaCatalog \u00b6 DeltaCatalog is a DelegatingCatalogExtension ( Spark SQL ) and a StagingTableCatalog ( Spark SQL ). DeltaCatalog is registered using spark.sql.catalog.spark_catalog configuration property (while creating a SparkSession in a Spark application). Altering Table \u00b6 alterTable ( ident : Identifier , changes : TableChange * ): Table alterTable is part of the TableCatalog ( Spark SQL ) abstraction. alterTable tries to load a table and continues only when it is a DeltaTableV2 . Otherwise, alterTable simply delegates to the parent TableCatalog . alterTable ...FIXME Creating Table \u00b6 createTable ( ident : Identifier , schema : StructType , partitions : Array [ Transform ], properties : util . Map [ String , String ]): Table createTable is part of the TableCatalog ( Spark SQL ) abstraction. createTable ...FIXME Loading Table \u00b6 loadTable ( ident : Identifier ): Table loadTable is part of the TableCatalog ( Spark SQL ) abstraction. loadTable loads a table by the given identifier from a catalog. If found and the table is a delta table (Spark SQL's V1Table with delta provider), loadTable creates a DeltaTableV2 . Creating Delta Table \u00b6 createDeltaTable ( ident : Identifier , schema : StructType , partitions : Array [ Transform ], properties : util . Map [ String , String ], sourceQuery : Option [ LogicalPlan ], operation : TableCreationModes . CreationMode ): Table createDeltaTable ...FIXME createDeltaTable is used when: DeltaCatalog is requested to createTable StagedDeltaTableV2 is requested to commitStagedChanges","title":"DeltaCatalog"},{"location":"DeltaCatalog/#deltacatalog","text":"DeltaCatalog is a DelegatingCatalogExtension ( Spark SQL ) and a StagingTableCatalog ( Spark SQL ). DeltaCatalog is registered using spark.sql.catalog.spark_catalog configuration property (while creating a SparkSession in a Spark application).","title":"DeltaCatalog"},{"location":"DeltaCatalog/#altering-table","text":"alterTable ( ident : Identifier , changes : TableChange * ): Table alterTable is part of the TableCatalog ( Spark SQL ) abstraction. alterTable tries to load a table and continues only when it is a DeltaTableV2 . Otherwise, alterTable simply delegates to the parent TableCatalog . alterTable ...FIXME","title":" Altering Table"},{"location":"DeltaCatalog/#creating-table","text":"createTable ( ident : Identifier , schema : StructType , partitions : Array [ Transform ], properties : util . Map [ String , String ]): Table createTable is part of the TableCatalog ( Spark SQL ) abstraction. createTable ...FIXME","title":" Creating Table"},{"location":"DeltaCatalog/#loading-table","text":"loadTable ( ident : Identifier ): Table loadTable is part of the TableCatalog ( Spark SQL ) abstraction. loadTable loads a table by the given identifier from a catalog. If found and the table is a delta table (Spark SQL's V1Table with delta provider), loadTable creates a DeltaTableV2 .","title":" Loading Table"},{"location":"DeltaCatalog/#creating-delta-table","text":"createDeltaTable ( ident : Identifier , schema : StructType , partitions : Array [ Transform ], properties : util . Map [ String , String ], sourceQuery : Option [ LogicalPlan ], operation : TableCreationModes . CreationMode ): Table createDeltaTable ...FIXME createDeltaTable is used when: DeltaCatalog is requested to createTable StagedDeltaTableV2 is requested to commitStagedChanges","title":" Creating Delta Table"},{"location":"DeltaConfig/","text":"DeltaConfig \u00b6 DeltaConfig (of type T ) represents a named configuration property of a delta table with values (of type T ). Creating Instance \u00b6 DeltaConfig takes the following to be created: Configuration Key Default Value Conversion function (from text representation of the DeltaConfig to the T type, i.e. String => T ) Validation function (that guards from incorrect values, i.e. T => Boolean ) Help message (optional) Minimum version of protocol supported (default: undefined) DeltaConfig is created when: DeltaConfigs utility is used to build a DeltaConfig Reading Configuration Property From Metadata \u00b6 fromMetaData ( metadata : Metadata ): T fromMetaData looks up the key in the configuration of the given Metadata . If not found, fromMetaData gives the default value . In the end, fromMetaData converts the text representation to the proper type using fromString conversion function. fromMetaData is used when: Checkpoints utility is used to buildCheckpoint DeltaErrors utility is used to logFileNotFoundException DeltaLog is requested for checkpointInterval and deletedFileRetentionDuration table properties, and to assert a table is not read-only MetadataCleanup is requested for the enableExpiredLogCleanup and the deltaRetentionMillis OptimisticTransactionImpl is requested to commit Snapshot is requested for the numIndexedCols Demo \u00b6 import org . apache . spark . sql . delta .{ DeltaConfig , DeltaConfigs } scala> :type DeltaConfigs.TOMBSTONE_RETENTION org.apache.spark.sql.delta.DeltaConfig[org.apache.spark.unsafe.types.CalendarInterval] import org . apache . spark . sql . delta . DeltaLog val path = \"/tmp/delta/t1\" val t1 = DeltaLog . forTable ( spark , path ) val metadata = t1 . snapshot . metadata val retention = DeltaConfigs . TOMBSTONE_RETENTION . fromMetaData ( metadata ) scala> :type retention org.apache.spark.unsafe.types.CalendarInterval","title":"DeltaConfig"},{"location":"DeltaConfig/#deltaconfig","text":"DeltaConfig (of type T ) represents a named configuration property of a delta table with values (of type T ).","title":"DeltaConfig"},{"location":"DeltaConfig/#creating-instance","text":"DeltaConfig takes the following to be created: Configuration Key Default Value Conversion function (from text representation of the DeltaConfig to the T type, i.e. String => T ) Validation function (that guards from incorrect values, i.e. T => Boolean ) Help message (optional) Minimum version of protocol supported (default: undefined) DeltaConfig is created when: DeltaConfigs utility is used to build a DeltaConfig","title":"Creating Instance"},{"location":"DeltaConfig/#reading-configuration-property-from-metadata","text":"fromMetaData ( metadata : Metadata ): T fromMetaData looks up the key in the configuration of the given Metadata . If not found, fromMetaData gives the default value . In the end, fromMetaData converts the text representation to the proper type using fromString conversion function. fromMetaData is used when: Checkpoints utility is used to buildCheckpoint DeltaErrors utility is used to logFileNotFoundException DeltaLog is requested for checkpointInterval and deletedFileRetentionDuration table properties, and to assert a table is not read-only MetadataCleanup is requested for the enableExpiredLogCleanup and the deltaRetentionMillis OptimisticTransactionImpl is requested to commit Snapshot is requested for the numIndexedCols","title":" Reading Configuration Property From Metadata"},{"location":"DeltaConfig/#demo","text":"import org . apache . spark . sql . delta .{ DeltaConfig , DeltaConfigs } scala> :type DeltaConfigs.TOMBSTONE_RETENTION org.apache.spark.sql.delta.DeltaConfig[org.apache.spark.unsafe.types.CalendarInterval] import org . apache . spark . sql . delta . DeltaLog val path = \"/tmp/delta/t1\" val t1 = DeltaLog . forTable ( spark , path ) val metadata = t1 . snapshot . metadata val retention = DeltaConfigs . TOMBSTONE_RETENTION . fromMetaData ( metadata ) scala> :type retention org.apache.spark.unsafe.types.CalendarInterval","title":"Demo"},{"location":"DeltaConfigs/","text":"DeltaConfigs (DeltaConfigsBase) \u00b6 DeltaConfigs holds the table properties that can be set on a delta table. Configuration Properties \u00b6 appendOnly \u00b6 Whether a delta table is append-only ( true ) or not ( false ). When enabled, a table allows appends only and no updates or deletes. Default: false Used when: DeltaLog is requested to assertRemovable (that in turn uses DeltaErrors utility to modifyAppendOnlyTableException ) Protocol utility is used to requiredMinimumProtocol autoOptimize \u00b6 Whether this delta table will automagically optimize the layout of files during writes. Default: false checkpointInterval \u00b6 How often to checkpoint the state of a delta table (at the end of transaction commit ) Default: 10 checkpointRetentionDuration \u00b6 How long to keep checkpoint files around before deleting them Default: interval 2 days The most recent checkpoint is never deleted. It is acceptable to keep checkpoint files beyond this duration until the next calendar day. checkpoint.writeStatsAsJson \u00b6 Controls whether to write file statistics in the checkpoint in JSON format as the stats column. Default: true checkpoint.writeStatsAsStruct \u00b6 Controls whether to write file statistics in the checkpoint in the struct format in the stats_parsed column and partition values as a struct as partitionValues_parsed Default: undefined ( Option[Boolean] ) compatibility.symlinkFormatManifest.enabled \u00b6 Whether to register the GenerateSymlinkManifest post-commit hook while committing a transaction or not Default: false dataSkippingNumIndexedCols \u00b6 The number of columns to collect stats on for data skipping. -1 means collecting stats for all columns. Default: 32 deletedFileRetentionDuration \u00b6 How long to keep logically deleted data files around before deleting them physically (to prevent failures in stale readers after compactions or partition overwrites) Default: interval 1 week enableExpiredLogCleanup \u00b6 Whether to clean up expired log files and checkpoints Default: true enableFullRetentionRollback \u00b6 Controls whether or not a delta table can be rolled back to any point within logRetentionDuration . When disabled, the table can be rolled back checkpointRetentionDuration only. Default: true logRetentionDuration \u00b6 How long to keep obsolete logs around before deleting them. Delta can keep logs beyond the duration until the next calendar day to avoid constantly creating checkpoints. Default: interval 30 days ( CalendarInterval ) minReaderVersion \u00b6 The protocol reader version Default: 1 This property is not stored as a table property in the Metadata action. It is stored as its own action. Having it modelled as a table property makes it easier to upgrade, and view the version. minWriterVersion \u00b6 The protocol reader version Default: 3 This property is not stored as a table property in the Metadata action. It is stored as its own action. Having it modelled as a table property makes it easier to upgrade, and view the version. randomizeFilePrefixes \u00b6 Whether to use a random prefix in a file path instead of partition information (may be required for very high volume S3 calls to better be partitioned across S3 servers) Default: false randomPrefixLength \u00b6 The length of the random prefix in a file path for randomizeFilePrefixes Default: 2 sampleRetentionDuration \u00b6 How long to keep delta sample files around before deleting them Default: interval 7 days Building Configuration \u00b6 buildConfig [ T ]( key : String , defaultValue : String , fromString : String => T , validationFunction : T => Boolean , helpMessage : String , minimumProtocolVersion : Option [ Protocol ] = None ): DeltaConfig [ T ] buildConfig creates a DeltaConfig for the given key (with delta prefix added) and adds it to the entries internal registry. buildConfig is used to define all of the configuration properties in a type-safe way and (as a side effect) register them with the system-wide entries internal registry. System-Wide Configuration Entries Registry \u00b6 entries : HashMap [ String , DeltaConfig [ _ ]] DeltaConfigs utility (a Scala object) uses entries internal registry of DeltaConfig s by their key. New entries are added in buildConfig . entries is used when: validateConfigurations mergeGlobalConfigs normalizeConfigKey and normalizeConfigKeys mergeGlobalConfigs Utility \u00b6 mergeGlobalConfigs ( sqlConfs : SQLConf , tableConf : Map [ String , String ], protocol : Protocol ): Map [ String , String ] mergeGlobalConfigs finds all spark.databricks.delta.properties.defaults -prefixed configuration properties among the entries . mergeGlobalConfigs is used when: OptimisticTransactionImpl is requested to withGlobalConfigDefaults InitialSnapshot is created validateConfigurations Utility \u00b6 validateConfigurations ( configurations : Map [ String , String ]): Map [ String , String ] validateConfigurations ...FIXME validateConfigurations is used when: DeltaCatalog is requested to verifyTableAndSolidify and alterTable normalizeConfigKeys Utility \u00b6 normalizeConfigKeys ( propKeys : Seq [ String ]): Seq [ String ] normalizeConfigKeys ...FIXME normalizeConfigKeys is used when: AlterTableUnsetPropertiesDeltaCommand is executed spark.databricks.delta.properties.defaults Prefix \u00b6 DeltaConfigs uses spark.databricks.delta.properties.defaults prefix for global configuration properties .","title":"DeltaConfigs"},{"location":"DeltaConfigs/#deltaconfigs-deltaconfigsbase","text":"DeltaConfigs holds the table properties that can be set on a delta table.","title":"DeltaConfigs (DeltaConfigsBase)"},{"location":"DeltaConfigs/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"DeltaConfigs/#appendonly","text":"Whether a delta table is append-only ( true ) or not ( false ). When enabled, a table allows appends only and no updates or deletes. Default: false Used when: DeltaLog is requested to assertRemovable (that in turn uses DeltaErrors utility to modifyAppendOnlyTableException ) Protocol utility is used to requiredMinimumProtocol","title":" appendOnly"},{"location":"DeltaConfigs/#autooptimize","text":"Whether this delta table will automagically optimize the layout of files during writes. Default: false","title":" autoOptimize"},{"location":"DeltaConfigs/#checkpointinterval","text":"How often to checkpoint the state of a delta table (at the end of transaction commit ) Default: 10","title":" checkpointInterval"},{"location":"DeltaConfigs/#checkpointretentionduration","text":"How long to keep checkpoint files around before deleting them Default: interval 2 days The most recent checkpoint is never deleted. It is acceptable to keep checkpoint files beyond this duration until the next calendar day.","title":" checkpointRetentionDuration"},{"location":"DeltaConfigs/#checkpointwritestatsasjson","text":"Controls whether to write file statistics in the checkpoint in JSON format as the stats column. Default: true","title":" checkpoint.writeStatsAsJson"},{"location":"DeltaConfigs/#checkpointwritestatsasstruct","text":"Controls whether to write file statistics in the checkpoint in the struct format in the stats_parsed column and partition values as a struct as partitionValues_parsed Default: undefined ( Option[Boolean] )","title":" checkpoint.writeStatsAsStruct"},{"location":"DeltaConfigs/#compatibilitysymlinkformatmanifestenabled","text":"Whether to register the GenerateSymlinkManifest post-commit hook while committing a transaction or not Default: false","title":" compatibility.symlinkFormatManifest.enabled"},{"location":"DeltaConfigs/#dataskippingnumindexedcols","text":"The number of columns to collect stats on for data skipping. -1 means collecting stats for all columns. Default: 32","title":" dataSkippingNumIndexedCols"},{"location":"DeltaConfigs/#deletedfileretentionduration","text":"How long to keep logically deleted data files around before deleting them physically (to prevent failures in stale readers after compactions or partition overwrites) Default: interval 1 week","title":" deletedFileRetentionDuration"},{"location":"DeltaConfigs/#enableexpiredlogcleanup","text":"Whether to clean up expired log files and checkpoints Default: true","title":" enableExpiredLogCleanup"},{"location":"DeltaConfigs/#enablefullretentionrollback","text":"Controls whether or not a delta table can be rolled back to any point within logRetentionDuration . When disabled, the table can be rolled back checkpointRetentionDuration only. Default: true","title":" enableFullRetentionRollback"},{"location":"DeltaConfigs/#logretentionduration","text":"How long to keep obsolete logs around before deleting them. Delta can keep logs beyond the duration until the next calendar day to avoid constantly creating checkpoints. Default: interval 30 days ( CalendarInterval )","title":" logRetentionDuration"},{"location":"DeltaConfigs/#minreaderversion","text":"The protocol reader version Default: 1 This property is not stored as a table property in the Metadata action. It is stored as its own action. Having it modelled as a table property makes it easier to upgrade, and view the version.","title":" minReaderVersion"},{"location":"DeltaConfigs/#minwriterversion","text":"The protocol reader version Default: 3 This property is not stored as a table property in the Metadata action. It is stored as its own action. Having it modelled as a table property makes it easier to upgrade, and view the version.","title":" minWriterVersion"},{"location":"DeltaConfigs/#randomizefileprefixes","text":"Whether to use a random prefix in a file path instead of partition information (may be required for very high volume S3 calls to better be partitioned across S3 servers) Default: false","title":" randomizeFilePrefixes"},{"location":"DeltaConfigs/#randomprefixlength","text":"The length of the random prefix in a file path for randomizeFilePrefixes Default: 2","title":" randomPrefixLength"},{"location":"DeltaConfigs/#sampleretentionduration","text":"How long to keep delta sample files around before deleting them Default: interval 7 days","title":" sampleRetentionDuration"},{"location":"DeltaConfigs/#building-configuration","text":"buildConfig [ T ]( key : String , defaultValue : String , fromString : String => T , validationFunction : T => Boolean , helpMessage : String , minimumProtocolVersion : Option [ Protocol ] = None ): DeltaConfig [ T ] buildConfig creates a DeltaConfig for the given key (with delta prefix added) and adds it to the entries internal registry. buildConfig is used to define all of the configuration properties in a type-safe way and (as a side effect) register them with the system-wide entries internal registry.","title":" Building Configuration"},{"location":"DeltaConfigs/#system-wide-configuration-entries-registry","text":"entries : HashMap [ String , DeltaConfig [ _ ]] DeltaConfigs utility (a Scala object) uses entries internal registry of DeltaConfig s by their key. New entries are added in buildConfig . entries is used when: validateConfigurations mergeGlobalConfigs normalizeConfigKey and normalizeConfigKeys","title":" System-Wide Configuration Entries Registry"},{"location":"DeltaConfigs/#mergeglobalconfigs-utility","text":"mergeGlobalConfigs ( sqlConfs : SQLConf , tableConf : Map [ String , String ], protocol : Protocol ): Map [ String , String ] mergeGlobalConfigs finds all spark.databricks.delta.properties.defaults -prefixed configuration properties among the entries . mergeGlobalConfigs is used when: OptimisticTransactionImpl is requested to withGlobalConfigDefaults InitialSnapshot is created","title":" mergeGlobalConfigs Utility"},{"location":"DeltaConfigs/#validateconfigurations-utility","text":"validateConfigurations ( configurations : Map [ String , String ]): Map [ String , String ] validateConfigurations ...FIXME validateConfigurations is used when: DeltaCatalog is requested to verifyTableAndSolidify and alterTable","title":" validateConfigurations Utility"},{"location":"DeltaConfigs/#normalizeconfigkeys-utility","text":"normalizeConfigKeys ( propKeys : Seq [ String ]): Seq [ String ] normalizeConfigKeys ...FIXME normalizeConfigKeys is used when: AlterTableUnsetPropertiesDeltaCommand is executed","title":" normalizeConfigKeys Utility"},{"location":"DeltaConfigs/#sparkdatabricksdeltapropertiesdefaults-prefix","text":"DeltaConfigs uses spark.databricks.delta.properties.defaults prefix for global configuration properties .","title":" spark.databricks.delta.properties.defaults Prefix"},{"location":"DeltaDataSource/","text":"DeltaDataSource \u00b6 DeltaDataSource is a DataSourceRegister and is the entry point to all the features provided by delta data source. DeltaDataSource is a RelationProvider . DeltaDataSource is a StreamSinkProvider for a streaming sink for streaming queries (Structured Streaming). DataSourceRegister and delta Alias \u00b6 DeltaDataSource is a DataSourceRegister ( Spark SQL ) and registers delta alias. DeltaDataSource is registered using META-INF/services/org.apache.spark.sql.sources.DataSourceRegister : org.apache.spark.sql.delta.sources.DeltaDataSource RelationProvider for Batch Queries \u00b6 DeltaDataSource is a RelationProvider ( Spark SQL ) for reading ( loading ) data from a delta table in a structured query. createRelation ( sqlContext : SQLContext , parameters : Map [ String , String ]): BaseRelation createRelation reads the path option from the given parameters. createRelation verifies the given parameters . createRelation extracts time travel specification from the given parameters. In the end, createRelation creates a DeltaTableV2 (for the path option and the time travel specification) and requests it for an insertable HadoopFsRelation . createRelation throws an IllegalArgumentException when path option is not specified: 'path' is not specified Source Schema \u00b6 sourceSchema ( sqlContext : SQLContext , schema : Option [ StructType ], providerName : String , parameters : Map [ String , String ]): ( String , StructType ) sourceSchema creates a DeltaLog for a Delta table in the directory specified by the required path option (in the parameters) and returns the delta name with the schema (of the Delta table). sourceSchema throws an IllegalArgumentException when the path option has not been specified: 'path' is not specified sourceSchema throws an AnalysisException when the path option uses time travel : Cannot time travel views, subqueries or streams. sourceSchema is part of the StreamSourceProvider abstraction ( Spark Structured Streaming ). CreatableRelationProvider \u00b6 DeltaDataSource is a CreatableRelationProvider ( Spark SQL ) for writing out the result of a structured query. Creating Streaming Source \u00b6 DeltaDataSource is a StreamSourceProvider ( Spark Structured Streaming ) for a streaming source in streaming queries. Creating Streaming Sink \u00b6 DeltaDataSource is a StreamSinkProvider ( Spark Structured Streaming ) for a streaming sink in streaming queries. DeltaDataSource supports Append and Complete output modes only. In the end, DeltaDataSource creates a DeltaSink . Tip Consult the demo Using Delta Lake (as Streaming Sink) in Streaming Queries . Loading Table \u00b6 getTable ( schema : StructType , partitioning : Array [ Transform ], properties : java . util . Map [ String , String ]): Table getTable ...FIXME getTable is part of the TableProvider (Spark SQL 3.0.0) abstraction. Utilities \u00b6 getTimeTravelVersion \u00b6 getTimeTravelVersion ( parameters : Map [ String , String ]): Option [ DeltaTimeTravelSpec ] getTimeTravelVersion ...FIXME getTimeTravelVersion is used when DeltaDataSource is requested to create a relation (as a RelationProvider) . parsePathIdentifier \u00b6 parsePathIdentifier ( spark : SparkSession , userPath : String ): ( Path , Seq [( String , String )], Option [ DeltaTimeTravelSpec ]) parsePathIdentifier ...FIXME parsePathIdentifier is used when DeltaTableV2 is requested for metadata (for a non-catalog table).","title":"DeltaDataSource"},{"location":"DeltaDataSource/#deltadatasource","text":"DeltaDataSource is a DataSourceRegister and is the entry point to all the features provided by delta data source. DeltaDataSource is a RelationProvider . DeltaDataSource is a StreamSinkProvider for a streaming sink for streaming queries (Structured Streaming).","title":"DeltaDataSource"},{"location":"DeltaDataSource/#datasourceregister-and-delta-alias","text":"DeltaDataSource is a DataSourceRegister ( Spark SQL ) and registers delta alias. DeltaDataSource is registered using META-INF/services/org.apache.spark.sql.sources.DataSourceRegister : org.apache.spark.sql.delta.sources.DeltaDataSource","title":" DataSourceRegister and delta Alias"},{"location":"DeltaDataSource/#relationprovider-for-batch-queries","text":"DeltaDataSource is a RelationProvider ( Spark SQL ) for reading ( loading ) data from a delta table in a structured query. createRelation ( sqlContext : SQLContext , parameters : Map [ String , String ]): BaseRelation createRelation reads the path option from the given parameters. createRelation verifies the given parameters . createRelation extracts time travel specification from the given parameters. In the end, createRelation creates a DeltaTableV2 (for the path option and the time travel specification) and requests it for an insertable HadoopFsRelation . createRelation throws an IllegalArgumentException when path option is not specified: 'path' is not specified","title":" RelationProvider for Batch Queries"},{"location":"DeltaDataSource/#source-schema","text":"sourceSchema ( sqlContext : SQLContext , schema : Option [ StructType ], providerName : String , parameters : Map [ String , String ]): ( String , StructType ) sourceSchema creates a DeltaLog for a Delta table in the directory specified by the required path option (in the parameters) and returns the delta name with the schema (of the Delta table). sourceSchema throws an IllegalArgumentException when the path option has not been specified: 'path' is not specified sourceSchema throws an AnalysisException when the path option uses time travel : Cannot time travel views, subqueries or streams. sourceSchema is part of the StreamSourceProvider abstraction ( Spark Structured Streaming ).","title":" Source Schema"},{"location":"DeltaDataSource/#creatablerelationprovider","text":"DeltaDataSource is a CreatableRelationProvider ( Spark SQL ) for writing out the result of a structured query.","title":" CreatableRelationProvider"},{"location":"DeltaDataSource/#creating-streaming-source","text":"DeltaDataSource is a StreamSourceProvider ( Spark Structured Streaming ) for a streaming source in streaming queries.","title":" Creating Streaming Source"},{"location":"DeltaDataSource/#creating-streaming-sink","text":"DeltaDataSource is a StreamSinkProvider ( Spark Structured Streaming ) for a streaming sink in streaming queries. DeltaDataSource supports Append and Complete output modes only. In the end, DeltaDataSource creates a DeltaSink . Tip Consult the demo Using Delta Lake (as Streaming Sink) in Streaming Queries .","title":" Creating Streaming Sink"},{"location":"DeltaDataSource/#loading-table","text":"getTable ( schema : StructType , partitioning : Array [ Transform ], properties : java . util . Map [ String , String ]): Table getTable ...FIXME getTable is part of the TableProvider (Spark SQL 3.0.0) abstraction.","title":" Loading Table"},{"location":"DeltaDataSource/#utilities","text":"","title":"Utilities"},{"location":"DeltaDataSource/#gettimetravelversion","text":"getTimeTravelVersion ( parameters : Map [ String , String ]): Option [ DeltaTimeTravelSpec ] getTimeTravelVersion ...FIXME getTimeTravelVersion is used when DeltaDataSource is requested to create a relation (as a RelationProvider) .","title":" getTimeTravelVersion"},{"location":"DeltaDataSource/#parsepathidentifier","text":"parsePathIdentifier ( spark : SparkSession , userPath : String ): ( Path , Seq [( String , String )], Option [ DeltaTimeTravelSpec ]) parsePathIdentifier ...FIXME parsePathIdentifier is used when DeltaTableV2 is requested for metadata (for a non-catalog table).","title":" parsePathIdentifier"},{"location":"DeltaErrors/","text":"DeltaErrors Utility \u00b6 modifyAppendOnlyTableException \u00b6 modifyAppendOnlyTableException : Throwable modifyAppendOnlyTableException throws an UnsupportedOperationException : This table is configured to only allow appends. If you would like to permit updates or deletes, use 'ALTER TABLE <table_name> SET TBLPROPERTIES (appendOnly=false)'. modifyAppendOnlyTableException is used when: DeltaLog is requested to assertRemovable Reporting Post-Commit Hook Failure \u00b6 postCommitHookFailedException ( failedHook : PostCommitHook , failedOnCommitVersion : Long , extraErrorMessage : String , error : Throwable ): Throwable postCommitHookFailedException throws a RuntimeException : Committing to the Delta table version [failedOnCommitVersion] succeeded but error while executing post-commit hook [failedHook]: [extraErrorMessage] postCommitHookFailedException is used when: GenerateSymlinkManifestImpl is requested to handleError","title":"DeltaErrors"},{"location":"DeltaErrors/#deltaerrors-utility","text":"","title":"DeltaErrors Utility"},{"location":"DeltaErrors/#modifyappendonlytableexception","text":"modifyAppendOnlyTableException : Throwable modifyAppendOnlyTableException throws an UnsupportedOperationException : This table is configured to only allow appends. If you would like to permit updates or deletes, use 'ALTER TABLE <table_name> SET TBLPROPERTIES (appendOnly=false)'. modifyAppendOnlyTableException is used when: DeltaLog is requested to assertRemovable","title":" modifyAppendOnlyTableException"},{"location":"DeltaErrors/#reporting-post-commit-hook-failure","text":"postCommitHookFailedException ( failedHook : PostCommitHook , failedOnCommitVersion : Long , extraErrorMessage : String , error : Throwable ): Throwable postCommitHookFailedException throws a RuntimeException : Committing to the Delta table version [failedOnCommitVersion] succeeded but error while executing post-commit hook [failedHook]: [extraErrorMessage] postCommitHookFailedException is used when: GenerateSymlinkManifestImpl is requested to handleError","title":" Reporting Post-Commit Hook Failure"},{"location":"DeltaFileFormat/","text":"DeltaFileFormat \u00b6 DeltaFileFormat is an abstraction of format metadata that specify the file format of a delta table . Contract \u00b6 FileFormat \u00b6 fileFormat : FileFormat FileFormat ( Spark SQL ) of a delta table Default: ParquetFileFormat ( Spark SQL ) Used when: DeltaLog is requested for a relation (in batch queries) and DataFrame DeltaCommand is requested for a relation TransactionalWrite is requested to write data out Implementations \u00b6 Snapshot","title":"DeltaFileFormat"},{"location":"DeltaFileFormat/#deltafileformat","text":"DeltaFileFormat is an abstraction of format metadata that specify the file format of a delta table .","title":"DeltaFileFormat"},{"location":"DeltaFileFormat/#contract","text":"","title":"Contract"},{"location":"DeltaFileFormat/#fileformat","text":"fileFormat : FileFormat FileFormat ( Spark SQL ) of a delta table Default: ParquetFileFormat ( Spark SQL ) Used when: DeltaLog is requested for a relation (in batch queries) and DataFrame DeltaCommand is requested for a relation TransactionalWrite is requested to write data out","title":" FileFormat"},{"location":"DeltaFileFormat/#implementations","text":"Snapshot","title":"Implementations"},{"location":"DeltaFileOperations/","text":"DeltaFileOperations Utilities \u00b6 listUsingLogStore \u00b6 listUsingLogStore ( logStore : LogStore , subDirs : Iterator [ String ], recurse : Boolean , hiddenFileNameFilter : String => Boolean ): Iterator [ SerializableFileStatus ] listUsingLogStore ...FIXME listUsingLogStore is used when: DeltaFileOperations utility is used to recurseDirectories , recursiveListDirs and localListDirs localListDirs \u00b6 localListDirs ( spark : SparkSession , dirs : Seq [ String ], recursive : Boolean = true , fileFilter : String => Boolean = defaultHiddenFileFilter ): Seq [ SerializableFileStatus ] localListDirs ...FIXME localListDirs seems not used. recurseDirectories \u00b6 recurseDirectories ( logStore : LogStore , filesAndDirs : Iterator [ SerializableFileStatus ], hiddenFileNameFilter : String => Boolean ): Iterator [ SerializableFileStatus ] recurseDirectories ...FIXME recurseDirectories is used when: DeltaFileOperations utility is used to listUsingLogStore and recursiveListDirs recursiveListDirs \u00b6 recursiveListDirs ( spark : SparkSession , subDirs : Seq [ String ], hadoopConf : Broadcast [ SerializableConfiguration ], hiddenFileNameFilter : String => Boolean = defaultHiddenFileFilter , fileListingParallelism : Option [ Int ] = None ): Dataset [ SerializableFileStatus ] recursiveListDirs ...FIXME recursiveListDirs is used when: ManualListingFileManifest is requested to doList VacuumCommand utility is used to gc tryDeleteNonRecursive \u00b6 tryDeleteNonRecursive ( fs : FileSystem , path : Path , tries : Int = 3 ): Boolean tryDeleteNonRecursive ...FIXME tryDeleteNonRecursive is used when: VacuumCommandImpl is requested to delete","title":"DeltaFileOperations"},{"location":"DeltaFileOperations/#deltafileoperations-utilities","text":"","title":"DeltaFileOperations Utilities"},{"location":"DeltaFileOperations/#listusinglogstore","text":"listUsingLogStore ( logStore : LogStore , subDirs : Iterator [ String ], recurse : Boolean , hiddenFileNameFilter : String => Boolean ): Iterator [ SerializableFileStatus ] listUsingLogStore ...FIXME listUsingLogStore is used when: DeltaFileOperations utility is used to recurseDirectories , recursiveListDirs and localListDirs","title":" listUsingLogStore"},{"location":"DeltaFileOperations/#locallistdirs","text":"localListDirs ( spark : SparkSession , dirs : Seq [ String ], recursive : Boolean = true , fileFilter : String => Boolean = defaultHiddenFileFilter ): Seq [ SerializableFileStatus ] localListDirs ...FIXME localListDirs seems not used.","title":" localListDirs"},{"location":"DeltaFileOperations/#recursedirectories","text":"recurseDirectories ( logStore : LogStore , filesAndDirs : Iterator [ SerializableFileStatus ], hiddenFileNameFilter : String => Boolean ): Iterator [ SerializableFileStatus ] recurseDirectories ...FIXME recurseDirectories is used when: DeltaFileOperations utility is used to listUsingLogStore and recursiveListDirs","title":" recurseDirectories"},{"location":"DeltaFileOperations/#recursivelistdirs","text":"recursiveListDirs ( spark : SparkSession , subDirs : Seq [ String ], hadoopConf : Broadcast [ SerializableConfiguration ], hiddenFileNameFilter : String => Boolean = defaultHiddenFileFilter , fileListingParallelism : Option [ Int ] = None ): Dataset [ SerializableFileStatus ] recursiveListDirs ...FIXME recursiveListDirs is used when: ManualListingFileManifest is requested to doList VacuumCommand utility is used to gc","title":" recursiveListDirs"},{"location":"DeltaFileOperations/#trydeletenonrecursive","text":"tryDeleteNonRecursive ( fs : FileSystem , path : Path , tries : Int = 3 ): Boolean tryDeleteNonRecursive ...FIXME tryDeleteNonRecursive is used when: VacuumCommandImpl is requested to delete","title":" tryDeleteNonRecursive"},{"location":"DeltaHistoryManager/","text":"DeltaHistoryManager \u00b6 DeltaHistoryManager is used for version and commit history of a delta table . Creating Instance \u00b6 DeltaHistoryManager takes the following to be created: DeltaLog Maximum number of keys (default: 1000 ) DeltaHistoryManager is created when: DeltaLog is requested for one DeltaTableOperations is requested to execute history command Version and Commit History \u00b6 getHistory ( start : Long , end : Option [ Long ] = None ): Seq [ CommitInfo ] getHistory ( limitOpt : Option [ Int ]): Seq [ CommitInfo ] getHistory ...FIXME getHistory is used when: DeltaTableOperations is requested to executeHistory (for DeltaTable.history operator) DescribeDeltaHistoryCommand is executed (for DESCRIBE HISTORY SQL command) getCommitInfo Utility \u00b6 getCommitInfo ( logStore : LogStore , basePath : Path , version : Long ): CommitInfo getCommitInfo ...FIXME getActiveCommitAtTime \u00b6 getActiveCommitAtTime ( timestamp : Timestamp , canReturnLastCommit : Boolean , mustBeRecreatable : Boolean = true , canReturnEarliestCommit : Boolean = false ): Commit getActiveCommitAtTime ...FIXME getActiveCommitAtTime is used when: DeltaTableUtils utility is used to resolveTimeTravelVersion DeltaSource is requested for getStartingVersion","title":"DeltaHistoryManager"},{"location":"DeltaHistoryManager/#deltahistorymanager","text":"DeltaHistoryManager is used for version and commit history of a delta table .","title":"DeltaHistoryManager"},{"location":"DeltaHistoryManager/#creating-instance","text":"DeltaHistoryManager takes the following to be created: DeltaLog Maximum number of keys (default: 1000 ) DeltaHistoryManager is created when: DeltaLog is requested for one DeltaTableOperations is requested to execute history command","title":"Creating Instance"},{"location":"DeltaHistoryManager/#version-and-commit-history","text":"getHistory ( start : Long , end : Option [ Long ] = None ): Seq [ CommitInfo ] getHistory ( limitOpt : Option [ Int ]): Seq [ CommitInfo ] getHistory ...FIXME getHistory is used when: DeltaTableOperations is requested to executeHistory (for DeltaTable.history operator) DescribeDeltaHistoryCommand is executed (for DESCRIBE HISTORY SQL command)","title":" Version and Commit History"},{"location":"DeltaHistoryManager/#getcommitinfo-utility","text":"getCommitInfo ( logStore : LogStore , basePath : Path , version : Long ): CommitInfo getCommitInfo ...FIXME","title":" getCommitInfo Utility"},{"location":"DeltaHistoryManager/#getactivecommitattime","text":"getActiveCommitAtTime ( timestamp : Timestamp , canReturnLastCommit : Boolean , mustBeRecreatable : Boolean = true , canReturnEarliestCommit : Boolean = false ): Commit getActiveCommitAtTime ...FIXME getActiveCommitAtTime is used when: DeltaTableUtils utility is used to resolveTimeTravelVersion DeltaSource is requested for getStartingVersion","title":" getActiveCommitAtTime"},{"location":"DeltaInvariantCheckerExec/","text":"DeltaInvariantCheckerExec Unary Physical Operator \u00b6 DeltaInvariantCheckerExec is...FIXME","title":"DeltaInvariantCheckerExec"},{"location":"DeltaInvariantCheckerExec/#deltainvariantcheckerexec-unary-physical-operator","text":"DeltaInvariantCheckerExec is...FIXME","title":"DeltaInvariantCheckerExec Unary Physical Operator"},{"location":"DeltaLog/","text":"DeltaLog \u00b6 DeltaLog is a transaction log ( change log ) of changes to the state of a Delta table (in the given data directory ). Creating Instance \u00b6 DeltaLog takes the following to be created: Log directory (Hadoop Path ) Data directory (Hadoop Path ) Clock DeltaLog is created (indirectly via DeltaLog.apply utility) when: DeltaLog.forTable utility is used _delta_log Metadata Directory \u00b6 DeltaLog uses _delta_log metadata directory for the transaction log of a Delta table. The _delta_log directory is in the given data path directory (when created using DeltaLog.forTable utility). The _delta_log directory is resolved (in the DeltaLog.apply utility) using the application-wide Hadoop Configuration . Once resolved and turned into a qualified path, the _delta_log directory is cached . DeltaLog.forTable Utility \u00b6 forTable ( spark : SparkSession , table : CatalogTable ): DeltaLog forTable ( spark : SparkSession , table : CatalogTable , clock : Clock ): DeltaLog forTable ( spark : SparkSession , deltaTable : DeltaTableIdentifier ): DeltaLog forTable ( spark : SparkSession , dataPath : File ): DeltaLog forTable ( spark : SparkSession , dataPath : File , clock : Clock ): DeltaLog forTable ( spark : SparkSession , dataPath : Path ): DeltaLog forTable ( spark : SparkSession , dataPath : Path , clock : Clock ): DeltaLog forTable ( spark : SparkSession , dataPath : String ): DeltaLog forTable ( spark : SparkSession , dataPath : String , clock : Clock ): DeltaLog forTable ( spark : SparkSession , tableName : TableIdentifier ): DeltaLog forTable ( spark : SparkSession , tableName : TableIdentifier , clock : Clock ): DeltaLog forTable creates a DeltaLog with _delta_log directory (in the given dataPath directory). forTable is used when: AlterTableSetLocationDeltaCommand , ConvertToDeltaCommand , VacuumTableCommand , CreateDeltaTableCommand , DeltaGenerateCommand , DescribeDeltaDetailCommand , DescribeDeltaHistoryCommand commands are executed DeltaDataSource is requested for the source schema , a source , and a relation DeltaTable.isDeltaTable utility is used DeltaTableUtils.combineWithCatalogMetadata utility is used DeltaTableIdentifier is requested to getDeltaLog DeltaCatalog is requested to createDeltaTable DeltaTableV2 is requested for the DeltaLog DeltaSink is created Looking Up Or Creating DeltaLog Instance \u00b6 apply ( spark : SparkSession , rawPath : Path , clock : Clock = new SystemClock ): DeltaLog Note rawPath is a Hadoop Path to the _delta_log directory at the root of the data of a delta table. apply ...FIXME tableExists \u00b6 tableExists : Boolean tableExists requests the current Snapshot for the version and checks out whether it is 0 or higher. is used when: DeltaTable utility is used to isDeltaTable DeltaUnsupportedOperationsCheck logical check rule is executed DeltaTableV2 is requested to toBaseRelation Demo: Creating DeltaLog \u00b6 import org . apache . spark . sql . SparkSession assert ( spark . isInstanceOf [ SparkSession ]) val dataPath = \"/tmp/delta/t1\" import org . apache . spark . sql . delta . DeltaLog val deltaLog = DeltaLog . forTable ( spark , dataPath ) import org . apache . hadoop . fs . Path val expected = new Path ( s\"file: $ dataPath /_delta_log/_last_checkpoint\" ) assert ( deltaLog . LAST_CHECKPOINT == expected ) Accessing Current Version \u00b6 A common idiom (if not the only way) to know the current version of the delta table is to request the DeltaLog for the current state (snapshot) and then for the version . import org.apache.spark.sql.delta.DeltaLog assert(deltaLog.isInstanceOf[DeltaLog]) val deltaVersion = deltaLog.snapshot.version scala> println(deltaVersion) 5 Initialization \u00b6 When created, DeltaLog does the following: Creates the LogStore based on spark.delta.logStore.class configuration property Initializes the current snapshot Updates state of the delta table when there is no metadata checkpoint (e.g. the version of the state is -1 ) In other words, the version of (the DeltaLog of) a delta table is at version 0 at the very minimum. assert ( deltaLog . snapshot . version >= 0 ) filterFileList Utility \u00b6 filterFileList ( partitionSchema : StructType , files : DataFrame , partitionFilters : Seq [ Expression ], partitionColumnPrefixes : Seq [ String ] = Nil ): DataFrame filterFileList ...FIXME filterFileList is used when: OptimisticTransactionImpl is requested to checkAndRetry PartitionFiltering is requested to filesForScan WriteIntoDelta is requested to write SnapshotIterator is requested to iterator TahoeBatchFileIndex is requested to matchingFiles DeltaDataSource utility is requested to verifyAndCreatePartitionFilters FileFormats \u00b6 DeltaLog defines two FileFormat s ( Spark SQL ): ParquetFileFormat for indices of delta files JsonFileFormat for indices of checkpoint files These FileFormat s are used to create DeltaLogFileIndex es for Snapshots that in turn used them for stateReconstruction . LogStore \u00b6 DeltaLog uses a LogStore for...FIXME Transaction Logs (DeltaLogs) per Fully-Qualified Path \u00b6 deltaLogCache : Cache [ Path , DeltaLog ] deltaLogCache is part of DeltaLog Scala object which makes it an application-wide cache \"for free\". Once used, deltaLogCache will only be one until the application that uses it stops. deltaLogCache is a registry of DeltaLogs by their fully-qualified _delta_log directories. A new instance of DeltaLog is added when DeltaLog.apply utility is used and the instance hasn't been created before for a path. deltaLogCache is invalidated: For a delta table using DeltaLog.invalidateCache utility For all delta tables using DeltaLog.clearCache utility Executing Single-Threaded Operation in New Transaction \u00b6 withNewTransaction [ T ]( thunk : OptimisticTransaction => T ): T withNewTransaction starts a new transaction (that is active for the whole thread) and executes the given thunk block. In the end, withNewTransaction makes the transaction no longer active . withNewTransaction is used when: DeleteCommand , MergeIntoCommand , UpdateCommand , and WriteIntoDelta commands are executed DeltaSink is requested to add a streaming micro-batch Starting New Transaction \u00b6 startTransaction (): OptimisticTransaction startTransaction updates and creates a new OptimisticTransaction (for this DeltaLog ). Note startTransaction is a \"subset\" of withNewTransaction . startTransaction is used when: DeltaLog is requested to upgradeProtocol AlterDeltaTableCommand is requested to startTransaction ConvertToDeltaCommand and CreateDeltaTableCommand are executed Throwing UnsupportedOperationException for Append-Only Tables \u00b6 assertRemovable (): Unit assertRemovable throws an UnsupportedOperationException for the appendOnly table property ( in the Metadata ) enabled ( true ): This table is configured to only allow appends. If you would like to permit updates or deletes, use 'ALTER TABLE <table_name> SET TBLPROPERTIES (appendOnly=false)'. assertRemovable is used when: OptimisticTransactionImpl is requested to prepareCommit DeleteCommand , UpdateCommand , WriteIntoDelta (with Overwrite mode) are executed DeltaSink is requested to addBatch (with Complete output mode) metadata \u00b6 metadata : Metadata metadata is part of the Checkpoints abstraction. metadata requests the current Snapshot for the metadata or creates a new one (if the current Snapshot is not initialized). update \u00b6 update ( stalenessAcceptable : Boolean = false ): Snapshot update branches off based on a combination of flags: the given stalenessAcceptable and isSnapshotStale . For the stalenessAcceptable not acceptable (default) and the snapshot not stale , update simply acquires the deltaLogLock lock and updateInternal (with isAsync flag off). For all other cases, update ...FIXME update is used when: DeltaHistoryManager is requested to getHistory , getActiveCommitAtTime , and checkVersionExists DeltaLog is created (with no checkpoint created), and requested to startTransaction and withNewTransaction OptimisticTransactionImpl is requested to doCommit and checkAndRetry ConvertToDeltaCommand is requested to run and streamWrite VacuumCommand utility is used to gc TahoeLogFileIndex is requested for the (historical or latest) snapshot DeltaDataSource is requested for a relation tryUpdate \u00b6 tryUpdate ( isAsync : Boolean = false ): Snapshot tryUpdate ...FIXME Current State Snapshot \u00b6 snapshot : Snapshot snapshot returns the current snapshot . snapshot is used when: OptimisticTransaction is created Checkpoints is requested to checkpoint DeltaLog is requested for the metadata , to upgradeProtocol , getSnapshotAt , createRelation OptimisticTransactionImpl is requested to getNextAttemptVersion DeleteCommand , DeltaGenerateCommand , DescribeDeltaDetailCommand , UpdateCommand commands are executed GenerateSymlinkManifest is executed DeltaCommand is requested to buildBaseRelation TahoeFileIndex is requested for the table version , partitionSchema TahoeLogFileIndex is requested for the table size DeltaDataSource is requested for the schema of the streaming delta source DeltaSource is created and requested for the getStartingOffset , getBatch Current State Snapshot \u00b6 currentSnapshot : Snapshot currentSnapshot is a Snapshot based on the metadata checkpoint if available or a new Snapshot instance (with version being -1 ). Note For a new Snapshot instance (with version being -1 ) DeltaLog immediately updates the state . Internally, currentSnapshot ...FIXME currentSnapshot is available using snapshot method. currentSnapshot is used when: DeltaLog is requested to updateInternal , update and tryUpdate Creating Insertable HadoopFsRelation For Batch Queries \u00b6 createRelation ( partitionFilters : Seq [ Expression ] = Nil , snapshotToUseOpt : Option [ Snapshot ] = None , isTimeTravelQuery : Boolean = false , cdcOptions : CaseInsensitiveStringMap = CaseInsensitiveStringMap . empty ): BaseRelation createRelation ...FIXME createRelation creates a TahoeLogFileIndex for the data path , the given partitionFilters and a version (if defined). createRelation ...FIXME In the end, createRelation creates a HadoopFsRelation for the TahoeLogFileIndex and...FIXME. The HadoopFsRelation is also an InsertableRelation . createRelation is used when: DeltaTableV2 is requested to toBaseRelation WriteIntoDeltaBuilder is requested to buildForV1Write DeltaDataSource is requested for a writable relation insert \u00b6 insert ( data : DataFrame , overwrite : Boolean ): Unit insert ...FIXME insert is part of the InsertableRelation ( Spark SQL ) abstraction. Retrieving State Of Delta Table At Given Version \u00b6 getSnapshotAt ( version : Long , commitTimestamp : Option [ Long ] = None , lastCheckpointHint : Option [ CheckpointInstance ] = None ): Snapshot getSnapshotAt ...FIXME getSnapshotAt is used when: DeltaLog is requested for a relation , and to updateInternal DeltaSource is requested for the snapshot of a delta table at a given version TahoeLogFileIndex is requested for historicalSnapshotOpt Checkpoint Interval \u00b6 checkpointInterval : Int checkpointInterval is the current value of checkpointInterval table property ( from the Metadata ). checkpointInterval is used when: OptimisticTransactionImpl is requested to postCommit Changes (Actions) Of Delta Version And Later \u00b6 getChanges ( startVersion : Long ): Iterator [( Long , Seq [ Action ])] getChanges gives all action s ( changes ) per delta log file for the given startVersion of a delta table and later. val dataPath = \"/tmp/delta/users\" import org . apache . spark . sql . delta . DeltaLog val deltaLog = DeltaLog . forTable ( spark , dataPath ) assert ( deltaLog . isInstanceOf [ DeltaLog ]) val changesPerVersion = deltaLog . getChanges ( startVersion = 0 ) Internally, getChanges requests the LogStore for files that are lexicographically greater or equal to the delta log file for the given startVersion (in the logPath ) and leaves only delta log files (e.g. files with numbers only as file name and .json file extension). For every delta file, getChanges requests the LogStore to read the JSON content (every line is an action ), and then deserializes it to an action . getChanges is used when: DeltaSource is requested for the indexed file additions (FileAdd actions) Creating DataFrame For Given AddFiles \u00b6 createDataFrame ( snapshot : Snapshot , addFiles : Seq [ AddFile ], isStreaming : Boolean = false , actionTypeOpt : Option [ String ] = None ): DataFrame createDataFrame uses the action type based on the optional action type (if defined) or uses the following based on the isStreaming flag: streaming when isStreaming flag is enabled ( true ) batch when isStreaming flag is disabled ( false ) Note actionTypeOpt seems not to be defined ever. createDataFrame creates a new TahoeBatchFileIndex (for the action type, and the given AddFile s and Snapshot ). createDataFrame creates a HadoopFsRelation ( Spark SQL ) with the TahoeBatchFileIndex and the other properties based on the given Snapshot (and the associated Metadata ). In the end, createDataFrame creates a DataFrame with a logical query plan with a LogicalRelation ( Spark SQL ) over the HadoopFsRelation . createDataFrame is used when: MergeIntoCommand is executed DeltaSource is requested for a DataFrame for data between start and end offsets minFileRetentionTimestamp Method \u00b6 minFileRetentionTimestamp : Long minFileRetentionTimestamp is the timestamp that is tombstoneRetentionMillis before the current time (per the given Clock ). minFileRetentionTimestamp is used when: DeltaLog is requested for the currentSnapshot , to updateInternal , and to getSnapshotAt VacuumCommand is requested for garbage collecting of a delta table tombstoneRetentionMillis Method \u00b6 tombstoneRetentionMillis : Long tombstoneRetentionMillis gives the value of deletedFileRetentionDuration table property ( from the Metadata ). tombstoneRetentionMillis is used when: DeltaLog is requested for minFileRetentionTimestamp VacuumCommand is requested for garbage collecting of a delta table updateInternal Internal Method \u00b6 updateInternal ( isAsync : Boolean ): Snapshot updateInternal ...FIXME updateInternal is used when: DeltaLog is requested to update (directly or via tryUpdate ) Invalidating Cached DeltaLog Instance By Path \u00b6 invalidateCache ( spark : SparkSession , dataPath : Path ): Unit invalidateCache ...FIXME invalidateCache is a public API and does not seem to be used at all. protocolRead \u00b6 protocolRead ( protocol : Protocol ): Unit protocolRead ...FIXME protocolRead is used when: OptimisticTransactionImpl is requested to validate and retry a commit Snapshot is created DeltaSource is requested to verifyStreamHygieneAndFilterAddFiles upgradeProtocol \u00b6 upgradeProtocol ( newVersion : Protocol = Protocol ()): Unit upgradeProtocol ...FIXME upgradeProtocol is used when: DeltaTable is requested to upgradeTableProtocol LogStoreProvider \u00b6 DeltaLog is a LogStoreProvider . Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.delta.DeltaLog logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.DeltaLog=ALL Refer to Logging .","title":"DeltaLog"},{"location":"DeltaLog/#deltalog","text":"DeltaLog is a transaction log ( change log ) of changes to the state of a Delta table (in the given data directory ).","title":"DeltaLog"},{"location":"DeltaLog/#creating-instance","text":"DeltaLog takes the following to be created: Log directory (Hadoop Path ) Data directory (Hadoop Path ) Clock DeltaLog is created (indirectly via DeltaLog.apply utility) when: DeltaLog.forTable utility is used","title":"Creating Instance"},{"location":"DeltaLog/#_delta_log-metadata-directory","text":"DeltaLog uses _delta_log metadata directory for the transaction log of a Delta table. The _delta_log directory is in the given data path directory (when created using DeltaLog.forTable utility). The _delta_log directory is resolved (in the DeltaLog.apply utility) using the application-wide Hadoop Configuration . Once resolved and turned into a qualified path, the _delta_log directory is cached .","title":" _delta_log Metadata Directory"},{"location":"DeltaLog/#deltalogfortable-utility","text":"forTable ( spark : SparkSession , table : CatalogTable ): DeltaLog forTable ( spark : SparkSession , table : CatalogTable , clock : Clock ): DeltaLog forTable ( spark : SparkSession , deltaTable : DeltaTableIdentifier ): DeltaLog forTable ( spark : SparkSession , dataPath : File ): DeltaLog forTable ( spark : SparkSession , dataPath : File , clock : Clock ): DeltaLog forTable ( spark : SparkSession , dataPath : Path ): DeltaLog forTable ( spark : SparkSession , dataPath : Path , clock : Clock ): DeltaLog forTable ( spark : SparkSession , dataPath : String ): DeltaLog forTable ( spark : SparkSession , dataPath : String , clock : Clock ): DeltaLog forTable ( spark : SparkSession , tableName : TableIdentifier ): DeltaLog forTable ( spark : SparkSession , tableName : TableIdentifier , clock : Clock ): DeltaLog forTable creates a DeltaLog with _delta_log directory (in the given dataPath directory). forTable is used when: AlterTableSetLocationDeltaCommand , ConvertToDeltaCommand , VacuumTableCommand , CreateDeltaTableCommand , DeltaGenerateCommand , DescribeDeltaDetailCommand , DescribeDeltaHistoryCommand commands are executed DeltaDataSource is requested for the source schema , a source , and a relation DeltaTable.isDeltaTable utility is used DeltaTableUtils.combineWithCatalogMetadata utility is used DeltaTableIdentifier is requested to getDeltaLog DeltaCatalog is requested to createDeltaTable DeltaTableV2 is requested for the DeltaLog DeltaSink is created","title":" DeltaLog.forTable Utility"},{"location":"DeltaLog/#looking-up-or-creating-deltalog-instance","text":"apply ( spark : SparkSession , rawPath : Path , clock : Clock = new SystemClock ): DeltaLog Note rawPath is a Hadoop Path to the _delta_log directory at the root of the data of a delta table. apply ...FIXME","title":" Looking Up Or Creating DeltaLog Instance"},{"location":"DeltaLog/#tableexists","text":"tableExists : Boolean tableExists requests the current Snapshot for the version and checks out whether it is 0 or higher. is used when: DeltaTable utility is used to isDeltaTable DeltaUnsupportedOperationsCheck logical check rule is executed DeltaTableV2 is requested to toBaseRelation","title":" tableExists"},{"location":"DeltaLog/#demo-creating-deltalog","text":"import org . apache . spark . sql . SparkSession assert ( spark . isInstanceOf [ SparkSession ]) val dataPath = \"/tmp/delta/t1\" import org . apache . spark . sql . delta . DeltaLog val deltaLog = DeltaLog . forTable ( spark , dataPath ) import org . apache . hadoop . fs . Path val expected = new Path ( s\"file: $ dataPath /_delta_log/_last_checkpoint\" ) assert ( deltaLog . LAST_CHECKPOINT == expected )","title":"Demo: Creating DeltaLog"},{"location":"DeltaLog/#accessing-current-version","text":"A common idiom (if not the only way) to know the current version of the delta table is to request the DeltaLog for the current state (snapshot) and then for the version . import org.apache.spark.sql.delta.DeltaLog assert(deltaLog.isInstanceOf[DeltaLog]) val deltaVersion = deltaLog.snapshot.version scala> println(deltaVersion) 5","title":"Accessing Current Version"},{"location":"DeltaLog/#initialization","text":"When created, DeltaLog does the following: Creates the LogStore based on spark.delta.logStore.class configuration property Initializes the current snapshot Updates state of the delta table when there is no metadata checkpoint (e.g. the version of the state is -1 ) In other words, the version of (the DeltaLog of) a delta table is at version 0 at the very minimum. assert ( deltaLog . snapshot . version >= 0 )","title":"Initialization"},{"location":"DeltaLog/#filterfilelist-utility","text":"filterFileList ( partitionSchema : StructType , files : DataFrame , partitionFilters : Seq [ Expression ], partitionColumnPrefixes : Seq [ String ] = Nil ): DataFrame filterFileList ...FIXME filterFileList is used when: OptimisticTransactionImpl is requested to checkAndRetry PartitionFiltering is requested to filesForScan WriteIntoDelta is requested to write SnapshotIterator is requested to iterator TahoeBatchFileIndex is requested to matchingFiles DeltaDataSource utility is requested to verifyAndCreatePartitionFilters","title":" filterFileList Utility"},{"location":"DeltaLog/#fileformats","text":"DeltaLog defines two FileFormat s ( Spark SQL ): ParquetFileFormat for indices of delta files JsonFileFormat for indices of checkpoint files These FileFormat s are used to create DeltaLogFileIndex es for Snapshots that in turn used them for stateReconstruction .","title":"FileFormats"},{"location":"DeltaLog/#logstore","text":"DeltaLog uses a LogStore for...FIXME","title":" LogStore"},{"location":"DeltaLog/#transaction-logs-deltalogs-per-fully-qualified-path","text":"deltaLogCache : Cache [ Path , DeltaLog ] deltaLogCache is part of DeltaLog Scala object which makes it an application-wide cache \"for free\". Once used, deltaLogCache will only be one until the application that uses it stops. deltaLogCache is a registry of DeltaLogs by their fully-qualified _delta_log directories. A new instance of DeltaLog is added when DeltaLog.apply utility is used and the instance hasn't been created before for a path. deltaLogCache is invalidated: For a delta table using DeltaLog.invalidateCache utility For all delta tables using DeltaLog.clearCache utility","title":" Transaction Logs (DeltaLogs) per Fully-Qualified Path"},{"location":"DeltaLog/#executing-single-threaded-operation-in-new-transaction","text":"withNewTransaction [ T ]( thunk : OptimisticTransaction => T ): T withNewTransaction starts a new transaction (that is active for the whole thread) and executes the given thunk block. In the end, withNewTransaction makes the transaction no longer active . withNewTransaction is used when: DeleteCommand , MergeIntoCommand , UpdateCommand , and WriteIntoDelta commands are executed DeltaSink is requested to add a streaming micro-batch","title":" Executing Single-Threaded Operation in New Transaction"},{"location":"DeltaLog/#starting-new-transaction","text":"startTransaction (): OptimisticTransaction startTransaction updates and creates a new OptimisticTransaction (for this DeltaLog ). Note startTransaction is a \"subset\" of withNewTransaction . startTransaction is used when: DeltaLog is requested to upgradeProtocol AlterDeltaTableCommand is requested to startTransaction ConvertToDeltaCommand and CreateDeltaTableCommand are executed","title":" Starting New Transaction"},{"location":"DeltaLog/#throwing-unsupportedoperationexception-for-append-only-tables","text":"assertRemovable (): Unit assertRemovable throws an UnsupportedOperationException for the appendOnly table property ( in the Metadata ) enabled ( true ): This table is configured to only allow appends. If you would like to permit updates or deletes, use 'ALTER TABLE <table_name> SET TBLPROPERTIES (appendOnly=false)'. assertRemovable is used when: OptimisticTransactionImpl is requested to prepareCommit DeleteCommand , UpdateCommand , WriteIntoDelta (with Overwrite mode) are executed DeltaSink is requested to addBatch (with Complete output mode)","title":" Throwing UnsupportedOperationException for Append-Only Tables"},{"location":"DeltaLog/#metadata","text":"metadata : Metadata metadata is part of the Checkpoints abstraction. metadata requests the current Snapshot for the metadata or creates a new one (if the current Snapshot is not initialized).","title":" metadata"},{"location":"DeltaLog/#update","text":"update ( stalenessAcceptable : Boolean = false ): Snapshot update branches off based on a combination of flags: the given stalenessAcceptable and isSnapshotStale . For the stalenessAcceptable not acceptable (default) and the snapshot not stale , update simply acquires the deltaLogLock lock and updateInternal (with isAsync flag off). For all other cases, update ...FIXME update is used when: DeltaHistoryManager is requested to getHistory , getActiveCommitAtTime , and checkVersionExists DeltaLog is created (with no checkpoint created), and requested to startTransaction and withNewTransaction OptimisticTransactionImpl is requested to doCommit and checkAndRetry ConvertToDeltaCommand is requested to run and streamWrite VacuumCommand utility is used to gc TahoeLogFileIndex is requested for the (historical or latest) snapshot DeltaDataSource is requested for a relation","title":" update"},{"location":"DeltaLog/#tryupdate","text":"tryUpdate ( isAsync : Boolean = false ): Snapshot tryUpdate ...FIXME","title":" tryUpdate"},{"location":"DeltaLog/#current-state-snapshot","text":"snapshot : Snapshot snapshot returns the current snapshot . snapshot is used when: OptimisticTransaction is created Checkpoints is requested to checkpoint DeltaLog is requested for the metadata , to upgradeProtocol , getSnapshotAt , createRelation OptimisticTransactionImpl is requested to getNextAttemptVersion DeleteCommand , DeltaGenerateCommand , DescribeDeltaDetailCommand , UpdateCommand commands are executed GenerateSymlinkManifest is executed DeltaCommand is requested to buildBaseRelation TahoeFileIndex is requested for the table version , partitionSchema TahoeLogFileIndex is requested for the table size DeltaDataSource is requested for the schema of the streaming delta source DeltaSource is created and requested for the getStartingOffset , getBatch","title":" Current State Snapshot"},{"location":"DeltaLog/#current-state-snapshot_1","text":"currentSnapshot : Snapshot currentSnapshot is a Snapshot based on the metadata checkpoint if available or a new Snapshot instance (with version being -1 ). Note For a new Snapshot instance (with version being -1 ) DeltaLog immediately updates the state . Internally, currentSnapshot ...FIXME currentSnapshot is available using snapshot method. currentSnapshot is used when: DeltaLog is requested to updateInternal , update and tryUpdate","title":" Current State Snapshot"},{"location":"DeltaLog/#creating-insertable-hadoopfsrelation-for-batch-queries","text":"createRelation ( partitionFilters : Seq [ Expression ] = Nil , snapshotToUseOpt : Option [ Snapshot ] = None , isTimeTravelQuery : Boolean = false , cdcOptions : CaseInsensitiveStringMap = CaseInsensitiveStringMap . empty ): BaseRelation createRelation ...FIXME createRelation creates a TahoeLogFileIndex for the data path , the given partitionFilters and a version (if defined). createRelation ...FIXME In the end, createRelation creates a HadoopFsRelation for the TahoeLogFileIndex and...FIXME. The HadoopFsRelation is also an InsertableRelation . createRelation is used when: DeltaTableV2 is requested to toBaseRelation WriteIntoDeltaBuilder is requested to buildForV1Write DeltaDataSource is requested for a writable relation","title":" Creating Insertable HadoopFsRelation For Batch Queries"},{"location":"DeltaLog/#insert","text":"insert ( data : DataFrame , overwrite : Boolean ): Unit insert ...FIXME insert is part of the InsertableRelation ( Spark SQL ) abstraction.","title":" insert"},{"location":"DeltaLog/#retrieving-state-of-delta-table-at-given-version","text":"getSnapshotAt ( version : Long , commitTimestamp : Option [ Long ] = None , lastCheckpointHint : Option [ CheckpointInstance ] = None ): Snapshot getSnapshotAt ...FIXME getSnapshotAt is used when: DeltaLog is requested for a relation , and to updateInternal DeltaSource is requested for the snapshot of a delta table at a given version TahoeLogFileIndex is requested for historicalSnapshotOpt","title":" Retrieving State Of Delta Table At Given Version"},{"location":"DeltaLog/#checkpoint-interval","text":"checkpointInterval : Int checkpointInterval is the current value of checkpointInterval table property ( from the Metadata ). checkpointInterval is used when: OptimisticTransactionImpl is requested to postCommit","title":" Checkpoint Interval"},{"location":"DeltaLog/#changes-actions-of-delta-version-and-later","text":"getChanges ( startVersion : Long ): Iterator [( Long , Seq [ Action ])] getChanges gives all action s ( changes ) per delta log file for the given startVersion of a delta table and later. val dataPath = \"/tmp/delta/users\" import org . apache . spark . sql . delta . DeltaLog val deltaLog = DeltaLog . forTable ( spark , dataPath ) assert ( deltaLog . isInstanceOf [ DeltaLog ]) val changesPerVersion = deltaLog . getChanges ( startVersion = 0 ) Internally, getChanges requests the LogStore for files that are lexicographically greater or equal to the delta log file for the given startVersion (in the logPath ) and leaves only delta log files (e.g. files with numbers only as file name and .json file extension). For every delta file, getChanges requests the LogStore to read the JSON content (every line is an action ), and then deserializes it to an action . getChanges is used when: DeltaSource is requested for the indexed file additions (FileAdd actions)","title":" Changes (Actions) Of Delta Version And Later"},{"location":"DeltaLog/#creating-dataframe-for-given-addfiles","text":"createDataFrame ( snapshot : Snapshot , addFiles : Seq [ AddFile ], isStreaming : Boolean = false , actionTypeOpt : Option [ String ] = None ): DataFrame createDataFrame uses the action type based on the optional action type (if defined) or uses the following based on the isStreaming flag: streaming when isStreaming flag is enabled ( true ) batch when isStreaming flag is disabled ( false ) Note actionTypeOpt seems not to be defined ever. createDataFrame creates a new TahoeBatchFileIndex (for the action type, and the given AddFile s and Snapshot ). createDataFrame creates a HadoopFsRelation ( Spark SQL ) with the TahoeBatchFileIndex and the other properties based on the given Snapshot (and the associated Metadata ). In the end, createDataFrame creates a DataFrame with a logical query plan with a LogicalRelation ( Spark SQL ) over the HadoopFsRelation . createDataFrame is used when: MergeIntoCommand is executed DeltaSource is requested for a DataFrame for data between start and end offsets","title":" Creating DataFrame For Given AddFiles"},{"location":"DeltaLog/#minfileretentiontimestamp-method","text":"minFileRetentionTimestamp : Long minFileRetentionTimestamp is the timestamp that is tombstoneRetentionMillis before the current time (per the given Clock ). minFileRetentionTimestamp is used when: DeltaLog is requested for the currentSnapshot , to updateInternal , and to getSnapshotAt VacuumCommand is requested for garbage collecting of a delta table","title":" minFileRetentionTimestamp Method"},{"location":"DeltaLog/#tombstoneretentionmillis-method","text":"tombstoneRetentionMillis : Long tombstoneRetentionMillis gives the value of deletedFileRetentionDuration table property ( from the Metadata ). tombstoneRetentionMillis is used when: DeltaLog is requested for minFileRetentionTimestamp VacuumCommand is requested for garbage collecting of a delta table","title":" tombstoneRetentionMillis Method"},{"location":"DeltaLog/#updateinternal-internal-method","text":"updateInternal ( isAsync : Boolean ): Snapshot updateInternal ...FIXME updateInternal is used when: DeltaLog is requested to update (directly or via tryUpdate )","title":" updateInternal Internal Method"},{"location":"DeltaLog/#invalidating-cached-deltalog-instance-by-path","text":"invalidateCache ( spark : SparkSession , dataPath : Path ): Unit invalidateCache ...FIXME invalidateCache is a public API and does not seem to be used at all.","title":" Invalidating Cached DeltaLog Instance By Path"},{"location":"DeltaLog/#protocolread","text":"protocolRead ( protocol : Protocol ): Unit protocolRead ...FIXME protocolRead is used when: OptimisticTransactionImpl is requested to validate and retry a commit Snapshot is created DeltaSource is requested to verifyStreamHygieneAndFilterAddFiles","title":" protocolRead"},{"location":"DeltaLog/#upgradeprotocol","text":"upgradeProtocol ( newVersion : Protocol = Protocol ()): Unit upgradeProtocol ...FIXME upgradeProtocol is used when: DeltaTable is requested to upgradeTableProtocol","title":" upgradeProtocol"},{"location":"DeltaLog/#logstoreprovider","text":"DeltaLog is a LogStoreProvider .","title":"LogStoreProvider"},{"location":"DeltaLog/#logging","text":"Enable ALL logging level for org.apache.spark.sql.delta.DeltaLog logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.DeltaLog=ALL Refer to Logging .","title":"Logging"},{"location":"DeltaLogFileIndex/","text":"DeltaLogFileIndex \u00b6 DeltaLogFileIndex is a FileIndex ( Spark SQL ) for Snapshot (for the commit and checkpoint files). Creating Instance \u00b6 DeltaLogFileIndex takes the following to be created: FileFormat Files (as Hadoop FileStatus es) While being created, DeltaLogFileIndex prints out the following INFO message to the logs: Created [this] DeltaLogFileIndex is created (indirectly using apply utility) when Snapshot is requested for DeltaLogFileIndex for commit or checkpoint files. FileFormat \u00b6 DeltaLogFileIndex is given a FileFormat ( Spark SQL ) when created : JsonFileFormat ( Spark SQL ) for commit files ParquetFileFormat ( Spark SQL ) for checkpoint files Text Representation \u00b6 toString : String toString returns the following (using the given FileFormat , the number of files and their estimated size): DeltaLogFileIndex([format], numFilesInSegment: [files], totalFileSize: [sizeInBytes]) Creating DeltaLogFileIndex \u00b6 apply ( format : FileFormat , files : Seq [ FileStatus ]): Option [ DeltaLogFileIndex ] apply creates a new DeltaLogFileIndex (for a non-empty collection of files). apply is used when Snapshot is requested for DeltaLogFileIndex for commit or checkpoint files. Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.delta.DeltaLogFileIndex logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.DeltaLogFileIndex=ALL Refer to Logging .","title":"DeltaLogFileIndex"},{"location":"DeltaLogFileIndex/#deltalogfileindex","text":"DeltaLogFileIndex is a FileIndex ( Spark SQL ) for Snapshot (for the commit and checkpoint files).","title":"DeltaLogFileIndex"},{"location":"DeltaLogFileIndex/#creating-instance","text":"DeltaLogFileIndex takes the following to be created: FileFormat Files (as Hadoop FileStatus es) While being created, DeltaLogFileIndex prints out the following INFO message to the logs: Created [this] DeltaLogFileIndex is created (indirectly using apply utility) when Snapshot is requested for DeltaLogFileIndex for commit or checkpoint files.","title":"Creating Instance"},{"location":"DeltaLogFileIndex/#fileformat","text":"DeltaLogFileIndex is given a FileFormat ( Spark SQL ) when created : JsonFileFormat ( Spark SQL ) for commit files ParquetFileFormat ( Spark SQL ) for checkpoint files","title":" FileFormat"},{"location":"DeltaLogFileIndex/#text-representation","text":"toString : String toString returns the following (using the given FileFormat , the number of files and their estimated size): DeltaLogFileIndex([format], numFilesInSegment: [files], totalFileSize: [sizeInBytes])","title":" Text Representation"},{"location":"DeltaLogFileIndex/#creating-deltalogfileindex","text":"apply ( format : FileFormat , files : Seq [ FileStatus ]): Option [ DeltaLogFileIndex ] apply creates a new DeltaLogFileIndex (for a non-empty collection of files). apply is used when Snapshot is requested for DeltaLogFileIndex for commit or checkpoint files.","title":" Creating DeltaLogFileIndex"},{"location":"DeltaLogFileIndex/#logging","text":"Enable ALL logging level for org.apache.spark.sql.delta.DeltaLogFileIndex logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.DeltaLogFileIndex=ALL Refer to Logging .","title":"Logging"},{"location":"DeltaOptionParser/","text":"DeltaOptionParser \u00b6 DeltaOptionParser is an abstraction of options for reading from and writing to delta tables. Contract \u00b6 SQLConf \u00b6 sqlConf : SQLConf Used when: DeltaWriteOptionsImpl is requested for canMergeSchema Options \u00b6 options : CaseInsensitiveMap [ String ] Implementations \u00b6 DeltaReadOptions DeltaWriteOptions DeltaWriteOptionsImpl","title":"DeltaOptionParser"},{"location":"DeltaOptionParser/#deltaoptionparser","text":"DeltaOptionParser is an abstraction of options for reading from and writing to delta tables.","title":"DeltaOptionParser"},{"location":"DeltaOptionParser/#contract","text":"","title":"Contract"},{"location":"DeltaOptionParser/#sqlconf","text":"sqlConf : SQLConf Used when: DeltaWriteOptionsImpl is requested for canMergeSchema","title":" SQLConf"},{"location":"DeltaOptionParser/#options","text":"options : CaseInsensitiveMap [ String ]","title":" Options"},{"location":"DeltaOptionParser/#implementations","text":"DeltaReadOptions DeltaWriteOptions DeltaWriteOptionsImpl","title":"Implementations"},{"location":"DeltaOptions/","text":"DeltaOptions \u00b6 DeltaOptions is a DeltaWriteOptions and DeltaReadOptions . DeltaOptions is a type-safe abstraction of write and read options. DeltaOptions is used to create WriteIntoDelta command, DeltaSink , and DeltaSource . DeltaOptions is a Serializable ( Java ) (so it can be used in Spark tasks). Creating Instance \u00b6 DeltaOptions takes the following to be created: Case-Insensitive Options SQLConf ( Spark SQL ) When created, DeltaOptions verifies the input options. DeltaOptions is created when: DeltaLog is requested for a relation (for DeltaDataSource as a CreatableRelationProvider and a RelationProvider ) DeltaCatalog is requested to createDeltaTable WriteIntoDeltaBuilder is requested to buildForV1Write CreateDeltaTableCommand is executed DeltaDataSource is requested for a streaming source (to create a DeltaSource for Structured Streaming), a streaming sink (to create a DeltaSink for Structured Streaming), and for an insertable HadoopFsRelation Verifying Options \u00b6 verifyOptions ( options : CaseInsensitiveMap [ String ]): Unit verifyOptions finds invalid options among the input options . Note In the open-source version verifyOptions does nothing. The underlying objects ( recordDeltaEvent and the others) are no-ops. verifyOptions is used when: DeltaOptions is created DeltaDataSource is requested for a relation (for loading data in batch queries)","title":"DeltaOptions"},{"location":"DeltaOptions/#deltaoptions","text":"DeltaOptions is a DeltaWriteOptions and DeltaReadOptions . DeltaOptions is a type-safe abstraction of write and read options. DeltaOptions is used to create WriteIntoDelta command, DeltaSink , and DeltaSource . DeltaOptions is a Serializable ( Java ) (so it can be used in Spark tasks).","title":"DeltaOptions"},{"location":"DeltaOptions/#creating-instance","text":"DeltaOptions takes the following to be created: Case-Insensitive Options SQLConf ( Spark SQL ) When created, DeltaOptions verifies the input options. DeltaOptions is created when: DeltaLog is requested for a relation (for DeltaDataSource as a CreatableRelationProvider and a RelationProvider ) DeltaCatalog is requested to createDeltaTable WriteIntoDeltaBuilder is requested to buildForV1Write CreateDeltaTableCommand is executed DeltaDataSource is requested for a streaming source (to create a DeltaSource for Structured Streaming), a streaming sink (to create a DeltaSink for Structured Streaming), and for an insertable HadoopFsRelation","title":"Creating Instance"},{"location":"DeltaOptions/#verifying-options","text":"verifyOptions ( options : CaseInsensitiveMap [ String ]): Unit verifyOptions finds invalid options among the input options . Note In the open-source version verifyOptions does nothing. The underlying objects ( recordDeltaEvent and the others) are no-ops. verifyOptions is used when: DeltaOptions is created DeltaDataSource is requested for a relation (for loading data in batch queries)","title":" Verifying Options"},{"location":"DeltaReadOptions/","text":"DeltaReadOptions \u00b6 DeltaReadOptions is...FIXME","title":"DeltaReadOptions"},{"location":"DeltaReadOptions/#deltareadoptions","text":"DeltaReadOptions is...FIXME","title":"DeltaReadOptions"},{"location":"DeltaSQLConf/","text":"DeltaSQLConf \u2014 spark.databricks.delta Configuration Properties \u00b6 DeltaSQLConf contains spark.databricks.delta -prefixed configuration properties to configure behaviour of Delta Lake. alterLocation.bypassSchemaCheck \u00b6 spark.databricks.delta.alterLocation.bypassSchemaCheck enables Alter Table Set Location on Delta to go through even if the Delta table in the new location has a different schema from the original Delta table Default: false checkLatestSchemaOnRead \u00b6 spark.databricks.delta.checkLatestSchemaOnRead (internal) enables a check that ensures that users won't read corrupt data if the source schema changes in an incompatible way. Default: true In Delta, we always try to give users the latest version of their data without having to call REFRESH TABLE or redefine their DataFrames when used in the context of streaming. There is a possibility that the schema of the latest version of the table may be incompatible with the schema at the time of DataFrame creation. checkpoint.partSize \u00b6 spark.databricks.delta.checkpoint.partSize (internal) is the limit at which we will start parallelizing the checkpoint. We will attempt to write maximum of this many actions per checkpoint. Default: 5000000 commitInfo.enabled \u00b6 spark.databricks.delta.commitInfo.enabled controls whether to log commit information into a Delta log . Default: true commitInfo.userMetadata \u00b6 spark.databricks.delta.commitInfo.userMetadata is an arbitrary user-defined metadata to include in CommitInfo (requires spark.databricks.delta.commitInfo.enabled ). Default: (empty) commitValidation.enabled \u00b6 spark.databricks.delta.commitValidation.enabled (internal) controls whether to perform validation checks before commit or not Default: true convert.metadataCheck.enabled \u00b6 spark.databricks.delta.convert.metadataCheck.enabled enables validation during convert to delta, if there is a difference between the catalog table's properties and the Delta table's configuration, we should error. If disabled, merge the two configurations with the same semantics as update and merge Default: true dummyFileManager.numOfFiles \u00b6 spark.databricks.delta.dummyFileManager.numOfFiles (internal) controls how many dummy files to write in DummyFileManager Default: 3 dummyFileManager.prefix \u00b6 spark.databricks.delta.dummyFileManager.prefix (internal) is the file prefix to use in DummyFileManager Default: .s3-optimization- history.maxKeysPerList \u00b6 spark.databricks.delta.history.maxKeysPerList (internal) controls how many commits to list when performing a parallel search. The default is the maximum keys returned by S3 per list call. Azure can return 5000, therefore we choose 1000. Default: 1000 history.metricsEnabled \u00b6 spark.databricks.delta.history.metricsEnabled enables metrics reporting in DESCRIBE HISTORY ( CommitInfo will record the operation metrics when a OptimisticTransactionImpl is committed and the spark.databricks.delta.commitInfo.enabled configuration property is enabled). Requires spark.databricks.delta.commitInfo.enabled configuration property to be enabled Default: true Used when: OptimisticTransactionImpl is requested to getOperationMetrics ConvertToDeltaCommand is requested to streamWrite SQLMetricsReporting is requested to registerSQLMetrics TransactionalWrite is requested to writeFiles import.batchSize.schemaInference \u00b6 spark.databricks.delta.import.batchSize.schemaInference (internal) is the number of files per batch for schema inference during import . Default: 1000000 import.batchSize.statsCollection \u00b6 spark.databricks.delta.import.batchSize.statsCollection (internal) is the number of files per batch for stats collection during import . Default: 50000 maxSnapshotLineageLength \u00b6 spark.databricks.delta.maxSnapshotLineageLength (internal) is the maximum lineage length of a Snapshot before Delta forces to build a Snapshot from scratch Default: 50 merge.maxInsertCount \u00b6 spark.databricks.delta.merge.maxInsertCount (internal) is the maximum row count of inserts in each MERGE execution Default: 10000L merge.optimizeInsertOnlyMerge.enabled \u00b6 spark.databricks.delta.merge.optimizeInsertOnlyMerge.enabled (internal) controls merge without any matched clause (i.e., insert-only merge) will be optimized by avoiding rewriting old files and just inserting new files Default: true merge.optimizeMatchedOnlyMerge.enabled \u00b6 spark.databricks.delta.merge.optimizeMatchedOnlyMerge.enabled (internal) controls merge without 'when not matched' clause will be optimized to use a right outer join instead of a full outer join Default: true merge.repartitionBeforeWrite.enabled \u00b6 spark.databricks.delta.merge.repartitionBeforeWrite.enabled (internal) controls merge will repartition the output by the table's partition columns before writing the files Default: false partitionColumnValidity.enabled \u00b6 spark.databricks.delta.partitionColumnValidity.enabled (internal) enables validation of the partition column names (just like the data columns) Default: true properties.defaults.minReaderVersion \u00b6 spark.databricks.delta.properties.defaults.minReaderVersion is the default reader protocol version to create new tables with, unless a feature that requires a higher version for correctness is enabled. Default: 1 Available values: 1 Used when: Protocol utility is used to create a Protocol properties.defaults.minWriterVersion \u00b6 spark.databricks.delta.properties.defaults.minWriterVersion is the default writer protocol version to create new tables with, unless a feature that requires a higher version for correctness is enabled. Default: 2 Available values: 1 , 2 , 3 Used when: Protocol utility is used to create a Protocol retentionDurationCheck.enabled \u00b6 spark.databricks.delta.retentionDurationCheck.enabled adds a check preventing users from running vacuum with a very short retention period, which may end up corrupting a Delta log. Default: true sampling.enabled \u00b6 spark.databricks.delta.sampling.enabled (internal) enables sample-based estimation Default: false schema.autoMerge.enabled \u00b6 spark.databricks.delta.schema.autoMerge.enabled enables schema merging on appends and overwrites. Default: false Equivalent DataFrame option: mergeSchema snapshotIsolation.enabled \u00b6 spark.databricks.delta.snapshotIsolation.enabled (internal) controls whether queries on Delta tables are guaranteed to have snapshot isolation Default: true snapshotPartitions \u00b6 spark.databricks.delta.snapshotPartitions (internal) is the number of partitions to use for state reconstruction (when building a snapshot of a Delta table). Default: 50 stalenessLimit \u00b6 spark.databricks.delta.stalenessLimit (in millis) allows you to query the last loaded state of the Delta table without blocking on a table update. You can use this configuration to reduce the latency on queries when up-to-date results are not a requirement. Table updates will be scheduled on a separate scheduler pool in a FIFO queue, and will share cluster resources fairly with your query. If a table hasn't updated past this time limit, we will block on a synchronous state update before running the query. Default: 0 (no tables can be stale) state.corruptionIsFatal \u00b6 spark.databricks.delta.state.corruptionIsFatal (internal) throws a fatal error when the recreated Delta State doesn't match committed checksum file Default: true stateReconstructionValidation.enabled \u00b6 spark.databricks.delta.stateReconstructionValidation.enabled (internal) controls whether to perform validation checks on the reconstructed state Default: true stats.collect \u00b6 spark.databricks.delta.stats.collect (internal) enables statistics to be collected while writing files into a Delta table Default: true stats.limitPushdown.enabled \u00b6 spark.databricks.delta.stats.limitPushdown.enabled (internal) enables using the limit clause and file statistics to prune files before they are collected to the driver Default: true stats.localCache.maxNumFiles \u00b6 spark.databricks.delta.stats.localCache.maxNumFiles (internal) is the maximum number of files for a table to be considered a delta small table . Some metadata operations (such as using data skipping) are optimized for small tables using driver local caching and local execution. Default: 2000 stats.skipping \u00b6 spark.databricks.delta.stats.skipping (internal) enables statistics used for skipping Default: true timeTravel.resolveOnIdentifier.enabled \u00b6 spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled (internal) controls whether to resolve patterns as @v123 and @yyyyMMddHHmmssSSS in path identifiers as time travel nodes. Default: true vacuum.parallelDelete.enabled \u00b6 spark.databricks.delta.vacuum.parallelDelete.enabled enables parallelizing the deletion of files during vacuum command. Default: false Enabling may result hitting rate limits on some storage backends. When enabled, parallelization is controlled by the default number of shuffle partitions.","title":"DeltaSQLConf"},{"location":"DeltaSQLConf/#deltasqlconf-sparkdatabricksdelta-configuration-properties","text":"DeltaSQLConf contains spark.databricks.delta -prefixed configuration properties to configure behaviour of Delta Lake.","title":"DeltaSQLConf &mdash; spark.databricks.delta Configuration Properties"},{"location":"DeltaSQLConf/#alterlocationbypassschemacheck","text":"spark.databricks.delta.alterLocation.bypassSchemaCheck enables Alter Table Set Location on Delta to go through even if the Delta table in the new location has a different schema from the original Delta table Default: false","title":" alterLocation.bypassSchemaCheck"},{"location":"DeltaSQLConf/#checklatestschemaonread","text":"spark.databricks.delta.checkLatestSchemaOnRead (internal) enables a check that ensures that users won't read corrupt data if the source schema changes in an incompatible way. Default: true In Delta, we always try to give users the latest version of their data without having to call REFRESH TABLE or redefine their DataFrames when used in the context of streaming. There is a possibility that the schema of the latest version of the table may be incompatible with the schema at the time of DataFrame creation.","title":" checkLatestSchemaOnRead"},{"location":"DeltaSQLConf/#checkpointpartsize","text":"spark.databricks.delta.checkpoint.partSize (internal) is the limit at which we will start parallelizing the checkpoint. We will attempt to write maximum of this many actions per checkpoint. Default: 5000000","title":" checkpoint.partSize"},{"location":"DeltaSQLConf/#commitinfoenabled","text":"spark.databricks.delta.commitInfo.enabled controls whether to log commit information into a Delta log . Default: true","title":" commitInfo.enabled"},{"location":"DeltaSQLConf/#commitinfousermetadata","text":"spark.databricks.delta.commitInfo.userMetadata is an arbitrary user-defined metadata to include in CommitInfo (requires spark.databricks.delta.commitInfo.enabled ). Default: (empty)","title":" commitInfo.userMetadata"},{"location":"DeltaSQLConf/#commitvalidationenabled","text":"spark.databricks.delta.commitValidation.enabled (internal) controls whether to perform validation checks before commit or not Default: true","title":" commitValidation.enabled"},{"location":"DeltaSQLConf/#convertmetadatacheckenabled","text":"spark.databricks.delta.convert.metadataCheck.enabled enables validation during convert to delta, if there is a difference between the catalog table's properties and the Delta table's configuration, we should error. If disabled, merge the two configurations with the same semantics as update and merge Default: true","title":" convert.metadataCheck.enabled"},{"location":"DeltaSQLConf/#dummyfilemanagernumoffiles","text":"spark.databricks.delta.dummyFileManager.numOfFiles (internal) controls how many dummy files to write in DummyFileManager Default: 3","title":" dummyFileManager.numOfFiles"},{"location":"DeltaSQLConf/#dummyfilemanagerprefix","text":"spark.databricks.delta.dummyFileManager.prefix (internal) is the file prefix to use in DummyFileManager Default: .s3-optimization-","title":" dummyFileManager.prefix"},{"location":"DeltaSQLConf/#historymaxkeysperlist","text":"spark.databricks.delta.history.maxKeysPerList (internal) controls how many commits to list when performing a parallel search. The default is the maximum keys returned by S3 per list call. Azure can return 5000, therefore we choose 1000. Default: 1000","title":" history.maxKeysPerList"},{"location":"DeltaSQLConf/#historymetricsenabled","text":"spark.databricks.delta.history.metricsEnabled enables metrics reporting in DESCRIBE HISTORY ( CommitInfo will record the operation metrics when a OptimisticTransactionImpl is committed and the spark.databricks.delta.commitInfo.enabled configuration property is enabled). Requires spark.databricks.delta.commitInfo.enabled configuration property to be enabled Default: true Used when: OptimisticTransactionImpl is requested to getOperationMetrics ConvertToDeltaCommand is requested to streamWrite SQLMetricsReporting is requested to registerSQLMetrics TransactionalWrite is requested to writeFiles","title":" history.metricsEnabled"},{"location":"DeltaSQLConf/#importbatchsizeschemainference","text":"spark.databricks.delta.import.batchSize.schemaInference (internal) is the number of files per batch for schema inference during import . Default: 1000000","title":" import.batchSize.schemaInference"},{"location":"DeltaSQLConf/#importbatchsizestatscollection","text":"spark.databricks.delta.import.batchSize.statsCollection (internal) is the number of files per batch for stats collection during import . Default: 50000","title":" import.batchSize.statsCollection"},{"location":"DeltaSQLConf/#maxsnapshotlineagelength","text":"spark.databricks.delta.maxSnapshotLineageLength (internal) is the maximum lineage length of a Snapshot before Delta forces to build a Snapshot from scratch Default: 50","title":" maxSnapshotLineageLength"},{"location":"DeltaSQLConf/#mergemaxinsertcount","text":"spark.databricks.delta.merge.maxInsertCount (internal) is the maximum row count of inserts in each MERGE execution Default: 10000L","title":" merge.maxInsertCount"},{"location":"DeltaSQLConf/#mergeoptimizeinsertonlymergeenabled","text":"spark.databricks.delta.merge.optimizeInsertOnlyMerge.enabled (internal) controls merge without any matched clause (i.e., insert-only merge) will be optimized by avoiding rewriting old files and just inserting new files Default: true","title":" merge.optimizeInsertOnlyMerge.enabled"},{"location":"DeltaSQLConf/#mergeoptimizematchedonlymergeenabled","text":"spark.databricks.delta.merge.optimizeMatchedOnlyMerge.enabled (internal) controls merge without 'when not matched' clause will be optimized to use a right outer join instead of a full outer join Default: true","title":" merge.optimizeMatchedOnlyMerge.enabled"},{"location":"DeltaSQLConf/#mergerepartitionbeforewriteenabled","text":"spark.databricks.delta.merge.repartitionBeforeWrite.enabled (internal) controls merge will repartition the output by the table's partition columns before writing the files Default: false","title":" merge.repartitionBeforeWrite.enabled"},{"location":"DeltaSQLConf/#partitioncolumnvalidityenabled","text":"spark.databricks.delta.partitionColumnValidity.enabled (internal) enables validation of the partition column names (just like the data columns) Default: true","title":" partitionColumnValidity.enabled"},{"location":"DeltaSQLConf/#propertiesdefaultsminreaderversion","text":"spark.databricks.delta.properties.defaults.minReaderVersion is the default reader protocol version to create new tables with, unless a feature that requires a higher version for correctness is enabled. Default: 1 Available values: 1 Used when: Protocol utility is used to create a Protocol","title":" properties.defaults.minReaderVersion"},{"location":"DeltaSQLConf/#propertiesdefaultsminwriterversion","text":"spark.databricks.delta.properties.defaults.minWriterVersion is the default writer protocol version to create new tables with, unless a feature that requires a higher version for correctness is enabled. Default: 2 Available values: 1 , 2 , 3 Used when: Protocol utility is used to create a Protocol","title":" properties.defaults.minWriterVersion"},{"location":"DeltaSQLConf/#retentiondurationcheckenabled","text":"spark.databricks.delta.retentionDurationCheck.enabled adds a check preventing users from running vacuum with a very short retention period, which may end up corrupting a Delta log. Default: true","title":" retentionDurationCheck.enabled"},{"location":"DeltaSQLConf/#samplingenabled","text":"spark.databricks.delta.sampling.enabled (internal) enables sample-based estimation Default: false","title":" sampling.enabled"},{"location":"DeltaSQLConf/#schemaautomergeenabled","text":"spark.databricks.delta.schema.autoMerge.enabled enables schema merging on appends and overwrites. Default: false Equivalent DataFrame option: mergeSchema","title":" schema.autoMerge.enabled"},{"location":"DeltaSQLConf/#snapshotisolationenabled","text":"spark.databricks.delta.snapshotIsolation.enabled (internal) controls whether queries on Delta tables are guaranteed to have snapshot isolation Default: true","title":" snapshotIsolation.enabled"},{"location":"DeltaSQLConf/#snapshotpartitions","text":"spark.databricks.delta.snapshotPartitions (internal) is the number of partitions to use for state reconstruction (when building a snapshot of a Delta table). Default: 50","title":" snapshotPartitions"},{"location":"DeltaSQLConf/#stalenesslimit","text":"spark.databricks.delta.stalenessLimit (in millis) allows you to query the last loaded state of the Delta table without blocking on a table update. You can use this configuration to reduce the latency on queries when up-to-date results are not a requirement. Table updates will be scheduled on a separate scheduler pool in a FIFO queue, and will share cluster resources fairly with your query. If a table hasn't updated past this time limit, we will block on a synchronous state update before running the query. Default: 0 (no tables can be stale)","title":" stalenessLimit"},{"location":"DeltaSQLConf/#statecorruptionisfatal","text":"spark.databricks.delta.state.corruptionIsFatal (internal) throws a fatal error when the recreated Delta State doesn't match committed checksum file Default: true","title":" state.corruptionIsFatal"},{"location":"DeltaSQLConf/#statereconstructionvalidationenabled","text":"spark.databricks.delta.stateReconstructionValidation.enabled (internal) controls whether to perform validation checks on the reconstructed state Default: true","title":" stateReconstructionValidation.enabled"},{"location":"DeltaSQLConf/#statscollect","text":"spark.databricks.delta.stats.collect (internal) enables statistics to be collected while writing files into a Delta table Default: true","title":" stats.collect"},{"location":"DeltaSQLConf/#statslimitpushdownenabled","text":"spark.databricks.delta.stats.limitPushdown.enabled (internal) enables using the limit clause and file statistics to prune files before they are collected to the driver Default: true","title":" stats.limitPushdown.enabled"},{"location":"DeltaSQLConf/#statslocalcachemaxnumfiles","text":"spark.databricks.delta.stats.localCache.maxNumFiles (internal) is the maximum number of files for a table to be considered a delta small table . Some metadata operations (such as using data skipping) are optimized for small tables using driver local caching and local execution. Default: 2000","title":" stats.localCache.maxNumFiles"},{"location":"DeltaSQLConf/#statsskipping","text":"spark.databricks.delta.stats.skipping (internal) enables statistics used for skipping Default: true","title":" stats.skipping"},{"location":"DeltaSQLConf/#timetravelresolveonidentifierenabled","text":"spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled (internal) controls whether to resolve patterns as @v123 and @yyyyMMddHHmmssSSS in path identifiers as time travel nodes. Default: true","title":" timeTravel.resolveOnIdentifier.enabled"},{"location":"DeltaSQLConf/#vacuumparalleldeleteenabled","text":"spark.databricks.delta.vacuum.parallelDelete.enabled enables parallelizing the deletion of files during vacuum command. Default: false Enabling may result hitting rate limits on some storage backends. When enabled, parallelization is controlled by the default number of shuffle partitions.","title":" vacuum.parallelDelete.enabled"},{"location":"DeltaSink/","text":"DeltaSink \u00b6 DeltaSink is the Sink ( Spark Structured Streaming ) of the delta data source for streaming queries. Creating Instance \u00b6 DeltaSink takes the following to be created: SQLContext Hadoop Path of the delta table (to write data to as configured by the path option) Names of the partition columns OutputMode ( Spark Structured Streaming ) DeltaOptions DeltaSink is created when: DeltaDataSource is requested for a streaming sink DeltaLog \u00b6 deltaLog : DeltaLog deltaLog is a DeltaLog that is created for the delta table when DeltaSink is created . deltaLog is used when: DeltaSink is requested to add a streaming micro-batch Adding Streaming Micro-Batch \u00b6 addBatch ( batchId : Long , data : DataFrame ): Unit addBatch is part of the Sink ( Spark Structured Streaming ) abstraction. addBatch requests the DeltaLog to start a new transaction . addBatch ...FIXME In the end, addBatch requests the OptimisticTransaction to commit . Text Representation \u00b6 toString (): String DeltaSink uses the following text representation (with the path ): DeltaSink[path] ImplicitMetadataOperation \u00b6 DeltaSink is an ImplicitMetadataOperation .","title":"DeltaSink"},{"location":"DeltaSink/#deltasink","text":"DeltaSink is the Sink ( Spark Structured Streaming ) of the delta data source for streaming queries.","title":"DeltaSink"},{"location":"DeltaSink/#creating-instance","text":"DeltaSink takes the following to be created: SQLContext Hadoop Path of the delta table (to write data to as configured by the path option) Names of the partition columns OutputMode ( Spark Structured Streaming ) DeltaOptions DeltaSink is created when: DeltaDataSource is requested for a streaming sink","title":"Creating Instance"},{"location":"DeltaSink/#deltalog","text":"deltaLog : DeltaLog deltaLog is a DeltaLog that is created for the delta table when DeltaSink is created . deltaLog is used when: DeltaSink is requested to add a streaming micro-batch","title":" DeltaLog"},{"location":"DeltaSink/#adding-streaming-micro-batch","text":"addBatch ( batchId : Long , data : DataFrame ): Unit addBatch is part of the Sink ( Spark Structured Streaming ) abstraction. addBatch requests the DeltaLog to start a new transaction . addBatch ...FIXME In the end, addBatch requests the OptimisticTransaction to commit .","title":" Adding Streaming Micro-Batch"},{"location":"DeltaSink/#text-representation","text":"toString (): String DeltaSink uses the following text representation (with the path ): DeltaSink[path]","title":" Text Representation"},{"location":"DeltaSink/#implicitmetadataoperation","text":"DeltaSink is an ImplicitMetadataOperation .","title":" ImplicitMetadataOperation"},{"location":"DeltaSource/","text":"DeltaSource \u00b6 DeltaSource is the Source ( Spark Structured Streaming ) of the delta data source for streaming queries. Creating Instance \u00b6 DeltaSource takes the following to be created: SparkSession ( Spark SQL ) DeltaLog DeltaOptions Filters (default: empty) DeltaSource is created when: DeltaDataSource is requested for a streaming source Demo \u00b6 val q = spark .readStream // Creating a streaming query .format(\"delta\") // Using delta data source .load(\"/tmp/delta/users\") // Over data in a delta table .writeStream .format(\"memory\") .option(\"queryName\", \"demo\") .start import org.apache.spark.sql.execution.streaming.{MicroBatchExecution, StreamingQueryWrapper} val plan = q.asInstanceOf[StreamingQueryWrapper] .streamingQuery .asInstanceOf[MicroBatchExecution] .logicalPlan import org.apache.spark.sql.execution.streaming.StreamingExecutionRelation val relation = plan.collect { case r: StreamingExecutionRelation => r }.head import org.apache.spark.sql.delta.sources.DeltaSource assert(relation.source.asInstanceOf[DeltaSource]) scala> println(relation.source) DeltaSource[file:/tmp/delta/users] Latest Available Offset \u00b6 latestOffset ( startOffset : streaming . Offset , limit : ReadLimit ): streaming . Offset latestOffset is part of the SupportsAdmissionControl ( Spark Structured Streaming ) abstraction. latestOffset determines the latest offset ( currentOffset ) based on whether the previousOffset internal registry is initialized or not . latestOffset prints out the following DEBUG message to the logs (using the previousOffset internal registry). previousOffset -> currentOffset: [previousOffset] -> [currentOffset] In the end, latestOffset returns the previousOffset if defined or null . No previousOffset \u00b6 For no previousOffset , getOffset retrieves the starting offset (with a new AdmissionLimits for the given ReadLimit ). previousOffset Available \u00b6 When the previousOffset is defined (which is when the DeltaSource is requested for another micro-batch), latestOffset gets the changes as an indexed AddFile s (with the previousOffset and a new AdmissionLimits for the given ReadLimit ). latestOffset takes the last AddFile if available. With no AddFile , latestOffset returns the previousOffset . With the previousOffset and the last indexed AddFile both available, latestOffset creates a new DeltaSourceOffset for the version, index, and isLast flag from the last indexed AddFile . Note isStartingVersion local value is enabled ( true ) when the following holds: Version of the last indexed AddFile is equal to the reservoirVersion of the previous ending offset isStartingVersion flag of the previous ending offset is enabled ( true ) getStartingOffset \u00b6 getStartingOffset ( limits : Option [ AdmissionLimits ]): Option [ Offset ] getStartingOffset ...FIXME (review me) getStartingOffset requests the DeltaLog for the version of the delta table (by requesting for the current state (snapshot) and then for the version ). getStartingOffset takes the last file from the files added (with rate limit) for the version of the delta table, -1L as the fromIndex , and the isStartingVersion flag enabled ( true ). getStartingOffset returns a new DeltaSourceOffset for the tableId , the version and the index of the last file added, and whether the last file added is the last file of its version. getStartingOffset returns None ( offset not available ) when either happens: the version of the delta table is negative (below 0 ) no files were added in the version getStartingOffset throws an AssertionError when the version of the last file added is smaller than the delta table's version: assertion failed: getChangesWithRateLimit returns an invalid version: [v] (expected: >= [version]) getChangesWithRateLimit \u00b6 getChangesWithRateLimit ( fromVersion : Long , fromIndex : Long , isStartingVersion : Boolean ): Iterator [ IndexedFile ] getChangesWithRateLimit gets the changes (as indexed AddFile s) for the given fromVersion , fromIndex , and isStartingVersion flag. getOffset \u00b6 getOffset : Option [ Offset ] getOffset is part of the Source ( Spark Structured Streaming ) abstraction. getOffset has been replaced by the newer latestOffset and so throws an UnsupportedOperationException when called: latestOffset(Offset, ReadLimit) should be called instead of this method Requesting Micro-Batch DataFrame \u00b6 getBatch ( start : Option [ Offset ], end : Offset ): DataFrame getBatch is part of the Source ( Spark Structured Streaming ) abstraction. getBatch creates an DeltaSourceOffset for the tableId (aka reservoirId ) and the given end offset. getBatch gets the changes ...FIXME getBatch prints out the following DEBUG message to the logs: start: [start] end: [end] [addFiles] In the end, getBatch requests the DeltaLog to createDataFrame (for the current snapshot of the DeltaLog , addFiles and isStreaming flag on). Snapshot Management \u00b6 DeltaSource uses internal registries for the DeltaSourceSnapshot and the version to avoid requesting the DeltaLog for getSnapshotAt . Snapshot \u00b6 DeltaSource uses initialState internal registry for the DeltaSourceSnapshot of the state of the delta table at the initialStateVersion . DeltaSourceSnapshot is used for AddFiles of the delta table at a given version . Initially uninitialized ( null ). DeltaSourceSnapshot is created ( initialized ) when uninitialized or the version requested is different from the current one . DeltaSourceSnapshot is closed and dereferenced ( null ) when DeltaSource is requested to cleanUpSnapshotResources (due to version change , another micro-batch or stop ). Version \u00b6 DeltaSource uses initialStateVersion internal registry to keep track of the version of DeltaSourceSnapshot (when requested for AddFiles of the delta table at a given version ). Changes (alongside the initialState ) to the version requested when DeltaSource is requested for the snapshot at a given version (only when the versions are different) Used when: DeltaSource is requested for AddFiles of the delta table at a given version and to cleanUpSnapshotResources (and unpersist the current snapshot) Stopping \u00b6 stop (): Unit stop is part of the Source ( Spark Structured Streaming ) abstraction. stop simply cleanUpSnapshotResources . Previous Offset \u00b6 Ending DeltaSourceOffset of the latest micro-batch Starts uninitialized ( null ). Used when DeltaSource is requested for the latest available offset . AddFiles of Delta Table at Given Version \u00b6 getSnapshotAt ( version : Long ): Iterator [ IndexedFile ] getSnapshotAt requests the DeltaSourceSnapshot for the data files (as indexed AddFile s). In case the DeltaSourceSnapshot hasn't been initialized yet ( null ) or the requested version is different from the initialStateVersion , getSnapshotAt does the following: cleanUpSnapshotResources Requests the DeltaLog for the state (snapshot) of the delta table at the version Creates a new DeltaSourceSnapshot for the state (snapshot) as the current DeltaSourceSnapshot Changes the initialStateVersion internal registry to the requested version getSnapshotAt is used when: DeltaSource is requested to getChanges (with isStartingVersion flag enabled) getChanges \u00b6 getChanges ( fromVersion : Long , fromIndex : Long , isStartingVersion : Boolean ): Iterator [ IndexedFile ] getChanges branches based on isStartingVersion flag (enabled or not): For isStartingVersion flag enabled ( true ), getChanges gets the state (snapshot) for the given fromVersion followed by (filtered out) indexed AddFiles for the next version after the given fromVersion For isStartingVersion flag disabled ( false ), getChanges simply gives (filtered out) indexed AddFiles for the given fromVersion Note isStartingVersion flag simply adds the state (snapshot) before (filtered out) indexed AddFiles when enabled ( true ). isStartingVersion flag is enabled when DeltaSource is requested for the following: Micro-batch with data between start and end offsets and the start offset is not given or is for the starting version Latest available offset with no end offset of the latest micro-batch or the end offset of the latest micro-batch for the starting version In the end, getChanges filters out ( excludes ) indexed AddFile s that are not with the version later than the given fromVersion or the index greater than the given fromIndex . getChanges is used when: DeltaSource is requested for the latest available offset (when requested for the files added (with rate limit) ) and getBatch filterAndIndexDeltaLogs \u00b6 filterAndIndexDeltaLogs ( startVersion : Long ): Iterator [ IndexedFile ] filterAndIndexDeltaLogs ...FIXME verifyStreamHygieneAndFilterAddFiles \u00b6 verifyStreamHygieneAndFilterAddFiles ( actions : Seq [ Action ], version : Long ): Seq [ Action ] verifyStreamHygieneAndFilterAddFiles ...FIXME cleanUpSnapshotResources \u00b6 cleanUpSnapshotResources (): Unit cleanUpSnapshotResources does the following when the initial DeltaSourceSnapshot internal registry is not empty: Requests the DeltaSourceSnapshot to close (with the unpersistSnapshot flag based on whether the initialStateVersion is earlier than the snapshot version ) Dereferences ( nullifies ) the DeltaSourceSnapshot Otherwise, cleanUpSnapshotResources does nothing. cleanUpSnapshotResources is used when: DeltaSource is requested to getSnapshotAt , getBatch and stop ReadLimit \u00b6 getDefaultReadLimit : ReadLimit getDefaultReadLimit is part of the SupportsAdmissionControl ( Spark Structured Streaming ) abstraction. getDefaultReadLimit creates a AdmissionLimits and requests it for a corresponding ReadLimit . Retrieving Last Element From Iterator \u00b6 iteratorLast [ T ]( iter : Iterator [ T ]): Option [ T ] iteratorLast simply returns the last element of the given Iterator ( Scala ) or None . iteratorLast is used when: DeltaSource is requested to getStartingOffset and getOffset Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.delta.sources.DeltaSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.sources.DeltaSource=ALL Refer to Logging .","title":"DeltaSource"},{"location":"DeltaSource/#deltasource","text":"DeltaSource is the Source ( Spark Structured Streaming ) of the delta data source for streaming queries.","title":"DeltaSource"},{"location":"DeltaSource/#creating-instance","text":"DeltaSource takes the following to be created: SparkSession ( Spark SQL ) DeltaLog DeltaOptions Filters (default: empty) DeltaSource is created when: DeltaDataSource is requested for a streaming source","title":"Creating Instance"},{"location":"DeltaSource/#demo","text":"val q = spark .readStream // Creating a streaming query .format(\"delta\") // Using delta data source .load(\"/tmp/delta/users\") // Over data in a delta table .writeStream .format(\"memory\") .option(\"queryName\", \"demo\") .start import org.apache.spark.sql.execution.streaming.{MicroBatchExecution, StreamingQueryWrapper} val plan = q.asInstanceOf[StreamingQueryWrapper] .streamingQuery .asInstanceOf[MicroBatchExecution] .logicalPlan import org.apache.spark.sql.execution.streaming.StreamingExecutionRelation val relation = plan.collect { case r: StreamingExecutionRelation => r }.head import org.apache.spark.sql.delta.sources.DeltaSource assert(relation.source.asInstanceOf[DeltaSource]) scala> println(relation.source) DeltaSource[file:/tmp/delta/users]","title":"Demo"},{"location":"DeltaSource/#latest-available-offset","text":"latestOffset ( startOffset : streaming . Offset , limit : ReadLimit ): streaming . Offset latestOffset is part of the SupportsAdmissionControl ( Spark Structured Streaming ) abstraction. latestOffset determines the latest offset ( currentOffset ) based on whether the previousOffset internal registry is initialized or not . latestOffset prints out the following DEBUG message to the logs (using the previousOffset internal registry). previousOffset -> currentOffset: [previousOffset] -> [currentOffset] In the end, latestOffset returns the previousOffset if defined or null .","title":" Latest Available Offset"},{"location":"DeltaSource/#no-previousoffset","text":"For no previousOffset , getOffset retrieves the starting offset (with a new AdmissionLimits for the given ReadLimit ).","title":" No previousOffset"},{"location":"DeltaSource/#previousoffset-available","text":"When the previousOffset is defined (which is when the DeltaSource is requested for another micro-batch), latestOffset gets the changes as an indexed AddFile s (with the previousOffset and a new AdmissionLimits for the given ReadLimit ). latestOffset takes the last AddFile if available. With no AddFile , latestOffset returns the previousOffset . With the previousOffset and the last indexed AddFile both available, latestOffset creates a new DeltaSourceOffset for the version, index, and isLast flag from the last indexed AddFile . Note isStartingVersion local value is enabled ( true ) when the following holds: Version of the last indexed AddFile is equal to the reservoirVersion of the previous ending offset isStartingVersion flag of the previous ending offset is enabled ( true )","title":" previousOffset Available"},{"location":"DeltaSource/#getstartingoffset","text":"getStartingOffset ( limits : Option [ AdmissionLimits ]): Option [ Offset ] getStartingOffset ...FIXME (review me) getStartingOffset requests the DeltaLog for the version of the delta table (by requesting for the current state (snapshot) and then for the version ). getStartingOffset takes the last file from the files added (with rate limit) for the version of the delta table, -1L as the fromIndex , and the isStartingVersion flag enabled ( true ). getStartingOffset returns a new DeltaSourceOffset for the tableId , the version and the index of the last file added, and whether the last file added is the last file of its version. getStartingOffset returns None ( offset not available ) when either happens: the version of the delta table is negative (below 0 ) no files were added in the version getStartingOffset throws an AssertionError when the version of the last file added is smaller than the delta table's version: assertion failed: getChangesWithRateLimit returns an invalid version: [v] (expected: >= [version])","title":" getStartingOffset"},{"location":"DeltaSource/#getchangeswithratelimit","text":"getChangesWithRateLimit ( fromVersion : Long , fromIndex : Long , isStartingVersion : Boolean ): Iterator [ IndexedFile ] getChangesWithRateLimit gets the changes (as indexed AddFile s) for the given fromVersion , fromIndex , and isStartingVersion flag.","title":" getChangesWithRateLimit"},{"location":"DeltaSource/#getoffset","text":"getOffset : Option [ Offset ] getOffset is part of the Source ( Spark Structured Streaming ) abstraction. getOffset has been replaced by the newer latestOffset and so throws an UnsupportedOperationException when called: latestOffset(Offset, ReadLimit) should be called instead of this method","title":" getOffset"},{"location":"DeltaSource/#requesting-micro-batch-dataframe","text":"getBatch ( start : Option [ Offset ], end : Offset ): DataFrame getBatch is part of the Source ( Spark Structured Streaming ) abstraction. getBatch creates an DeltaSourceOffset for the tableId (aka reservoirId ) and the given end offset. getBatch gets the changes ...FIXME getBatch prints out the following DEBUG message to the logs: start: [start] end: [end] [addFiles] In the end, getBatch requests the DeltaLog to createDataFrame (for the current snapshot of the DeltaLog , addFiles and isStreaming flag on).","title":" Requesting Micro-Batch DataFrame"},{"location":"DeltaSource/#snapshot-management","text":"DeltaSource uses internal registries for the DeltaSourceSnapshot and the version to avoid requesting the DeltaLog for getSnapshotAt .","title":"Snapshot Management"},{"location":"DeltaSource/#snapshot","text":"DeltaSource uses initialState internal registry for the DeltaSourceSnapshot of the state of the delta table at the initialStateVersion . DeltaSourceSnapshot is used for AddFiles of the delta table at a given version . Initially uninitialized ( null ). DeltaSourceSnapshot is created ( initialized ) when uninitialized or the version requested is different from the current one . DeltaSourceSnapshot is closed and dereferenced ( null ) when DeltaSource is requested to cleanUpSnapshotResources (due to version change , another micro-batch or stop ).","title":" Snapshot"},{"location":"DeltaSource/#version","text":"DeltaSource uses initialStateVersion internal registry to keep track of the version of DeltaSourceSnapshot (when requested for AddFiles of the delta table at a given version ). Changes (alongside the initialState ) to the version requested when DeltaSource is requested for the snapshot at a given version (only when the versions are different) Used when: DeltaSource is requested for AddFiles of the delta table at a given version and to cleanUpSnapshotResources (and unpersist the current snapshot)","title":" Version"},{"location":"DeltaSource/#stopping","text":"stop (): Unit stop is part of the Source ( Spark Structured Streaming ) abstraction. stop simply cleanUpSnapshotResources .","title":" Stopping"},{"location":"DeltaSource/#previous-offset","text":"Ending DeltaSourceOffset of the latest micro-batch Starts uninitialized ( null ). Used when DeltaSource is requested for the latest available offset .","title":" Previous Offset"},{"location":"DeltaSource/#addfiles-of-delta-table-at-given-version","text":"getSnapshotAt ( version : Long ): Iterator [ IndexedFile ] getSnapshotAt requests the DeltaSourceSnapshot for the data files (as indexed AddFile s). In case the DeltaSourceSnapshot hasn't been initialized yet ( null ) or the requested version is different from the initialStateVersion , getSnapshotAt does the following: cleanUpSnapshotResources Requests the DeltaLog for the state (snapshot) of the delta table at the version Creates a new DeltaSourceSnapshot for the state (snapshot) as the current DeltaSourceSnapshot Changes the initialStateVersion internal registry to the requested version getSnapshotAt is used when: DeltaSource is requested to getChanges (with isStartingVersion flag enabled)","title":" AddFiles of Delta Table at Given Version"},{"location":"DeltaSource/#getchanges","text":"getChanges ( fromVersion : Long , fromIndex : Long , isStartingVersion : Boolean ): Iterator [ IndexedFile ] getChanges branches based on isStartingVersion flag (enabled or not): For isStartingVersion flag enabled ( true ), getChanges gets the state (snapshot) for the given fromVersion followed by (filtered out) indexed AddFiles for the next version after the given fromVersion For isStartingVersion flag disabled ( false ), getChanges simply gives (filtered out) indexed AddFiles for the given fromVersion Note isStartingVersion flag simply adds the state (snapshot) before (filtered out) indexed AddFiles when enabled ( true ). isStartingVersion flag is enabled when DeltaSource is requested for the following: Micro-batch with data between start and end offsets and the start offset is not given or is for the starting version Latest available offset with no end offset of the latest micro-batch or the end offset of the latest micro-batch for the starting version In the end, getChanges filters out ( excludes ) indexed AddFile s that are not with the version later than the given fromVersion or the index greater than the given fromIndex . getChanges is used when: DeltaSource is requested for the latest available offset (when requested for the files added (with rate limit) ) and getBatch","title":" getChanges"},{"location":"DeltaSource/#filterandindexdeltalogs","text":"filterAndIndexDeltaLogs ( startVersion : Long ): Iterator [ IndexedFile ] filterAndIndexDeltaLogs ...FIXME","title":" filterAndIndexDeltaLogs"},{"location":"DeltaSource/#verifystreamhygieneandfilteraddfiles","text":"verifyStreamHygieneAndFilterAddFiles ( actions : Seq [ Action ], version : Long ): Seq [ Action ] verifyStreamHygieneAndFilterAddFiles ...FIXME","title":" verifyStreamHygieneAndFilterAddFiles"},{"location":"DeltaSource/#cleanupsnapshotresources","text":"cleanUpSnapshotResources (): Unit cleanUpSnapshotResources does the following when the initial DeltaSourceSnapshot internal registry is not empty: Requests the DeltaSourceSnapshot to close (with the unpersistSnapshot flag based on whether the initialStateVersion is earlier than the snapshot version ) Dereferences ( nullifies ) the DeltaSourceSnapshot Otherwise, cleanUpSnapshotResources does nothing. cleanUpSnapshotResources is used when: DeltaSource is requested to getSnapshotAt , getBatch and stop","title":" cleanUpSnapshotResources"},{"location":"DeltaSource/#readlimit","text":"getDefaultReadLimit : ReadLimit getDefaultReadLimit is part of the SupportsAdmissionControl ( Spark Structured Streaming ) abstraction. getDefaultReadLimit creates a AdmissionLimits and requests it for a corresponding ReadLimit .","title":" ReadLimit"},{"location":"DeltaSource/#retrieving-last-element-from-iterator","text":"iteratorLast [ T ]( iter : Iterator [ T ]): Option [ T ] iteratorLast simply returns the last element of the given Iterator ( Scala ) or None . iteratorLast is used when: DeltaSource is requested to getStartingOffset and getOffset","title":" Retrieving Last Element From Iterator"},{"location":"DeltaSource/#logging","text":"Enable ALL logging level for org.apache.spark.sql.delta.sources.DeltaSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.sources.DeltaSource=ALL Refer to Logging .","title":"Logging"},{"location":"DeltaSourceOffset/","text":"DeltaSourceOffset \u2014 Streaming Offset \u00b6 DeltaSourceOffset is an Offset ( Spark Structured Streaming ) for DeltaSource . Creating Instance \u00b6 DeltaSourceOffset takes the following to be created: Source Version Reservoir ID (aka Table ID ) Reservoir Version Index isStartingVersion flag DeltaSourceOffset is created (using apply utility) when: DeltaSource is requested for the starting and latest offsets Creating DeltaSourceOffset \u00b6 apply ( reservoirId : String , offset : Offset ): DeltaSourceOffset apply ( reservoirId : String , reservoirVersion : Long , index : Long , isStartingVersion : Boolean ): DeltaSourceOffset apply creates a DeltaSourceOffset (for the version and the given arguments) or converts a SerializedOffset to a DeltaSourceOffset . apply is used when: DeltaSource is requested for the starting and latest offsets validateSourceVersion \u00b6 validateSourceVersion ( json : String ): Unit validateSourceVersion ...FIXME Source Version \u00b6 DeltaSourceOffset uses 1 for the version (and does not allow changing it). The version is used when: DeltaSourceOffset.apply and validateSourceVersion utilities are used","title":"DeltaSourceOffset"},{"location":"DeltaSourceOffset/#deltasourceoffset-streaming-offset","text":"DeltaSourceOffset is an Offset ( Spark Structured Streaming ) for DeltaSource .","title":"DeltaSourceOffset &mdash; Streaming Offset"},{"location":"DeltaSourceOffset/#creating-instance","text":"DeltaSourceOffset takes the following to be created: Source Version Reservoir ID (aka Table ID ) Reservoir Version Index isStartingVersion flag DeltaSourceOffset is created (using apply utility) when: DeltaSource is requested for the starting and latest offsets","title":"Creating Instance"},{"location":"DeltaSourceOffset/#creating-deltasourceoffset","text":"apply ( reservoirId : String , offset : Offset ): DeltaSourceOffset apply ( reservoirId : String , reservoirVersion : Long , index : Long , isStartingVersion : Boolean ): DeltaSourceOffset apply creates a DeltaSourceOffset (for the version and the given arguments) or converts a SerializedOffset to a DeltaSourceOffset . apply is used when: DeltaSource is requested for the starting and latest offsets","title":" Creating DeltaSourceOffset"},{"location":"DeltaSourceOffset/#validatesourceversion","text":"validateSourceVersion ( json : String ): Unit validateSourceVersion ...FIXME","title":" validateSourceVersion"},{"location":"DeltaSourceOffset/#source-version","text":"DeltaSourceOffset uses 1 for the version (and does not allow changing it). The version is used when: DeltaSourceOffset.apply and validateSourceVersion utilities are used","title":" Source Version"},{"location":"DeltaSourceSnapshot/","text":"DeltaSourceSnapshot \u00b6 DeltaSourceSnapshot is a SnapshotIterator and a StateCache for DeltaSource . Creating Instance \u00b6 DeltaSourceSnapshot takes the following to be created: SparkSession ( Spark SQL ) Snapshot Filter Expressions ( Spark SQL ) DeltaSourceSnapshot is created when: DeltaSource is requested for the snapshot of a delta table at a given version Initial Files (Indexed AddFiles) \u00b6 initialFiles : Dataset [ IndexedFile ] Dataset of Indexed AddFiles \u00b6 initialFiles requests the Snapshot for all AddFiles (in the snapshot) ( Dataset[AddFile] ). initialFiles sorts the AddFile dataset ( Dataset[AddFile] ) by modificationTime and path in ascending order. initialFiles indexes the AddFiles (using RDD.zipWithIndex operator) that gives a RDD[(AddFile, Long)] . initialFiles converts the RDD to a DataFrame of two columns: add and index . initialFiles adds the two new columns: version isLast as false literal initialFiles converts ( projects ) DataFrame to Dataset[IndexedFile] . Creating CachedDS \u00b6 initialFiles creates a CachedDS with the following name (with the version and path of the Snapshot ): Delta Source Snapshot #[version] - [path] Cached Dataset of Indexed AddFiles \u00b6 In the end, initialFiles requests the CachedDS to getDS . Usage \u00b6 initialFiles is used when: SnapshotIterator is requested for the AddFiles Closing \u00b6 close ( unpersistSnapshot : Boolean ): Unit close is part of the SnapshotIterator abstraction. close requests the Snapshot to uncache when the given unpersistSnapshot flag is enabled.","title":"DeltaSourceSnapshot"},{"location":"DeltaSourceSnapshot/#deltasourcesnapshot","text":"DeltaSourceSnapshot is a SnapshotIterator and a StateCache for DeltaSource .","title":"DeltaSourceSnapshot"},{"location":"DeltaSourceSnapshot/#creating-instance","text":"DeltaSourceSnapshot takes the following to be created: SparkSession ( Spark SQL ) Snapshot Filter Expressions ( Spark SQL ) DeltaSourceSnapshot is created when: DeltaSource is requested for the snapshot of a delta table at a given version","title":"Creating Instance"},{"location":"DeltaSourceSnapshot/#initial-files-indexed-addfiles","text":"initialFiles : Dataset [ IndexedFile ]","title":" Initial Files (Indexed AddFiles)"},{"location":"DeltaSourceSnapshot/#dataset-of-indexed-addfiles","text":"initialFiles requests the Snapshot for all AddFiles (in the snapshot) ( Dataset[AddFile] ). initialFiles sorts the AddFile dataset ( Dataset[AddFile] ) by modificationTime and path in ascending order. initialFiles indexes the AddFiles (using RDD.zipWithIndex operator) that gives a RDD[(AddFile, Long)] . initialFiles converts the RDD to a DataFrame of two columns: add and index . initialFiles adds the two new columns: version isLast as false literal initialFiles converts ( projects ) DataFrame to Dataset[IndexedFile] .","title":" Dataset of Indexed AddFiles"},{"location":"DeltaSourceSnapshot/#creating-cachedds","text":"initialFiles creates a CachedDS with the following name (with the version and path of the Snapshot ): Delta Source Snapshot #[version] - [path]","title":" Creating CachedDS"},{"location":"DeltaSourceSnapshot/#cached-dataset-of-indexed-addfiles","text":"In the end, initialFiles requests the CachedDS to getDS .","title":" Cached Dataset of Indexed AddFiles"},{"location":"DeltaSourceSnapshot/#usage","text":"initialFiles is used when: SnapshotIterator is requested for the AddFiles","title":" Usage"},{"location":"DeltaSourceSnapshot/#closing","text":"close ( unpersistSnapshot : Boolean ): Unit close is part of the SnapshotIterator abstraction. close requests the Snapshot to uncache when the given unpersistSnapshot flag is enabled.","title":" Closing"},{"location":"DeltaSparkSessionExtension/","text":"DeltaSparkSessionExtension \u00b6 DeltaSparkSessionExtension is used to register ( inject ) the following extensions to a SparkSession : Delta SQL support (using DeltaSqlParser ) DeltaAnalysis logical resolution rule DeltaUnsupportedOperationsCheck PreprocessTableUpdate logical resolution rule PreprocessTableMerge logical resolution rule PreprocessTableDelete logical resolution rule ActiveOptimisticTransactionRule logical optimization rule DeltaSparkSessionExtension is registered using spark.sql.extensions configuration property (while creating a SparkSession in a Spark application).","title":"DeltaSparkSessionExtension"},{"location":"DeltaSparkSessionExtension/#deltasparksessionextension","text":"DeltaSparkSessionExtension is used to register ( inject ) the following extensions to a SparkSession : Delta SQL support (using DeltaSqlParser ) DeltaAnalysis logical resolution rule DeltaUnsupportedOperationsCheck PreprocessTableUpdate logical resolution rule PreprocessTableMerge logical resolution rule PreprocessTableDelete logical resolution rule ActiveOptimisticTransactionRule logical optimization rule DeltaSparkSessionExtension is registered using spark.sql.extensions configuration property (while creating a SparkSession in a Spark application).","title":"DeltaSparkSessionExtension"},{"location":"DeltaTable/","text":"DeltaTable \u00b6 DeltaTable is the management interface of a delta table. DeltaTable is created using utilities (e.g. DeltaTable.forName , DeltaTable.convertToDelta ). io.delta.tables Package \u00b6 DeltaTable belongs to io.delta.tables package. import io . delta . tables . DeltaTable DeltaLog \u00b6 deltaLog : DeltaLog deltaLog is a DeltaLog of the DeltaTableV2 . Utilities (Static Methods) \u00b6 convertToDelta \u00b6 convertToDelta ( spark : SparkSession , identifier : String ): DeltaTable convertToDelta ( spark : SparkSession , identifier : String , partitionSchema : String ): DeltaTable convertToDelta ( spark : SparkSession , identifier : String , partitionSchema : StructType ): DeltaTable convertToDelta converts a parquet table to delta format (and makes the table available in Delta Lake). Note Refer to Demo: Converting Parquet Dataset Into Delta Format for a demo of DeltaTable.convertToDelta . Internally, convertToDelta requests the SparkSession for the SQL parser ( ParserInterface ) that is in turn requested to parse the given table identifier (to get a TableIdentifier ). Tip Read up on ParserInterface in The Internals of Spark SQL online book. In the end, convertToDelta uses the DeltaConvert utility to convert the parquet table to delta format and creates a DeltaTable . forName \u00b6 forName ( sparkSession : SparkSession , tableName : String ): DeltaTable forName ( tableOrViewName : String ): DeltaTable forName uses ParserInterface (of the given SparkSession ) to parse the given table name. forName checks whether the given table name is of a Delta table and, if so, creates a DeltaTable with the following: Dataset that represents loading data from the specified table name (using SparkSession.table operator) DeltaLog of the specified table forName throws an AnalysisException when the given table name is for non-Delta table: [deltaTableIdentifier] is not a Delta table. forName is used internally when DeltaConvert utility is used to executeConvert . forPath \u00b6 forPath ( sparkSession : SparkSession , path : String ): DeltaTable forPath ( path : String ): DeltaTable forPath creates a DeltaTable instance for data in the given directory ( path ) when the given directory is part of a delta table already (as the root or a child directory). assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession]) val tableId = \"/tmp/delta-table/users\" import io.delta.tables.DeltaTable assert(DeltaTable.isDeltaTable(tableId), s\"$tableId should be a Delta table\") val dt = DeltaTable.forPath(\"delta-table\") forPath throws an AnalysisException when the given path does not belong to a delta table: [deltaTableIdentifier] is not a Delta table. Internally, forPath creates a new DeltaTable with the following: Dataset that represents loading data from the specified path using delta data source DeltaLog for the (transaction log in) the specified path forPath is used internally in DeltaTable.convertToDelta (via DeltaConvert utility). isDeltaTable \u00b6 isDeltaTable ( sparkSession : SparkSession , identifier : String ): Boolean isDeltaTable ( identifier : String ): Boolean isDeltaTable checks whether the provided identifier string is a file path that points to the root of a Delta table or one of the subdirectories. Internally, isDeltaTable simply relays to DeltaTableUtils.isDeltaTable utility. Creating Instance \u00b6 DeltaTable takes the following to be created: Table Data ( Dataset[Row] ) DeltaTableV2 DeltaTable is created using DeltaTable.forPath or DeltaTable.forName utilities. Operators \u00b6 alias \u00b6 alias ( alias : String ): DeltaTable Applies an alias to the DeltaTable (equivalent to as ) as \u00b6 as ( alias : String ): DeltaTable Applies an alias to the DeltaTable delete \u00b6 delete (): Unit delete ( condition : Column ): Unit delete ( condition : String ): Unit Deletes data from the DeltaTable that matches the given condition delete executes DeleteFromTable command . generate \u00b6 generate ( mode : String ): Unit Generates a manifest for the delta table generate executes the DeltaGenerateCommand with the table ID of the format delta.`path` (where the path is the data directory of the DeltaLog ) and the given mode. history \u00b6 history (): DataFrame history ( limit : Int ): DataFrame Gets available commits ( history ) of the DeltaTable history requests the DeltaHistoryManager for history . merge \u00b6 merge ( source : DataFrame , condition : Column ): DeltaMergeBuilder merge ( source : DataFrame , condition : String ): DeltaMergeBuilder Creates a DeltaMergeBuilder toDF \u00b6 toDF : Dataset [ Row ] Returns the DataFrame representation of the DeltaTable update \u00b6 update ( condition : Column , set : Map [ String , Column ]): Unit update ( set : Map [ String , Column ]): Unit Updates data in the DeltaTable on the rows that match the given condition based on the rules defined by set update executes UpdateTable command . updateExpr \u00b6 updateExpr ( set : Map [ String , String ]): Unit updateExpr ( condition : String , set : Map [ String , String ]): Unit Updates data in the DeltaTable on the rows that match the given condition based on the rules defined by set update executes UpdateTable command . upgradeTableProtocol \u00b6 upgradeTableProtocol ( readerVersion : Int , writerVersion : Int ): Unit Updates the protocol version of the table to leverage new features. Upgrading the reader version will prevent all clients that have an older version of Delta Lake from accessing this table. Upgrading the writer version will prevent older versions of Delta Lake to write to this table. The reader or writer version cannot be downgraded. Internally, upgradeTableProtocol creates a new Protocol (with the given versions) and requests the DeltaLog to upgradeProtocol . [SC-44271][DELTA] Introduce default protocol version for Delta tables upgradeTableProtocol was introduced in [SC-44271][DELTA] Introduce default protocol version for Delta tables commit. vacuum \u00b6 vacuum (): DataFrame vacuum ( retentionHours : Double ): DataFrame Deletes files and directories (recursively) in the DeltaTable that are not needed by the table (and maintains older versions up to the given retention threshold). vacuum executes vacuum command .","title":"DeltaTable"},{"location":"DeltaTable/#deltatable","text":"DeltaTable is the management interface of a delta table. DeltaTable is created using utilities (e.g. DeltaTable.forName , DeltaTable.convertToDelta ).","title":"DeltaTable"},{"location":"DeltaTable/#iodeltatables-package","text":"DeltaTable belongs to io.delta.tables package. import io . delta . tables . DeltaTable","title":"io.delta.tables Package"},{"location":"DeltaTable/#deltalog","text":"deltaLog : DeltaLog deltaLog is a DeltaLog of the DeltaTableV2 .","title":" DeltaLog"},{"location":"DeltaTable/#utilities-static-methods","text":"","title":" Utilities (Static Methods)"},{"location":"DeltaTable/#converttodelta","text":"convertToDelta ( spark : SparkSession , identifier : String ): DeltaTable convertToDelta ( spark : SparkSession , identifier : String , partitionSchema : String ): DeltaTable convertToDelta ( spark : SparkSession , identifier : String , partitionSchema : StructType ): DeltaTable convertToDelta converts a parquet table to delta format (and makes the table available in Delta Lake). Note Refer to Demo: Converting Parquet Dataset Into Delta Format for a demo of DeltaTable.convertToDelta . Internally, convertToDelta requests the SparkSession for the SQL parser ( ParserInterface ) that is in turn requested to parse the given table identifier (to get a TableIdentifier ). Tip Read up on ParserInterface in The Internals of Spark SQL online book. In the end, convertToDelta uses the DeltaConvert utility to convert the parquet table to delta format and creates a DeltaTable .","title":" convertToDelta"},{"location":"DeltaTable/#forname","text":"forName ( sparkSession : SparkSession , tableName : String ): DeltaTable forName ( tableOrViewName : String ): DeltaTable forName uses ParserInterface (of the given SparkSession ) to parse the given table name. forName checks whether the given table name is of a Delta table and, if so, creates a DeltaTable with the following: Dataset that represents loading data from the specified table name (using SparkSession.table operator) DeltaLog of the specified table forName throws an AnalysisException when the given table name is for non-Delta table: [deltaTableIdentifier] is not a Delta table. forName is used internally when DeltaConvert utility is used to executeConvert .","title":" forName"},{"location":"DeltaTable/#forpath","text":"forPath ( sparkSession : SparkSession , path : String ): DeltaTable forPath ( path : String ): DeltaTable forPath creates a DeltaTable instance for data in the given directory ( path ) when the given directory is part of a delta table already (as the root or a child directory). assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession]) val tableId = \"/tmp/delta-table/users\" import io.delta.tables.DeltaTable assert(DeltaTable.isDeltaTable(tableId), s\"$tableId should be a Delta table\") val dt = DeltaTable.forPath(\"delta-table\") forPath throws an AnalysisException when the given path does not belong to a delta table: [deltaTableIdentifier] is not a Delta table. Internally, forPath creates a new DeltaTable with the following: Dataset that represents loading data from the specified path using delta data source DeltaLog for the (transaction log in) the specified path forPath is used internally in DeltaTable.convertToDelta (via DeltaConvert utility).","title":" forPath"},{"location":"DeltaTable/#isdeltatable","text":"isDeltaTable ( sparkSession : SparkSession , identifier : String ): Boolean isDeltaTable ( identifier : String ): Boolean isDeltaTable checks whether the provided identifier string is a file path that points to the root of a Delta table or one of the subdirectories. Internally, isDeltaTable simply relays to DeltaTableUtils.isDeltaTable utility.","title":" isDeltaTable"},{"location":"DeltaTable/#creating-instance","text":"DeltaTable takes the following to be created: Table Data ( Dataset[Row] ) DeltaTableV2 DeltaTable is created using DeltaTable.forPath or DeltaTable.forName utilities.","title":"Creating Instance"},{"location":"DeltaTable/#operators","text":"","title":"Operators"},{"location":"DeltaTable/#alias","text":"alias ( alias : String ): DeltaTable Applies an alias to the DeltaTable (equivalent to as )","title":" alias"},{"location":"DeltaTable/#as","text":"as ( alias : String ): DeltaTable Applies an alias to the DeltaTable","title":" as"},{"location":"DeltaTable/#delete","text":"delete (): Unit delete ( condition : Column ): Unit delete ( condition : String ): Unit Deletes data from the DeltaTable that matches the given condition delete executes DeleteFromTable command .","title":" delete"},{"location":"DeltaTable/#generate","text":"generate ( mode : String ): Unit Generates a manifest for the delta table generate executes the DeltaGenerateCommand with the table ID of the format delta.`path` (where the path is the data directory of the DeltaLog ) and the given mode.","title":" generate"},{"location":"DeltaTable/#history","text":"history (): DataFrame history ( limit : Int ): DataFrame Gets available commits ( history ) of the DeltaTable history requests the DeltaHistoryManager for history .","title":" history"},{"location":"DeltaTable/#merge","text":"merge ( source : DataFrame , condition : Column ): DeltaMergeBuilder merge ( source : DataFrame , condition : String ): DeltaMergeBuilder Creates a DeltaMergeBuilder","title":" merge"},{"location":"DeltaTable/#todf","text":"toDF : Dataset [ Row ] Returns the DataFrame representation of the DeltaTable","title":" toDF"},{"location":"DeltaTable/#update","text":"update ( condition : Column , set : Map [ String , Column ]): Unit update ( set : Map [ String , Column ]): Unit Updates data in the DeltaTable on the rows that match the given condition based on the rules defined by set update executes UpdateTable command .","title":" update"},{"location":"DeltaTable/#updateexpr","text":"updateExpr ( set : Map [ String , String ]): Unit updateExpr ( condition : String , set : Map [ String , String ]): Unit Updates data in the DeltaTable on the rows that match the given condition based on the rules defined by set update executes UpdateTable command .","title":" updateExpr"},{"location":"DeltaTable/#upgradetableprotocol","text":"upgradeTableProtocol ( readerVersion : Int , writerVersion : Int ): Unit Updates the protocol version of the table to leverage new features. Upgrading the reader version will prevent all clients that have an older version of Delta Lake from accessing this table. Upgrading the writer version will prevent older versions of Delta Lake to write to this table. The reader or writer version cannot be downgraded. Internally, upgradeTableProtocol creates a new Protocol (with the given versions) and requests the DeltaLog to upgradeProtocol . [SC-44271][DELTA] Introduce default protocol version for Delta tables upgradeTableProtocol was introduced in [SC-44271][DELTA] Introduce default protocol version for Delta tables commit.","title":" upgradeTableProtocol"},{"location":"DeltaTable/#vacuum","text":"vacuum (): DataFrame vacuum ( retentionHours : Double ): DataFrame Deletes files and directories (recursively) in the DeltaTable that are not needed by the table (and maintains older versions up to the given retention threshold). vacuum executes vacuum command .","title":" vacuum"},{"location":"DeltaTableIdentifier/","text":"DeltaTableIdentifier \u00b6 DeltaTableIdentifier is an identifier of a delta table by TableIdentifier or directory depending whether it is a catalog table or not (and living non-cataloged). Creating Instance \u00b6 DeltaTableIdentifier takes the following to be created: Directory (default: undefined) TableIdentifier (default: undefined) Creating DeltaTableIdentifier \u00b6 apply ( spark : SparkSession , identifier : TableIdentifier ): Option [ DeltaTableIdentifier ] apply creates a new DeltaTableIdentifier for the given TableIdentifier if the specified table identifier represents a Delta table or None . apply is used when: VacuumTableCommand , DeltaGenerateCommand , DescribeDeltaDetailCommand and DescribeDeltaHistoryCommand are executed Creating DeltaLog \u00b6 getDeltaLog ( spark : SparkSession ): DeltaLog getDeltaLog creates a DeltaLog (for the location ). Note getDeltaLog does not seem to be used. Location Path \u00b6 getPath ( spark : SparkSession ): Path getPath creates a Hadoop Path for the path if defined or requests SessionCatalog ( Spark SQL ) for the table metadata and uses the locationUri .","title":"DeltaTableIdentifier"},{"location":"DeltaTableIdentifier/#deltatableidentifier","text":"DeltaTableIdentifier is an identifier of a delta table by TableIdentifier or directory depending whether it is a catalog table or not (and living non-cataloged).","title":"DeltaTableIdentifier"},{"location":"DeltaTableIdentifier/#creating-instance","text":"DeltaTableIdentifier takes the following to be created: Directory (default: undefined) TableIdentifier (default: undefined)","title":"Creating Instance"},{"location":"DeltaTableIdentifier/#creating-deltatableidentifier","text":"apply ( spark : SparkSession , identifier : TableIdentifier ): Option [ DeltaTableIdentifier ] apply creates a new DeltaTableIdentifier for the given TableIdentifier if the specified table identifier represents a Delta table or None . apply is used when: VacuumTableCommand , DeltaGenerateCommand , DescribeDeltaDetailCommand and DescribeDeltaHistoryCommand are executed","title":" Creating DeltaTableIdentifier"},{"location":"DeltaTableIdentifier/#creating-deltalog","text":"getDeltaLog ( spark : SparkSession ): DeltaLog getDeltaLog creates a DeltaLog (for the location ). Note getDeltaLog does not seem to be used.","title":" Creating DeltaLog"},{"location":"DeltaTableIdentifier/#location-path","text":"getPath ( spark : SparkSession ): Path getPath creates a Hadoop Path for the path if defined or requests SessionCatalog ( Spark SQL ) for the table metadata and uses the locationUri .","title":" Location Path"},{"location":"DeltaTableOperations/","text":"DeltaTableOperations \u2014 Delta DML Operations \u00b6 DeltaTableOperations is an abstraction of management services for executing delete , generate , history , update , and vacuum operations ( commands ). DeltaTableOperations is assumed to be associated with a DeltaTable . Method Command DeltaTable Operator executeDelete DeltaDelete DeltaTable.delete executeGenerate DeltaGenerateCommand DeltaTable.generate executeHistory DeltaHistoryManager.getHistory DeltaTable.history executeUpdate UpdateTable ( Spark SQL ) DeltaTable.update and DeltaTable.updateExpr executeVacuum VacuumCommand.gc DeltaTable.vacuum Implementations \u00b6 DeltaTable","title":"DeltaTableOperations"},{"location":"DeltaTableOperations/#deltatableoperations-delta-dml-operations","text":"DeltaTableOperations is an abstraction of management services for executing delete , generate , history , update , and vacuum operations ( commands ). DeltaTableOperations is assumed to be associated with a DeltaTable . Method Command DeltaTable Operator executeDelete DeltaDelete DeltaTable.delete executeGenerate DeltaGenerateCommand DeltaTable.generate executeHistory DeltaHistoryManager.getHistory DeltaTable.history executeUpdate UpdateTable ( Spark SQL ) DeltaTable.update and DeltaTable.updateExpr executeVacuum VacuumCommand.gc DeltaTable.vacuum","title":"DeltaTableOperations &mdash; Delta DML Operations"},{"location":"DeltaTableOperations/#implementations","text":"DeltaTable","title":"Implementations"},{"location":"DeltaTableUtils/","text":"DeltaTableUtils Utility \u00b6 extractIfPathContainsTimeTravel \u00b6 extractIfPathContainsTimeTravel ( session : SparkSession , path : String ): ( String , Option [ DeltaTimeTravelSpec ]) extractIfPathContainsTimeTravel uses the internal spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled configuration property to find time travel patterns in the given path . extractIfPathContainsTimeTravel ...FIXME extractIfPathContainsTimeTravel is used when: DeltaDataSource is requested to sourceSchema and parsePathIdentifier findDeltaTableRoot \u00b6 findDeltaTableRoot ( spark : SparkSession , path : Path , options : Map [ String , String ] = Map . empty ): Option [ Path ] findDeltaTableRoot traverses the Hadoop DFS-compliant path upwards (to the root directory of the file system) until _delta_log or _samples directories are found, or the root directory is reached. For _delta_log or _samples directories, findDeltaTableRoot returns the parent directory (of _delta_log directory). findDeltaTableRoot is used when: DeltaTable.isDeltaTable utility is used VacuumTableCommand is executed DeltaTableUtils utility is used to isDeltaTable DeltaDataSource utility is used to parsePathIdentifier isPredicatePartitionColumnsOnly \u00b6 isPredicatePartitionColumnsOnly ( condition : Expression , partitionColumns : Seq [ String ], spark : SparkSession ): Boolean isPredicatePartitionColumnsOnly holds true when all of the references of the condition expression are among the partitionColumns . isPredicatePartitionColumnsOnly is used when: DeltaTableUtils is used to isPredicateMetadataOnly OptimisticTransactionImpl is requested for the filterFiles DeltaSourceSnapshot is requested for the partition and data filters isDeltaTable \u00b6 isDeltaTable ( table : CatalogTable ): Boolean isDeltaTable ( spark : SparkSession , path : Path ): Boolean isDeltaTable ( spark : SparkSession , tableName : TableIdentifier ): Boolean isDeltaTable ...FIXME isDeltaTable is used when: DeltaCatalog is requested to loadTable DeltaTable.forName , DeltaTable.forPath and DeltaTable.isDeltaTable utilities are used DeltaTableIdentifier utility is used to create a DeltaTableIdentifier from a TableIdentifier DeltaUnsupportedOperationsCheck is requested to fail resolveTimeTravelVersion \u00b6 resolveTimeTravelVersion ( conf : SQLConf , deltaLog : DeltaLog , tt : DeltaTimeTravelSpec ): ( Long , String ) resolveTimeTravelVersion ...FIXME resolveTimeTravelVersion is used when: DeltaLog is requested to create a relation (per partition filters and time travel) DeltaTableV2 is requested for a Snapshot splitMetadataAndDataPredicates \u00b6 splitMetadataAndDataPredicates ( condition : Expression , partitionColumns : Seq [ String ], spark : SparkSession ): ( Seq [ Expression ], Seq [ Expression ]) splitMetadataAndDataPredicates splits conjunctive ( and ) predicates in the given condition expression and partitions them into two collections based on the isPredicateMetadataOnly predicate (with the given partitionColumns ). splitMetadataAndDataPredicates is used when: PartitionFiltering is requested for filesForScan DeleteCommand is executed (with a delete condition) UpdateCommand is executed isPredicateMetadataOnly \u00b6 isPredicateMetadataOnly ( condition : Expression , partitionColumns : Seq [ String ], spark : SparkSession ): Boolean isPredicateMetadataOnly holds true when the following hold about the given condition : Is partition column only (given the partitionColumns ) Does not contain a subquery","title":"DeltaTableUtils"},{"location":"DeltaTableUtils/#deltatableutils-utility","text":"","title":"DeltaTableUtils Utility"},{"location":"DeltaTableUtils/#extractifpathcontainstimetravel","text":"extractIfPathContainsTimeTravel ( session : SparkSession , path : String ): ( String , Option [ DeltaTimeTravelSpec ]) extractIfPathContainsTimeTravel uses the internal spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled configuration property to find time travel patterns in the given path . extractIfPathContainsTimeTravel ...FIXME extractIfPathContainsTimeTravel is used when: DeltaDataSource is requested to sourceSchema and parsePathIdentifier","title":" extractIfPathContainsTimeTravel"},{"location":"DeltaTableUtils/#finddeltatableroot","text":"findDeltaTableRoot ( spark : SparkSession , path : Path , options : Map [ String , String ] = Map . empty ): Option [ Path ] findDeltaTableRoot traverses the Hadoop DFS-compliant path upwards (to the root directory of the file system) until _delta_log or _samples directories are found, or the root directory is reached. For _delta_log or _samples directories, findDeltaTableRoot returns the parent directory (of _delta_log directory). findDeltaTableRoot is used when: DeltaTable.isDeltaTable utility is used VacuumTableCommand is executed DeltaTableUtils utility is used to isDeltaTable DeltaDataSource utility is used to parsePathIdentifier","title":" findDeltaTableRoot"},{"location":"DeltaTableUtils/#ispredicatepartitioncolumnsonly","text":"isPredicatePartitionColumnsOnly ( condition : Expression , partitionColumns : Seq [ String ], spark : SparkSession ): Boolean isPredicatePartitionColumnsOnly holds true when all of the references of the condition expression are among the partitionColumns . isPredicatePartitionColumnsOnly is used when: DeltaTableUtils is used to isPredicateMetadataOnly OptimisticTransactionImpl is requested for the filterFiles DeltaSourceSnapshot is requested for the partition and data filters","title":" isPredicatePartitionColumnsOnly"},{"location":"DeltaTableUtils/#isdeltatable","text":"isDeltaTable ( table : CatalogTable ): Boolean isDeltaTable ( spark : SparkSession , path : Path ): Boolean isDeltaTable ( spark : SparkSession , tableName : TableIdentifier ): Boolean isDeltaTable ...FIXME isDeltaTable is used when: DeltaCatalog is requested to loadTable DeltaTable.forName , DeltaTable.forPath and DeltaTable.isDeltaTable utilities are used DeltaTableIdentifier utility is used to create a DeltaTableIdentifier from a TableIdentifier DeltaUnsupportedOperationsCheck is requested to fail","title":" isDeltaTable"},{"location":"DeltaTableUtils/#resolvetimetravelversion","text":"resolveTimeTravelVersion ( conf : SQLConf , deltaLog : DeltaLog , tt : DeltaTimeTravelSpec ): ( Long , String ) resolveTimeTravelVersion ...FIXME resolveTimeTravelVersion is used when: DeltaLog is requested to create a relation (per partition filters and time travel) DeltaTableV2 is requested for a Snapshot","title":" resolveTimeTravelVersion"},{"location":"DeltaTableUtils/#splitmetadataanddatapredicates","text":"splitMetadataAndDataPredicates ( condition : Expression , partitionColumns : Seq [ String ], spark : SparkSession ): ( Seq [ Expression ], Seq [ Expression ]) splitMetadataAndDataPredicates splits conjunctive ( and ) predicates in the given condition expression and partitions them into two collections based on the isPredicateMetadataOnly predicate (with the given partitionColumns ). splitMetadataAndDataPredicates is used when: PartitionFiltering is requested for filesForScan DeleteCommand is executed (with a delete condition) UpdateCommand is executed","title":" splitMetadataAndDataPredicates"},{"location":"DeltaTableUtils/#ispredicatemetadataonly","text":"isPredicateMetadataOnly ( condition : Expression , partitionColumns : Seq [ String ], spark : SparkSession ): Boolean isPredicateMetadataOnly holds true when the following hold about the given condition : Is partition column only (given the partitionColumns ) Does not contain a subquery","title":" isPredicateMetadataOnly"},{"location":"DeltaTableV2/","text":"DeltaTableV2 \u00b6 DeltaTableV2 is a logical representation of a writable Delta table. In Spark SQL 3's terms, DeltaTableV2 is a Table ( Spark SQL ) that SupportsWrite ( Spark SQL ). Creating Instance \u00b6 DeltaTableV2 takes the following to be created: SparkSession Hadoop Path Optional Catalog Metadata ( Option[CatalogTable] ) Optional Table ID ( Option[String] ) Optional DeltaTimeTravelSpec DeltaTableV2 is created when: DeltaCatalog is requested to load a table DeltaDataSource is requested to load a table or create a table relation DeltaTimeTravelSpec \u00b6 DeltaTableV2 may be given a DeltaTimeTravelSpec when created . DeltaTimeTravelSpec is assumed not to be defined by default ( None ). DeltaTableV2 is given a DeltaTimeTravelSpec when: DeltaDataSource is requested for a BaseRelation DeltaTimeTravelSpec is used for timeTravelSpec . Snapshot \u00b6 snapshot : Snapshot DeltaTableV2 has a Snapshot . In other words, DeltaTableV2 represents a Delta table at a specific version. Scala lazy value snapshot is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged. DeltaTableV2 uses the DeltaLog to load it at a given version (based on the optional timeTravelSpec ) or update to the latest version . snapshot is used when DeltaTableV2 is requested for the schema , partitioning and properties . DeltaTimeTravelSpec \u00b6 timeTravelSpec : Option [ DeltaTimeTravelSpec ] DeltaTableV2 may have a DeltaTimeTravelSpec specified that is either given or extracted from the path (for timeTravelByPath ). timeTravelSpec throws an AnalysisException when timeTravelOpt and timeTravelByPath are both defined: Cannot specify time travel in multiple formats. timeTravelSpec is used when DeltaTableV2 is requested for a Snapshot and BaseRelation . DeltaTimeTravelSpec by Path \u00b6 timeTravelByPath : Option [ DeltaTimeTravelSpec ] Scala lazy value timeTravelByPath is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged. timeTravelByPath is undefined when CatalogTable is defined. With no CatalogTable defined, DeltaTableV2 parses the given Path for the timeTravelByPath (that resolvePath under the covers). Converting to Insertable HadoopFsRelation \u00b6 toBaseRelation : BaseRelation toBaseRelation verifyAndCreatePartitionFilters for the Path , the current Snapshot and partitionFilters . In the end, toBaseRelation requests the DeltaLog for an insertable HadoopFsRelation . toBaseRelation is used when: DeltaDataSource is requested to createRelation DeltaRelation utility is used to fromV2Relation","title":"DeltaTableV2"},{"location":"DeltaTableV2/#deltatablev2","text":"DeltaTableV2 is a logical representation of a writable Delta table. In Spark SQL 3's terms, DeltaTableV2 is a Table ( Spark SQL ) that SupportsWrite ( Spark SQL ).","title":"DeltaTableV2"},{"location":"DeltaTableV2/#creating-instance","text":"DeltaTableV2 takes the following to be created: SparkSession Hadoop Path Optional Catalog Metadata ( Option[CatalogTable] ) Optional Table ID ( Option[String] ) Optional DeltaTimeTravelSpec DeltaTableV2 is created when: DeltaCatalog is requested to load a table DeltaDataSource is requested to load a table or create a table relation","title":"Creating Instance"},{"location":"DeltaTableV2/#deltatimetravelspec","text":"DeltaTableV2 may be given a DeltaTimeTravelSpec when created . DeltaTimeTravelSpec is assumed not to be defined by default ( None ). DeltaTableV2 is given a DeltaTimeTravelSpec when: DeltaDataSource is requested for a BaseRelation DeltaTimeTravelSpec is used for timeTravelSpec .","title":" DeltaTimeTravelSpec"},{"location":"DeltaTableV2/#snapshot","text":"snapshot : Snapshot DeltaTableV2 has a Snapshot . In other words, DeltaTableV2 represents a Delta table at a specific version. Scala lazy value snapshot is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged. DeltaTableV2 uses the DeltaLog to load it at a given version (based on the optional timeTravelSpec ) or update to the latest version . snapshot is used when DeltaTableV2 is requested for the schema , partitioning and properties .","title":" Snapshot"},{"location":"DeltaTableV2/#deltatimetravelspec_1","text":"timeTravelSpec : Option [ DeltaTimeTravelSpec ] DeltaTableV2 may have a DeltaTimeTravelSpec specified that is either given or extracted from the path (for timeTravelByPath ). timeTravelSpec throws an AnalysisException when timeTravelOpt and timeTravelByPath are both defined: Cannot specify time travel in multiple formats. timeTravelSpec is used when DeltaTableV2 is requested for a Snapshot and BaseRelation .","title":" DeltaTimeTravelSpec"},{"location":"DeltaTableV2/#deltatimetravelspec-by-path","text":"timeTravelByPath : Option [ DeltaTimeTravelSpec ] Scala lazy value timeTravelByPath is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged. timeTravelByPath is undefined when CatalogTable is defined. With no CatalogTable defined, DeltaTableV2 parses the given Path for the timeTravelByPath (that resolvePath under the covers).","title":" DeltaTimeTravelSpec by Path"},{"location":"DeltaTableV2/#converting-to-insertable-hadoopfsrelation","text":"toBaseRelation : BaseRelation toBaseRelation verifyAndCreatePartitionFilters for the Path , the current Snapshot and partitionFilters . In the end, toBaseRelation requests the DeltaLog for an insertable HadoopFsRelation . toBaseRelation is used when: DeltaDataSource is requested to createRelation DeltaRelation utility is used to fromV2Relation","title":" Converting to Insertable HadoopFsRelation"},{"location":"DeltaTimeTravelSpec/","text":"DeltaTimeTravelSpec \u00b6 Time Travel Patterns \u00b6 DeltaTimeTravelSpec defines regular expressions for timestamp- and version-based time travel identifiers: Version URI: (path)@[vV](some numbers) Timestamp URI: (path)@(yyyyMMddHHmmssSSS) Creating Instance \u00b6 DeltaTimeTravelSpec takes the following to be created: Timestamp Version creationSource identifier DeltaTimeTravelSpec asserts that either version or timestamp is provided (and throws an AssertionError ). DeltaTimeTravelSpec is created when: DeltaTimeTravelSpec utility is used to resolve a path DeltaDataSource utility is used to getTimeTravelVersion Resolving Path \u00b6 resolvePath ( conf : SQLConf , identifier : String ): ( DeltaTimeTravelSpec , String ) resolvePath ...FIXME resolvePath is used when DeltaTableUtils utility is used to extractIfPathContainsTimeTravel . getTimestamp \u00b6 getTimestamp ( timeZone : String ): Timestamp getTimestamp ...FIXME getTimestamp is used when DeltaTableUtils utility is used to resolveTimeTravelVersion . isApplicable \u00b6 isApplicable ( conf : SQLConf , identifier : String ): Boolean isApplicable is true when all of the following hold: spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled is true identifierContainsTimeTravel is true isApplicable is used when DeltaTableUtils utility is used to extractIfPathContainsTimeTravel . identifierContainsTimeTravel \u00b6 identifierContainsTimeTravel ( identifier : String ): Boolean identifierContainsTimeTravel is true when the given identifier is either timestamp or version time travel pattern.","title":"DeltaTimeTravelSpec"},{"location":"DeltaTimeTravelSpec/#deltatimetravelspec","text":"","title":"DeltaTimeTravelSpec"},{"location":"DeltaTimeTravelSpec/#time-travel-patterns","text":"DeltaTimeTravelSpec defines regular expressions for timestamp- and version-based time travel identifiers: Version URI: (path)@[vV](some numbers) Timestamp URI: (path)@(yyyyMMddHHmmssSSS)","title":" Time Travel Patterns"},{"location":"DeltaTimeTravelSpec/#creating-instance","text":"DeltaTimeTravelSpec takes the following to be created: Timestamp Version creationSource identifier DeltaTimeTravelSpec asserts that either version or timestamp is provided (and throws an AssertionError ). DeltaTimeTravelSpec is created when: DeltaTimeTravelSpec utility is used to resolve a path DeltaDataSource utility is used to getTimeTravelVersion","title":"Creating Instance"},{"location":"DeltaTimeTravelSpec/#resolving-path","text":"resolvePath ( conf : SQLConf , identifier : String ): ( DeltaTimeTravelSpec , String ) resolvePath ...FIXME resolvePath is used when DeltaTableUtils utility is used to extractIfPathContainsTimeTravel .","title":" Resolving Path"},{"location":"DeltaTimeTravelSpec/#gettimestamp","text":"getTimestamp ( timeZone : String ): Timestamp getTimestamp ...FIXME getTimestamp is used when DeltaTableUtils utility is used to resolveTimeTravelVersion .","title":" getTimestamp"},{"location":"DeltaTimeTravelSpec/#isapplicable","text":"isApplicable ( conf : SQLConf , identifier : String ): Boolean isApplicable is true when all of the following hold: spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled is true identifierContainsTimeTravel is true isApplicable is used when DeltaTableUtils utility is used to extractIfPathContainsTimeTravel .","title":" isApplicable"},{"location":"DeltaTimeTravelSpec/#identifiercontainstimetravel","text":"identifierContainsTimeTravel ( identifier : String ): Boolean identifierContainsTimeTravel is true when the given identifier is either timestamp or version time travel pattern.","title":" identifierContainsTimeTravel"},{"location":"DeltaUnsupportedOperationsCheck/","text":"DeltaUnsupportedOperationsCheck \u00b6 DeltaUnsupportedOperationsCheck is a logical check rule that adds helpful error messages when Delta is being used with unsupported Hive operations or if an unsupported operation is executed.","title":"DeltaUnsupportedOperationsCheck"},{"location":"DeltaUnsupportedOperationsCheck/#deltaunsupportedoperationscheck","text":"DeltaUnsupportedOperationsCheck is a logical check rule that adds helpful error messages when Delta is being used with unsupported Hive operations or if an unsupported operation is executed.","title":"DeltaUnsupportedOperationsCheck"},{"location":"DeltaWriteOptions/","text":"DeltaWriteOptions \u00b6 DeltaWriteOptions is a type-safe abstraction of the write-related DeltaOptions . DeltaWriteOptions is DeltaWriteOptionsImpl and DeltaOptionParser . replaceWhere \u00b6 replaceWhere : Option [ String ] replaceWhere is the value of replaceWhere option. replaceWhere is used when: WriteIntoDelta command is created and executed CreateDeltaTableCommand command is requested for a Delta Operation (for history purposes) userMetadata \u00b6 userMetadata : Option [ String ] userMetadata is the value of userMetadata option. optimizeWrite \u00b6 optimizeWrite : Option [ Boolean ] optimizeWrite is the value of optimizeWrite option.","title":"DeltaWriteOptions"},{"location":"DeltaWriteOptions/#deltawriteoptions","text":"DeltaWriteOptions is a type-safe abstraction of the write-related DeltaOptions . DeltaWriteOptions is DeltaWriteOptionsImpl and DeltaOptionParser .","title":"DeltaWriteOptions"},{"location":"DeltaWriteOptions/#replacewhere","text":"replaceWhere : Option [ String ] replaceWhere is the value of replaceWhere option. replaceWhere is used when: WriteIntoDelta command is created and executed CreateDeltaTableCommand command is requested for a Delta Operation (for history purposes)","title":" replaceWhere"},{"location":"DeltaWriteOptions/#usermetadata","text":"userMetadata : Option [ String ] userMetadata is the value of userMetadata option.","title":" userMetadata"},{"location":"DeltaWriteOptions/#optimizewrite","text":"optimizeWrite : Option [ Boolean ] optimizeWrite is the value of optimizeWrite option.","title":" optimizeWrite"},{"location":"DeltaWriteOptionsImpl/","text":"DeltaWriteOptionsImpl \u00b6 DeltaWriteOptionsImpl is a DeltaOptionParser . canMergeSchema \u00b6 canMergeSchema : Boolean canMergeSchema is the value of mergeSchema option (if defined) or spark.databricks.delta.schema.autoMerge.enabled configuration property. canMergeSchema is used when: WriteIntoDelta is created DeltaSink is created canOverwriteSchema \u00b6 canOverwriteSchema : Boolean canOverwriteSchema is the value of overwriteSchema option. canOverwriteSchema is used when: CreateDeltaTableCommand is requested to replaceMetadataIfNecessary WriteIntoDelta is created DeltaSink is created rearrangeOnly \u00b6 rearrangeOnly : Boolean rearrangeOnly is the value of dataChange option. rearrangeOnly is used when: WriteIntoDelta is requested to write","title":"DeltaWriteOptionsImpl"},{"location":"DeltaWriteOptionsImpl/#deltawriteoptionsimpl","text":"DeltaWriteOptionsImpl is a DeltaOptionParser .","title":"DeltaWriteOptionsImpl"},{"location":"DeltaWriteOptionsImpl/#canmergeschema","text":"canMergeSchema : Boolean canMergeSchema is the value of mergeSchema option (if defined) or spark.databricks.delta.schema.autoMerge.enabled configuration property. canMergeSchema is used when: WriteIntoDelta is created DeltaSink is created","title":" canMergeSchema"},{"location":"DeltaWriteOptionsImpl/#canoverwriteschema","text":"canOverwriteSchema : Boolean canOverwriteSchema is the value of overwriteSchema option. canOverwriteSchema is used when: CreateDeltaTableCommand is requested to replaceMetadataIfNecessary WriteIntoDelta is created DeltaSink is created","title":" canOverwriteSchema"},{"location":"DeltaWriteOptionsImpl/#rearrangeonly","text":"rearrangeOnly : Boolean rearrangeOnly is the value of dataChange option. rearrangeOnly is used when: WriteIntoDelta is requested to write","title":" rearrangeOnly"},{"location":"FileAction/","text":"FileAction \u00b6 FileAction is an extension of the Action abstraction for actions that can add or remove files. Contract \u00b6 Path \u00b6 path : String dataChange \u00b6 dataChange : Boolean Controls the transaction isolation level for committing a transaction Isolation Level Description SnapshotIsolation No data changes ( dataChange is false for all FileAction s to be committed) Serializable There can be no RemoveFile s with dataChange enabled for appendOnly unmodifiable tables (or an UnsupportedOperationException is thrown ). dataChange Value When false InMemoryLogReplay is requested to replay a version true ConvertToDeltaCommand is executed (and requested to create an AddFile with the flag turned on) Opposite of dataChange option WriteIntoDelta is requested to write (with dataChange option turned off for rearrange-only writes) dataChange is used when: OptimisticTransactionImpl is requested to commit (and determines the isolation level), prepareCommit , attempt a commit (for bytesNew statistics) DeltaSource is requested to getChanges (and verifyStreamHygieneAndFilterAddFiles ) Implementations \u00b6 AddCDCFile AddFile RemoveFile Sealed Trait FileAction is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).","title":"FileAction"},{"location":"FileAction/#fileaction","text":"FileAction is an extension of the Action abstraction for actions that can add or remove files.","title":"FileAction"},{"location":"FileAction/#contract","text":"","title":"Contract"},{"location":"FileAction/#path","text":"path : String","title":" Path"},{"location":"FileAction/#datachange","text":"dataChange : Boolean Controls the transaction isolation level for committing a transaction Isolation Level Description SnapshotIsolation No data changes ( dataChange is false for all FileAction s to be committed) Serializable There can be no RemoveFile s with dataChange enabled for appendOnly unmodifiable tables (or an UnsupportedOperationException is thrown ). dataChange Value When false InMemoryLogReplay is requested to replay a version true ConvertToDeltaCommand is executed (and requested to create an AddFile with the flag turned on) Opposite of dataChange option WriteIntoDelta is requested to write (with dataChange option turned off for rearrange-only writes) dataChange is used when: OptimisticTransactionImpl is requested to commit (and determines the isolation level), prepareCommit , attempt a commit (for bytesNew statistics) DeltaSource is requested to getChanges (and verifyStreamHygieneAndFilterAddFiles )","title":" dataChange"},{"location":"FileAction/#implementations","text":"AddCDCFile AddFile RemoveFile Sealed Trait FileAction is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).","title":"Implementations"},{"location":"FileNames/","text":"= FileNames Utility [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | checkpointPrefix a| [[checkpointPrefix]] Creates a Hadoop Path for a file name with a given version : [version][%020d].checkpoint E.g. 00000000000000000005.checkpoint | isCheckpointFile a| [[isCheckpointFile]] | isDeltaFile a| [[isDeltaFile]] |=== == [[deltaFile]] Creating Hadoop Path To Delta File -- deltaFile Utility [source, scala] \u00b6 deltaFile( path: Path, version: Long): Path deltaFile creates a Hadoop Path to a file of the format [version][%020d].json in the path directory, e.g. 00000000000000000001.json . NOTE: deltaFile is used when...FIXME","title":"FileNames"},{"location":"FileNames/#source-scala","text":"deltaFile( path: Path, version: Long): Path deltaFile creates a Hadoop Path to a file of the format [version][%020d].json in the path directory, e.g. 00000000000000000001.json . NOTE: deltaFile is used when...FIXME","title":"[source, scala]"},{"location":"HDFSLogStore/","text":"= HDFSLogStore HDFSLogStore is...FIXME","title":"HDFSLogStore"},{"location":"HadoopFileSystemLogStore/","text":"HadoopFileSystemLogStore \u00b6 HadoopFileSystemLogStore is an extension of the LogStore abstraction for Hadoop DFS-based log stores . Implementations \u00b6 AzureLogStore HDFSLogStore LocalLogStore S3SingleDriverLogStore Creating Instance \u00b6 HadoopFileSystemLogStore takes the following to be created: SparkConf ( Apache Spark ) Hadoop Configuration Abstract Class HadoopFileSystemLogStore is an abstract class and cannot be created directly. It is created indirectly for the concrete HadoopFileSystemLogStores .","title":"HadoopFileSystemLogStore"},{"location":"HadoopFileSystemLogStore/#hadoopfilesystemlogstore","text":"HadoopFileSystemLogStore is an extension of the LogStore abstraction for Hadoop DFS-based log stores .","title":"HadoopFileSystemLogStore"},{"location":"HadoopFileSystemLogStore/#implementations","text":"AzureLogStore HDFSLogStore LocalLogStore S3SingleDriverLogStore","title":"Implementations"},{"location":"HadoopFileSystemLogStore/#creating-instance","text":"HadoopFileSystemLogStore takes the following to be created: SparkConf ( Apache Spark ) Hadoop Configuration Abstract Class HadoopFileSystemLogStore is an abstract class and cannot be created directly. It is created indirectly for the concrete HadoopFileSystemLogStores .","title":"Creating Instance"},{"location":"ImplicitMetadataOperation/","text":"ImplicitMetadataOperation \u00b6 ImplicitMetadataOperation is an abstraction of operations that can update metadata of a delta table (while writing out a new data). ImplicitMetadataOperation operations can update schema by merging and overwriting schema. Contract \u00b6 canMergeSchema \u00b6 canMergeSchema : Boolean Used when: MergeIntoCommand command is executed ImplicitMetadataOperation is requested to updateMetadata canOverwriteSchema \u00b6 canOverwriteSchema : Boolean Used when: ImplicitMetadataOperation is requested to updateMetadata Implementations \u00b6 DeltaSink MergeIntoCommand WriteIntoDelta Updating Metadata \u00b6 updateMetadata ( // (1) txn : OptimisticTransaction , data : Dataset [ _ ], partitionColumns : Seq [ String ], configuration : Map [ String , String ], isOverwriteMode : Boolean , rearrangeOnly : Boolean = false ): Unit updateMetadata ( spark : SparkSession , txn : OptimisticTransaction , schema : StructType , partitionColumns : Seq [ String ], configuration : Map [ String , String ], isOverwriteMode : Boolean , rearrangeOnly : Boolean ): Unit Uses the SparkSession and the schema of the given Dataset and assumes the rearrangeOnly to be off updateMetadata ...FIXME updateMetadata is used when: MergeIntoCommand and WriteIntoDelta commands are executed DeltaSink is requested to add a streaming micro-batch Normalizing Partition Columns \u00b6 normalizePartitionColumns ( spark : SparkSession , partitionCols : Seq [ String ], schema : StructType ): Seq [ String ] normalizePartitionColumns ...FIXME","title":"ImplicitMetadataOperation"},{"location":"ImplicitMetadataOperation/#implicitmetadataoperation","text":"ImplicitMetadataOperation is an abstraction of operations that can update metadata of a delta table (while writing out a new data). ImplicitMetadataOperation operations can update schema by merging and overwriting schema.","title":"ImplicitMetadataOperation"},{"location":"ImplicitMetadataOperation/#contract","text":"","title":"Contract"},{"location":"ImplicitMetadataOperation/#canmergeschema","text":"canMergeSchema : Boolean Used when: MergeIntoCommand command is executed ImplicitMetadataOperation is requested to updateMetadata","title":" canMergeSchema"},{"location":"ImplicitMetadataOperation/#canoverwriteschema","text":"canOverwriteSchema : Boolean Used when: ImplicitMetadataOperation is requested to updateMetadata","title":" canOverwriteSchema"},{"location":"ImplicitMetadataOperation/#implementations","text":"DeltaSink MergeIntoCommand WriteIntoDelta","title":"Implementations"},{"location":"ImplicitMetadataOperation/#updating-metadata","text":"updateMetadata ( // (1) txn : OptimisticTransaction , data : Dataset [ _ ], partitionColumns : Seq [ String ], configuration : Map [ String , String ], isOverwriteMode : Boolean , rearrangeOnly : Boolean = false ): Unit updateMetadata ( spark : SparkSession , txn : OptimisticTransaction , schema : StructType , partitionColumns : Seq [ String ], configuration : Map [ String , String ], isOverwriteMode : Boolean , rearrangeOnly : Boolean ): Unit Uses the SparkSession and the schema of the given Dataset and assumes the rearrangeOnly to be off updateMetadata ...FIXME updateMetadata is used when: MergeIntoCommand and WriteIntoDelta commands are executed DeltaSink is requested to add a streaming micro-batch","title":" Updating Metadata"},{"location":"ImplicitMetadataOperation/#normalizing-partition-columns","text":"normalizePartitionColumns ( spark : SparkSession , partitionCols : Seq [ String ], schema : StructType ): Seq [ String ] normalizePartitionColumns ...FIXME","title":" Normalizing Partition Columns"},{"location":"InMemoryLogReplay/","text":"InMemoryLogReplay \u00b6 InMemoryLogReplay is used at the very last phase of state reconstruction (of a cached delta state ). InMemoryLogReplay handles a single partition of the state reconstruction dataset (based on the spark.databricks.delta.snapshotPartitions configuration property). Creating Instance \u00b6 InMemoryLogReplay takes the following to be created: minFileRetentionTimestamp ( Snapshot.minFileRetentionTimestamp ) InMemoryLogReplay is created when: Snapshot is requested for state reconstruction Lifecycle \u00b6 The lifecycle of InMemoryLogReplay is as follows: Created (with Snapshot.minFileRetentionTimestamp ) Append all SingleAction s of a partition (based on the spark.databricks.delta.snapshotPartitions configuration property) Checkpoint Replaying Version \u00b6 append ( version : Long , actions : Iterator [ Action ]): Unit append sets the currentVersion to the given version . append adds the given actions to their respective registries. Action Registry SetTransaction transactions by appId Metadata currentMetaData Protocol currentProtocolVersion AddFile 1. activeFiles by path and with dataChange flag disabled 2. Removes the path from tombstones (so there's only one FileAction for a path) RemoveFile 1. Removes the path from activeFiles (so there's only one FileAction for a path) 2. tombstones by path and with dataChange flag disabled CommitInfo Ignored AddCDCFile Ignored append throws an AssertionError when the currentVersion is -1 or one before the given version : Attempted to replay version [version], but state is at [currentVersion] Current State of Delta Table \u00b6 checkpoint : Iterator [ Action ] checkpoint returns an Iterator ( Scala ) of Action s in the following order: currentProtocolVersion if defined (non- null ) currentMetaData if defined (non- null ) SetTransaction s AddFile s and RemoveFile s sorted by path (lexicographically) getTombstones \u00b6 getTombstones : Iterable [ FileAction ] getTombstones uses the tombstones internal registry for RemoveFile s with deletionTimestamp after ( greater than ) the minFileRetentionTimestamp .","title":"InMemoryLogReplay"},{"location":"InMemoryLogReplay/#inmemorylogreplay","text":"InMemoryLogReplay is used at the very last phase of state reconstruction (of a cached delta state ). InMemoryLogReplay handles a single partition of the state reconstruction dataset (based on the spark.databricks.delta.snapshotPartitions configuration property).","title":"InMemoryLogReplay"},{"location":"InMemoryLogReplay/#creating-instance","text":"InMemoryLogReplay takes the following to be created: minFileRetentionTimestamp ( Snapshot.minFileRetentionTimestamp ) InMemoryLogReplay is created when: Snapshot is requested for state reconstruction","title":"Creating Instance"},{"location":"InMemoryLogReplay/#lifecycle","text":"The lifecycle of InMemoryLogReplay is as follows: Created (with Snapshot.minFileRetentionTimestamp ) Append all SingleAction s of a partition (based on the spark.databricks.delta.snapshotPartitions configuration property) Checkpoint","title":"Lifecycle"},{"location":"InMemoryLogReplay/#replaying-version","text":"append ( version : Long , actions : Iterator [ Action ]): Unit append sets the currentVersion to the given version . append adds the given actions to their respective registries. Action Registry SetTransaction transactions by appId Metadata currentMetaData Protocol currentProtocolVersion AddFile 1. activeFiles by path and with dataChange flag disabled 2. Removes the path from tombstones (so there's only one FileAction for a path) RemoveFile 1. Removes the path from activeFiles (so there's only one FileAction for a path) 2. tombstones by path and with dataChange flag disabled CommitInfo Ignored AddCDCFile Ignored append throws an AssertionError when the currentVersion is -1 or one before the given version : Attempted to replay version [version], but state is at [currentVersion]","title":" Replaying Version"},{"location":"InMemoryLogReplay/#current-state-of-delta-table","text":"checkpoint : Iterator [ Action ] checkpoint returns an Iterator ( Scala ) of Action s in the following order: currentProtocolVersion if defined (non- null ) currentMetaData if defined (non- null ) SetTransaction s AddFile s and RemoveFile s sorted by path (lexicographically)","title":" Current State of Delta Table"},{"location":"InMemoryLogReplay/#gettombstones","text":"getTombstones : Iterable [ FileAction ] getTombstones uses the tombstones internal registry for RemoveFile s with deletionTimestamp after ( greater than ) the minFileRetentionTimestamp .","title":" getTombstones"},{"location":"Invariants/","text":"Invariants \u00b6 Invariants is...FIXME","title":"Invariants"},{"location":"Invariants/#invariants","text":"Invariants is...FIXME","title":"Invariants"},{"location":"IsolationLevel/","text":"IsolationLevel \u00b6 IsolationLevel is an abstraction of consistency guarantees to be provided when OptimisticTransaction is committed . Implementations \u00b6 Sealed Trait IsolationLevel is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file). Serializable \u00b6 Serializable is the most strict consistency guarantee . Serializable is the isolation level for data-changing commit s. For Serializable commits, OptimisticTransactionImpl adds extra addedFilesToCheckForConflicts ( changedData or blindAppend AddFile s) when checkForConflicts . Serializable is a valid table isolation level . For operations that do not modify data in a table, there is no difference between Serializable and SnapshotIsolation . Serializable is used for ConvertToDeltaCommand command. SnapshotIsolation \u00b6 SnapshotIsolation is the least strict consistency guarantee . SnapshotIsolation is the isolation level for commit s with no data changed . For SnapshotIsolation commits, OptimisticTransactionImpl adds no extra addedFilesToCheckForConflicts when checkForConflicts . For operations that do not modify data in a table, there is no difference between Serializable and SnapshotIsolation . WriteSerializable \u00b6 The default IsolationLevel For WriteSerializable commits, OptimisticTransactionImpl adds extra addedFilesToCheckForConflicts ( changedData AddFile s) when checkForConflicts . Blind appends don't ( seem to ) conflict with WriteSerializable commits. WriteSerializable a valid table isolation level . Consistency Guarantee Strictness Ordering \u00b6 The following are all the isolation levels in descending order of guarantees provided: Serializable (the most strict level) WriteSerializable SnapshotIsolation (the least strict one) Valid Table Isolation Levels \u00b6 Not Used in OSS Delta Lake This feature is not used. The following are the valid isolation levels that can be specified as the table isolation level: Serializable WriteSerializable","title":"IsolationLevel"},{"location":"IsolationLevel/#isolationlevel","text":"IsolationLevel is an abstraction of consistency guarantees to be provided when OptimisticTransaction is committed .","title":"IsolationLevel"},{"location":"IsolationLevel/#implementations","text":"Sealed Trait IsolationLevel is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).","title":"Implementations"},{"location":"IsolationLevel/#serializable","text":"Serializable is the most strict consistency guarantee . Serializable is the isolation level for data-changing commit s. For Serializable commits, OptimisticTransactionImpl adds extra addedFilesToCheckForConflicts ( changedData or blindAppend AddFile s) when checkForConflicts . Serializable is a valid table isolation level . For operations that do not modify data in a table, there is no difference between Serializable and SnapshotIsolation . Serializable is used for ConvertToDeltaCommand command.","title":" Serializable"},{"location":"IsolationLevel/#snapshotisolation","text":"SnapshotIsolation is the least strict consistency guarantee . SnapshotIsolation is the isolation level for commit s with no data changed . For SnapshotIsolation commits, OptimisticTransactionImpl adds no extra addedFilesToCheckForConflicts when checkForConflicts . For operations that do not modify data in a table, there is no difference between Serializable and SnapshotIsolation .","title":" SnapshotIsolation"},{"location":"IsolationLevel/#writeserializable","text":"The default IsolationLevel For WriteSerializable commits, OptimisticTransactionImpl adds extra addedFilesToCheckForConflicts ( changedData AddFile s) when checkForConflicts . Blind appends don't ( seem to ) conflict with WriteSerializable commits. WriteSerializable a valid table isolation level .","title":" WriteSerializable"},{"location":"IsolationLevel/#consistency-guarantee-strictness-ordering","text":"The following are all the isolation levels in descending order of guarantees provided: Serializable (the most strict level) WriteSerializable SnapshotIsolation (the least strict one)","title":" Consistency Guarantee Strictness Ordering"},{"location":"IsolationLevel/#valid-table-isolation-levels","text":"Not Used in OSS Delta Lake This feature is not used. The following are the valid isolation levels that can be specified as the table isolation level: Serializable WriteSerializable","title":" Valid Table Isolation Levels"},{"location":"LogStore/","text":"LogStore \u00b6 LogStore is an abstraction of transaction log stores (to read and write physical log files and checkpoints). LogStore is created using LogStoreProvider based on spark.delta.logStore.class configuration property. Contract \u00b6 invalidateCache \u00b6 invalidateCache (): Unit isPartialWriteVisible \u00b6 isPartialWriteVisible ( path : Path ): Boolean Default: true Used when: Checkpoints utility is used to writeCheckpoint listFrom \u00b6 listFrom ( path : Path ): Iterator [ FileStatus ] listFrom ( path : String ): Iterator [ FileStatus ] Used when: Checkpoints is requested to findLastCompleteCheckpoint DeltaHistoryManager is requested to getEarliestDeltaFile , getEarliestReproducibleCommit and getCommits DeltaLog is requested to getChanges MetadataCleanup is requested to listExpiredDeltaLogs SnapshotManagement is requested to listFrom DeltaFileOperations utility is used to listUsingLogStore read \u00b6 read ( path : Path ): Seq [ String ] read ( path : String ): Seq [ String ] Used when: Checkpoints is requested to loadMetadataFromFile ReadChecksum is requested to readChecksum DeltaLog is requested to getChanges OptimisticTransactionImpl is requested to checkForConflicts LogStore is requested to readAsIterator write \u00b6 write ( path : Path , actions : Iterator [ String ], overwrite : Boolean = false ): Unit write ( path : String , actions : Iterator [ String ]): Unit Used when: Checkpoints is requested to checkpoint OptimisticTransactionImpl is requested to doCommit DeltaCommand is requested to commitLarge GenerateSymlinkManifestImpl is requested to writeManifestFiles Implementations \u00b6 HadoopFileSystemLogStore Creating LogStore \u00b6 apply ( sc : SparkContext ): LogStore apply ( sparkConf : SparkConf , hadoopConf : Configuration ): LogStore apply creates a LogStore . apply is used when: GenerateSymlinkManifestImpl is requested to writeManifestFiles and writeSingleManifestFile DeltaHistoryManager is requested to getHistory and getActiveCommitAtTime DeltaFileOperations is requested to recursiveListDirs and localListDirs","title":"LogStore"},{"location":"LogStore/#logstore","text":"LogStore is an abstraction of transaction log stores (to read and write physical log files and checkpoints). LogStore is created using LogStoreProvider based on spark.delta.logStore.class configuration property.","title":"LogStore"},{"location":"LogStore/#contract","text":"","title":"Contract"},{"location":"LogStore/#invalidatecache","text":"invalidateCache (): Unit","title":" invalidateCache"},{"location":"LogStore/#ispartialwritevisible","text":"isPartialWriteVisible ( path : Path ): Boolean Default: true Used when: Checkpoints utility is used to writeCheckpoint","title":" isPartialWriteVisible"},{"location":"LogStore/#listfrom","text":"listFrom ( path : Path ): Iterator [ FileStatus ] listFrom ( path : String ): Iterator [ FileStatus ] Used when: Checkpoints is requested to findLastCompleteCheckpoint DeltaHistoryManager is requested to getEarliestDeltaFile , getEarliestReproducibleCommit and getCommits DeltaLog is requested to getChanges MetadataCleanup is requested to listExpiredDeltaLogs SnapshotManagement is requested to listFrom DeltaFileOperations utility is used to listUsingLogStore","title":" listFrom"},{"location":"LogStore/#read","text":"read ( path : Path ): Seq [ String ] read ( path : String ): Seq [ String ] Used when: Checkpoints is requested to loadMetadataFromFile ReadChecksum is requested to readChecksum DeltaLog is requested to getChanges OptimisticTransactionImpl is requested to checkForConflicts LogStore is requested to readAsIterator","title":" read"},{"location":"LogStore/#write","text":"write ( path : Path , actions : Iterator [ String ], overwrite : Boolean = false ): Unit write ( path : String , actions : Iterator [ String ]): Unit Used when: Checkpoints is requested to checkpoint OptimisticTransactionImpl is requested to doCommit DeltaCommand is requested to commitLarge GenerateSymlinkManifestImpl is requested to writeManifestFiles","title":" write"},{"location":"LogStore/#implementations","text":"HadoopFileSystemLogStore","title":"Implementations"},{"location":"LogStore/#creating-logstore","text":"apply ( sc : SparkContext ): LogStore apply ( sparkConf : SparkConf , hadoopConf : Configuration ): LogStore apply creates a LogStore . apply is used when: GenerateSymlinkManifestImpl is requested to writeManifestFiles and writeSingleManifestFile DeltaHistoryManager is requested to getHistory and getActiveCommitAtTime DeltaFileOperations is requested to recursiveListDirs and localListDirs","title":" Creating LogStore"},{"location":"LogStoreProvider/","text":"LogStoreProvider \u00b6 LogStoreProvider is an abstraction of providers of LogStores . spark.delta.logStore.class \u00b6 LogStoreProvider uses the spark.delta.logStore.class configuration property for the LogStore to create (for a DeltaLog , a DeltaHistoryManager , and DeltaFileOperations ). Creating LogStore \u00b6 createLogStore ( spark : SparkSession ): LogStore createLogStore ( sparkConf : SparkConf , hadoopConf : Configuration ): LogStore createLogStore ...FIXME createLogStore is used when: DeltaLog is created LogStore.apply utility is used","title":"LogStoreProvider"},{"location":"LogStoreProvider/#logstoreprovider","text":"LogStoreProvider is an abstraction of providers of LogStores .","title":"LogStoreProvider"},{"location":"LogStoreProvider/#sparkdeltalogstoreclass","text":"LogStoreProvider uses the spark.delta.logStore.class configuration property for the LogStore to create (for a DeltaLog , a DeltaHistoryManager , and DeltaFileOperations ).","title":" spark.delta.logStore.class"},{"location":"LogStoreProvider/#creating-logstore","text":"createLogStore ( spark : SparkSession ): LogStore createLogStore ( sparkConf : SparkConf , hadoopConf : Configuration ): LogStore createLogStore ...FIXME createLogStore is used when: DeltaLog is created LogStore.apply utility is used","title":" Creating LogStore"},{"location":"Metadata/","text":"Metadata \u00b6 Metadata is an Action to update the metadata of a delta table (indirectly via the Snapshot ). Use DescribeDeltaDetailCommand to review the metadata of a delta table. Creating Instance \u00b6 Metadata takes the following to be created: Id (default: random UUID) Name (default: null ) Description (default: null ) Format (default: empty) Schema (default: null ) Partition Columns (default: Nil ) Table Configuration (default: Map.empty ) Created Time (default: current time) Metadata is created when: DeltaLog is requested for the metadata (but that should be rare) InitialSnapshot is created ConvertToDeltaCommand is executed ImplicitMetadataOperation is requested to updateMetadata Updating Metadata \u00b6 Metadata can be updated in a transaction once only (and only when created for an uninitialized table, when readVersion is -1 ). txn . metadata Demo \u00b6 val path = \"/tmp/delta/users\" import org . apache . spark . sql . delta . DeltaLog val deltaLog = DeltaLog . forTable ( spark , path ) import org . apache . spark . sql . delta . actions . Metadata assert ( deltaLog . snapshot . metadata . isInstanceOf [ Metadata ]) deltaLog . snapshot . metadata . id Table ID \u00b6 Metadata uses a Table ID (aka reservoirId ) to uniquely identify a delta table and is never going to change through the history of the table. Table ID is given Metadata is created or defaults to a random UUID ( Java ). Note When I asked the question tableId and reservoirId - Why two different names for metadata ID? on delta-users mailing list, Tathagata Das wrote: Any reference to \"reservoir\" is just legacy code. In the early days of this project, the project was called \"Tahoe\" and each table is called a \"reservoir\" (Tahoe is one of the 2 nd deepest lake in US, and is a very large reservoir of water ;) ). So you may still find those two terms all around the codebase. In some cases, like DeltaSourceOffset, the term reservoirId is in the json that is written to the streaming checkpoint directory. So we cannot change that for backward compatibility.","title":"Metadata"},{"location":"Metadata/#metadata","text":"Metadata is an Action to update the metadata of a delta table (indirectly via the Snapshot ). Use DescribeDeltaDetailCommand to review the metadata of a delta table.","title":"Metadata"},{"location":"Metadata/#creating-instance","text":"Metadata takes the following to be created: Id (default: random UUID) Name (default: null ) Description (default: null ) Format (default: empty) Schema (default: null ) Partition Columns (default: Nil ) Table Configuration (default: Map.empty ) Created Time (default: current time) Metadata is created when: DeltaLog is requested for the metadata (but that should be rare) InitialSnapshot is created ConvertToDeltaCommand is executed ImplicitMetadataOperation is requested to updateMetadata","title":"Creating Instance"},{"location":"Metadata/#updating-metadata","text":"Metadata can be updated in a transaction once only (and only when created for an uninitialized table, when readVersion is -1 ). txn . metadata","title":"Updating Metadata"},{"location":"Metadata/#demo","text":"val path = \"/tmp/delta/users\" import org . apache . spark . sql . delta . DeltaLog val deltaLog = DeltaLog . forTable ( spark , path ) import org . apache . spark . sql . delta . actions . Metadata assert ( deltaLog . snapshot . metadata . isInstanceOf [ Metadata ]) deltaLog . snapshot . metadata . id","title":"Demo"},{"location":"Metadata/#table-id","text":"Metadata uses a Table ID (aka reservoirId ) to uniquely identify a delta table and is never going to change through the history of the table. Table ID is given Metadata is created or defaults to a random UUID ( Java ). Note When I asked the question tableId and reservoirId - Why two different names for metadata ID? on delta-users mailing list, Tathagata Das wrote: Any reference to \"reservoir\" is just legacy code. In the early days of this project, the project was called \"Tahoe\" and each table is called a \"reservoir\" (Tahoe is one of the 2 nd deepest lake in US, and is a very large reservoir of water ;) ). So you may still find those two terms all around the codebase. In some cases, like DeltaSourceOffset, the term reservoirId is in the json that is written to the streaming checkpoint directory. So we cannot change that for backward compatibility.","title":" Table ID"},{"location":"MetadataCleanup/","text":"MetadataCleanup \u00b6 MetadataCleanup is an abstraction of metadata cleaners that can clean up expired checkpoints and delta logs of a delta table . MetadataCleanup requires to be used with DeltaLog (or subtypes) only. Implementations \u00b6 DeltaLog Table Properties \u00b6 enableExpiredLogCleanup \u00b6 MetadataCleanup uses enableExpiredLogCleanup table configuration to enable log cleanup . logRetentionDuration \u00b6 MetadataCleanup uses logRetentionDuration table configuration for how long to keep around obsolete logs. Cleaning Up Expired Logs \u00b6 doLogCleanup (): Unit doLogCleanup is part of the Checkpoints abstraction. doLogCleanup cleanUpExpiredLogs when enabled . cleanUpExpiredLogs \u00b6 cleanUpExpiredLogs (): Unit cleanUpExpiredLogs calculates a fileCutOffTime based on the current time and the logRetentionDuration table property. cleanUpExpiredLogs prints out the following INFO message to the logs: Starting the deletion of log files older than [date] cleanUpExpiredLogs finds the expired delta logs (based on the fileCutOffTime ) and deletes the files (using Hadoop's FileSystem.delete non-recursively). cleanUpExpiredLogs counts the files deleted (and uses it in the summary INFO message). In the end, cleanUpExpiredLogs prints out the following INFO message to the logs: Deleted [numDeleted] log files older than [date] Finding Expired Log Files \u00b6 listExpiredDeltaLogs ( fileCutOffTime : Long ): Iterator [ FileStatus ] listExpiredDeltaLogs loads the most recent checkpoint if available. If the last checkpoint is not available, listExpiredDeltaLogs returns an empty iterator. listExpiredDeltaLogs requests the LogStore for the paths (in the same directory) that are (lexicographically) greater or equal to the 0 th checkpoint file (per checkpointPrefix format) of the checkpoint and delta files in the log directory . In the end, listExpiredDeltaLogs creates a BufferingLogDeletionIterator that...FIXME Logging \u00b6 Enable ALL logging level for the Implementations logger to see what happens inside.","title":"MetadataCleanup"},{"location":"MetadataCleanup/#metadatacleanup","text":"MetadataCleanup is an abstraction of metadata cleaners that can clean up expired checkpoints and delta logs of a delta table . MetadataCleanup requires to be used with DeltaLog (or subtypes) only.","title":"MetadataCleanup"},{"location":"MetadataCleanup/#implementations","text":"DeltaLog","title":"Implementations"},{"location":"MetadataCleanup/#table-properties","text":"","title":"Table Properties"},{"location":"MetadataCleanup/#enableexpiredlogcleanup","text":"MetadataCleanup uses enableExpiredLogCleanup table configuration to enable log cleanup .","title":" enableExpiredLogCleanup"},{"location":"MetadataCleanup/#logretentionduration","text":"MetadataCleanup uses logRetentionDuration table configuration for how long to keep around obsolete logs.","title":" logRetentionDuration"},{"location":"MetadataCleanup/#cleaning-up-expired-logs","text":"doLogCleanup (): Unit doLogCleanup is part of the Checkpoints abstraction. doLogCleanup cleanUpExpiredLogs when enabled .","title":" Cleaning Up Expired Logs"},{"location":"MetadataCleanup/#cleanupexpiredlogs","text":"cleanUpExpiredLogs (): Unit cleanUpExpiredLogs calculates a fileCutOffTime based on the current time and the logRetentionDuration table property. cleanUpExpiredLogs prints out the following INFO message to the logs: Starting the deletion of log files older than [date] cleanUpExpiredLogs finds the expired delta logs (based on the fileCutOffTime ) and deletes the files (using Hadoop's FileSystem.delete non-recursively). cleanUpExpiredLogs counts the files deleted (and uses it in the summary INFO message). In the end, cleanUpExpiredLogs prints out the following INFO message to the logs: Deleted [numDeleted] log files older than [date]","title":" cleanUpExpiredLogs"},{"location":"MetadataCleanup/#finding-expired-log-files","text":"listExpiredDeltaLogs ( fileCutOffTime : Long ): Iterator [ FileStatus ] listExpiredDeltaLogs loads the most recent checkpoint if available. If the last checkpoint is not available, listExpiredDeltaLogs returns an empty iterator. listExpiredDeltaLogs requests the LogStore for the paths (in the same directory) that are (lexicographically) greater or equal to the 0 th checkpoint file (per checkpointPrefix format) of the checkpoint and delta files in the log directory . In the end, listExpiredDeltaLogs creates a BufferingLogDeletionIterator that...FIXME","title":" Finding Expired Log Files"},{"location":"MetadataCleanup/#logging","text":"Enable ALL logging level for the Implementations logger to see what happens inside.","title":"Logging"},{"location":"Operation/","text":"Operation \u00b6 Operation is an abstraction of operations that can be executed on Delta tables. Operation is described by a name and parameters (that are simply used to create a CommitInfo for OptimisticTransactionImpl when committed and, as a way to bypass a transaction, ConvertToDeltaCommand ). Operation may have performance metrics . Contract \u00b6 Parameters \u00b6 parameters : Map [ String , Any ] Parameters of the operation (to create a CommitInfo with the JSON-encoded values ) Used when Operation is requested for parameters with the values in JSON format Implementations \u00b6 Sealed Abstract Class Operation is a Scala sealed abstract class which means that all of the implementations are in the same compilation unit (a single file). AddColumns \u00b6 AddConstraint \u00b6 ChangeColumn \u00b6 Convert \u00b6 CreateTable \u00b6 Delete \u00b6 DropConstraint \u00b6 ManualUpdate \u00b6 Merge \u00b6 ReplaceColumns \u00b6 ReplaceTable \u00b6 SetTableProperties \u00b6 Name : SET TBLPROPERTIES Parameters : properties Used when: AlterTableSetPropertiesDeltaCommand is executed StreamingUpdate \u00b6 Name : STREAMING UPDATE Parameters : outputMode queryId epochId Used when: DeltaSink is requested to addBatch Truncate \u00b6 UnsetTableProperties \u00b6 Update \u00b6 UpdateColumnMetadata \u00b6 UpdateSchema \u00b6 UpgradeProtocol \u00b6 Write \u00b6 Merge \u00b6 Recorded when a merge operation is committed to a Delta table (when MergeIntoCommand is executed) Creating Instance \u00b6 Operation takes the following to be created: Name of this operation Abstract Class Operation is an abstract class and cannot be created directly. It is created indirectly for the concrete Operations . Serializing Parameter Values (to JSON Format) \u00b6 jsonEncodedValues : Map [ String , String ] jsonEncodedValues converts the values of the parameters to JSON format. jsonEncodedValues is used when: OptimisticTransactionImpl is requested to commit ConvertToDeltaCommand command is requested to streamWrite operationMetrics Registry \u00b6 operationMetrics : Set [ String ] operationMetrics is empty by default (and is expected to be overriden by concrete operations ) operationMetrics is used when Operation is requested to transformMetrics . transformMetrics \u00b6 transformMetrics ( metrics : Map [ String , SQLMetric ]): Map [ String , String ] transformMetrics returns a collection of performance metrics ( SQLMetric ) and their values (as a text) that are defined as the operationMetrics . transformMetrics is used when SQLMetricsReporting is requested to getMetricsForOperation .","title":"Operation"},{"location":"Operation/#operation","text":"Operation is an abstraction of operations that can be executed on Delta tables. Operation is described by a name and parameters (that are simply used to create a CommitInfo for OptimisticTransactionImpl when committed and, as a way to bypass a transaction, ConvertToDeltaCommand ). Operation may have performance metrics .","title":"Operation"},{"location":"Operation/#contract","text":"","title":"Contract"},{"location":"Operation/#parameters","text":"parameters : Map [ String , Any ] Parameters of the operation (to create a CommitInfo with the JSON-encoded values ) Used when Operation is requested for parameters with the values in JSON format","title":" Parameters"},{"location":"Operation/#implementations","text":"Sealed Abstract Class Operation is a Scala sealed abstract class which means that all of the implementations are in the same compilation unit (a single file).","title":"Implementations"},{"location":"Operation/#addcolumns","text":"","title":"AddColumns"},{"location":"Operation/#addconstraint","text":"","title":"AddConstraint"},{"location":"Operation/#changecolumn","text":"","title":"ChangeColumn"},{"location":"Operation/#convert","text":"","title":"Convert"},{"location":"Operation/#createtable","text":"","title":"CreateTable"},{"location":"Operation/#delete","text":"","title":"Delete"},{"location":"Operation/#dropconstraint","text":"","title":"DropConstraint"},{"location":"Operation/#manualupdate","text":"","title":"ManualUpdate"},{"location":"Operation/#merge","text":"","title":"Merge"},{"location":"Operation/#replacecolumns","text":"","title":"ReplaceColumns"},{"location":"Operation/#replacetable","text":"","title":"ReplaceTable"},{"location":"Operation/#settableproperties","text":"Name : SET TBLPROPERTIES Parameters : properties Used when: AlterTableSetPropertiesDeltaCommand is executed","title":"SetTableProperties"},{"location":"Operation/#streamingupdate","text":"Name : STREAMING UPDATE Parameters : outputMode queryId epochId Used when: DeltaSink is requested to addBatch","title":" StreamingUpdate"},{"location":"Operation/#truncate","text":"","title":"Truncate"},{"location":"Operation/#unsettableproperties","text":"","title":"UnsetTableProperties"},{"location":"Operation/#update","text":"","title":"Update"},{"location":"Operation/#updatecolumnmetadata","text":"","title":"UpdateColumnMetadata"},{"location":"Operation/#updateschema","text":"","title":"UpdateSchema"},{"location":"Operation/#upgradeprotocol","text":"","title":"UpgradeProtocol"},{"location":"Operation/#write","text":"","title":"Write"},{"location":"Operation/#merge_1","text":"Recorded when a merge operation is committed to a Delta table (when MergeIntoCommand is executed)","title":"Merge"},{"location":"Operation/#creating-instance","text":"Operation takes the following to be created: Name of this operation Abstract Class Operation is an abstract class and cannot be created directly. It is created indirectly for the concrete Operations .","title":"Creating Instance"},{"location":"Operation/#serializing-parameter-values-to-json-format","text":"jsonEncodedValues : Map [ String , String ] jsonEncodedValues converts the values of the parameters to JSON format. jsonEncodedValues is used when: OptimisticTransactionImpl is requested to commit ConvertToDeltaCommand command is requested to streamWrite","title":" Serializing Parameter Values (to JSON Format)"},{"location":"Operation/#operationmetrics-registry","text":"operationMetrics : Set [ String ] operationMetrics is empty by default (and is expected to be overriden by concrete operations ) operationMetrics is used when Operation is requested to transformMetrics .","title":" operationMetrics Registry"},{"location":"Operation/#transformmetrics","text":"transformMetrics ( metrics : Map [ String , SQLMetric ]): Map [ String , String ] transformMetrics returns a collection of performance metrics ( SQLMetric ) and their values (as a text) that are defined as the operationMetrics . transformMetrics is used when SQLMetricsReporting is requested to getMetricsForOperation .","title":" transformMetrics"},{"location":"OptimisticTransaction/","text":"OptimisticTransaction \u00b6 OptimisticTransaction is an OptimisticTransactionImpl (which seems more of a class name change than anything more important). OptimisticTransaction is created for changes to a delta table at a given version . When OptimisticTransaction (as a OptimisticTransactionImpl ) is about to be committed (that does doCommit internally), the LogStore (of the delta table ) is requested to write actions to a delta file (e.g. _delta_log/00000000000000000001.json for the attempt version 1 ). Unless a FileAlreadyExistsException is thrown a commit is considered successful or retried . OptimisticTransaction can be associated with a thread as an active transaction . Demo \u00b6 import org.apache.spark.sql.delta.DeltaLog val dir = \"/tmp/delta/users\" val log = DeltaLog.forTable(spark, dir) val txn = log.startTransaction() // ...changes to a delta table... val addFile = AddFile(\"foo\", Map.empty, 1L, System.currentTimeMillis(), dataChange = true) val removeFile = addFile.remove val actions = addFile :: removeFile :: Nil txn.commit(actions, op) Alternatively, you could do the following instead. deltaLog.withNewTransaction { txn => // ...transactional changes to a delta table } Creating Instance \u00b6 OptimisticTransaction takes the following to be created: DeltaLog Snapshot Clock Note The DeltaLog and Snapshot are part of the OptimisticTransactionImpl abstraction (which in turn inherits them as a TransactionalWrite and simply changes to val from def ). OptimisticTransaction is created when DeltaLog is used for the following: Starting a new transaction Executing a single-threaded operation (in a new transaction) Active Thread-Local OptimisticTransaction \u00b6 active : ThreadLocal [ OptimisticTransaction ] active is a Java ThreadLocal with the OptimisticTransaction of the current thread. ThreadLocal ThreadLocal provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its get or set method) has its own, independently initialized copy of the variable. ThreadLocal instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID). active is assigned to the current thread using setActive utility and cleared in clearActive . active is available using getActive utility. There can only be one active OptimisticTransaction (or an IllegalStateException is thrown). setActive \u00b6 setActive ( txn : OptimisticTransaction ): Unit setActive associates the given OptimisticTransaction as active with the current thread. setActive throws an IllegalStateException if there is an active OptimisticTransaction already associated: Cannot set a new txn as active when one is already active setActive is used when: DeltaLog is requested to execute an operation in a new transaction clearActive \u00b6 clearActive (): Unit clearActive clears the active transaction (so no transaction is associated with the current thread). clearActive is used when: DeltaLog is requested to execute an operation in a new transaction getActive \u00b6 getActive (): Option [ OptimisticTransaction ] getActive returns the active transaction (if available ). getActive seems unused. Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.delta.OptimisticTransaction logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.OptimisticTransaction=ALL Refer to Logging .","title":"OptimisticTransaction"},{"location":"OptimisticTransaction/#optimistictransaction","text":"OptimisticTransaction is an OptimisticTransactionImpl (which seems more of a class name change than anything more important). OptimisticTransaction is created for changes to a delta table at a given version . When OptimisticTransaction (as a OptimisticTransactionImpl ) is about to be committed (that does doCommit internally), the LogStore (of the delta table ) is requested to write actions to a delta file (e.g. _delta_log/00000000000000000001.json for the attempt version 1 ). Unless a FileAlreadyExistsException is thrown a commit is considered successful or retried . OptimisticTransaction can be associated with a thread as an active transaction .","title":"OptimisticTransaction"},{"location":"OptimisticTransaction/#demo","text":"import org.apache.spark.sql.delta.DeltaLog val dir = \"/tmp/delta/users\" val log = DeltaLog.forTable(spark, dir) val txn = log.startTransaction() // ...changes to a delta table... val addFile = AddFile(\"foo\", Map.empty, 1L, System.currentTimeMillis(), dataChange = true) val removeFile = addFile.remove val actions = addFile :: removeFile :: Nil txn.commit(actions, op) Alternatively, you could do the following instead. deltaLog.withNewTransaction { txn => // ...transactional changes to a delta table }","title":"Demo"},{"location":"OptimisticTransaction/#creating-instance","text":"OptimisticTransaction takes the following to be created: DeltaLog Snapshot Clock Note The DeltaLog and Snapshot are part of the OptimisticTransactionImpl abstraction (which in turn inherits them as a TransactionalWrite and simply changes to val from def ). OptimisticTransaction is created when DeltaLog is used for the following: Starting a new transaction Executing a single-threaded operation (in a new transaction)","title":"Creating Instance"},{"location":"OptimisticTransaction/#active-thread-local-optimistictransaction","text":"active : ThreadLocal [ OptimisticTransaction ] active is a Java ThreadLocal with the OptimisticTransaction of the current thread. ThreadLocal ThreadLocal provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its get or set method) has its own, independently initialized copy of the variable. ThreadLocal instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID). active is assigned to the current thread using setActive utility and cleared in clearActive . active is available using getActive utility. There can only be one active OptimisticTransaction (or an IllegalStateException is thrown).","title":" Active Thread-Local OptimisticTransaction"},{"location":"OptimisticTransaction/#setactive","text":"setActive ( txn : OptimisticTransaction ): Unit setActive associates the given OptimisticTransaction as active with the current thread. setActive throws an IllegalStateException if there is an active OptimisticTransaction already associated: Cannot set a new txn as active when one is already active setActive is used when: DeltaLog is requested to execute an operation in a new transaction","title":" setActive"},{"location":"OptimisticTransaction/#clearactive","text":"clearActive (): Unit clearActive clears the active transaction (so no transaction is associated with the current thread). clearActive is used when: DeltaLog is requested to execute an operation in a new transaction","title":" clearActive"},{"location":"OptimisticTransaction/#getactive","text":"getActive (): Option [ OptimisticTransaction ] getActive returns the active transaction (if available ). getActive seems unused.","title":" getActive"},{"location":"OptimisticTransaction/#logging","text":"Enable ALL logging level for org.apache.spark.sql.delta.OptimisticTransaction logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.OptimisticTransaction=ALL Refer to Logging .","title":"Logging"},{"location":"OptimisticTransactionImpl/","text":"OptimisticTransactionImpl \u00b6 OptimisticTransactionImpl is an extension of the TransactionalWrite abstraction for optimistic transactions that can modify a delta table (at a given version ) and can be committed eventually. In other words, OptimisticTransactionImpl is a set of actions as part of an Operation that changes the state of a delta table transactionally. Contract \u00b6 Clock \u00b6 clock : Clock DeltaLog \u00b6 deltaLog : DeltaLog DeltaLog (of a delta table) that this transaction is changing deltaLog is part of the TransactionalWrite abstraction and seems to change it to val (from def ). Snapshot \u00b6 snapshot : Snapshot Snapshot (of the delta table ) that this transaction is changing snapshot is part of the TransactionalWrite contract and seems to change it to val (from def ). Implementations \u00b6 OptimisticTransaction Table Version at Reading Time \u00b6 readVersion : Long readVersion requests the Snapshot for the version . readVersion is used when: OptimisticTransactionImpl is requested to updateMetadata and commit AlterDeltaTableCommand , ConvertToDeltaCommand , CreateDeltaTableCommand commands are executed DeltaCommand is requested to commitLarge WriteIntoDelta is requested to write ImplicitMetadataOperation is requested to updateMetadata Transactional Commit \u00b6 commit ( actions : Seq [ Action ], op : DeltaOperations . Operation ): Long commit attempts to commit the transaction (with the Action s and the Operation ) and gives the commit version. Usage \u00b6 commit is used when: DeltaLog is requested to upgrade the protocol ALTER delta table commands ( AlterTableSetPropertiesDeltaCommand , AlterTableUnsetPropertiesDeltaCommand , AlterTableAddColumnsDeltaCommand , AlterTableChangeColumnDeltaCommand , AlterTableReplaceColumnsDeltaCommand , AlterTableAddConstraintDeltaCommand , AlterTableDropConstraintDeltaCommand ) are executed ConvertToDeltaCommand command is executed CreateDeltaTableCommand command is executed DeleteCommand command is executed MergeIntoCommand command is executed UpdateCommand command is executed WriteIntoDelta command is executed DeltaSink is requested to addBatch Preparing Commit \u00b6 commit firstly prepares a commit (that gives the final actions to commit that may be different from the given action s). Isolation Level \u00b6 commit determines the isolation level based on FileAction s (in the given action s) and their dataChange flag. With all action s with dataChange flag disabled ( false ), commit assumes no data changed and chooses SnapshotIsolation else Serializable . Blind Append \u00b6 commit is considered blind append when the following all hold: There are only AddFile s among FileAction s in the actions ( onlyAddFiles ) It does not depend on files, i.e. the readPredicates and readFiles are empty ( dependsOnFiles ) CommitInfo \u00b6 commit ...FIXME Registering Post-Commit Hook \u00b6 commit registers the GenerateSymlinkManifest post-commit hook when there is a FileAction among the actions and the compatibility.symlinkFormatManifest.enabled table property is enabled. Commit Version \u00b6 commit doCommit with the next version, the actions, attempt number 0 , and the select isolation level. commit prints out the following INFO message to the logs: Committed delta #[commitVersion] to [logPath] Performing Post-Commit Operations \u00b6 commit postCommit (with the version committed and the actions). Executing Post-Commit Hooks \u00b6 In the end, commit runs post-commit hooks and returns the version of the successful commit. doCommitRetryIteratively \u00b6 doCommitRetryIteratively ( attemptVersion : Long , actions : Seq [ Action ], isolationLevel : IsolationLevel ): Long doCommitRetryIteratively ...FIXME Checking Logical Conflicts with Concurrent Updates \u00b6 checkForConflicts ( checkVersion : Long , actions : Seq [ Action ], attemptNumber : Int , commitIsolationLevel : IsolationLevel ): Long checkForConflicts checks for logical conflicts (of the given actions ) with concurrent updates (actions of the commits since the transaction has started). checkForConflicts gives the next possible commit version unless the following happened between the time of read ( checkVersion ) and the time of this commit attempt: Client is up to date with the table protocol for reading and writing (and hence allowed to access the table) Protocol version has changed Metadata has changed AddFile s have been added that the txn should have read based on the given IsolationLevel ( Concurrent Append ) AddFile s that the txn read have been deleted ( Concurrent Delete ) Files have been deleted by the txn and since the time of read ( Concurrent Delete ) Idempotent transactions have conflicted ( Multiple Streaming Queries with the same checkpoint location) checkForConflicts takes the next possible commit version . For every commit since the time of read ( checkVersion ) and this commit attempt, checkForConflicts does the following: FIXME Prints out the following INFO message to the logs: Completed checking for conflicts Version: [version] Attempt: [attemptNumber] Time: [totalCheckAndRetryTime] ms In the end, checkForConflicts prints out the following INFO message to the logs: No logical conflicts with deltas [[checkVersion], [nextAttemptVersion]), retrying. getPrettyPartitionMessage \u00b6 getPrettyPartitionMessage ( partitionValues : Map [ String , String ]): String getPrettyPartitionMessage ...FIXME postCommit \u00b6 postCommit ( commitVersion : Long , commitActions : Seq [ Action ]): Unit postCommit turns the committed flag on. postCommit requests the DeltaLog to checkpoint when the given commitVersion is not 0 ( first commit ) and the checkpoint interval has been reached (based on the given commitVersion ). Note commitActions argument is not used. postCommit prints out the following WARN message to the logs in case of IllegalStateException : Failed to checkpoint table state. prepareCommit \u00b6 prepareCommit ( actions : Seq [ Action ], op : DeltaOperations . Operation ): Seq [ Action ] prepareCommit adds the newMetadata action (if available) to the given action s. prepareCommit verifyNewMetadata if there was one. prepareCommit ...FIXME prepareCommit requests the DeltaLog to protocolWrite . prepareCommit ...FIXME Multiple Metadata Changes Not Allowed \u00b6 prepareCommit throws an AssertionError when there are multiple metadata changes in the transaction (by means of Metadata actions): Cannot change the metadata more than once in a transaction. Committing Transaction Allowed Once Only \u00b6 prepareCommit throws an AssertionError when the committed internal flag is enabled: Transaction already committed. Registering Post-Commit Hook \u00b6 registerPostCommitHook ( hook : PostCommitHook ): Unit registerPostCommitHook registers ( adds ) the given PostCommitHook to the postCommitHooks internal registry. runPostCommitHooks \u00b6 runPostCommitHooks ( version : Long , committedActions : Seq [ Action ]): Unit runPostCommitHooks simply runs every post-commit hook registered (in the postCommitHooks internal registry). runPostCommitHooks clears the active transaction (making all follow-up operations non-transactional). Note Hooks may create new transactions. Handling Non-Fatal Exceptions \u00b6 For non-fatal exceptions, runPostCommitHooks prints out the following ERROR message to the logs, records the delta event, and requests the post-commit hook to handle the error . Error when executing post-commit hook [name] for commit [version] AssertionError \u00b6 runPostCommitHooks throws an AssertionError when committed flag is disabled: Can't call post commit hooks before committing Next Possible Commit Version \u00b6 getNextAttemptVersion ( previousAttemptVersion : Long ): Long getNextAttemptVersion requests the DeltaLog to update (and give the latest state snapshot of the delta table). In the end, getNextAttemptVersion requests the Snapshot for the version and increments it. Note The input previousAttemptVersion argument is not used. Operation Metrics \u00b6 getOperationMetrics ( op : Operation ): Option [ Map [ String , String ]] getOperationMetrics gives the metrics for the given Operation when the spark.databricks.delta.history.metricsEnabled configuration property is enabled. Otherwise, getOperationMetrics gives None . CommitInfo \u00b6 OptimisticTransactionImpl creates a CommitInfo when requested to commit with spark.databricks.delta.commitInfo.enabled configuration enabled. OptimisticTransactionImpl uses the CommitInfo to recordDeltaEvent (as a CommitStats ). Attempting Commit \u00b6 doCommit ( attemptVersion : Long , actions : Seq [ Action ], attemptNumber : Int , isolationLevel : IsolationLevel ): Long doCommit returns the given attemptVersion as the commit version if successful or checkAndRetry . doCommit is used when: OptimisticTransactionImpl is requested to commit (and checkAndRetry ). Internally, doCommit prints out the following DEBUG message to the logs: Attempting to commit version [attemptVersion] with [n] actions with [isolationLevel] isolation level Writing Out \u00b6 doCommit requests the DeltaLog for the LogStore to write out the given action s to a delta file in the log directory with the attemptVersion version, e.g. 00000000000000000001.json doCommit writes the action s out in JSON format . Note LogStores must throw a java.nio.file.FileAlreadyExistsException exception if the delta file already exists. Any FileAlreadyExistsExceptions are caught by doCommit itself to checkAndRetry . Post-Commit Snapshot \u00b6 doCommit requests the DeltaLog to update . IllegalStateException \u00b6 doCommit throws an IllegalStateException when the version of the snapshot after update is smaller than the given attemptVersion version. The committed version is [attemptVersion] but the current version is [version]. CommitStats \u00b6 doCommit records a new CommitStats and returns the given attemptVersion as the commit version. FileAlreadyExistsExceptions \u00b6 doCommit catches FileAlreadyExistsExceptions and checkAndRetry . Retrying Commit \u00b6 checkAndRetry ( checkVersion : Long , actions : Seq [ Action ], attemptNumber : Int ): Long checkAndRetry ...FIXME checkAndRetry is used when OptimisticTransactionImpl is requested to commit (and attempts a commit that failed with an FileAlreadyExistsException ). verifyNewMetadata \u00b6 verifyNewMetadata ( metadata : Metadata ): Unit verifyNewMetadata ...FIXME verifyNewMetadata is used when: OptimisticTransactionImpl is requested to prepareCommit and updateMetadata withGlobalConfigDefaults \u00b6 withGlobalConfigDefaults ( metadata : Metadata ): Metadata withGlobalConfigDefaults ...FIXME withGlobalConfigDefaults is used when: OptimisticTransactionImpl is requested to updateMetadata and updateMetadataForNewTable Looking Up Transaction Version For Given (Streaming Query) ID \u00b6 txnVersion ( id : String ): Long txnVersion simply registers ( adds ) the given ID in the readTxn internal registry. In the end, txnVersion requests the Snapshot for the transaction version for the given ID or -1 . txnVersion is used when: DeltaSink is requested to add a streaming micro-batch User-Defined Metadata \u00b6 getUserMetadata ( op : Operation ): Option [ String ] getUserMetadata returns the Operation.md#userMetadata[userMetadata] of the given Operation.md[] (if defined) or the value of DeltaSQLConf.md#DELTA_USER_METADATA[spark.databricks.delta.commitInfo.userMetadata] configuration property. getUserMetadata is used when: OptimisticTransactionImpl is requested to commit (and spark.databricks.delta.commitInfo.enabled configuration property is enabled) ConvertToDeltaCommand is executed (and in turn requests DeltaCommand to commitLarge ) Internal Registries \u00b6 Post-Commit Hooks \u00b6 postCommitHooks : ArrayBuffer [ PostCommitHook ] OptimisticTransactionImpl manages PostCommitHook s that will be executed right after a commit is successful. Post-commit hooks can be registered , but only the GenerateSymlinkManifest post-commit hook is supported. newMetadata \u00b6 newMetadata : Option [ Metadata ] OptimisticTransactionImpl uses the newMetadata internal registry for a new Metadata that should be committed with this transaction. newMetadata is initially undefined ( None ). It can be updated only once and before the transaction writes out any files . newMetadata is used when prepareCommit and doCommit (for statistics). newMetadata is available using metadata method. readFiles \u00b6 readFiles : HashSet [ AddFile ] OptimisticTransactionImpl uses readFiles registry to track AddFile s that have been seen ( scanned ) by this transaction (when requested to filterFiles ). Used to determine isBlindAppend and checkForConflicts (and fail if the files have been deleted that the txn read). readPredicates \u00b6 readPredicates : ArrayBuffer [ Expression ] readPredicates holds predicate expressions for partitions the transaction is modifying. readPredicates is added a new predicate expression when filterFiles and readWholeTable . readPredicates is used when checkAndRetry . Internal Properties \u00b6 committed \u00b6 Controls whether the transaction has been committed or not (and prevents prepareCommit from being executed again) Default: false Enabled in postCommit readTxn \u00b6 Streaming query IDs that have been seen by this transaction A new queryId is added when OptimisticTransactionImpl is requested for txnVersion Used when OptimisticTransactionImpl is requested to checkAndRetry (to fail with a ConcurrentTransactionException for idempotent transactions that have conflicted) snapshotMetadata \u00b6 Metadata of the Snapshot readWholeTable \u00b6 readWholeTable (): Unit readWholeTable simply adds True literal to the readPredicates internal registry. readWholeTable is used when: DeltaSink is requested to add a streaming micro-batch (and the batch reads the same Delta table as this sink is going to write to) updateMetadataForNewTable \u00b6 updateMetadataForNewTable ( metadata : Metadata ): Unit updateMetadataForNewTable ...FIXME updateMetadataForNewTable is used when: ConvertToDeltaCommand and CreateDeltaTableCommand are executed Metadata \u00b6 metadata : Metadata metadata is part of the TransactionalWrite abstraction. metadata is either the newMetadata (if defined) or the snapshotMetadata . Updating Metadata \u00b6 updateMetadata ( metadata : Metadata ): Unit updateMetadata updates the newMetadata internal property based on the readVersion : For -1 , updateMetadata updates the configuration of the given metadata with a new metadata based on the SQLConf (of the active SparkSession ), the configuration of the given metadata and a new Protocol For other versions, updateMetadata leaves the given Metadata unchanged updateMetadata is used when: OptimisticTransactionImpl is requested to updateMetadataForNewTable AlterTableSetPropertiesDeltaCommand , AlterTableUnsetPropertiesDeltaCommand , AlterTableAddColumnsDeltaCommand , AlterTableChangeColumnDeltaCommand , AlterTableReplaceColumnsDeltaCommand are executed ConvertToDeltaCommand is executed ImplicitMetadataOperation is requested to updateMetadata AssertionError \u00b6 updateMetadata throws an AssertionError when the hasWritten flag is enabled: Cannot update the metadata in a transaction that has already written data. AssertionError \u00b6 updateMetadata throws an AssertionError when the newMetadata is not empty: Cannot change the metadata more than once in a transaction. Files To Scan Matching Given Predicates \u00b6 filterFiles (): Seq [ AddFile ] // (1) filterFiles ( filters : Seq [ Expression ]): Seq [ AddFile ] No filters = all files filterFiles gives the files to scan for the given predicates ( filter expressions ). Internally, filterFiles requests the Snapshot for the filesForScan (for no projection attributes and the given filters). filterFiles finds the partition predicates among the given filters (and the partition columns of the Metadata ). filterFiles registers ( adds ) the partition predicates (in the readPredicates internal registry) and the files to scan (in the readFiles internal registry). filterFiles is used when: ActiveOptimisticTransactionRule is executed DeltaSink is requested to add a streaming micro-batch (with Complete output mode) DeleteCommand , MergeIntoCommand and UpdateCommand , WriteIntoDelta are executed CreateDeltaTableCommand is executed","title":"OptimisticTransactionImpl"},{"location":"OptimisticTransactionImpl/#optimistictransactionimpl","text":"OptimisticTransactionImpl is an extension of the TransactionalWrite abstraction for optimistic transactions that can modify a delta table (at a given version ) and can be committed eventually. In other words, OptimisticTransactionImpl is a set of actions as part of an Operation that changes the state of a delta table transactionally.","title":"OptimisticTransactionImpl"},{"location":"OptimisticTransactionImpl/#contract","text":"","title":"Contract"},{"location":"OptimisticTransactionImpl/#clock","text":"clock : Clock","title":" Clock"},{"location":"OptimisticTransactionImpl/#deltalog","text":"deltaLog : DeltaLog DeltaLog (of a delta table) that this transaction is changing deltaLog is part of the TransactionalWrite abstraction and seems to change it to val (from def ).","title":" DeltaLog"},{"location":"OptimisticTransactionImpl/#snapshot","text":"snapshot : Snapshot Snapshot (of the delta table ) that this transaction is changing snapshot is part of the TransactionalWrite contract and seems to change it to val (from def ).","title":" Snapshot"},{"location":"OptimisticTransactionImpl/#implementations","text":"OptimisticTransaction","title":"Implementations"},{"location":"OptimisticTransactionImpl/#table-version-at-reading-time","text":"readVersion : Long readVersion requests the Snapshot for the version . readVersion is used when: OptimisticTransactionImpl is requested to updateMetadata and commit AlterDeltaTableCommand , ConvertToDeltaCommand , CreateDeltaTableCommand commands are executed DeltaCommand is requested to commitLarge WriteIntoDelta is requested to write ImplicitMetadataOperation is requested to updateMetadata","title":" Table Version at Reading Time"},{"location":"OptimisticTransactionImpl/#transactional-commit","text":"commit ( actions : Seq [ Action ], op : DeltaOperations . Operation ): Long commit attempts to commit the transaction (with the Action s and the Operation ) and gives the commit version.","title":" Transactional Commit"},{"location":"OptimisticTransactionImpl/#usage","text":"commit is used when: DeltaLog is requested to upgrade the protocol ALTER delta table commands ( AlterTableSetPropertiesDeltaCommand , AlterTableUnsetPropertiesDeltaCommand , AlterTableAddColumnsDeltaCommand , AlterTableChangeColumnDeltaCommand , AlterTableReplaceColumnsDeltaCommand , AlterTableAddConstraintDeltaCommand , AlterTableDropConstraintDeltaCommand ) are executed ConvertToDeltaCommand command is executed CreateDeltaTableCommand command is executed DeleteCommand command is executed MergeIntoCommand command is executed UpdateCommand command is executed WriteIntoDelta command is executed DeltaSink is requested to addBatch","title":" Usage"},{"location":"OptimisticTransactionImpl/#preparing-commit","text":"commit firstly prepares a commit (that gives the final actions to commit that may be different from the given action s).","title":" Preparing Commit"},{"location":"OptimisticTransactionImpl/#isolation-level","text":"commit determines the isolation level based on FileAction s (in the given action s) and their dataChange flag. With all action s with dataChange flag disabled ( false ), commit assumes no data changed and chooses SnapshotIsolation else Serializable .","title":" Isolation Level"},{"location":"OptimisticTransactionImpl/#blind-append","text":"commit is considered blind append when the following all hold: There are only AddFile s among FileAction s in the actions ( onlyAddFiles ) It does not depend on files, i.e. the readPredicates and readFiles are empty ( dependsOnFiles )","title":" Blind Append"},{"location":"OptimisticTransactionImpl/#commitinfo","text":"commit ...FIXME","title":" CommitInfo"},{"location":"OptimisticTransactionImpl/#registering-post-commit-hook","text":"commit registers the GenerateSymlinkManifest post-commit hook when there is a FileAction among the actions and the compatibility.symlinkFormatManifest.enabled table property is enabled.","title":" Registering Post-Commit Hook"},{"location":"OptimisticTransactionImpl/#commit-version","text":"commit doCommit with the next version, the actions, attempt number 0 , and the select isolation level. commit prints out the following INFO message to the logs: Committed delta #[commitVersion] to [logPath]","title":" Commit Version"},{"location":"OptimisticTransactionImpl/#performing-post-commit-operations","text":"commit postCommit (with the version committed and the actions).","title":" Performing Post-Commit Operations"},{"location":"OptimisticTransactionImpl/#executing-post-commit-hooks","text":"In the end, commit runs post-commit hooks and returns the version of the successful commit.","title":" Executing Post-Commit Hooks"},{"location":"OptimisticTransactionImpl/#docommitretryiteratively","text":"doCommitRetryIteratively ( attemptVersion : Long , actions : Seq [ Action ], isolationLevel : IsolationLevel ): Long doCommitRetryIteratively ...FIXME","title":" doCommitRetryIteratively"},{"location":"OptimisticTransactionImpl/#checking-logical-conflicts-with-concurrent-updates","text":"checkForConflicts ( checkVersion : Long , actions : Seq [ Action ], attemptNumber : Int , commitIsolationLevel : IsolationLevel ): Long checkForConflicts checks for logical conflicts (of the given actions ) with concurrent updates (actions of the commits since the transaction has started). checkForConflicts gives the next possible commit version unless the following happened between the time of read ( checkVersion ) and the time of this commit attempt: Client is up to date with the table protocol for reading and writing (and hence allowed to access the table) Protocol version has changed Metadata has changed AddFile s have been added that the txn should have read based on the given IsolationLevel ( Concurrent Append ) AddFile s that the txn read have been deleted ( Concurrent Delete ) Files have been deleted by the txn and since the time of read ( Concurrent Delete ) Idempotent transactions have conflicted ( Multiple Streaming Queries with the same checkpoint location) checkForConflicts takes the next possible commit version . For every commit since the time of read ( checkVersion ) and this commit attempt, checkForConflicts does the following: FIXME Prints out the following INFO message to the logs: Completed checking for conflicts Version: [version] Attempt: [attemptNumber] Time: [totalCheckAndRetryTime] ms In the end, checkForConflicts prints out the following INFO message to the logs: No logical conflicts with deltas [[checkVersion], [nextAttemptVersion]), retrying.","title":" Checking Logical Conflicts with Concurrent Updates"},{"location":"OptimisticTransactionImpl/#getprettypartitionmessage","text":"getPrettyPartitionMessage ( partitionValues : Map [ String , String ]): String getPrettyPartitionMessage ...FIXME","title":" getPrettyPartitionMessage"},{"location":"OptimisticTransactionImpl/#postcommit","text":"postCommit ( commitVersion : Long , commitActions : Seq [ Action ]): Unit postCommit turns the committed flag on. postCommit requests the DeltaLog to checkpoint when the given commitVersion is not 0 ( first commit ) and the checkpoint interval has been reached (based on the given commitVersion ). Note commitActions argument is not used. postCommit prints out the following WARN message to the logs in case of IllegalStateException : Failed to checkpoint table state.","title":" postCommit"},{"location":"OptimisticTransactionImpl/#preparecommit","text":"prepareCommit ( actions : Seq [ Action ], op : DeltaOperations . Operation ): Seq [ Action ] prepareCommit adds the newMetadata action (if available) to the given action s. prepareCommit verifyNewMetadata if there was one. prepareCommit ...FIXME prepareCommit requests the DeltaLog to protocolWrite . prepareCommit ...FIXME","title":" prepareCommit"},{"location":"OptimisticTransactionImpl/#multiple-metadata-changes-not-allowed","text":"prepareCommit throws an AssertionError when there are multiple metadata changes in the transaction (by means of Metadata actions): Cannot change the metadata more than once in a transaction.","title":" Multiple Metadata Changes Not Allowed"},{"location":"OptimisticTransactionImpl/#committing-transaction-allowed-once-only","text":"prepareCommit throws an AssertionError when the committed internal flag is enabled: Transaction already committed.","title":" Committing Transaction Allowed Once Only"},{"location":"OptimisticTransactionImpl/#registering-post-commit-hook_1","text":"registerPostCommitHook ( hook : PostCommitHook ): Unit registerPostCommitHook registers ( adds ) the given PostCommitHook to the postCommitHooks internal registry.","title":" Registering Post-Commit Hook"},{"location":"OptimisticTransactionImpl/#runpostcommithooks","text":"runPostCommitHooks ( version : Long , committedActions : Seq [ Action ]): Unit runPostCommitHooks simply runs every post-commit hook registered (in the postCommitHooks internal registry). runPostCommitHooks clears the active transaction (making all follow-up operations non-transactional). Note Hooks may create new transactions.","title":" runPostCommitHooks"},{"location":"OptimisticTransactionImpl/#handling-non-fatal-exceptions","text":"For non-fatal exceptions, runPostCommitHooks prints out the following ERROR message to the logs, records the delta event, and requests the post-commit hook to handle the error . Error when executing post-commit hook [name] for commit [version]","title":" Handling Non-Fatal Exceptions"},{"location":"OptimisticTransactionImpl/#assertionerror","text":"runPostCommitHooks throws an AssertionError when committed flag is disabled: Can't call post commit hooks before committing","title":" AssertionError"},{"location":"OptimisticTransactionImpl/#next-possible-commit-version","text":"getNextAttemptVersion ( previousAttemptVersion : Long ): Long getNextAttemptVersion requests the DeltaLog to update (and give the latest state snapshot of the delta table). In the end, getNextAttemptVersion requests the Snapshot for the version and increments it. Note The input previousAttemptVersion argument is not used.","title":" Next Possible Commit Version"},{"location":"OptimisticTransactionImpl/#operation-metrics","text":"getOperationMetrics ( op : Operation ): Option [ Map [ String , String ]] getOperationMetrics gives the metrics for the given Operation when the spark.databricks.delta.history.metricsEnabled configuration property is enabled. Otherwise, getOperationMetrics gives None .","title":" Operation Metrics"},{"location":"OptimisticTransactionImpl/#commitinfo_1","text":"OptimisticTransactionImpl creates a CommitInfo when requested to commit with spark.databricks.delta.commitInfo.enabled configuration enabled. OptimisticTransactionImpl uses the CommitInfo to recordDeltaEvent (as a CommitStats ).","title":" CommitInfo"},{"location":"OptimisticTransactionImpl/#attempting-commit","text":"doCommit ( attemptVersion : Long , actions : Seq [ Action ], attemptNumber : Int , isolationLevel : IsolationLevel ): Long doCommit returns the given attemptVersion as the commit version if successful or checkAndRetry . doCommit is used when: OptimisticTransactionImpl is requested to commit (and checkAndRetry ). Internally, doCommit prints out the following DEBUG message to the logs: Attempting to commit version [attemptVersion] with [n] actions with [isolationLevel] isolation level","title":" Attempting Commit"},{"location":"OptimisticTransactionImpl/#writing-out","text":"doCommit requests the DeltaLog for the LogStore to write out the given action s to a delta file in the log directory with the attemptVersion version, e.g. 00000000000000000001.json doCommit writes the action s out in JSON format . Note LogStores must throw a java.nio.file.FileAlreadyExistsException exception if the delta file already exists. Any FileAlreadyExistsExceptions are caught by doCommit itself to checkAndRetry .","title":" Writing Out"},{"location":"OptimisticTransactionImpl/#post-commit-snapshot","text":"doCommit requests the DeltaLog to update .","title":" Post-Commit Snapshot"},{"location":"OptimisticTransactionImpl/#illegalstateexception","text":"doCommit throws an IllegalStateException when the version of the snapshot after update is smaller than the given attemptVersion version. The committed version is [attemptVersion] but the current version is [version].","title":" IllegalStateException"},{"location":"OptimisticTransactionImpl/#commitstats","text":"doCommit records a new CommitStats and returns the given attemptVersion as the commit version.","title":" CommitStats"},{"location":"OptimisticTransactionImpl/#filealreadyexistsexceptions","text":"doCommit catches FileAlreadyExistsExceptions and checkAndRetry .","title":" FileAlreadyExistsExceptions"},{"location":"OptimisticTransactionImpl/#retrying-commit","text":"checkAndRetry ( checkVersion : Long , actions : Seq [ Action ], attemptNumber : Int ): Long checkAndRetry ...FIXME checkAndRetry is used when OptimisticTransactionImpl is requested to commit (and attempts a commit that failed with an FileAlreadyExistsException ).","title":" Retrying Commit"},{"location":"OptimisticTransactionImpl/#verifynewmetadata","text":"verifyNewMetadata ( metadata : Metadata ): Unit verifyNewMetadata ...FIXME verifyNewMetadata is used when: OptimisticTransactionImpl is requested to prepareCommit and updateMetadata","title":" verifyNewMetadata"},{"location":"OptimisticTransactionImpl/#withglobalconfigdefaults","text":"withGlobalConfigDefaults ( metadata : Metadata ): Metadata withGlobalConfigDefaults ...FIXME withGlobalConfigDefaults is used when: OptimisticTransactionImpl is requested to updateMetadata and updateMetadataForNewTable","title":" withGlobalConfigDefaults"},{"location":"OptimisticTransactionImpl/#looking-up-transaction-version-for-given-streaming-query-id","text":"txnVersion ( id : String ): Long txnVersion simply registers ( adds ) the given ID in the readTxn internal registry. In the end, txnVersion requests the Snapshot for the transaction version for the given ID or -1 . txnVersion is used when: DeltaSink is requested to add a streaming micro-batch","title":" Looking Up Transaction Version For Given (Streaming Query) ID"},{"location":"OptimisticTransactionImpl/#user-defined-metadata","text":"getUserMetadata ( op : Operation ): Option [ String ] getUserMetadata returns the Operation.md#userMetadata[userMetadata] of the given Operation.md[] (if defined) or the value of DeltaSQLConf.md#DELTA_USER_METADATA[spark.databricks.delta.commitInfo.userMetadata] configuration property. getUserMetadata is used when: OptimisticTransactionImpl is requested to commit (and spark.databricks.delta.commitInfo.enabled configuration property is enabled) ConvertToDeltaCommand is executed (and in turn requests DeltaCommand to commitLarge )","title":" User-Defined Metadata"},{"location":"OptimisticTransactionImpl/#internal-registries","text":"","title":"Internal Registries"},{"location":"OptimisticTransactionImpl/#post-commit-hooks","text":"postCommitHooks : ArrayBuffer [ PostCommitHook ] OptimisticTransactionImpl manages PostCommitHook s that will be executed right after a commit is successful. Post-commit hooks can be registered , but only the GenerateSymlinkManifest post-commit hook is supported.","title":" Post-Commit Hooks"},{"location":"OptimisticTransactionImpl/#newmetadata","text":"newMetadata : Option [ Metadata ] OptimisticTransactionImpl uses the newMetadata internal registry for a new Metadata that should be committed with this transaction. newMetadata is initially undefined ( None ). It can be updated only once and before the transaction writes out any files . newMetadata is used when prepareCommit and doCommit (for statistics). newMetadata is available using metadata method.","title":" newMetadata"},{"location":"OptimisticTransactionImpl/#readfiles","text":"readFiles : HashSet [ AddFile ] OptimisticTransactionImpl uses readFiles registry to track AddFile s that have been seen ( scanned ) by this transaction (when requested to filterFiles ). Used to determine isBlindAppend and checkForConflicts (and fail if the files have been deleted that the txn read).","title":" readFiles"},{"location":"OptimisticTransactionImpl/#readpredicates","text":"readPredicates : ArrayBuffer [ Expression ] readPredicates holds predicate expressions for partitions the transaction is modifying. readPredicates is added a new predicate expression when filterFiles and readWholeTable . readPredicates is used when checkAndRetry .","title":" readPredicates"},{"location":"OptimisticTransactionImpl/#internal-properties","text":"","title":"Internal Properties"},{"location":"OptimisticTransactionImpl/#committed","text":"Controls whether the transaction has been committed or not (and prevents prepareCommit from being executed again) Default: false Enabled in postCommit","title":" committed"},{"location":"OptimisticTransactionImpl/#readtxn","text":"Streaming query IDs that have been seen by this transaction A new queryId is added when OptimisticTransactionImpl is requested for txnVersion Used when OptimisticTransactionImpl is requested to checkAndRetry (to fail with a ConcurrentTransactionException for idempotent transactions that have conflicted)","title":" readTxn"},{"location":"OptimisticTransactionImpl/#snapshotmetadata","text":"Metadata of the Snapshot","title":" snapshotMetadata"},{"location":"OptimisticTransactionImpl/#readwholetable","text":"readWholeTable (): Unit readWholeTable simply adds True literal to the readPredicates internal registry. readWholeTable is used when: DeltaSink is requested to add a streaming micro-batch (and the batch reads the same Delta table as this sink is going to write to)","title":" readWholeTable"},{"location":"OptimisticTransactionImpl/#updatemetadatafornewtable","text":"updateMetadataForNewTable ( metadata : Metadata ): Unit updateMetadataForNewTable ...FIXME updateMetadataForNewTable is used when: ConvertToDeltaCommand and CreateDeltaTableCommand are executed","title":" updateMetadataForNewTable"},{"location":"OptimisticTransactionImpl/#metadata","text":"metadata : Metadata metadata is part of the TransactionalWrite abstraction. metadata is either the newMetadata (if defined) or the snapshotMetadata .","title":" Metadata"},{"location":"OptimisticTransactionImpl/#updating-metadata","text":"updateMetadata ( metadata : Metadata ): Unit updateMetadata updates the newMetadata internal property based on the readVersion : For -1 , updateMetadata updates the configuration of the given metadata with a new metadata based on the SQLConf (of the active SparkSession ), the configuration of the given metadata and a new Protocol For other versions, updateMetadata leaves the given Metadata unchanged updateMetadata is used when: OptimisticTransactionImpl is requested to updateMetadataForNewTable AlterTableSetPropertiesDeltaCommand , AlterTableUnsetPropertiesDeltaCommand , AlterTableAddColumnsDeltaCommand , AlterTableChangeColumnDeltaCommand , AlterTableReplaceColumnsDeltaCommand are executed ConvertToDeltaCommand is executed ImplicitMetadataOperation is requested to updateMetadata","title":" Updating Metadata"},{"location":"OptimisticTransactionImpl/#assertionerror_1","text":"updateMetadata throws an AssertionError when the hasWritten flag is enabled: Cannot update the metadata in a transaction that has already written data.","title":" AssertionError"},{"location":"OptimisticTransactionImpl/#assertionerror_2","text":"updateMetadata throws an AssertionError when the newMetadata is not empty: Cannot change the metadata more than once in a transaction.","title":" AssertionError"},{"location":"OptimisticTransactionImpl/#files-to-scan-matching-given-predicates","text":"filterFiles (): Seq [ AddFile ] // (1) filterFiles ( filters : Seq [ Expression ]): Seq [ AddFile ] No filters = all files filterFiles gives the files to scan for the given predicates ( filter expressions ). Internally, filterFiles requests the Snapshot for the filesForScan (for no projection attributes and the given filters). filterFiles finds the partition predicates among the given filters (and the partition columns of the Metadata ). filterFiles registers ( adds ) the partition predicates (in the readPredicates internal registry) and the files to scan (in the readFiles internal registry). filterFiles is used when: ActiveOptimisticTransactionRule is executed DeltaSink is requested to add a streaming micro-batch (with Complete output mode) DeleteCommand , MergeIntoCommand and UpdateCommand , WriteIntoDelta are executed CreateDeltaTableCommand is executed","title":" Files To Scan Matching Given Predicates"},{"location":"PartitionFiltering/","text":"PartitionFiltering \u00b6 PartitionFiltering is an abstraction of snapshots with partition filtering for scan . Implementations \u00b6 Snapshot is the default and only known PartitionFiltering in Delta Lake. Files to Scan (Matching Projection Attributes and Predicates) \u00b6 filesForScan ( projection : Seq [ Attribute ], filters : Seq [ Expression ], keepStats : Boolean = false ): DeltaScan filesForScan ...FIXME filesForScan is used when: OptimisticTransactionImpl is requested for the files to scan matching given predicates TahoeLogFileIndex is requested for the files matching predicates and the input files","title":"PartitionFiltering"},{"location":"PartitionFiltering/#partitionfiltering","text":"PartitionFiltering is an abstraction of snapshots with partition filtering for scan .","title":"PartitionFiltering"},{"location":"PartitionFiltering/#implementations","text":"Snapshot is the default and only known PartitionFiltering in Delta Lake.","title":"Implementations"},{"location":"PartitionFiltering/#files-to-scan-matching-projection-attributes-and-predicates","text":"filesForScan ( projection : Seq [ Attribute ], filters : Seq [ Expression ], keepStats : Boolean = false ): DeltaScan filesForScan ...FIXME filesForScan is used when: OptimisticTransactionImpl is requested for the files to scan matching given predicates TahoeLogFileIndex is requested for the files matching predicates and the input files","title":" Files to Scan (Matching Projection Attributes and Predicates)"},{"location":"PinnedTahoeFileIndex/","text":"PinnedTahoeFileIndex \u00b6 PinnedTahoeFileIndex is a TahoeFileIndex . Creating Instance \u00b6 PinnedTahoeFileIndex takes the following to be created: SparkSession ( Spark SQL ) DeltaLog Hadoop Path Snapshot PinnedTahoeFileIndex is created when: ActiveOptimisticTransactionRule logical optimization rule is executed","title":"PinnedTahoeFileIndex"},{"location":"PinnedTahoeFileIndex/#pinnedtahoefileindex","text":"PinnedTahoeFileIndex is a TahoeFileIndex .","title":"PinnedTahoeFileIndex"},{"location":"PinnedTahoeFileIndex/#creating-instance","text":"PinnedTahoeFileIndex takes the following to be created: SparkSession ( Spark SQL ) DeltaLog Hadoop Path Snapshot PinnedTahoeFileIndex is created when: ActiveOptimisticTransactionRule logical optimization rule is executed","title":"Creating Instance"},{"location":"PostCommitHook/","text":"= PostCommitHook PostCommitHook is an < > of < > that have a < > and can be < > (when OptimisticTransactionImpl is < >). [[contract]] .PostCommitHook Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | handleError a| [[handleError]] [source, scala] \u00b6 handleError( error: Throwable, version: Long): Unit = {} Handles an error while < > Used when OptimisticTransactionImpl is requested to < > (when < >) | name a| [[name]] [source, scala] \u00b6 name: String \u00b6 User-friendly name of the hook for error reporting Used when: DeltaErrors utility is used to < > OptimisticTransactionImpl is requested to < > (when < >) GenerateSymlinkManifestImpl is requested to handle an error | run a| [[run]] [source, scala] \u00b6 run( spark: SparkSession, txn: OptimisticTransactionImpl, committedActions: Seq[Action]): Unit Executes the post-commit hook Used when OptimisticTransactionImpl is requested to < > (when < >) |===","title":"Post-Commit Hooks"},{"location":"PostCommitHook/#source-scala","text":"handleError( error: Throwable, version: Long): Unit = {} Handles an error while < > Used when OptimisticTransactionImpl is requested to < > (when < >) | name a| [[name]]","title":"[source, scala]"},{"location":"PostCommitHook/#source-scala_1","text":"","title":"[source, scala]"},{"location":"PostCommitHook/#name-string","text":"User-friendly name of the hook for error reporting Used when: DeltaErrors utility is used to < > OptimisticTransactionImpl is requested to < > (when < >) GenerateSymlinkManifestImpl is requested to handle an error | run a| [[run]]","title":"name: String"},{"location":"PostCommitHook/#source-scala_2","text":"run( spark: SparkSession, txn: OptimisticTransactionImpl, committedActions: Seq[Action]): Unit Executes the post-commit hook Used when OptimisticTransactionImpl is requested to < > (when < >) |===","title":"[source, scala]"},{"location":"PreprocessTableDelete/","text":"PreprocessTableDelete Logical Resolution Rule \u00b6 PreprocessTableDelete is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to resolve DeltaDelete commands in a logical query plan into DeleteCommand s. PreprocessTableDelete is installed (injected) into a SparkSession using DeltaSparkSessionExtension . Creating Instance \u00b6 PreprocessTableDelete takes the following to be created: SQLConf ( Spark SQL ) PreprocessTableDelete is created when: DeltaSparkSessionExtension is executed (and registers Delta SQL support) Executing Rule \u00b6 apply ( plan : LogicalPlan ): LogicalPlan apply is part of the Rule ( Spark SQL ) abstraction. apply resolves ( replaces ) DeltaDelete logical commands (in a logical query plan) into DeleteCommand s. toCommand \u00b6 toCommand ( d : DeltaDelete ): DeleteCommand toCommand ...FIXME","title":"PreprocessTableDelete"},{"location":"PreprocessTableDelete/#preprocesstabledelete-logical-resolution-rule","text":"PreprocessTableDelete is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to resolve DeltaDelete commands in a logical query plan into DeleteCommand s. PreprocessTableDelete is installed (injected) into a SparkSession using DeltaSparkSessionExtension .","title":"PreprocessTableDelete Logical Resolution Rule"},{"location":"PreprocessTableDelete/#creating-instance","text":"PreprocessTableDelete takes the following to be created: SQLConf ( Spark SQL ) PreprocessTableDelete is created when: DeltaSparkSessionExtension is executed (and registers Delta SQL support)","title":"Creating Instance"},{"location":"PreprocessTableDelete/#executing-rule","text":"apply ( plan : LogicalPlan ): LogicalPlan apply is part of the Rule ( Spark SQL ) abstraction. apply resolves ( replaces ) DeltaDelete logical commands (in a logical query plan) into DeleteCommand s.","title":" Executing Rule"},{"location":"PreprocessTableDelete/#tocommand","text":"toCommand ( d : DeltaDelete ): DeleteCommand toCommand ...FIXME","title":" toCommand"},{"location":"PreprocessTableMerge/","text":"PreprocessTableMerge Logical Resolution Rule \u00b6 PreprocessTableMerge is a post-hoc logical resolution rule ( Spark SQL ) to resolve DeltaMergeInto logical commands (in a logical query plan) into MergeIntoCommand s. PreprocessTableMerge is injected ( installed ) into a SparkSession using DeltaSparkSessionExtension . Creating Instance \u00b6 PreprocessTableMerge takes the following to be created: SQLConf ( Spark SQL ) PreprocessTableMerge is created when: DeltaSparkSessionExtension is requested to register Delta SQL support DeltaMergeBuilder is requested to execute Executing Rule \u00b6 apply ( plan : LogicalPlan ): LogicalPlan apply is part of the Rule ( Spark SQL ) abstraction. In summary, apply resolves ( replaces ) DeltaMergeInto logical commands (in a logical query plan) into corresponding MergeIntoCommand s. Internally, apply ...FIXME","title":"PreprocessTableMerge"},{"location":"PreprocessTableMerge/#preprocesstablemerge-logical-resolution-rule","text":"PreprocessTableMerge is a post-hoc logical resolution rule ( Spark SQL ) to resolve DeltaMergeInto logical commands (in a logical query plan) into MergeIntoCommand s. PreprocessTableMerge is injected ( installed ) into a SparkSession using DeltaSparkSessionExtension .","title":"PreprocessTableMerge Logical Resolution Rule"},{"location":"PreprocessTableMerge/#creating-instance","text":"PreprocessTableMerge takes the following to be created: SQLConf ( Spark SQL ) PreprocessTableMerge is created when: DeltaSparkSessionExtension is requested to register Delta SQL support DeltaMergeBuilder is requested to execute","title":"Creating Instance"},{"location":"PreprocessTableMerge/#executing-rule","text":"apply ( plan : LogicalPlan ): LogicalPlan apply is part of the Rule ( Spark SQL ) abstraction. In summary, apply resolves ( replaces ) DeltaMergeInto logical commands (in a logical query plan) into corresponding MergeIntoCommand s. Internally, apply ...FIXME","title":" Executing Rule"},{"location":"PreprocessTableUpdate/","text":"PreprocessTableUpdate Logical Resolution Rule \u00b6 PreprocessTableUpdate is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to resolve DeltaUpdateTable commands in a logical query plan into UpdateCommand s. PreprocessTableUpdate is installed (injected) into a SparkSession using DeltaSparkSessionExtension . Creating Instance \u00b6 PreprocessTableUpdate takes the following to be created: SQLConf ( Spark SQL ) PreprocessTableUpdate is created when: DeltaSparkSessionExtension is executed (and registers Delta SQL support) Executing Rule \u00b6 apply ( plan : LogicalPlan ): LogicalPlan apply is part of the Rule ( Spark SQL ) abstraction. apply resolves ( replaces ) DeltaUpdateTable logical commands (in a logical query plan) into UpdateCommand s. toCommand \u00b6 toCommand ( update : DeltaUpdateTable ): UpdateCommand toCommand ...FIXME","title":"PreprocessTableUpdate"},{"location":"PreprocessTableUpdate/#preprocesstableupdate-logical-resolution-rule","text":"PreprocessTableUpdate is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to resolve DeltaUpdateTable commands in a logical query plan into UpdateCommand s. PreprocessTableUpdate is installed (injected) into a SparkSession using DeltaSparkSessionExtension .","title":"PreprocessTableUpdate Logical Resolution Rule"},{"location":"PreprocessTableUpdate/#creating-instance","text":"PreprocessTableUpdate takes the following to be created: SQLConf ( Spark SQL ) PreprocessTableUpdate is created when: DeltaSparkSessionExtension is executed (and registers Delta SQL support)","title":"Creating Instance"},{"location":"PreprocessTableUpdate/#executing-rule","text":"apply ( plan : LogicalPlan ): LogicalPlan apply is part of the Rule ( Spark SQL ) abstraction. apply resolves ( replaces ) DeltaUpdateTable logical commands (in a logical query plan) into UpdateCommand s.","title":" Executing Rule"},{"location":"PreprocessTableUpdate/#tocommand","text":"toCommand ( update : DeltaUpdateTable ): UpdateCommand toCommand ...FIXME","title":" toCommand"},{"location":"Protocol/","text":"Protocol \u00b6 Protocol is an Action . Creating Instance \u00b6 Protocol takes the following to be created: Minimum Reader Version Allowed (default: 1 ) Minimum Writer Version Allowed (default: 3 ) Protocol is created when: DeltaTable is requested to upgradeTableProtocol FIXME forNewTable Utility \u00b6 forNewTable ( spark : SparkSession , metadata : Metadata ): Protocol forNewTable creates a new Protocol for the given SparkSession and Metadata . forNewTable is used when: OptimisticTransactionImpl is requested to updateMetadata and updateMetadataForNewTable InitialSnapshot is requested to computedState apply \u00b6 apply ( spark : SparkSession , metadataOpt : Option [ Metadata ]): Protocol apply ...FIXME checkProtocolRequirements Utility \u00b6 checkProtocolRequirements ( spark : SparkSession , metadata : Metadata , current : Protocol ): Option [ Protocol ] checkProtocolRequirements ...FIXME checkProtocolRequirements is used when: OptimisticTransactionImpl is requested to verifyNewMetadata requiredMinimumProtocol Utility \u00b6 requiredMinimumProtocol ( spark : SparkSession , metadata : Metadata ): ( Protocol , Seq [ String ]) requiredMinimumProtocol ...FIXME requiredMinimumProtocol is used when: Protocol utility is used to create a Protocol and checkProtocolRequirements","title":"Protocol"},{"location":"Protocol/#protocol","text":"Protocol is an Action .","title":"Protocol"},{"location":"Protocol/#creating-instance","text":"Protocol takes the following to be created: Minimum Reader Version Allowed (default: 1 ) Minimum Writer Version Allowed (default: 3 ) Protocol is created when: DeltaTable is requested to upgradeTableProtocol FIXME","title":"Creating Instance"},{"location":"Protocol/#fornewtable-utility","text":"forNewTable ( spark : SparkSession , metadata : Metadata ): Protocol forNewTable creates a new Protocol for the given SparkSession and Metadata . forNewTable is used when: OptimisticTransactionImpl is requested to updateMetadata and updateMetadataForNewTable InitialSnapshot is requested to computedState","title":" forNewTable Utility"},{"location":"Protocol/#apply","text":"apply ( spark : SparkSession , metadataOpt : Option [ Metadata ]): Protocol apply ...FIXME","title":" apply"},{"location":"Protocol/#checkprotocolrequirements-utility","text":"checkProtocolRequirements ( spark : SparkSession , metadata : Metadata , current : Protocol ): Option [ Protocol ] checkProtocolRequirements ...FIXME checkProtocolRequirements is used when: OptimisticTransactionImpl is requested to verifyNewMetadata","title":" checkProtocolRequirements Utility"},{"location":"Protocol/#requiredminimumprotocol-utility","text":"requiredMinimumProtocol ( spark : SparkSession , metadata : Metadata ): ( Protocol , Seq [ String ]) requiredMinimumProtocol ...FIXME requiredMinimumProtocol is used when: Protocol utility is used to create a Protocol and checkProtocolRequirements","title":" requiredMinimumProtocol Utility"},{"location":"ReadChecksum/","text":"ReadChecksum \u00b6 ReadChecksum is...FIXME","title":"ReadChecksum"},{"location":"ReadChecksum/#readchecksum","text":"ReadChecksum is...FIXME","title":"ReadChecksum"},{"location":"RemoveFile/","text":"RemoveFile \u00b6 RemoveFile is a FileAction that represents an action of removing ( deleting ) a file from a delta table. Creating Instance \u00b6 RemoveFile takes the following to be created: Path Deletion Timestamp (optional) dataChange flag extendedFileMetadata flag (default: false ) Partition values (default: null ) Size (in bytes) (default: 0 ) Tags ( Map[String, String] ) (default: null ) RemoveFile is created when: AddFile action is requested to removeWithTimestamp","title":"RemoveFile"},{"location":"RemoveFile/#removefile","text":"RemoveFile is a FileAction that represents an action of removing ( deleting ) a file from a delta table.","title":"RemoveFile"},{"location":"RemoveFile/#creating-instance","text":"RemoveFile takes the following to be created: Path Deletion Timestamp (optional) dataChange flag extendedFileMetadata flag (default: false ) Partition values (default: null ) Size (in bytes) (default: 0 ) Tags ( Map[String, String] ) (default: null ) RemoveFile is created when: AddFile action is requested to removeWithTimestamp","title":"Creating Instance"},{"location":"S3SingleDriverLogStore/","text":"S3SingleDriverLogStore \u00b6 S3SingleDriverLogStore is a HadoopFileSystemLogStore . Creating Instance \u00b6 S3SingleDriverLogStore takes the following to be created: SparkConf ( Apache Spark ) Hadoop Configuration","title":"S3SingleDriverLogStore"},{"location":"S3SingleDriverLogStore/#s3singledriverlogstore","text":"S3SingleDriverLogStore is a HadoopFileSystemLogStore .","title":"S3SingleDriverLogStore"},{"location":"S3SingleDriverLogStore/#creating-instance","text":"S3SingleDriverLogStore takes the following to be created: SparkConf ( Apache Spark ) Hadoop Configuration","title":"Creating Instance"},{"location":"SQLMetricsReporting/","text":"SQLMetricsReporting \u00b6 SQLMetricsReporting is an extension for OptimisticTransactionImpl to track SQL metrics of Operations . Implementations \u00b6 OptimisticTransactionImpl operationSQLMetrics Registry \u00b6 operationSQLMetrics ( spark : SparkSession , metrics : Map [ String , SQLMetric ]): Unit operationSQLMetrics ...FIXME operationSQLMetrics is used when...FIXME registerSQLMetrics \u00b6 registerSQLMetrics ( spark : SparkSession , metrics : Map [ String , SQLMetric ]): Unit registerSQLMetrics ...FIXME registerSQLMetrics is used when...FIXME Operation Metrics \u00b6 getMetricsForOperation ( operation : Operation ): Map [ String , String ] getMetricsForOperation requests the given Operation to transform the operation metrics . getMetricsForOperation is used when: OptimisticTransactionImpl is requested to getOperationMetrics","title":"SQLMetricsReporting"},{"location":"SQLMetricsReporting/#sqlmetricsreporting","text":"SQLMetricsReporting is an extension for OptimisticTransactionImpl to track SQL metrics of Operations .","title":"SQLMetricsReporting"},{"location":"SQLMetricsReporting/#implementations","text":"OptimisticTransactionImpl","title":"Implementations"},{"location":"SQLMetricsReporting/#operationsqlmetrics-registry","text":"operationSQLMetrics ( spark : SparkSession , metrics : Map [ String , SQLMetric ]): Unit operationSQLMetrics ...FIXME operationSQLMetrics is used when...FIXME","title":" operationSQLMetrics Registry"},{"location":"SQLMetricsReporting/#registersqlmetrics","text":"registerSQLMetrics ( spark : SparkSession , metrics : Map [ String , SQLMetric ]): Unit registerSQLMetrics ...FIXME registerSQLMetrics is used when...FIXME","title":" registerSQLMetrics"},{"location":"SQLMetricsReporting/#operation-metrics","text":"getMetricsForOperation ( operation : Operation ): Map [ String , String ] getMetricsForOperation requests the given Operation to transform the operation metrics . getMetricsForOperation is used when: OptimisticTransactionImpl is requested to getOperationMetrics","title":" Operation Metrics"},{"location":"SchemaUtils/","text":"SchemaUtils Utility \u00b6 mergeSchemas \u00b6 mergeSchemas ( tableSchema : StructType , dataSchema : StructType , allowImplicitConversions : Boolean = false ): StructType mergeSchemas ...FIXME mergeSchemas is used when: PreprocessTableMerge logical resolution rule is executed ConvertToDeltaCommand is executed ImplicitMetadataOperation is requested to update metadata","title":"SchemaUtils"},{"location":"SchemaUtils/#schemautils-utility","text":"","title":"SchemaUtils Utility"},{"location":"SchemaUtils/#mergeschemas","text":"mergeSchemas ( tableSchema : StructType , dataSchema : StructType , allowImplicitConversions : Boolean = false ): StructType mergeSchemas ...FIXME mergeSchemas is used when: PreprocessTableMerge logical resolution rule is executed ConvertToDeltaCommand is executed ImplicitMetadataOperation is requested to update metadata","title":" mergeSchemas"},{"location":"SetTransaction/","text":"SetTransaction \u00b6 SetTransaction is an Action defined by the following properties: Application ID (i.e. streaming query ID) Version (i.e. micro-batch ID) Last Updated (optional) (i.e. milliseconds since the epoch) SetTransaction is created when: DeltaSink is requested to add a streaming micro-batch (for STREAMING UPDATE operation idempotence at query restart) Demo \u00b6 val path = \"/tmp/delta/users\" import org . apache . spark . sql . delta . DeltaLog val deltaLog = DeltaLog . forTable ( spark , path ) import org . apache . spark . sql . delta . actions . SetTransaction assert ( deltaLog . snapshot . setTransactions . isInstanceOf [ Seq [ SetTransaction ]]) deltaLog . snapshot . setTransactions","title":"SetTransaction"},{"location":"SetTransaction/#settransaction","text":"SetTransaction is an Action defined by the following properties: Application ID (i.e. streaming query ID) Version (i.e. micro-batch ID) Last Updated (optional) (i.e. milliseconds since the epoch) SetTransaction is created when: DeltaSink is requested to add a streaming micro-batch (for STREAMING UPDATE operation idempotence at query restart)","title":"SetTransaction"},{"location":"SetTransaction/#demo","text":"val path = \"/tmp/delta/users\" import org . apache . spark . sql . delta . DeltaLog val deltaLog = DeltaLog . forTable ( spark , path ) import org . apache . spark . sql . delta . actions . SetTransaction assert ( deltaLog . snapshot . setTransactions . isInstanceOf [ Seq [ SetTransaction ]]) deltaLog . snapshot . setTransactions","title":"Demo"},{"location":"SingleAction/","text":"SingleAction \u00b6 SingleAction is...FIXME","title":"SingleAction"},{"location":"SingleAction/#singleaction","text":"SingleAction is...FIXME","title":"SingleAction"},{"location":"Snapshot/","text":"Snapshot \u00b6 Snapshot is an immutable snapshot of the state of the Delta table at the version . Tip Use Demo: DeltaTable, DeltaLog And Snapshots to learn more. Creating Instance \u00b6 Snapshot takes the following to be created: Hadoop Path to the log directory Version LogSegment minFileRetentionTimestamp (that is exactly DeltaLog.minFileRetentionTimestamp ) DeltaLog Timestamp Optional VersionChecksum While being created, Snapshot prints out the following INFO message to the logs and initialize : Created snapshot [this] Snapshot is created when SnapshotManagement is requested for one . Initializing \u00b6 init (): Unit init requests the DeltaLog for the protocolRead for the Protocol . Computed State \u00b6 computedState : State Scala lazy value computedState is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val computedState: State computedState takes the current cached set of actions and reads the latest state (executes a state.select(...).first() query). Note The state.select(...).first() query uses aggregate standard functions (e.g. last , collect_set , sum , count ) and so uses groupBy over the whole dataset indirectly. computedState assumes that the protocol and metadata (actions) are defined. computedState throws an IllegalStateException when the actions are not defined and spark.databricks.delta.stateReconstructionValidation.enabled configuration property is enabled. The [action] of your Delta table couldn't be recovered while Reconstructing version: [version]. Did you manually delete files in the _delta_log directory? Note The state.select(...).first() query uses last with ignoreNulls flag true and so may give no rows for first() . computedState makes sure that the State to be returned has at least the default protocol and metadata (actions) defined. Configuration Properties \u00b6 spark.databricks.delta.snapshotPartitions \u00b6 Snapshot uses the spark.databricks.delta.snapshotPartitions configuration property for the number of partitions to use for state reconstruction . spark.databricks.delta.stateReconstructionValidation.enabled \u00b6 Snapshot uses the spark.databricks.delta.stateReconstructionValidation.enabled configuration property for reconstructing state . State Dataset (of Actions) \u00b6 state : Dataset [ SingleAction ] state requests the cached delta table state for the current state (from the cache) . state is used when: Checkpoints utility is used to writeCheckpoint Snapshot is requested for computedState , all files and files removed (tombstones) VacuumCommand utility is requested for garbage collection Cached Delta Table State \u00b6 lazy val cachedState : CachedDS [ SingleAction ] cachedState is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. cachedState creates a Cached Delta State with the following: The dataset part is the stateReconstruction dataset of SingleAction s The name in the format Delta Table State #version - [redactedPath] (with the version and the path redacted) All AddFiles \u00b6 allFiles : Dataset [ AddFile ] allFiles simply takes the state dataset and selects AddFiles (adds where clause for add IS NOT NULL and select over the fields of AddFiles ). Note allFiles simply adds where and select clauses. No computation happens yet as it is (a description of) a distributed computation as a Dataset[AddFile] . import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, \"/tmp/delta/users\") val files = deltaLog.snapshot.allFiles scala> :type files org.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.AddFile] scala> files.show(truncate = false) +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ |path |partitionValues|size|modificationTime|dataChange|stats|tags| +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ |part-00000-4050db39-e0f5-485d-ab3b-3ca72307f621-c000.snappy.parquet|[] |262 |1578083748000 |false |null |null| |part-00000-ba39f292-2970-4528-a40c-8f0aa5f796de-c000.snappy.parquet|[] |262 |1578083570000 |false |null |null| |part-00003-99f9d902-24a7-4f76-a15a-6971940bc245-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00007-03d987f1-5bb3-4b5b-8db9-97b6667107e2-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00011-a759a8c2-507d-46dd-9da7-dc722316214b-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00015-2e685d29-25ed-4262-90a7-5491847fd8d0-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00015-ee0ac1af-e1e0-4422-8245-12da91ced0a2-c000.snappy.parquet|[] |429 |1578083570000 |false |null |null| +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ allFiles is used when: PartitionFiltering is requested for the files to scan (matching projection attributes and predicates) DeltaSourceSnapshot is requested for the initial files (indexed AddFiles ) GenerateSymlinkManifestImpl is requested to generateIncrementalManifest and generateFullManifest DeltaDataSource is requested for an Insertable HadoopFsRelation stateReconstruction Dataset of Actions \u00b6 stateReconstruction : Dataset [ SingleAction ] Note stateReconstruction returns a Dataset[SingleAction] and so does not do any computation per se. stateReconstruction is a Dataset of SingleActions (that is the dataset part) of the cachedState . stateReconstruction loads the log file indices (that gives a Dataset[SingleAction] ). stateReconstruction maps over partitions (using Dataset.mapPartitions ) and canonicalize the paths for AddFile and RemoveFile actions. stateReconstruction adds file column that uses a UDF to assert that input_file_name() belongs to the Delta table. Note This UDF-based check is very clever. stateReconstruction repartitions the Dataset using the path of add or remove actions (with the configurable number of partitions ) and Dataset.sortWithinPartitions by the file column. In the end, stateReconstruction maps over partitions (using Dataset.mapPartitions ) that creates a InMemoryLogReplay , requests it to append the actions (as version 0 ) and checkpoint . stateReconstruction is used when: Snapshot is requested for a cached Delta table state Loading Actions \u00b6 loadActions : Dataset [ SingleAction ] loadActions creates a union of Dataset[SingleAction] s for the indices (as LogicalRelation s over a HadoopFsRelation ) or defaults to an empty dataset . indexToRelation \u00b6 indexToRelation ( index : DeltaLogFileIndex , schema : StructType = logSchema ): LogicalRelation indexToRelation converts the DeltaLogFileIndex to a LogicalRelation ( Spark SQL ) leaf logical operator (using the logSchema ). indexToRelation creates a LogicalRelation over a HadoopFsRelation ( Spark SQL ) with the given index and the schema. emptyActions Dataset (of Actions) \u00b6 emptyActions : Dataset [ SingleAction ] emptyActions is an empty dataset of SingleActions for loadActions (and InitialSnapshot 's state ). fileIndices \u00b6 fileIndices : Seq [ DeltaLogFileIndex ] Scala lazy value fileIndices is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val fileIndices: Seq[DeltaLogFileIndex] fileIndices is a collection of the checkpointFileIndexOpt and the deltaFileIndexOpt (if they are available). Commit File Index \u00b6 deltaFileIndexOpt : Option [ DeltaLogFileIndex ] Scala lazy value deltaFileIndexOpt is a Scala lazy value and is initialized once when first accessed. Once computed, it stays unchanged for the Snapshot instance. lazy val deltaFileIndexOpt: Option[DeltaLogFileIndex] deltaFileIndexOpt is a DeltaLogFileIndex (in JsonFileFormat ) for the checkpoint file of the LogSegment . Checkpoint File Index \u00b6 checkpointFileIndexOpt : Option [ DeltaLogFileIndex ] Scala lazy value checkpointFileIndexOpt is a Scala lazy value and is initialized once when first accessed. Once computed, it stays unchanged for the Snapshot instance. lazy val checkpointFileIndexOpt: Option[DeltaLogFileIndex] checkpointFileIndexOpt is a DeltaLogFileIndex (in ParquetFileFormat ) for the delta files of the LogSegment . Transaction Version By App ID \u00b6 transactions : Map [ String , Long ] transactions takes the SetTransaction actions (from the state dataset) and makes them a lookup table of transaction version by appId . Scala lazy value transactions is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val transactions: Map[String, Long] transactions is used when OptimisticTransactionImpl is requested for the transaction version for a given (streaming query) id . All RemoveFile Actions (Tombstones) \u00b6 tombstones : Dataset [ RemoveFile ] tombstones ...FIXME scala> deltaLog.snapshot.tombstones.show(false) +----+-----------------+----------+ |path|deletionTimestamp|dataChange| +----+-----------------+----------+ +----+-----------------+----------+","title":"Snapshot"},{"location":"Snapshot/#snapshot","text":"Snapshot is an immutable snapshot of the state of the Delta table at the version . Tip Use Demo: DeltaTable, DeltaLog And Snapshots to learn more.","title":"Snapshot"},{"location":"Snapshot/#creating-instance","text":"Snapshot takes the following to be created: Hadoop Path to the log directory Version LogSegment minFileRetentionTimestamp (that is exactly DeltaLog.minFileRetentionTimestamp ) DeltaLog Timestamp Optional VersionChecksum While being created, Snapshot prints out the following INFO message to the logs and initialize : Created snapshot [this] Snapshot is created when SnapshotManagement is requested for one .","title":"Creating Instance"},{"location":"Snapshot/#initializing","text":"init (): Unit init requests the DeltaLog for the protocolRead for the Protocol .","title":" Initializing"},{"location":"Snapshot/#computed-state","text":"computedState : State Scala lazy value computedState is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val computedState: State computedState takes the current cached set of actions and reads the latest state (executes a state.select(...).first() query). Note The state.select(...).first() query uses aggregate standard functions (e.g. last , collect_set , sum , count ) and so uses groupBy over the whole dataset indirectly. computedState assumes that the protocol and metadata (actions) are defined. computedState throws an IllegalStateException when the actions are not defined and spark.databricks.delta.stateReconstructionValidation.enabled configuration property is enabled. The [action] of your Delta table couldn't be recovered while Reconstructing version: [version]. Did you manually delete files in the _delta_log directory? Note The state.select(...).first() query uses last with ignoreNulls flag true and so may give no rows for first() . computedState makes sure that the State to be returned has at least the default protocol and metadata (actions) defined.","title":" Computed State"},{"location":"Snapshot/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"Snapshot/#sparkdatabricksdeltasnapshotpartitions","text":"Snapshot uses the spark.databricks.delta.snapshotPartitions configuration property for the number of partitions to use for state reconstruction .","title":" spark.databricks.delta.snapshotPartitions"},{"location":"Snapshot/#sparkdatabricksdeltastatereconstructionvalidationenabled","text":"Snapshot uses the spark.databricks.delta.stateReconstructionValidation.enabled configuration property for reconstructing state .","title":"spark.databricks.delta.stateReconstructionValidation.enabled"},{"location":"Snapshot/#state-dataset-of-actions","text":"state : Dataset [ SingleAction ] state requests the cached delta table state for the current state (from the cache) . state is used when: Checkpoints utility is used to writeCheckpoint Snapshot is requested for computedState , all files and files removed (tombstones) VacuumCommand utility is requested for garbage collection","title":" State Dataset (of Actions)"},{"location":"Snapshot/#cached-delta-table-state","text":"lazy val cachedState : CachedDS [ SingleAction ] cachedState is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. cachedState creates a Cached Delta State with the following: The dataset part is the stateReconstruction dataset of SingleAction s The name in the format Delta Table State #version - [redactedPath] (with the version and the path redacted)","title":" Cached Delta Table State"},{"location":"Snapshot/#all-addfiles","text":"allFiles : Dataset [ AddFile ] allFiles simply takes the state dataset and selects AddFiles (adds where clause for add IS NOT NULL and select over the fields of AddFiles ). Note allFiles simply adds where and select clauses. No computation happens yet as it is (a description of) a distributed computation as a Dataset[AddFile] . import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, \"/tmp/delta/users\") val files = deltaLog.snapshot.allFiles scala> :type files org.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.AddFile] scala> files.show(truncate = false) +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ |path |partitionValues|size|modificationTime|dataChange|stats|tags| +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ |part-00000-4050db39-e0f5-485d-ab3b-3ca72307f621-c000.snappy.parquet|[] |262 |1578083748000 |false |null |null| |part-00000-ba39f292-2970-4528-a40c-8f0aa5f796de-c000.snappy.parquet|[] |262 |1578083570000 |false |null |null| |part-00003-99f9d902-24a7-4f76-a15a-6971940bc245-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00007-03d987f1-5bb3-4b5b-8db9-97b6667107e2-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00011-a759a8c2-507d-46dd-9da7-dc722316214b-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00015-2e685d29-25ed-4262-90a7-5491847fd8d0-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00015-ee0ac1af-e1e0-4422-8245-12da91ced0a2-c000.snappy.parquet|[] |429 |1578083570000 |false |null |null| +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ allFiles is used when: PartitionFiltering is requested for the files to scan (matching projection attributes and predicates) DeltaSourceSnapshot is requested for the initial files (indexed AddFiles ) GenerateSymlinkManifestImpl is requested to generateIncrementalManifest and generateFullManifest DeltaDataSource is requested for an Insertable HadoopFsRelation","title":" All AddFiles"},{"location":"Snapshot/#statereconstruction-dataset-of-actions","text":"stateReconstruction : Dataset [ SingleAction ] Note stateReconstruction returns a Dataset[SingleAction] and so does not do any computation per se. stateReconstruction is a Dataset of SingleActions (that is the dataset part) of the cachedState . stateReconstruction loads the log file indices (that gives a Dataset[SingleAction] ). stateReconstruction maps over partitions (using Dataset.mapPartitions ) and canonicalize the paths for AddFile and RemoveFile actions. stateReconstruction adds file column that uses a UDF to assert that input_file_name() belongs to the Delta table. Note This UDF-based check is very clever. stateReconstruction repartitions the Dataset using the path of add or remove actions (with the configurable number of partitions ) and Dataset.sortWithinPartitions by the file column. In the end, stateReconstruction maps over partitions (using Dataset.mapPartitions ) that creates a InMemoryLogReplay , requests it to append the actions (as version 0 ) and checkpoint . stateReconstruction is used when: Snapshot is requested for a cached Delta table state","title":" stateReconstruction Dataset of Actions"},{"location":"Snapshot/#loading-actions","text":"loadActions : Dataset [ SingleAction ] loadActions creates a union of Dataset[SingleAction] s for the indices (as LogicalRelation s over a HadoopFsRelation ) or defaults to an empty dataset .","title":" Loading Actions"},{"location":"Snapshot/#indextorelation","text":"indexToRelation ( index : DeltaLogFileIndex , schema : StructType = logSchema ): LogicalRelation indexToRelation converts the DeltaLogFileIndex to a LogicalRelation ( Spark SQL ) leaf logical operator (using the logSchema ). indexToRelation creates a LogicalRelation over a HadoopFsRelation ( Spark SQL ) with the given index and the schema.","title":" indexToRelation"},{"location":"Snapshot/#emptyactions-dataset-of-actions","text":"emptyActions : Dataset [ SingleAction ] emptyActions is an empty dataset of SingleActions for loadActions (and InitialSnapshot 's state ).","title":" emptyActions Dataset (of Actions)"},{"location":"Snapshot/#fileindices","text":"fileIndices : Seq [ DeltaLogFileIndex ] Scala lazy value fileIndices is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val fileIndices: Seq[DeltaLogFileIndex] fileIndices is a collection of the checkpointFileIndexOpt and the deltaFileIndexOpt (if they are available).","title":" fileIndices"},{"location":"Snapshot/#commit-file-index","text":"deltaFileIndexOpt : Option [ DeltaLogFileIndex ] Scala lazy value deltaFileIndexOpt is a Scala lazy value and is initialized once when first accessed. Once computed, it stays unchanged for the Snapshot instance. lazy val deltaFileIndexOpt: Option[DeltaLogFileIndex] deltaFileIndexOpt is a DeltaLogFileIndex (in JsonFileFormat ) for the checkpoint file of the LogSegment .","title":" Commit File Index"},{"location":"Snapshot/#checkpoint-file-index","text":"checkpointFileIndexOpt : Option [ DeltaLogFileIndex ] Scala lazy value checkpointFileIndexOpt is a Scala lazy value and is initialized once when first accessed. Once computed, it stays unchanged for the Snapshot instance. lazy val checkpointFileIndexOpt: Option[DeltaLogFileIndex] checkpointFileIndexOpt is a DeltaLogFileIndex (in ParquetFileFormat ) for the delta files of the LogSegment .","title":" Checkpoint File Index"},{"location":"Snapshot/#transaction-version-by-app-id","text":"transactions : Map [ String , Long ] transactions takes the SetTransaction actions (from the state dataset) and makes them a lookup table of transaction version by appId . Scala lazy value transactions is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val transactions: Map[String, Long] transactions is used when OptimisticTransactionImpl is requested for the transaction version for a given (streaming query) id .","title":" Transaction Version By App ID"},{"location":"Snapshot/#all-removefile-actions-tombstones","text":"tombstones : Dataset [ RemoveFile ] tombstones ...FIXME scala> deltaLog.snapshot.tombstones.show(false) +----+-----------------+----------+ |path|deletionTimestamp|dataChange| +----+-----------------+----------+ +----+-----------------+----------+","title":" All RemoveFile Actions (Tombstones)"},{"location":"SnapshotIterator/","text":"SnapshotIterator \u00b6 SnapshotIterator is an abstraction of iterators over indexed AddFile actions in a Delta log for DeltaSourceSnapshot s. Iterator of Indexed AddFiles \u00b6 iterator (): Iterator [ IndexedFile ] iterator returns an Iterator ( Scala ) of IndexedFile s ( AddFile actions in a Delta log with extra metadata) of filterFileList . iterator is used when: DeltaSource is requested for the snapshot of a delta table at a given version Closing Iterator (Cleaning Up Internal Resources) \u00b6 close ( unpersistSnapshot : Boolean ): Unit close is a no-op (and leaves proper operation to implementations ). close is used when: DeltaSource is requested to cleanUpSnapshotResources Implementations \u00b6 DeltaSourceSnapshot","title":"SnapshotIterator"},{"location":"SnapshotIterator/#snapshotiterator","text":"SnapshotIterator is an abstraction of iterators over indexed AddFile actions in a Delta log for DeltaSourceSnapshot s.","title":"SnapshotIterator"},{"location":"SnapshotIterator/#iterator-of-indexed-addfiles","text":"iterator (): Iterator [ IndexedFile ] iterator returns an Iterator ( Scala ) of IndexedFile s ( AddFile actions in a Delta log with extra metadata) of filterFileList . iterator is used when: DeltaSource is requested for the snapshot of a delta table at a given version","title":" Iterator of Indexed AddFiles"},{"location":"SnapshotIterator/#closing-iterator-cleaning-up-internal-resources","text":"close ( unpersistSnapshot : Boolean ): Unit close is a no-op (and leaves proper operation to implementations ). close is used when: DeltaSource is requested to cleanUpSnapshotResources","title":" Closing Iterator (Cleaning Up Internal Resources)"},{"location":"SnapshotIterator/#implementations","text":"DeltaSourceSnapshot","title":"Implementations"},{"location":"SnapshotManagement/","text":"SnapshotManagement \u00b6 SnapshotManagement is an extension for DeltaLog to manage Snapshot s. Demo \u00b6 val name = \"employees\" val dataPath = s\"/tmp/delta/$name\" sql(s\"DROP TABLE $name\") sql(s\"\"\" | CREATE TABLE $name (id bigint, name string, city string) | USING delta | OPTIONS (path='$dataPath') \"\"\".stripMargin) import org.apache.spark.sql.delta.DeltaLog val log = DeltaLog.forTable(spark, dataPath) import org.apache.spark.sql.delta.SnapshotManagement assert(log.isInstanceOf[SnapshotManagement], \"DeltaLog is a SnapshotManagement\") val snapshot = log.update(stalenessAcceptable = false) scala> :type snapshot org.apache.spark.sql.delta.Snapshot assert(snapshot.version == 0) Current Snapshot \u00b6 currentSnapshot : Snapshot currentSnapshot is a registry with the current Snapshot of a Delta table. When DeltaLog is created, currentSnapshot is initialized as getSnapshotAtInit and changed every update . currentSnapshot ...FIXME currentSnapshot is used when: SnapshotManagement is requested to...FIXME Updating Current Snapshot \u00b6 update ( stalenessAcceptable : Boolean = false ): Snapshot update ...FIXME update is used when: DeltaLog is requested to start a transaction and withNewTransaction OptimisticTransactionImpl is requested to doCommit and getNextAttemptVersion DeltaTableV2 is requested for a Snapshot TahoeLogFileIndex is requested for a Snapshot DeltaHistoryManager is requested to getHistory , getActiveCommitAtTime , checkVersionExists In Delta commands ... tryUpdate \u00b6 tryUpdate ( isAsync : Boolean = false ): Snapshot tryUpdate ...FIXME tryUpdate is used when SnapshotManagement is requested to update . updateInternal \u00b6 updateInternal ( isAsync : Boolean ): Snapshot updateInternal ...FIXME updateInternal is used when SnapshotManagement is requested to update (and tryUpdate ). Loading Latest Snapshot \u00b6 getSnapshotAtInit : Snapshot getSnapshotAtInit getLogSegmentFrom for the last checkpoint . getSnapshotAtInit prints out the following INFO message to the logs: Loading version [version][startCheckpoint] getSnapshotAtInit creates a Snapshot for the log segment. getSnapshotAtInit records the current time in lastUpdateTimestamp registry. getSnapshotAtInit prints out the following INFO message to the logs: Returning initial snapshot [snapshot] getSnapshotAtInit is used when SnapshotManagement is created (and initializes the currentSnapshot registry). getLogSegmentFrom \u00b6 getLogSegmentFrom ( startingCheckpoint : Option [ CheckpointMetaData ]): LogSegment getLogSegmentFrom getLogSegmentForVersion for the version of the given CheckpointMetaData (if specified) as a start checkpoint version or leaves it undefined. getLogSegmentFrom is used when SnapshotManagement is requested for getSnapshotAtInit . getLogSegmentForVersion \u00b6 getLogSegmentForVersion ( startCheckpoint : Option [ Long ], versionToLoad : Option [ Long ] = None ): LogSegment getLogSegmentForVersion ...FIXME getLogSegmentForVersion is used when SnapshotManagement is requested for getLogSegmentFrom , updateInternal and getSnapshotAt . listFrom \u00b6 listFrom ( startVersion : Long ): Iterator [ FileStatus ] listFrom ...FIXME Creating Snapshot \u00b6 createSnapshot ( segment : LogSegment , minFileRetentionTimestamp : Long , timestamp : Long ): Snapshot createSnapshot readChecksum (for the version of the given LogSegment ) and creates a Snapshot . createSnapshot is used when SnapshotManagement is requested for getSnapshotAtInit , getSnapshotAt and update . Last Successful Update Timestamp \u00b6 SnapshotManagement uses lastUpdateTimestamp internal registry for the timestamp of the last successful update.","title":"SnapshotManagement"},{"location":"SnapshotManagement/#snapshotmanagement","text":"SnapshotManagement is an extension for DeltaLog to manage Snapshot s.","title":"SnapshotManagement"},{"location":"SnapshotManagement/#demo","text":"val name = \"employees\" val dataPath = s\"/tmp/delta/$name\" sql(s\"DROP TABLE $name\") sql(s\"\"\" | CREATE TABLE $name (id bigint, name string, city string) | USING delta | OPTIONS (path='$dataPath') \"\"\".stripMargin) import org.apache.spark.sql.delta.DeltaLog val log = DeltaLog.forTable(spark, dataPath) import org.apache.spark.sql.delta.SnapshotManagement assert(log.isInstanceOf[SnapshotManagement], \"DeltaLog is a SnapshotManagement\") val snapshot = log.update(stalenessAcceptable = false) scala> :type snapshot org.apache.spark.sql.delta.Snapshot assert(snapshot.version == 0)","title":"Demo"},{"location":"SnapshotManagement/#current-snapshot","text":"currentSnapshot : Snapshot currentSnapshot is a registry with the current Snapshot of a Delta table. When DeltaLog is created, currentSnapshot is initialized as getSnapshotAtInit and changed every update . currentSnapshot ...FIXME currentSnapshot is used when: SnapshotManagement is requested to...FIXME","title":" Current Snapshot"},{"location":"SnapshotManagement/#updating-current-snapshot","text":"update ( stalenessAcceptable : Boolean = false ): Snapshot update ...FIXME update is used when: DeltaLog is requested to start a transaction and withNewTransaction OptimisticTransactionImpl is requested to doCommit and getNextAttemptVersion DeltaTableV2 is requested for a Snapshot TahoeLogFileIndex is requested for a Snapshot DeltaHistoryManager is requested to getHistory , getActiveCommitAtTime , checkVersionExists In Delta commands ...","title":" Updating Current Snapshot"},{"location":"SnapshotManagement/#tryupdate","text":"tryUpdate ( isAsync : Boolean = false ): Snapshot tryUpdate ...FIXME tryUpdate is used when SnapshotManagement is requested to update .","title":" tryUpdate"},{"location":"SnapshotManagement/#updateinternal","text":"updateInternal ( isAsync : Boolean ): Snapshot updateInternal ...FIXME updateInternal is used when SnapshotManagement is requested to update (and tryUpdate ).","title":" updateInternal"},{"location":"SnapshotManagement/#loading-latest-snapshot","text":"getSnapshotAtInit : Snapshot getSnapshotAtInit getLogSegmentFrom for the last checkpoint . getSnapshotAtInit prints out the following INFO message to the logs: Loading version [version][startCheckpoint] getSnapshotAtInit creates a Snapshot for the log segment. getSnapshotAtInit records the current time in lastUpdateTimestamp registry. getSnapshotAtInit prints out the following INFO message to the logs: Returning initial snapshot [snapshot] getSnapshotAtInit is used when SnapshotManagement is created (and initializes the currentSnapshot registry).","title":" Loading Latest Snapshot"},{"location":"SnapshotManagement/#getlogsegmentfrom","text":"getLogSegmentFrom ( startingCheckpoint : Option [ CheckpointMetaData ]): LogSegment getLogSegmentFrom getLogSegmentForVersion for the version of the given CheckpointMetaData (if specified) as a start checkpoint version or leaves it undefined. getLogSegmentFrom is used when SnapshotManagement is requested for getSnapshotAtInit .","title":" getLogSegmentFrom"},{"location":"SnapshotManagement/#getlogsegmentforversion","text":"getLogSegmentForVersion ( startCheckpoint : Option [ Long ], versionToLoad : Option [ Long ] = None ): LogSegment getLogSegmentForVersion ...FIXME getLogSegmentForVersion is used when SnapshotManagement is requested for getLogSegmentFrom , updateInternal and getSnapshotAt .","title":" getLogSegmentForVersion"},{"location":"SnapshotManagement/#listfrom","text":"listFrom ( startVersion : Long ): Iterator [ FileStatus ] listFrom ...FIXME","title":" listFrom"},{"location":"SnapshotManagement/#creating-snapshot","text":"createSnapshot ( segment : LogSegment , minFileRetentionTimestamp : Long , timestamp : Long ): Snapshot createSnapshot readChecksum (for the version of the given LogSegment ) and creates a Snapshot . createSnapshot is used when SnapshotManagement is requested for getSnapshotAtInit , getSnapshotAt and update .","title":" Creating Snapshot"},{"location":"SnapshotManagement/#last-successful-update-timestamp","text":"SnapshotManagement uses lastUpdateTimestamp internal registry for the timestamp of the last successful update.","title":" Last Successful Update Timestamp"},{"location":"StagedDeltaTableV2/","text":"StagedDeltaTableV2 \u00b6 StagedDeltaTableV2 is a StagedTable ( Spark SQL ) and a SupportsWrite ( Spark SQL ). Creating Instance \u00b6 StagedDeltaTableV2 takes the following to be created: Identifier Schema Partitions ( Array[Transform] ) Properties Operation (one of Create , CreateOrReplace , Replace ) StagedDeltaTableV2 is created when DeltaCatalog is requested to stageReplace , stageCreateOrReplace or stageCreate . commitStagedChanges \u00b6 commitStagedChanges (): Unit commitStagedChanges ...FIXME commitStagedChanges is part of the StagedTable ( Spark SQL ) abstraction. abortStagedChanges \u00b6 abortStagedChanges (): Unit abortStagedChanges does nothing. abortStagedChanges is part of the StagedTable ( Spark SQL ) abstraction. Creating WriteBuilder \u00b6 newWriteBuilder ( info : LogicalWriteInfo ): V1WriteBuilder newWriteBuilder ...FIXME newWriteBuilder is part of the SupportsWrite ( Spark SQL ) abstraction.","title":"StagedDeltaTableV2"},{"location":"StagedDeltaTableV2/#stageddeltatablev2","text":"StagedDeltaTableV2 is a StagedTable ( Spark SQL ) and a SupportsWrite ( Spark SQL ).","title":"StagedDeltaTableV2"},{"location":"StagedDeltaTableV2/#creating-instance","text":"StagedDeltaTableV2 takes the following to be created: Identifier Schema Partitions ( Array[Transform] ) Properties Operation (one of Create , CreateOrReplace , Replace ) StagedDeltaTableV2 is created when DeltaCatalog is requested to stageReplace , stageCreateOrReplace or stageCreate .","title":"Creating Instance"},{"location":"StagedDeltaTableV2/#commitstagedchanges","text":"commitStagedChanges (): Unit commitStagedChanges ...FIXME commitStagedChanges is part of the StagedTable ( Spark SQL ) abstraction.","title":" commitStagedChanges"},{"location":"StagedDeltaTableV2/#abortstagedchanges","text":"abortStagedChanges (): Unit abortStagedChanges does nothing. abortStagedChanges is part of the StagedTable ( Spark SQL ) abstraction.","title":" abortStagedChanges"},{"location":"StagedDeltaTableV2/#creating-writebuilder","text":"newWriteBuilder ( info : LogicalWriteInfo ): V1WriteBuilder newWriteBuilder ...FIXME newWriteBuilder is part of the SupportsWrite ( Spark SQL ) abstraction.","title":" Creating WriteBuilder"},{"location":"StateCache/","text":"StateCache \u00b6 StateCache is an abstraction of state caches that can cache a Dataset and uncache them all . Contract \u00b6 SparkSession \u00b6 spark : SparkSession SparkSession the cached RDDs belong to Implementations \u00b6 DeltaSourceSnapshot Snapshot Cached RDDs \u00b6 cached : ArrayBuffer [ RDD [ _ ]] StateCache tracks cached RDDs in cached internal registry. cached is given a new RDD when StateCache is requested to cache a Dataset . cached is used when StateCache is requested to get a cached Dataset and uncache . Caching Dataset \u00b6 cacheDS [ A ]( ds : Dataset [ A ], name : String ): CachedDS [ A ] cacheDS creates a new CachedDS . cacheDS is used when: Snapshot is requested for a cached state ) DeltaSourceSnapshot is requested to initialFiles Uncaching All Cached Datasets \u00b6 uncache [ A ]( ds : Dataset [ A ], name : String ): CachedDS [ A ] uncache uses the isCached internal flag to avoid multiple executions. uncache is used when: DeltaLog utility is used to access deltaLogCache and a cached entry expires SnapshotManagement is requested to update state of a Delta table DeltaSourceSnapshot is requested to close","title":"StateCache"},{"location":"StateCache/#statecache","text":"StateCache is an abstraction of state caches that can cache a Dataset and uncache them all .","title":"StateCache"},{"location":"StateCache/#contract","text":"","title":"Contract"},{"location":"StateCache/#sparksession","text":"spark : SparkSession SparkSession the cached RDDs belong to","title":" SparkSession"},{"location":"StateCache/#implementations","text":"DeltaSourceSnapshot Snapshot","title":"Implementations"},{"location":"StateCache/#cached-rdds","text":"cached : ArrayBuffer [ RDD [ _ ]] StateCache tracks cached RDDs in cached internal registry. cached is given a new RDD when StateCache is requested to cache a Dataset . cached is used when StateCache is requested to get a cached Dataset and uncache .","title":" Cached RDDs"},{"location":"StateCache/#caching-dataset","text":"cacheDS [ A ]( ds : Dataset [ A ], name : String ): CachedDS [ A ] cacheDS creates a new CachedDS . cacheDS is used when: Snapshot is requested for a cached state ) DeltaSourceSnapshot is requested to initialFiles","title":" Caching Dataset"},{"location":"StateCache/#uncaching-all-cached-datasets","text":"uncache [ A ]( ds : Dataset [ A ], name : String ): CachedDS [ A ] uncache uses the isCached internal flag to avoid multiple executions. uncache is used when: DeltaLog utility is used to access deltaLogCache and a cached entry expires SnapshotManagement is requested to update state of a Delta table DeltaSourceSnapshot is requested to close","title":" Uncaching All Cached Datasets"},{"location":"TahoeBatchFileIndex/","text":"TahoeBatchFileIndex \u00b6 TahoeBatchFileIndex is a file index of a delta table at a given version . Creating Instance \u00b6 TahoeBatchFileIndex takes the following to be created: SparkSession ( Spark SQL ) Action Type AddFile s DeltaLog Data directory (as Hadoop Path ) Snapshot TahoeBatchFileIndex is created when: DeltaLog is requested for a DataFrame for given AddFiles DeleteCommand and UpdateCommand are executed (and DeltaCommand is requested for a HadoopFsRelation ) Action Type \u00b6 TahoeBatchFileIndex is given an Action Type identifier when created : batch or streaming when DeltaLog is requested for a batch or streaming DataFrame for given AddFiles , respectively delete for DeleteCommand update for UpdateCommand Important Action Type seems not to be used ever. tableVersion \u00b6 tableVersion : Long tableVersion is part of the TahoeFileIndex abstraction. tableVersion is always the version of the Snapshot . matchingFiles \u00b6 matchingFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ], keepStats : Boolean = false ): Seq [ AddFile ] matchingFiles is part of the TahoeFileIndex abstraction. matchingFiles filterFileList (that gives a DataFrame ) and collects the AddFile s (using Dataset.collect ). Input Files \u00b6 inputFiles : Array [ String ] inputFiles is part of the FileIndex ( Spark SQL ) abstraction. inputFiles returns the paths of all the given AddFiles . Partitions \u00b6 partitionSchema : StructType partitionSchema is part of the FileIndex ( Spark SQL ) abstraction. partitionSchema requests the Snapshot for the metadata that is in turn requested for the partitionSchema . Estimated Size of Relation \u00b6 sizeInBytes : Long sizeInBytes is part of the FileIndex ( Spark SQL ) abstraction. sizeInBytes is a sum of the sizes of all the given AddFiles .","title":"TahoeBatchFileIndex"},{"location":"TahoeBatchFileIndex/#tahoebatchfileindex","text":"TahoeBatchFileIndex is a file index of a delta table at a given version .","title":"TahoeBatchFileIndex"},{"location":"TahoeBatchFileIndex/#creating-instance","text":"TahoeBatchFileIndex takes the following to be created: SparkSession ( Spark SQL ) Action Type AddFile s DeltaLog Data directory (as Hadoop Path ) Snapshot TahoeBatchFileIndex is created when: DeltaLog is requested for a DataFrame for given AddFiles DeleteCommand and UpdateCommand are executed (and DeltaCommand is requested for a HadoopFsRelation )","title":"Creating Instance"},{"location":"TahoeBatchFileIndex/#action-type","text":"TahoeBatchFileIndex is given an Action Type identifier when created : batch or streaming when DeltaLog is requested for a batch or streaming DataFrame for given AddFiles , respectively delete for DeleteCommand update for UpdateCommand Important Action Type seems not to be used ever.","title":" Action Type"},{"location":"TahoeBatchFileIndex/#tableversion","text":"tableVersion : Long tableVersion is part of the TahoeFileIndex abstraction. tableVersion is always the version of the Snapshot .","title":" tableVersion"},{"location":"TahoeBatchFileIndex/#matchingfiles","text":"matchingFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ], keepStats : Boolean = false ): Seq [ AddFile ] matchingFiles is part of the TahoeFileIndex abstraction. matchingFiles filterFileList (that gives a DataFrame ) and collects the AddFile s (using Dataset.collect ).","title":" matchingFiles"},{"location":"TahoeBatchFileIndex/#input-files","text":"inputFiles : Array [ String ] inputFiles is part of the FileIndex ( Spark SQL ) abstraction. inputFiles returns the paths of all the given AddFiles .","title":" Input Files"},{"location":"TahoeBatchFileIndex/#partitions","text":"partitionSchema : StructType partitionSchema is part of the FileIndex ( Spark SQL ) abstraction. partitionSchema requests the Snapshot for the metadata that is in turn requested for the partitionSchema .","title":" Partitions"},{"location":"TahoeBatchFileIndex/#estimated-size-of-relation","text":"sizeInBytes : Long sizeInBytes is part of the FileIndex ( Spark SQL ) abstraction. sizeInBytes is a sum of the sizes of all the given AddFiles .","title":" Estimated Size of Relation"},{"location":"TahoeFileIndex/","text":"TahoeFileIndex \u00b6 TahoeFileIndex is an extension of the FileIndex abstraction ( Spark SQL ) for file indices of delta tables that can list data files to scan (based on partition and data filters ). The aim of TahoeFileIndex (and FileIndex in general) is to reduce usage of very expensive disk access for file-related information using Hadoop FileSystem API. Contract \u00b6 matchingFiles \u00b6 matchingFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ]): Seq [ AddFile ] AddFile s matching given partition and data filters (predicates) Used for listing data files Implementations \u00b6 PinnedTahoeFileIndex TahoeBatchFileIndex TahoeLogFileIndex Creating Instance \u00b6 TahoeFileIndex takes the following to be created: SparkSession DeltaLog Hadoop Path Abstract Class TahoeFileIndex is an abstract class and cannot be created directly. It is created indirectly for the concrete TahoeFileIndexes . Root Paths \u00b6 rootPaths : Seq [ Path ] rootPaths is the path only. rootPaths is part of the FileIndex abstraction ( Spark SQL ). Listing Files \u00b6 listFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ]): Seq [ PartitionDirectory ] listFiles is the path only. listFiles is part of the FileIndex abstraction ( Spark SQL ). Partitions \u00b6 partitionSchema : StructType partitionSchema is the partition schema of (the Metadata of the Snapshot ) of the DeltaLog . partitionSchema is part of the FileIndex abstraction ( Spark SQL ). Version of Delta Table \u00b6 tableVersion : Long tableVersion is the version of (the snapshot of) the DeltaLog . tableVersion is used when TahoeFileIndex is requested for the human-friendly textual representation . Textual Representation \u00b6 toString : String toString returns the following text (using the version and the path of the Delta table): Delta[version=[tableVersion], [truncatedPath]] toString is part of the java.lang.Object contract for a string representation of the object.","title":"TahoeFileIndex"},{"location":"TahoeFileIndex/#tahoefileindex","text":"TahoeFileIndex is an extension of the FileIndex abstraction ( Spark SQL ) for file indices of delta tables that can list data files to scan (based on partition and data filters ). The aim of TahoeFileIndex (and FileIndex in general) is to reduce usage of very expensive disk access for file-related information using Hadoop FileSystem API.","title":"TahoeFileIndex"},{"location":"TahoeFileIndex/#contract","text":"","title":"Contract"},{"location":"TahoeFileIndex/#matchingfiles","text":"matchingFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ]): Seq [ AddFile ] AddFile s matching given partition and data filters (predicates) Used for listing data files","title":" matchingFiles"},{"location":"TahoeFileIndex/#implementations","text":"PinnedTahoeFileIndex TahoeBatchFileIndex TahoeLogFileIndex","title":"Implementations"},{"location":"TahoeFileIndex/#creating-instance","text":"TahoeFileIndex takes the following to be created: SparkSession DeltaLog Hadoop Path Abstract Class TahoeFileIndex is an abstract class and cannot be created directly. It is created indirectly for the concrete TahoeFileIndexes .","title":"Creating Instance"},{"location":"TahoeFileIndex/#root-paths","text":"rootPaths : Seq [ Path ] rootPaths is the path only. rootPaths is part of the FileIndex abstraction ( Spark SQL ).","title":" Root Paths"},{"location":"TahoeFileIndex/#listing-files","text":"listFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ]): Seq [ PartitionDirectory ] listFiles is the path only. listFiles is part of the FileIndex abstraction ( Spark SQL ).","title":" Listing Files"},{"location":"TahoeFileIndex/#partitions","text":"partitionSchema : StructType partitionSchema is the partition schema of (the Metadata of the Snapshot ) of the DeltaLog . partitionSchema is part of the FileIndex abstraction ( Spark SQL ).","title":" Partitions"},{"location":"TahoeFileIndex/#version-of-delta-table","text":"tableVersion : Long tableVersion is the version of (the snapshot of) the DeltaLog . tableVersion is used when TahoeFileIndex is requested for the human-friendly textual representation .","title":" Version of Delta Table"},{"location":"TahoeFileIndex/#textual-representation","text":"toString : String toString returns the following text (using the version and the path of the Delta table): Delta[version=[tableVersion], [truncatedPath]] toString is part of the java.lang.Object contract for a string representation of the object.","title":" Textual Representation"},{"location":"TahoeLogFileIndex/","text":"TahoeLogFileIndex \u00b6 TahoeLogFileIndex is a file index . Creating Instance \u00b6 TahoeLogFileIndex takes the following to be created: SparkSession ( Spark SQL ) DeltaLog Data directory of the Delta table (as a Hadoop Path ) Snapshot at analysis Partition Filters (as Catalyst expressions; default: empty) isTimeTravelQuery flag (default: false ) TahoeLogFileIndex is created when: DeltaLog is requested for an Insertable HadoopFsRelation isTimeTravelQuery flag \u00b6 TahoeLogFileIndex is given a isTimeTravelQuery flag when created . isTimeTravelQuery flag is false by default and can be different when DeltaLog is requested to create a BaseRelation (when DeltaTableV2 is requested for a BaseRelation based on DeltaTimeTravelSpec ). Demo \u00b6 val q = spark.read.format(\"delta\").load(\"/tmp/delta/users\") val plan = q.queryExecution.executedPlan import org.apache.spark.sql.execution.FileSourceScanExec val scan = plan.collect { case e: FileSourceScanExec => e }.head import org.apache.spark.sql.delta.files.TahoeLogFileIndex val index = scan.relation.location.asInstanceOf[TahoeLogFileIndex] scala> println(index) Delta[version=1, file:/tmp/delta/users] matchingFiles Method \u00b6 matchingFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ], keepStats : Boolean = false ): Seq [ AddFile ] matchingFiles gets the snapshot (with stalenessAcceptable flag off) and requests it for the files to scan (for the index's partition filters , the given partitionFilters and dataFilters ). Note inputFiles and matchingFiles are similar. Both get the snapshot (of the delta table), but they use different filtering expressions and return value types. matchingFiles is part of the TahoeFileIndex abstraction. inputFiles Method \u00b6 inputFiles : Array [ String ] inputFiles gets the snapshot (with stalenessAcceptable flag off) and requests it for the files to scan (for the index's partition filters only). Note inputFiles and matchingFiles are similar. Both get the snapshot , but they use different filtering expressions and return value types. inputFiles is part of the FileIndex contract (Spark SQL). Historical Or Latest Snapshot \u00b6 getSnapshot ( stalenessAcceptable : Boolean ): Snapshot getSnapshot returns a Snapshot that is either the historical snapshot (for the snapshot version if specified) or requests the DeltaLog to update (and give one). getSnapshot is used when TahoeLogFileIndex is requested for the matching files and the input files . Internal Properties \u00b6 historicalSnapshotOpt \u00b6 Historical snapshot that is the Snapshot for the versionToUse if defined. Used when TahoeLogFileIndex is requested for the (historical or latest) snapshot and the schema of the partition columns","title":"TahoeLogFileIndex"},{"location":"TahoeLogFileIndex/#tahoelogfileindex","text":"TahoeLogFileIndex is a file index .","title":"TahoeLogFileIndex"},{"location":"TahoeLogFileIndex/#creating-instance","text":"TahoeLogFileIndex takes the following to be created: SparkSession ( Spark SQL ) DeltaLog Data directory of the Delta table (as a Hadoop Path ) Snapshot at analysis Partition Filters (as Catalyst expressions; default: empty) isTimeTravelQuery flag (default: false ) TahoeLogFileIndex is created when: DeltaLog is requested for an Insertable HadoopFsRelation","title":"Creating Instance"},{"location":"TahoeLogFileIndex/#istimetravelquery-flag","text":"TahoeLogFileIndex is given a isTimeTravelQuery flag when created . isTimeTravelQuery flag is false by default and can be different when DeltaLog is requested to create a BaseRelation (when DeltaTableV2 is requested for a BaseRelation based on DeltaTimeTravelSpec ).","title":" isTimeTravelQuery flag"},{"location":"TahoeLogFileIndex/#demo","text":"val q = spark.read.format(\"delta\").load(\"/tmp/delta/users\") val plan = q.queryExecution.executedPlan import org.apache.spark.sql.execution.FileSourceScanExec val scan = plan.collect { case e: FileSourceScanExec => e }.head import org.apache.spark.sql.delta.files.TahoeLogFileIndex val index = scan.relation.location.asInstanceOf[TahoeLogFileIndex] scala> println(index) Delta[version=1, file:/tmp/delta/users]","title":"Demo"},{"location":"TahoeLogFileIndex/#matchingfiles-method","text":"matchingFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ], keepStats : Boolean = false ): Seq [ AddFile ] matchingFiles gets the snapshot (with stalenessAcceptable flag off) and requests it for the files to scan (for the index's partition filters , the given partitionFilters and dataFilters ). Note inputFiles and matchingFiles are similar. Both get the snapshot (of the delta table), but they use different filtering expressions and return value types. matchingFiles is part of the TahoeFileIndex abstraction.","title":" matchingFiles Method"},{"location":"TahoeLogFileIndex/#inputfiles-method","text":"inputFiles : Array [ String ] inputFiles gets the snapshot (with stalenessAcceptable flag off) and requests it for the files to scan (for the index's partition filters only). Note inputFiles and matchingFiles are similar. Both get the snapshot , but they use different filtering expressions and return value types. inputFiles is part of the FileIndex contract (Spark SQL).","title":" inputFiles Method"},{"location":"TahoeLogFileIndex/#historical-or-latest-snapshot","text":"getSnapshot ( stalenessAcceptable : Boolean ): Snapshot getSnapshot returns a Snapshot that is either the historical snapshot (for the snapshot version if specified) or requests the DeltaLog to update (and give one). getSnapshot is used when TahoeLogFileIndex is requested for the matching files and the input files .","title":" Historical Or Latest Snapshot"},{"location":"TahoeLogFileIndex/#internal-properties","text":"","title":"Internal Properties"},{"location":"TahoeLogFileIndex/#historicalsnapshotopt","text":"Historical snapshot that is the Snapshot for the versionToUse if defined. Used when TahoeLogFileIndex is requested for the (historical or latest) snapshot and the schema of the partition columns","title":" historicalSnapshotOpt"},{"location":"TransactionalWrite/","text":"TransactionalWrite \u00b6 TransactionalWrite is an abstraction of optimistic transactional writers that can write a structured query out to a Delta table . Contract \u00b6 DeltaLog \u00b6 deltaLog : DeltaLog DeltaLog (of a delta table) that this transaction is changing Used when: ActiveOptimisticTransactionRule logical rule is executed OptimisticTransactionImpl is requested to prepare a commit , doCommit , checkAndRetry , and perform post-commit operations (and execute delta log checkpoint ) ConvertToDeltaCommand is executed DeltaCommand is requested to buildBaseRelation and commitLarge MergeIntoCommand is executed TransactionalWrite is requested to write a structured query out to a delta table GenerateSymlinkManifest post-commit hook is executed ImplicitMetadataOperation is requested to updateMetadata DeltaSink is requested to addBatch Metadata \u00b6 metadata : Metadata Metadata (of the delta table ) that this transaction is changing Protocol \u00b6 protocol : Protocol Protocol (of the delta table ) that this transaction is changing Used when: OptimisticTransactionImpl is requested to updateMetadata , verifyNewMetadata and prepareCommit ConvertToDeltaCommand is executed Snapshot \u00b6 snapshot : Snapshot Snapshot (of the delta table ) that this transaction is reading at Implementations \u00b6 OptimisticTransaction hasWritten Flag \u00b6 hasWritten : Boolean = false TransactionalWrite uses hasWritten internal registry to prevent OptimisticTransactionImpl from updating metadata after having written out files . hasWritten is initially false and changes to true after having written out files . Writing Data Out (Result Of Structured Query) \u00b6 writeFiles ( data : Dataset [ _ ]): Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], writeOptions : Option [ DeltaOptions ]): Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], isOptimize : Boolean ): Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], writeOptions : Option [ DeltaOptions ], isOptimize : Boolean ): Seq [ AddFile ] writeFiles creates a DeltaInvariantCheckerExec and a DelayedCommitProtocol to write out files to the data path (of the DeltaLog ). Learn more writeFiles uses Spark SQL's FileFormatWriter utility to write out a result of a streaming query. Learn about FileFormatWriter in The Internals of Spark SQL online book. writeFiles is executed within SQLExecution.withNewExecutionId . Learn more writeFiles can be tracked using web UI or SQLAppStatusListener (using SparkListenerSQLExecutionStart and SparkListenerSQLExecutionEnd events). Learn about SQLAppStatusListener in The Internals of Spark SQL online book. In the end, writeFiles returns the addedStatuses of the DelayedCommitProtocol committer. Internally, writeFiles turns the hasWritten flag on ( true ). Note After writeFiles , no metadata updates in the transaction are permitted. writeFiles normalize the given data dataset (based on the partitionColumns of the Metadata ). writeFiles getPartitioningColumns based on the partitionSchema of the Metadata . DelayedCommitProtocol Committer \u00b6 writeFiles creates a DelayedCommitProtocol committer for the data path of the DeltaLog . writeFiles gets the invariants from the schema of the Metadata . DeltaInvariantCheckerExec \u00b6 writeFiles requests a new Execution ID (that is used to track all Spark jobs of FileFormatWriter.write in Spark SQL) with a physical query plan of a new DeltaInvariantCheckerExec unary physical operator (with the executed plan of the normalized query execution as the child operator). Creating Committer \u00b6 getCommitter ( outputPath : Path ): DelayedCommitProtocol getCommitter creates a new DelayedCommitProtocol with the delta job ID and the given outputPath (and no random prefix). getPartitioningColumns \u00b6 getPartitioningColumns ( partitionSchema : StructType , output : Seq [ Attribute ], colsDropped : Boolean ): Seq [ Attribute ] getPartitioningColumns ...FIXME normalizeData \u00b6 normalizeData ( data : Dataset [ _ ], partitionCols : Seq [ String ]): ( QueryExecution , Seq [ Attribute ]) normalizeData ...FIXME makeOutputNullable \u00b6 makeOutputNullable ( output : Seq [ Attribute ]): Seq [ Attribute ] makeOutputNullable ...FIXME Usage \u00b6 writeFiles is used when: DeleteCommand , MergeIntoCommand , UpdateCommand , and WriteIntoDelta commands are executed DeltaSink is requested to add a streaming micro-batch","title":"TransactionalWrite"},{"location":"TransactionalWrite/#transactionalwrite","text":"TransactionalWrite is an abstraction of optimistic transactional writers that can write a structured query out to a Delta table .","title":"TransactionalWrite"},{"location":"TransactionalWrite/#contract","text":"","title":"Contract"},{"location":"TransactionalWrite/#deltalog","text":"deltaLog : DeltaLog DeltaLog (of a delta table) that this transaction is changing Used when: ActiveOptimisticTransactionRule logical rule is executed OptimisticTransactionImpl is requested to prepare a commit , doCommit , checkAndRetry , and perform post-commit operations (and execute delta log checkpoint ) ConvertToDeltaCommand is executed DeltaCommand is requested to buildBaseRelation and commitLarge MergeIntoCommand is executed TransactionalWrite is requested to write a structured query out to a delta table GenerateSymlinkManifest post-commit hook is executed ImplicitMetadataOperation is requested to updateMetadata DeltaSink is requested to addBatch","title":" DeltaLog"},{"location":"TransactionalWrite/#metadata","text":"metadata : Metadata Metadata (of the delta table ) that this transaction is changing","title":" Metadata"},{"location":"TransactionalWrite/#protocol","text":"protocol : Protocol Protocol (of the delta table ) that this transaction is changing Used when: OptimisticTransactionImpl is requested to updateMetadata , verifyNewMetadata and prepareCommit ConvertToDeltaCommand is executed","title":" Protocol"},{"location":"TransactionalWrite/#snapshot","text":"snapshot : Snapshot Snapshot (of the delta table ) that this transaction is reading at","title":" Snapshot"},{"location":"TransactionalWrite/#implementations","text":"OptimisticTransaction","title":"Implementations"},{"location":"TransactionalWrite/#haswritten-flag","text":"hasWritten : Boolean = false TransactionalWrite uses hasWritten internal registry to prevent OptimisticTransactionImpl from updating metadata after having written out files . hasWritten is initially false and changes to true after having written out files .","title":" hasWritten Flag"},{"location":"TransactionalWrite/#writing-data-out-result-of-structured-query","text":"writeFiles ( data : Dataset [ _ ]): Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], writeOptions : Option [ DeltaOptions ]): Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], isOptimize : Boolean ): Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], writeOptions : Option [ DeltaOptions ], isOptimize : Boolean ): Seq [ AddFile ] writeFiles creates a DeltaInvariantCheckerExec and a DelayedCommitProtocol to write out files to the data path (of the DeltaLog ). Learn more writeFiles uses Spark SQL's FileFormatWriter utility to write out a result of a streaming query. Learn about FileFormatWriter in The Internals of Spark SQL online book. writeFiles is executed within SQLExecution.withNewExecutionId . Learn more writeFiles can be tracked using web UI or SQLAppStatusListener (using SparkListenerSQLExecutionStart and SparkListenerSQLExecutionEnd events). Learn about SQLAppStatusListener in The Internals of Spark SQL online book. In the end, writeFiles returns the addedStatuses of the DelayedCommitProtocol committer. Internally, writeFiles turns the hasWritten flag on ( true ). Note After writeFiles , no metadata updates in the transaction are permitted. writeFiles normalize the given data dataset (based on the partitionColumns of the Metadata ). writeFiles getPartitioningColumns based on the partitionSchema of the Metadata .","title":" Writing Data Out (Result Of Structured Query)"},{"location":"TransactionalWrite/#delayedcommitprotocol-committer","text":"writeFiles creates a DelayedCommitProtocol committer for the data path of the DeltaLog . writeFiles gets the invariants from the schema of the Metadata .","title":" DelayedCommitProtocol Committer"},{"location":"TransactionalWrite/#deltainvariantcheckerexec","text":"writeFiles requests a new Execution ID (that is used to track all Spark jobs of FileFormatWriter.write in Spark SQL) with a physical query plan of a new DeltaInvariantCheckerExec unary physical operator (with the executed plan of the normalized query execution as the child operator).","title":" DeltaInvariantCheckerExec"},{"location":"TransactionalWrite/#creating-committer","text":"getCommitter ( outputPath : Path ): DelayedCommitProtocol getCommitter creates a new DelayedCommitProtocol with the delta job ID and the given outputPath (and no random prefix).","title":" Creating Committer"},{"location":"TransactionalWrite/#getpartitioningcolumns","text":"getPartitioningColumns ( partitionSchema : StructType , output : Seq [ Attribute ], colsDropped : Boolean ): Seq [ Attribute ] getPartitioningColumns ...FIXME","title":" getPartitioningColumns"},{"location":"TransactionalWrite/#normalizedata","text":"normalizeData ( data : Dataset [ _ ], partitionCols : Seq [ String ]): ( QueryExecution , Seq [ Attribute ]) normalizeData ...FIXME","title":" normalizeData"},{"location":"TransactionalWrite/#makeoutputnullable","text":"makeOutputNullable ( output : Seq [ Attribute ]): Seq [ Attribute ] makeOutputNullable ...FIXME","title":" makeOutputNullable"},{"location":"TransactionalWrite/#usage","text":"writeFiles is used when: DeleteCommand , MergeIntoCommand , UpdateCommand , and WriteIntoDelta commands are executed DeltaSink is requested to add a streaming micro-batch","title":" Usage"},{"location":"VerifyChecksum/","text":"= VerifyChecksum VerifyChecksum is...FIXME == [[validateChecksum]] validateChecksum Method [source, scala] \u00b6 validateChecksum(snapshot: Snapshot): Unit \u00b6 validateChecksum ...FIXME NOTE: validateChecksum is used when...FIXME","title":"VerifyChecksum"},{"location":"VerifyChecksum/#source-scala","text":"","title":"[source, scala]"},{"location":"VerifyChecksum/#validatechecksumsnapshot-snapshot-unit","text":"validateChecksum ...FIXME NOTE: validateChecksum is used when...FIXME","title":"validateChecksum(snapshot: Snapshot): Unit"},{"location":"WriteIntoDeltaBuilder/","text":"WriteIntoDeltaBuilder \u00b6 WriteIntoDeltaBuilder is a WriteBuilder ( Spark SQL ) with support for the following capabilities: SupportsOverwrite ( Spark SQL ) SupportsTruncate ( Spark SQL ) V1WriteBuilder ( Spark SQL ) Creating Instance \u00b6 WriteIntoDeltaBuilder takes the following to be created: DeltaLog Write-Specific Options WriteIntoDeltaBuilder is created when: DeltaTableV2 is requested for a WriteBuilder buildForV1Write \u00b6 buildForV1Write (): InsertableRelation buildForV1Write is part of the V1WriteBuilder ( Spark SQL ) abstraction. buildForV1Write creates an InsertableRelation ( Spark SQL ) that does the following when requested to insert : Creates and executes a WriteIntoDelta command Re-cache all cached plans (by requesting the CacheManager to recacheByPlan for a LogicalRelation over the BaseRelation of the DeltaLog )","title":"WriteIntoDeltaBuilder"},{"location":"WriteIntoDeltaBuilder/#writeintodeltabuilder","text":"WriteIntoDeltaBuilder is a WriteBuilder ( Spark SQL ) with support for the following capabilities: SupportsOverwrite ( Spark SQL ) SupportsTruncate ( Spark SQL ) V1WriteBuilder ( Spark SQL )","title":"WriteIntoDeltaBuilder"},{"location":"WriteIntoDeltaBuilder/#creating-instance","text":"WriteIntoDeltaBuilder takes the following to be created: DeltaLog Write-Specific Options WriteIntoDeltaBuilder is created when: DeltaTableV2 is requested for a WriteBuilder","title":"Creating Instance"},{"location":"WriteIntoDeltaBuilder/#buildforv1write","text":"buildForV1Write (): InsertableRelation buildForV1Write is part of the V1WriteBuilder ( Spark SQL ) abstraction. buildForV1Write creates an InsertableRelation ( Spark SQL ) that does the following when requested to insert : Creates and executes a WriteIntoDelta command Re-cache all cached plans (by requesting the CacheManager to recacheByPlan for a LogicalRelation over the BaseRelation of the DeltaLog )","title":" buildForV1Write"},{"location":"configuration-properties/","text":"Configuration Properties \u00b6 spark.delta.logStore.class \u00b6 The fully-qualified class name of a LogStore Default: HDFSLogStore Used when: LogStoreProvider is requested for a LogStore","title":"Configuration Properties"},{"location":"configuration-properties/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"configuration-properties/#sparkdeltalogstoreclass","text":"The fully-qualified class name of a LogStore Default: HDFSLogStore Used when: LogStoreProvider is requested for a LogStore","title":" spark.delta.logStore.class"},{"location":"installation/","text":"Installation \u00b6 Delta Lake is a Spark data source and as such installation boils down to using spark-submit's --packages command-line option with the following configuration properties for DeltaSparkSessionExtension and DeltaCatalog : spark.sql.extensions ( Spark SQL ) spark.sql.catalog.spark_catalog ( Spark SQL ) Spark SQL Application \u00b6 import org . apache . spark . sql . SparkSession val spark = SparkSession . builder () . appName ( \"...\" ) . config ( \"spark.sql.extensions\" , \"io.delta.sql.DeltaSparkSessionExtension\" ) . config ( \"spark.sql.catalog.spark_catalog\" , \"org.apache.spark.sql.delta.catalog.DeltaCatalog\" ) . getOrCreate Spark Shell \u00b6 ./bin/spark-shell \\ --packages io.delta:delta-core_2.12:1.0.0-SNAPSHOT \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog","title":"Installation"},{"location":"installation/#installation","text":"Delta Lake is a Spark data source and as such installation boils down to using spark-submit's --packages command-line option with the following configuration properties for DeltaSparkSessionExtension and DeltaCatalog : spark.sql.extensions ( Spark SQL ) spark.sql.catalog.spark_catalog ( Spark SQL )","title":"Installation"},{"location":"installation/#spark-sql-application","text":"import org . apache . spark . sql . SparkSession val spark = SparkSession . builder () . appName ( \"...\" ) . config ( \"spark.sql.extensions\" , \"io.delta.sql.DeltaSparkSessionExtension\" ) . config ( \"spark.sql.catalog.spark_catalog\" , \"org.apache.spark.sql.delta.catalog.DeltaCatalog\" ) . getOrCreate","title":" Spark SQL Application"},{"location":"installation/#spark-shell","text":"./bin/spark-shell \\ --packages io.delta:delta-core_2.12:1.0.0-SNAPSHOT \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog","title":" Spark Shell"},{"location":"options/","text":"Options \u00b6 Delta Lake comes with options to fine-tune its uses. They can be defined using option method of the following: DataFrameReader ( Spark SQL ) and DataFrameWriter ( Spark SQL ) for batch queries DataStreamReader ( Spark Structured Streaming ) and DataStreamWriter ( Spark Structured Streaming ) for streaming queries SQL queries checkpointLocation \u00b6 Checkpoint directory for storing checkpoint data of streaming queries ( Spark Structured Streaming ). dataChange \u00b6 Whether to write new data to the table or just rearrange data that is already part of the table. This option declares that the data being written by this job does not change any data in the table and merely rearranges existing data. This makes sure streaming queries reading from this table will not see any new changes Used when: DeltaWriteOptionsImpl is requested for rearrangeOnly Demo Learn more in Demo: dataChange . excludeRegex \u00b6 ignoreChanges \u00b6 ignoreDeletes \u00b6 ignoreFileDeletion \u00b6 maxBytesPerTrigger \u00b6 maxFilesPerTrigger \u00b6 Maximum number of files ( AddFiles ) that DeltaSource is supposed to scan ( read ) in a streaming micro-batch ( trigger ) Default: 1000 Must be at least 1 mergeSchema \u00b6 Enables schema migration (and allows automatic schema merging during a write operation for WriteIntoDelta and DeltaSink ) Equivalent SQL Session configuration: spark.databricks.delta.schema.autoMerge.enabled optimizeWrite \u00b6 Enables...FIXME overwriteSchema \u00b6 path \u00b6 (required) Directory on a Hadoop DFS-compliant file system with an optional time travel identifier Default: (undefined) Note Can also be specified using load method of DataFrameReader and DataStreamReader . queryName \u00b6 replaceWhere \u00b6 Available as DeltaWriteOptions.replaceWhere Demo Learn more in Demo: replaceWhere . timestampAsOf \u00b6 Timestamp of the version of a Delta table for Time Travel Mutually exclusive with versionAsOf option and the time travel identifier of the path option. userMetadata \u00b6 Defines a user-defined commit metadata Take precedence over spark.databricks.delta.commitInfo.userMetadata Available by inspecting CommitInfo s using DESCRIBE HISTORY or DeltaTable.history . Demo Learn more in Demo: User Metadata for Labelling Commits . versionAsOf \u00b6 Version of a Delta table for Time Travel Mutually exclusive with timestampAsOf option and the time travel identifier of the path option. Used when: DeltaDataSource is requested for a relation","title":"Options"},{"location":"options/#options","text":"Delta Lake comes with options to fine-tune its uses. They can be defined using option method of the following: DataFrameReader ( Spark SQL ) and DataFrameWriter ( Spark SQL ) for batch queries DataStreamReader ( Spark Structured Streaming ) and DataStreamWriter ( Spark Structured Streaming ) for streaming queries SQL queries","title":"Options"},{"location":"options/#checkpointlocation","text":"Checkpoint directory for storing checkpoint data of streaming queries ( Spark Structured Streaming ).","title":" checkpointLocation"},{"location":"options/#datachange","text":"Whether to write new data to the table or just rearrange data that is already part of the table. This option declares that the data being written by this job does not change any data in the table and merely rearranges existing data. This makes sure streaming queries reading from this table will not see any new changes Used when: DeltaWriteOptionsImpl is requested for rearrangeOnly Demo Learn more in Demo: dataChange .","title":" dataChange"},{"location":"options/#excluderegex","text":"","title":" excludeRegex"},{"location":"options/#ignorechanges","text":"","title":" ignoreChanges"},{"location":"options/#ignoredeletes","text":"","title":" ignoreDeletes"},{"location":"options/#ignorefiledeletion","text":"","title":" ignoreFileDeletion"},{"location":"options/#maxbytespertrigger","text":"","title":" maxBytesPerTrigger"},{"location":"options/#maxfilespertrigger","text":"Maximum number of files ( AddFiles ) that DeltaSource is supposed to scan ( read ) in a streaming micro-batch ( trigger ) Default: 1000 Must be at least 1","title":" maxFilesPerTrigger"},{"location":"options/#mergeschema","text":"Enables schema migration (and allows automatic schema merging during a write operation for WriteIntoDelta and DeltaSink ) Equivalent SQL Session configuration: spark.databricks.delta.schema.autoMerge.enabled","title":" mergeSchema"},{"location":"options/#optimizewrite","text":"Enables...FIXME","title":" optimizeWrite"},{"location":"options/#overwriteschema","text":"","title":" overwriteSchema"},{"location":"options/#path","text":"(required) Directory on a Hadoop DFS-compliant file system with an optional time travel identifier Default: (undefined) Note Can also be specified using load method of DataFrameReader and DataStreamReader .","title":" path"},{"location":"options/#queryname","text":"","title":" queryName"},{"location":"options/#replacewhere","text":"Available as DeltaWriteOptions.replaceWhere Demo Learn more in Demo: replaceWhere .","title":" replaceWhere"},{"location":"options/#timestampasof","text":"Timestamp of the version of a Delta table for Time Travel Mutually exclusive with versionAsOf option and the time travel identifier of the path option.","title":" timestampAsOf"},{"location":"options/#usermetadata","text":"Defines a user-defined commit metadata Take precedence over spark.databricks.delta.commitInfo.userMetadata Available by inspecting CommitInfo s using DESCRIBE HISTORY or DeltaTable.history . Demo Learn more in Demo: User Metadata for Labelling Commits .","title":" userMetadata"},{"location":"options/#versionasof","text":"Version of a Delta table for Time Travel Mutually exclusive with timestampAsOf option and the time travel identifier of the path option. Used when: DeltaDataSource is requested for a relation","title":" versionAsOf"},{"location":"overview/","text":"Delta Lake \u00b6 Delta Lake is an open-source Apache Spark -based storage layer that brings ACID transactions and time travel to Spark and other big data workloads. As it was well said : \"Delta is a storage format while Spark is an execution engine...to separate storage from compute.\" Important As of 0.7.0 Delta Lake requires Spark 3. Please note that Spark 3.1.1 is not yet supported . Use Spark 3.0.2 instead. Delta Lake is a table format. It introduces DeltaTable abstraction that is simply a parquet table with a transactional log . Changes to (the state of) a delta table are reflected as actions and persisted to the transactional log (in JSON format ). Delta Lake uses OptimisticTransaction for transactional writes . A commit is successful when the transaction can write the actions to a delta file (in the transactional log ). In case the delta file for the commit version already exists, the transaction is retried . Structured queries can write (transactionally) to a delta table using the following interfaces: WriteIntoDelta command for batch queries (Spark SQL) DeltaSink for streaming queries (Spark Structured Streaming) More importantly, multiple queries can write to the same delta table simultaneously (at exactly the same time). Delta Lake provides DeltaTable API to programmatically access Delta tables. A delta table can be created based on a parquet table or from scratch . Delta Lake supports batch and streaming queries (Spark SQL and Structured Streaming, respectively) using delta format. In order to fine tune queries over data in Delta Lake use options . Among the options path option is mandatory. Delta Lake supports reading and writing in batch queries: Batch reads (as a RelationProvider ) Batch writes (as a CreatableRelationProvider ) Delta Lake supports reading and writing in streaming queries: Stream reads (as a Source ) Stream writes (as a Sink ) Delta Lake uses LogStore abstraction to read and write physical log files and checkpoints (using Hadoop FileSystem API ). Delta Tables in Logical Query Plans \u00b6 Delta Table defines DeltaTable Scala extractor to find delta tables in a logical query plan. The extractor finds LogicalRelation s ( Spark SQL ) with HadoopFsRelation ( Spark SQL ) and TahoeFileIndex . Put simply, delta tables are LogicalRelation s with HadoopFsRelation with TahoeFileIndex in logical query plans. Concurrent Blind Append Transactions \u00b6 A transaction can be blind append when simply appends new data to a table with no reliance on existing data (and without reading or modifying it). They are marked in the commit info to distinguish them from read-modify-appends (deletes, merges or updates) and assume no conflict between concurrent transactions. Blind Append Transactions allow for concurrent updates. df . format ( \"delta\" ). mode ( \"append\" ). save (...)","title":"Overview"},{"location":"overview/#delta-lake","text":"Delta Lake is an open-source Apache Spark -based storage layer that brings ACID transactions and time travel to Spark and other big data workloads. As it was well said : \"Delta is a storage format while Spark is an execution engine...to separate storage from compute.\" Important As of 0.7.0 Delta Lake requires Spark 3. Please note that Spark 3.1.1 is not yet supported . Use Spark 3.0.2 instead. Delta Lake is a table format. It introduces DeltaTable abstraction that is simply a parquet table with a transactional log . Changes to (the state of) a delta table are reflected as actions and persisted to the transactional log (in JSON format ). Delta Lake uses OptimisticTransaction for transactional writes . A commit is successful when the transaction can write the actions to a delta file (in the transactional log ). In case the delta file for the commit version already exists, the transaction is retried . Structured queries can write (transactionally) to a delta table using the following interfaces: WriteIntoDelta command for batch queries (Spark SQL) DeltaSink for streaming queries (Spark Structured Streaming) More importantly, multiple queries can write to the same delta table simultaneously (at exactly the same time). Delta Lake provides DeltaTable API to programmatically access Delta tables. A delta table can be created based on a parquet table or from scratch . Delta Lake supports batch and streaming queries (Spark SQL and Structured Streaming, respectively) using delta format. In order to fine tune queries over data in Delta Lake use options . Among the options path option is mandatory. Delta Lake supports reading and writing in batch queries: Batch reads (as a RelationProvider ) Batch writes (as a CreatableRelationProvider ) Delta Lake supports reading and writing in streaming queries: Stream reads (as a Source ) Stream writes (as a Sink ) Delta Lake uses LogStore abstraction to read and write physical log files and checkpoints (using Hadoop FileSystem API ).","title":"Delta Lake"},{"location":"overview/#delta-tables-in-logical-query-plans","text":"Delta Table defines DeltaTable Scala extractor to find delta tables in a logical query plan. The extractor finds LogicalRelation s ( Spark SQL ) with HadoopFsRelation ( Spark SQL ) and TahoeFileIndex . Put simply, delta tables are LogicalRelation s with HadoopFsRelation with TahoeFileIndex in logical query plans.","title":"Delta Tables in Logical Query Plans"},{"location":"overview/#concurrent-blind-append-transactions","text":"A transaction can be blind append when simply appends new data to a table with no reliance on existing data (and without reading or modifying it). They are marked in the commit info to distinguish them from read-modify-appends (deletes, merges or updates) and assume no conflict between concurrent transactions. Blind Append Transactions allow for concurrent updates. df . format ( \"delta\" ). mode ( \"append\" ). save (...)","title":"Concurrent Blind Append Transactions"},{"location":"spark-logging/","text":"Logging \u00b6 Spark uses log4j for logging. Note Learn more on Spark Logging in The Internals of Apache Spark online book.","title":"Logging"},{"location":"spark-logging/#logging","text":"Spark uses log4j for logging. Note Learn more on Spark Logging in The Internals of Apache Spark online book.","title":"Logging"},{"location":"table-properties/","text":"Table Properties \u00b6 Delta Lake allows setting up table properties for a custom behaviour of a delta table. SHOW TBLPROPERTIES \u00b6 Table properties can be displayed using SHOW TBLPROPERTIES SQL command: SHOW TBLPROPERTIES < table_name > [( comma - separated properties )] sql ( \"SHOW TBLPROPERTIES delta.`/tmp/delta/t1`\" ). show ( truncate = false ) +----------------------+-----+ |key |value| +----------------------+-----+ |delta.minReaderVersion|1 | |delta.minWriterVersion|2 | +----------------------+-----+ sql ( \"SHOW TBLPROPERTIES delta.`/tmp/delta/t1` (delta.minReaderVersion)\" ). show ( truncate = false ) +----------------------+-----+ |key |value| +----------------------+-----+ |delta.minReaderVersion|1 | +----------------------+-----+ ALTER TABLE SET TBLPROPERTIES \u00b6 Table properties can be set a value or unset using ALTER TABLE SQL command: ALTER TABLE < table_name > SET TBLPROPERTIES ( < key >=< value > ) ALTER TABLE table1 UNSET TBLPROPERTIES [ IF EXISTS ] ( 'key1' , 'key2' , ...); sql(\"ALTER TABLE delta.`/tmp/delta/t1` SET TBLPROPERTIES (delta.enableExpiredLogCleanup=true)\") sql ( \"SHOW TBLPROPERTIES delta.`/tmp/delta/t1` (delta.enableExpiredLogCleanup)\" ). show ( truncate = false ) +-----------------------------+-----+ |key |value| +-----------------------------+-----+ |delta.enableExpiredLogCleanup|true | +-----------------------------+-----+","title":"Table Properties"},{"location":"table-properties/#table-properties","text":"Delta Lake allows setting up table properties for a custom behaviour of a delta table.","title":"Table Properties"},{"location":"table-properties/#show-tblproperties","text":"Table properties can be displayed using SHOW TBLPROPERTIES SQL command: SHOW TBLPROPERTIES < table_name > [( comma - separated properties )] sql ( \"SHOW TBLPROPERTIES delta.`/tmp/delta/t1`\" ). show ( truncate = false ) +----------------------+-----+ |key |value| +----------------------+-----+ |delta.minReaderVersion|1 | |delta.minWriterVersion|2 | +----------------------+-----+ sql ( \"SHOW TBLPROPERTIES delta.`/tmp/delta/t1` (delta.minReaderVersion)\" ). show ( truncate = false ) +----------------------+-----+ |key |value| +----------------------+-----+ |delta.minReaderVersion|1 | +----------------------+-----+","title":"SHOW TBLPROPERTIES"},{"location":"table-properties/#alter-table-set-tblproperties","text":"Table properties can be set a value or unset using ALTER TABLE SQL command: ALTER TABLE < table_name > SET TBLPROPERTIES ( < key >=< value > ) ALTER TABLE table1 UNSET TBLPROPERTIES [ IF EXISTS ] ( 'key1' , 'key2' , ...); sql(\"ALTER TABLE delta.`/tmp/delta/t1` SET TBLPROPERTIES (delta.enableExpiredLogCleanup=true)\") sql ( \"SHOW TBLPROPERTIES delta.`/tmp/delta/t1` (delta.enableExpiredLogCleanup)\" ). show ( truncate = false ) +-----------------------------+-----+ |key |value| +-----------------------------+-----+ |delta.enableExpiredLogCleanup|true | +-----------------------------+-----+","title":"ALTER TABLE SET TBLPROPERTIES"},{"location":"time-travel/","text":"Time Travel \u00b6 Delta Lake supports time travelling which is loading a Delta table at a given version or timestamp (defined by path , versionAsOf or timestampAsOf options). Delta Lake allows path option to include time travel patterns ( @v123 and @yyyyMMddHHmmssSSS ) unless the internal spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled configuration property is turned off. Time Travel cannot be specified for catalog delta tables. Time travel is described using DeltaTimeTravelSpec .","title":"Time Travel"},{"location":"time-travel/#time-travel","text":"Delta Lake supports time travelling which is loading a Delta table at a given version or timestamp (defined by path , versionAsOf or timestampAsOf options). Delta Lake allows path option to include time travel patterns ( @v123 and @yyyyMMddHHmmssSSS ) unless the internal spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled configuration property is turned off. Time Travel cannot be specified for catalog delta tables. Time travel is described using DeltaTimeTravelSpec .","title":"Time Travel"},{"location":"commands/","text":"Commands \u00b6","title":"Commands"},{"location":"commands/#commands","text":"","title":"Commands"},{"location":"commands/AlterDeltaTableCommand/","text":"AlterDeltaTableCommand \u00b6 AlterDeltaTableCommand is an extension of the DeltaCommand abstraction for Delta commands that alter a DeltaTableV2 . Contract \u00b6 table \u00b6 table : DeltaTableV2 DeltaTableV2 Used when: AlterDeltaTableCommand is requested to startTransaction Implementations \u00b6 AlterTableAddColumnsDeltaCommand AlterTableChangeColumnDeltaCommand AlterTableReplaceColumnsDeltaCommand AlterTableSetLocationDeltaCommand AlterTableSetPropertiesDeltaCommand AlterTableUnsetPropertiesDeltaCommand startTransaction \u00b6 startTransaction (): OptimisticTransaction startTransaction simply requests the DeltaTableV2 for the DeltaLog that in turn is requested to startTransaction .","title":"AlterDeltaTableCommand"},{"location":"commands/AlterDeltaTableCommand/#alterdeltatablecommand","text":"AlterDeltaTableCommand is an extension of the DeltaCommand abstraction for Delta commands that alter a DeltaTableV2 .","title":"AlterDeltaTableCommand"},{"location":"commands/AlterDeltaTableCommand/#contract","text":"","title":"Contract"},{"location":"commands/AlterDeltaTableCommand/#table","text":"table : DeltaTableV2 DeltaTableV2 Used when: AlterDeltaTableCommand is requested to startTransaction","title":" table"},{"location":"commands/AlterDeltaTableCommand/#implementations","text":"AlterTableAddColumnsDeltaCommand AlterTableChangeColumnDeltaCommand AlterTableReplaceColumnsDeltaCommand AlterTableSetLocationDeltaCommand AlterTableSetPropertiesDeltaCommand AlterTableUnsetPropertiesDeltaCommand","title":"Implementations"},{"location":"commands/AlterDeltaTableCommand/#starttransaction","text":"startTransaction (): OptimisticTransaction startTransaction simply requests the DeltaTableV2 for the DeltaLog that in turn is requested to startTransaction .","title":" startTransaction"},{"location":"commands/AlterTableAddColumnsDeltaCommand/","text":"AlterTableAddColumnsDeltaCommand \u00b6 AlterTableAddColumnsDeltaCommand is...FIXME","title":"AlterTableAddColumnsDeltaCommand"},{"location":"commands/AlterTableAddColumnsDeltaCommand/#altertableaddcolumnsdeltacommand","text":"AlterTableAddColumnsDeltaCommand is...FIXME","title":"AlterTableAddColumnsDeltaCommand"},{"location":"commands/AlterTableAddConstraintDeltaCommand/","text":"AlterTableAddConstraintDeltaCommand \u00b6 AlterTableAddConstraintDeltaCommand is...FIXME","title":"AlterTableAddConstraintDeltaCommand"},{"location":"commands/AlterTableAddConstraintDeltaCommand/#altertableaddconstraintdeltacommand","text":"AlterTableAddConstraintDeltaCommand is...FIXME","title":"AlterTableAddConstraintDeltaCommand"},{"location":"commands/AlterTableChangeColumnDeltaCommand/","text":"AlterTableChangeColumnDeltaCommand \u00b6 AlterTableChangeColumnDeltaCommand is...FIXME","title":"AlterTableChangeColumnDeltaCommand"},{"location":"commands/AlterTableChangeColumnDeltaCommand/#altertablechangecolumndeltacommand","text":"AlterTableChangeColumnDeltaCommand is...FIXME","title":"AlterTableChangeColumnDeltaCommand"},{"location":"commands/AlterTableDropConstraintDeltaCommand/","text":"AlterTableDropConstraintDeltaCommand \u00b6 AlterTableDropConstraintDeltaCommand is...FIXME","title":"AlterTableDropConstraintDeltaCommand"},{"location":"commands/AlterTableDropConstraintDeltaCommand/#altertabledropconstraintdeltacommand","text":"AlterTableDropConstraintDeltaCommand is...FIXME","title":"AlterTableDropConstraintDeltaCommand"},{"location":"commands/AlterTableReplaceColumnsDeltaCommand/","text":"AlterTableReplaceColumnsDeltaCommand \u00b6 AlterTableReplaceColumnsDeltaCommand is...FIXME","title":"AlterTableReplaceColumnsDeltaCommand"},{"location":"commands/AlterTableReplaceColumnsDeltaCommand/#altertablereplacecolumnsdeltacommand","text":"AlterTableReplaceColumnsDeltaCommand is...FIXME","title":"AlterTableReplaceColumnsDeltaCommand"},{"location":"commands/AlterTableSetLocationDeltaCommand/","text":"AlterTableSetLocationDeltaCommand \u00b6 AlterTableSetLocationDeltaCommand is...FIXME","title":"AlterTableSetLocationDeltaCommand"},{"location":"commands/AlterTableSetLocationDeltaCommand/#altertablesetlocationdeltacommand","text":"AlterTableSetLocationDeltaCommand is...FIXME","title":"AlterTableSetLocationDeltaCommand"},{"location":"commands/AlterTableSetPropertiesDeltaCommand/","text":"AlterTableSetPropertiesDeltaCommand \u00b6 AlterTableSetPropertiesDeltaCommand is a AlterDeltaTableCommand . AlterTableSetPropertiesDeltaCommand is a RunnableCommand ( Spark SQL ) logical operator. Creating Instance \u00b6 AlterTableSetPropertiesDeltaCommand takes the following to be created: DeltaTableV2 Configuration ( Map[String, String] ) AlterTableSetPropertiesDeltaCommand is created when: DeltaCatalog is requested to alterTable","title":"AlterTableSetPropertiesDeltaCommand"},{"location":"commands/AlterTableSetPropertiesDeltaCommand/#altertablesetpropertiesdeltacommand","text":"AlterTableSetPropertiesDeltaCommand is a AlterDeltaTableCommand . AlterTableSetPropertiesDeltaCommand is a RunnableCommand ( Spark SQL ) logical operator.","title":"AlterTableSetPropertiesDeltaCommand"},{"location":"commands/AlterTableSetPropertiesDeltaCommand/#creating-instance","text":"AlterTableSetPropertiesDeltaCommand takes the following to be created: DeltaTableV2 Configuration ( Map[String, String] ) AlterTableSetPropertiesDeltaCommand is created when: DeltaCatalog is requested to alterTable","title":"Creating Instance"},{"location":"commands/AlterTableUnsetPropertiesDeltaCommand/","text":"AlterTableUnsetPropertiesDeltaCommand \u00b6 AlterTableUnsetPropertiesDeltaCommand is...FIXME","title":"AlterTableUnsetPropertiesDeltaCommand"},{"location":"commands/AlterTableUnsetPropertiesDeltaCommand/#altertableunsetpropertiesdeltacommand","text":"AlterTableUnsetPropertiesDeltaCommand is...FIXME","title":"AlterTableUnsetPropertiesDeltaCommand"},{"location":"commands/CreateDeltaTableCommand/","text":"CreateDeltaTableCommand \u00b6 CreateDeltaTableCommand is a RunnableCommand ( Spark SQL ). Creating Instance \u00b6 CreateDeltaTableCommand takes the following to be created: CatalogTable ( Spark SQL ) Existing CatalogTable (if available) SaveMode Optional Data Query ( LogicalPlan ) CreationMode (default: TableCreationModes.Create ) tableByPath flag (default: false ) CreateDeltaTableCommand is created when: DeltaCatalog is requested to create a Delta table Executing Command \u00b6 run ( sparkSession : SparkSession ): Seq [ Row ] run creates a DeltaLog (for the given table based on a table location) and a DeltaOptions . run starts a transaction (on the DeltaLog ). run branches off based on the optional data query . For data query defined, run creates a WriteIntoDelta and requests it to write . Otherwise, run creates an empty table. Note run does a bit more, but I don't think it's of much interest. run commits the transaction . In the end, run updateCatalog . run is part of the RunnableCommand abstraction. updateCatalog \u00b6 updateCatalog ( spark : SparkSession , table : CatalogTable ): Unit updateCatalog uses the given SparkSession to access SessionCatalog to createTable or alterTable when the tableByPath flag is off. Otherwise, updateCatalog does nothing. getOperation \u00b6 getOperation ( metadata : Metadata , isManagedTable : Boolean , options : Option [ DeltaOptions ]): DeltaOperations . Operation getOperation ...FIXME","title":"CreateDeltaTableCommand"},{"location":"commands/CreateDeltaTableCommand/#createdeltatablecommand","text":"CreateDeltaTableCommand is a RunnableCommand ( Spark SQL ).","title":"CreateDeltaTableCommand"},{"location":"commands/CreateDeltaTableCommand/#creating-instance","text":"CreateDeltaTableCommand takes the following to be created: CatalogTable ( Spark SQL ) Existing CatalogTable (if available) SaveMode Optional Data Query ( LogicalPlan ) CreationMode (default: TableCreationModes.Create ) tableByPath flag (default: false ) CreateDeltaTableCommand is created when: DeltaCatalog is requested to create a Delta table","title":"Creating Instance"},{"location":"commands/CreateDeltaTableCommand/#executing-command","text":"run ( sparkSession : SparkSession ): Seq [ Row ] run creates a DeltaLog (for the given table based on a table location) and a DeltaOptions . run starts a transaction (on the DeltaLog ). run branches off based on the optional data query . For data query defined, run creates a WriteIntoDelta and requests it to write . Otherwise, run creates an empty table. Note run does a bit more, but I don't think it's of much interest. run commits the transaction . In the end, run updateCatalog . run is part of the RunnableCommand abstraction.","title":" Executing Command"},{"location":"commands/CreateDeltaTableCommand/#updatecatalog","text":"updateCatalog ( spark : SparkSession , table : CatalogTable ): Unit updateCatalog uses the given SparkSession to access SessionCatalog to createTable or alterTable when the tableByPath flag is off. Otherwise, updateCatalog does nothing.","title":" updateCatalog"},{"location":"commands/CreateDeltaTableCommand/#getoperation","text":"getOperation ( metadata : Metadata , isManagedTable : Boolean , options : Option [ DeltaOptions ]): DeltaOperations . Operation getOperation ...FIXME","title":" getOperation"},{"location":"commands/DeltaCommand/","text":"DeltaCommand \u00b6 DeltaCommand is a marker interface for commands to work with data in delta tables. Implementations \u00b6 AlterDeltaTableCommand ConvertToDeltaCommand DeleteCommand MergeIntoCommand UpdateCommand VacuumCommandImpl WriteIntoDelta parsePartitionPredicates Method \u00b6 parsePartitionPredicates ( spark : SparkSession , predicate : String ): Seq [ Expression ] parsePartitionPredicates ...FIXME parsePartitionPredicates is used when...FIXME verifyPartitionPredicates Method \u00b6 verifyPartitionPredicates ( spark : SparkSession , partitionColumns : Seq [ String ], predicates : Seq [ Expression ]): Unit verifyPartitionPredicates ...FIXME verifyPartitionPredicates is used when...FIXME generateCandidateFileMap Method \u00b6 generateCandidateFileMap ( basePath : Path , candidateFiles : Seq [ AddFile ]): Map [ String , AddFile ] generateCandidateFileMap ...FIXME generateCandidateFileMap is used when...FIXME removeFilesFromPaths Method \u00b6 removeFilesFromPaths ( deltaLog : DeltaLog , nameToAddFileMap : Map [ String , AddFile ], filesToRewrite : Seq [ String ], operationTimestamp : Long ): Seq [ RemoveFile ] removeFilesFromPaths ...FIXME removeFilesFromPaths is used when: DeleteCommand and UpdateCommand commands are executed Creating HadoopFsRelation (with TahoeBatchFileIndex) \u00b6 buildBaseRelation ( spark : SparkSession , txn : OptimisticTransaction , actionType : String , rootPath : Path , inputLeafFiles : Seq [ String ], nameToAddFileMap : Map [ String , AddFile ]): HadoopFsRelation buildBaseRelation converts the given inputLeafFiles to AddFiles (with the given rootPath and nameToAddFileMap ). buildBaseRelation creates a TahoeBatchFileIndex for the AddFile s (with the input actionType and rootPath ). In the end, buildBaseRelation creates a HadoopFsRelation ( Spark SQL ) with the TahoeBatchFileIndex (and the other properties based on the metadata of the given OptimisticTransaction ). buildBaseRelation is used when: DeleteCommand and UpdateCommand commands are executed (with delete and update action types, respectively) getTouchedFile Method \u00b6 getTouchedFile ( basePath : Path , filePath : String , nameToAddFileMap : Map [ String , AddFile ]): AddFile getTouchedFile ...FIXME getTouchedFile is used when: DeltaCommand is requested to removeFilesFromPaths and create a HadoopFsRelation (for DeleteCommand and UpdateCommand commands) MergeIntoCommand is executed isCatalogTable Method \u00b6 isCatalogTable ( analyzer : Analyzer , tableIdent : TableIdentifier ): Boolean isCatalogTable ...FIXME isCatalogTable is used when...FIXME isPathIdentifier Method \u00b6 isPathIdentifier ( tableIdent : TableIdentifier ): Boolean isPathIdentifier ...FIXME isPathIdentifier is used when...FIXME commitLarge \u00b6 commitLarge ( spark : SparkSession , txn : OptimisticTransaction , actions : Iterator [ Action ], op : DeltaOperations . Operation , context : Map [ String , String ], metrics : Map [ String , String ]): Long commitLarge ...FIXME commitLarge is used when: ConvertToDeltaCommand command is executed updateAndCheckpoint \u00b6 updateAndCheckpoint ( spark : SparkSession , deltaLog : DeltaLog , commitSize : Int , attemptVersion : Long ): Unit updateAndCheckpoint ...FIXME","title":"DeltaCommand"},{"location":"commands/DeltaCommand/#deltacommand","text":"DeltaCommand is a marker interface for commands to work with data in delta tables.","title":"DeltaCommand"},{"location":"commands/DeltaCommand/#implementations","text":"AlterDeltaTableCommand ConvertToDeltaCommand DeleteCommand MergeIntoCommand UpdateCommand VacuumCommandImpl WriteIntoDelta","title":"Implementations"},{"location":"commands/DeltaCommand/#parsepartitionpredicates-method","text":"parsePartitionPredicates ( spark : SparkSession , predicate : String ): Seq [ Expression ] parsePartitionPredicates ...FIXME parsePartitionPredicates is used when...FIXME","title":" parsePartitionPredicates Method"},{"location":"commands/DeltaCommand/#verifypartitionpredicates-method","text":"verifyPartitionPredicates ( spark : SparkSession , partitionColumns : Seq [ String ], predicates : Seq [ Expression ]): Unit verifyPartitionPredicates ...FIXME verifyPartitionPredicates is used when...FIXME","title":" verifyPartitionPredicates Method"},{"location":"commands/DeltaCommand/#generatecandidatefilemap-method","text":"generateCandidateFileMap ( basePath : Path , candidateFiles : Seq [ AddFile ]): Map [ String , AddFile ] generateCandidateFileMap ...FIXME generateCandidateFileMap is used when...FIXME","title":" generateCandidateFileMap Method"},{"location":"commands/DeltaCommand/#removefilesfrompaths-method","text":"removeFilesFromPaths ( deltaLog : DeltaLog , nameToAddFileMap : Map [ String , AddFile ], filesToRewrite : Seq [ String ], operationTimestamp : Long ): Seq [ RemoveFile ] removeFilesFromPaths ...FIXME removeFilesFromPaths is used when: DeleteCommand and UpdateCommand commands are executed","title":" removeFilesFromPaths Method"},{"location":"commands/DeltaCommand/#creating-hadoopfsrelation-with-tahoebatchfileindex","text":"buildBaseRelation ( spark : SparkSession , txn : OptimisticTransaction , actionType : String , rootPath : Path , inputLeafFiles : Seq [ String ], nameToAddFileMap : Map [ String , AddFile ]): HadoopFsRelation buildBaseRelation converts the given inputLeafFiles to AddFiles (with the given rootPath and nameToAddFileMap ). buildBaseRelation creates a TahoeBatchFileIndex for the AddFile s (with the input actionType and rootPath ). In the end, buildBaseRelation creates a HadoopFsRelation ( Spark SQL ) with the TahoeBatchFileIndex (and the other properties based on the metadata of the given OptimisticTransaction ). buildBaseRelation is used when: DeleteCommand and UpdateCommand commands are executed (with delete and update action types, respectively)","title":" Creating HadoopFsRelation (with TahoeBatchFileIndex)"},{"location":"commands/DeltaCommand/#gettouchedfile-method","text":"getTouchedFile ( basePath : Path , filePath : String , nameToAddFileMap : Map [ String , AddFile ]): AddFile getTouchedFile ...FIXME getTouchedFile is used when: DeltaCommand is requested to removeFilesFromPaths and create a HadoopFsRelation (for DeleteCommand and UpdateCommand commands) MergeIntoCommand is executed","title":" getTouchedFile Method"},{"location":"commands/DeltaCommand/#iscatalogtable-method","text":"isCatalogTable ( analyzer : Analyzer , tableIdent : TableIdentifier ): Boolean isCatalogTable ...FIXME isCatalogTable is used when...FIXME","title":" isCatalogTable Method"},{"location":"commands/DeltaCommand/#ispathidentifier-method","text":"isPathIdentifier ( tableIdent : TableIdentifier ): Boolean isPathIdentifier ...FIXME isPathIdentifier is used when...FIXME","title":" isPathIdentifier Method"},{"location":"commands/DeltaCommand/#commitlarge","text":"commitLarge ( spark : SparkSession , txn : OptimisticTransaction , actions : Iterator [ Action ], op : DeltaOperations . Operation , context : Map [ String , String ], metrics : Map [ String , String ]): Long commitLarge ...FIXME commitLarge is used when: ConvertToDeltaCommand command is executed","title":" commitLarge"},{"location":"commands/DeltaCommand/#updateandcheckpoint","text":"updateAndCheckpoint ( spark : SparkSession , deltaLog : DeltaLog , commitSize : Int , attemptVersion : Long ): Unit updateAndCheckpoint ...FIXME","title":" updateAndCheckpoint"},{"location":"commands/DeltaMergeBuilder/","text":"DeltaMergeBuilder \u00b6 DeltaMergeBuilder is a builder interface to describe how to merge data from a source DataFrame into the target delta table (using whenMatched and whenNotMatched conditions). DeltaMergeBuilder is created using DeltaTable.merge operator. In the end, DeltaMergeBuilder is supposed to be executed to take action. DeltaMergeBuilder creates a DeltaMergeInto logical command that is resolved to a MergeIntoCommand runnable logical command (using PreprocessTableMerge logical resolution rule). Creating Instance \u00b6 DeltaMergeBuilder takes the following to be created: Target DeltaTable Source DataFrame Condition Column When Clauses DeltaMergeBuilder is created using DeltaTable.merge operator. Operators \u00b6 whenMatched \u00b6 whenMatched (): DeltaMergeMatchedActionBuilder whenMatched ( condition : Column ): DeltaMergeMatchedActionBuilder whenMatched ( condition : String ): DeltaMergeMatchedActionBuilder Creates a DeltaMergeMatchedActionBuilder (for the DeltaMergeBuilder and a condition) whenNotMatched \u00b6 whenNotMatched (): DeltaMergeNotMatchedActionBuilder whenNotMatched ( condition : Column ): DeltaMergeNotMatchedActionBuilder whenNotMatched ( condition : String ): DeltaMergeNotMatchedActionBuilder Creates a DeltaMergeNotMatchedActionBuilder (for the DeltaMergeBuilder and a condition) Executing Merge \u00b6 execute (): Unit execute creates a merge plan (that is DeltaMergeInto logical command) and resolves column references . execute runs PreprocessTableMerge logical resolution rule on the DeltaMergeInto logical command (that gives MergeIntoCommand runnable logical command). In the end, execute executes the MergeIntoCommand logical command. Creating Logical Plan for Merge \u00b6 mergePlan : DeltaMergeInto mergePlan creates a DeltaMergeInto logical command. mergePlan is used when DeltaMergeBuilder is requested to execute . Creating DeltaMergeBuilder \u00b6 apply ( targetTable : DeltaTable , source : DataFrame , onCondition : Column ): DeltaMergeBuilder apply utility creates a new DeltaMergeBuilder for the given parameters and no DeltaMergeIntoClauses . apply is used for DeltaTable.merge operator. Adding DeltaMergeIntoClause \u00b6 withClause ( clause : DeltaMergeIntoClause ): DeltaMergeBuilder withClause creates a new DeltaMergeBuilder (based on the existing properties, e.g. the DeltaTable ) with the given DeltaMergeIntoClause added to the existing DeltaMergeIntoClauses (to create a more refined DeltaMergeBuilder ). withClause is used when: DeltaMergeMatchedActionBuilder is requested to updateAll , delete and addUpdateClause DeltaMergeNotMatchedActionBuilder is requested to insertAll and addInsertClause","title":"DeltaMergeBuilder"},{"location":"commands/DeltaMergeBuilder/#deltamergebuilder","text":"DeltaMergeBuilder is a builder interface to describe how to merge data from a source DataFrame into the target delta table (using whenMatched and whenNotMatched conditions). DeltaMergeBuilder is created using DeltaTable.merge operator. In the end, DeltaMergeBuilder is supposed to be executed to take action. DeltaMergeBuilder creates a DeltaMergeInto logical command that is resolved to a MergeIntoCommand runnable logical command (using PreprocessTableMerge logical resolution rule).","title":"DeltaMergeBuilder"},{"location":"commands/DeltaMergeBuilder/#creating-instance","text":"DeltaMergeBuilder takes the following to be created: Target DeltaTable Source DataFrame Condition Column When Clauses DeltaMergeBuilder is created using DeltaTable.merge operator.","title":"Creating Instance"},{"location":"commands/DeltaMergeBuilder/#operators","text":"","title":"Operators"},{"location":"commands/DeltaMergeBuilder/#whenmatched","text":"whenMatched (): DeltaMergeMatchedActionBuilder whenMatched ( condition : Column ): DeltaMergeMatchedActionBuilder whenMatched ( condition : String ): DeltaMergeMatchedActionBuilder Creates a DeltaMergeMatchedActionBuilder (for the DeltaMergeBuilder and a condition)","title":" whenMatched"},{"location":"commands/DeltaMergeBuilder/#whennotmatched","text":"whenNotMatched (): DeltaMergeNotMatchedActionBuilder whenNotMatched ( condition : Column ): DeltaMergeNotMatchedActionBuilder whenNotMatched ( condition : String ): DeltaMergeNotMatchedActionBuilder Creates a DeltaMergeNotMatchedActionBuilder (for the DeltaMergeBuilder and a condition)","title":" whenNotMatched"},{"location":"commands/DeltaMergeBuilder/#executing-merge","text":"execute (): Unit execute creates a merge plan (that is DeltaMergeInto logical command) and resolves column references . execute runs PreprocessTableMerge logical resolution rule on the DeltaMergeInto logical command (that gives MergeIntoCommand runnable logical command). In the end, execute executes the MergeIntoCommand logical command.","title":" Executing Merge"},{"location":"commands/DeltaMergeBuilder/#creating-logical-plan-for-merge","text":"mergePlan : DeltaMergeInto mergePlan creates a DeltaMergeInto logical command. mergePlan is used when DeltaMergeBuilder is requested to execute .","title":" Creating Logical Plan for Merge"},{"location":"commands/DeltaMergeBuilder/#creating-deltamergebuilder","text":"apply ( targetTable : DeltaTable , source : DataFrame , onCondition : Column ): DeltaMergeBuilder apply utility creates a new DeltaMergeBuilder for the given parameters and no DeltaMergeIntoClauses . apply is used for DeltaTable.merge operator.","title":" Creating DeltaMergeBuilder"},{"location":"commands/DeltaMergeBuilder/#adding-deltamergeintoclause","text":"withClause ( clause : DeltaMergeIntoClause ): DeltaMergeBuilder withClause creates a new DeltaMergeBuilder (based on the existing properties, e.g. the DeltaTable ) with the given DeltaMergeIntoClause added to the existing DeltaMergeIntoClauses (to create a more refined DeltaMergeBuilder ). withClause is used when: DeltaMergeMatchedActionBuilder is requested to updateAll , delete and addUpdateClause DeltaMergeNotMatchedActionBuilder is requested to insertAll and addInsertClause","title":" Adding DeltaMergeIntoClause"},{"location":"commands/DeltaMergeInto/","text":"DeltaMergeInto Logical Command \u00b6 DeltaMergeInto is a logical Command ( Spark SQL ). Creating Instance \u00b6 DeltaMergeInto takes the following to be created: Target LogicalPlan Source LogicalPlan Condition Expression Matched Clauses ( Seq[DeltaMergeIntoMatchedClause] ) Optional Non-Matched Clause ( Option[DeltaMergeIntoInsertClause] ) Optional Migrated Schema (default: undefined ) DeltaMergeInto is created (using apply and resolveReferences utilities) when: DeltaMergeBuilder is requested to execute DeltaAnalysis logical resolution rule is executed Utilities \u00b6 apply \u00b6 apply ( target : LogicalPlan , source : LogicalPlan , condition : Expression , whenClauses : Seq [ DeltaMergeIntoClause ]): DeltaMergeInto apply ...FIXME apply is used when: DeltaMergeBuilder is requested to execute (when mergePlan ) DeltaAnalysis logical resolution rule is executed (and resolves MergeIntoTable logical command) resolveReferences \u00b6 resolveReferences ( merge : DeltaMergeInto , conf : SQLConf )( resolveExpr : ( Expression , LogicalPlan ) => Expression ): DeltaMergeInto resolveReferences ...FIXME resolveReferences is used when: DeltaMergeBuilder is requested to execute DeltaAnalysis logical resolution rule is executed (and resolves MergeIntoTable logical command)","title":"DeltaMergeInto"},{"location":"commands/DeltaMergeInto/#deltamergeinto-logical-command","text":"DeltaMergeInto is a logical Command ( Spark SQL ).","title":"DeltaMergeInto Logical Command"},{"location":"commands/DeltaMergeInto/#creating-instance","text":"DeltaMergeInto takes the following to be created: Target LogicalPlan Source LogicalPlan Condition Expression Matched Clauses ( Seq[DeltaMergeIntoMatchedClause] ) Optional Non-Matched Clause ( Option[DeltaMergeIntoInsertClause] ) Optional Migrated Schema (default: undefined ) DeltaMergeInto is created (using apply and resolveReferences utilities) when: DeltaMergeBuilder is requested to execute DeltaAnalysis logical resolution rule is executed","title":"Creating Instance"},{"location":"commands/DeltaMergeInto/#utilities","text":"","title":"Utilities"},{"location":"commands/DeltaMergeInto/#apply","text":"apply ( target : LogicalPlan , source : LogicalPlan , condition : Expression , whenClauses : Seq [ DeltaMergeIntoClause ]): DeltaMergeInto apply ...FIXME apply is used when: DeltaMergeBuilder is requested to execute (when mergePlan ) DeltaAnalysis logical resolution rule is executed (and resolves MergeIntoTable logical command)","title":" apply"},{"location":"commands/DeltaMergeInto/#resolvereferences","text":"resolveReferences ( merge : DeltaMergeInto , conf : SQLConf )( resolveExpr : ( Expression , LogicalPlan ) => Expression ): DeltaMergeInto resolveReferences ...FIXME resolveReferences is used when: DeltaMergeBuilder is requested to execute DeltaAnalysis logical resolution rule is executed (and resolves MergeIntoTable logical command)","title":" resolveReferences"},{"location":"commands/DeltaMergeIntoClause/","text":"DeltaMergeIntoClause \u00b6 DeltaMergeIntoClause is...FIXME","title":"DeltaMergeIntoClause"},{"location":"commands/DeltaMergeIntoClause/#deltamergeintoclause","text":"DeltaMergeIntoClause is...FIXME","title":"DeltaMergeIntoClause"},{"location":"commands/DeltaMergeMatchedActionBuilder/","text":"DeltaMergeMatchedActionBuilder \u00b6 DeltaMergeMatchedActionBuilder is a builder interface for DeltaMergeBuilder.whenMatched operator. Creating Instance \u00b6 DeltaMergeMatchedActionBuilder takes the following to be created: DeltaMergeBuilder Optional match condition DeltaMergeMatchedActionBuilder is created when DeltaMergeBuilder is requested to whenMatched (using apply factory method). Operators \u00b6 delete \u00b6 delete (): DeltaMergeBuilder Adds a DeltaMergeIntoDeleteClause (with the matchCondition ) to the DeltaMergeBuilder . update \u00b6 update ( set : Map [ String , Column ]): DeltaMergeBuilder updateAll \u00b6 updateAll (): DeltaMergeBuilder updateExpr \u00b6 updateExpr ( set : Map [ String , String ]): DeltaMergeBuilder Creating DeltaMergeMatchedActionBuilder \u00b6 apply ( mergeBuilder : DeltaMergeBuilder , matchCondition : Option [ Column ]): DeltaMergeMatchedActionBuilder apply creates a DeltaMergeMatchedActionBuilder (for the given parameters). apply is used when DeltaMergeBuilder is requested to whenMatched .","title":"DeltaMergeMatchedActionBuilder"},{"location":"commands/DeltaMergeMatchedActionBuilder/#deltamergematchedactionbuilder","text":"DeltaMergeMatchedActionBuilder is a builder interface for DeltaMergeBuilder.whenMatched operator.","title":"DeltaMergeMatchedActionBuilder"},{"location":"commands/DeltaMergeMatchedActionBuilder/#creating-instance","text":"DeltaMergeMatchedActionBuilder takes the following to be created: DeltaMergeBuilder Optional match condition DeltaMergeMatchedActionBuilder is created when DeltaMergeBuilder is requested to whenMatched (using apply factory method).","title":"Creating Instance"},{"location":"commands/DeltaMergeMatchedActionBuilder/#operators","text":"","title":"Operators"},{"location":"commands/DeltaMergeMatchedActionBuilder/#delete","text":"delete (): DeltaMergeBuilder Adds a DeltaMergeIntoDeleteClause (with the matchCondition ) to the DeltaMergeBuilder .","title":" delete"},{"location":"commands/DeltaMergeMatchedActionBuilder/#update","text":"update ( set : Map [ String , Column ]): DeltaMergeBuilder","title":" update"},{"location":"commands/DeltaMergeMatchedActionBuilder/#updateall","text":"updateAll (): DeltaMergeBuilder","title":" updateAll"},{"location":"commands/DeltaMergeMatchedActionBuilder/#updateexpr","text":"updateExpr ( set : Map [ String , String ]): DeltaMergeBuilder","title":" updateExpr"},{"location":"commands/DeltaMergeMatchedActionBuilder/#creating-deltamergematchedactionbuilder","text":"apply ( mergeBuilder : DeltaMergeBuilder , matchCondition : Option [ Column ]): DeltaMergeMatchedActionBuilder apply creates a DeltaMergeMatchedActionBuilder (for the given parameters). apply is used when DeltaMergeBuilder is requested to whenMatched .","title":" Creating DeltaMergeMatchedActionBuilder"},{"location":"commands/DeltaMergeNotMatchedActionBuilder/","text":"DeltaMergeNotMatchedActionBuilder \u00b6 DeltaMergeNotMatchedActionBuilder is...FIXME","title":"DeltaMergeNotMatchedActionBuilder"},{"location":"commands/DeltaMergeNotMatchedActionBuilder/#deltamergenotmatchedactionbuilder","text":"DeltaMergeNotMatchedActionBuilder is...FIXME","title":"DeltaMergeNotMatchedActionBuilder"},{"location":"commands/JoinedRowProcessor/","text":"JoinedRowProcessor \u00b6 JoinedRowProcessor is...FIXME Creating Instance \u00b6 JoinedRowProcessor takes the following to be created: targetRowHasNoMatch Expression sourceRowHasNoMatch Expression Optional matchedCondition1 Expression Optional matchedOutput1 Expressions Optional matchedCondition2 Expression Optional matchedOutput2 Expressions Optional notMatchedCondition Expression Optional notMatchedOutput Expressions Optional noopCopyOutput Expression deleteRowOutput Expressions joinedAttributes Attributes joinedRowEncoder ExpressionEncoder outputRowEncoder ExpressionEncoder JoinedRowProcessor is created when MergeIntoCommand is requested to writeAllChanges . Processing Partition \u00b6 processPartition ( rowIterator : Iterator [ Row ]): Iterator [ Row ] processPartition ...FIXME processPartition is used when MergeIntoCommand is requested to writeAllChanges .","title":"JoinedRowProcessor"},{"location":"commands/JoinedRowProcessor/#joinedrowprocessor","text":"JoinedRowProcessor is...FIXME","title":"JoinedRowProcessor"},{"location":"commands/JoinedRowProcessor/#creating-instance","text":"JoinedRowProcessor takes the following to be created: targetRowHasNoMatch Expression sourceRowHasNoMatch Expression Optional matchedCondition1 Expression Optional matchedOutput1 Expressions Optional matchedCondition2 Expression Optional matchedOutput2 Expressions Optional notMatchedCondition Expression Optional notMatchedOutput Expressions Optional noopCopyOutput Expression deleteRowOutput Expressions joinedAttributes Attributes joinedRowEncoder ExpressionEncoder outputRowEncoder ExpressionEncoder JoinedRowProcessor is created when MergeIntoCommand is requested to writeAllChanges .","title":"Creating Instance"},{"location":"commands/JoinedRowProcessor/#processing-partition","text":"processPartition ( rowIterator : Iterator [ Row ]): Iterator [ Row ] processPartition ...FIXME processPartition is used when MergeIntoCommand is requested to writeAllChanges .","title":" Processing Partition"},{"location":"commands/MergeIntoCommand/","text":"MergeIntoCommand \u00b6 MergeIntoCommand is a DeltaCommand that represents a DeltaMergeInto logical command at execution. MergeIntoCommand is a RunnableCommand logical operator ( Spark SQL ). Tip Learn more in Demo: Merge Operation . Performance Metrics \u00b6 Name web UI numSourceRows number of source rows numTargetRowsCopied number of target rows rewritten unmodified numTargetRowsInserted number of inserted rows numTargetRowsUpdated number of updated rows numTargetRowsDeleted number of deleted rows numTargetFilesBeforeSkipping number of target files before skipping numTargetFilesAfterSkipping number of target files after skipping numTargetFilesRemoved number of files removed to target numTargetFilesAdded number of files added to target number of target rows rewritten unmodified \u00b6 numTargetRowsCopied performance metric (like the other metrics ) is turned into a non-deterministic user-defined function (UDF). numTargetRowsCopied becomes incrNoopCountExpr UDF. incrNoopCountExpr UDF is resolved on a joined plan and used to create a JoinedRowProcessor for processing partitions of the joined plan Dataset . Creating Instance \u00b6 MergeIntoCommand takes the following to be created: Source Data Target Data ( LogicalPlan ) TahoeFileIndex Condition Expression Matched Clauses ( Seq[DeltaMergeIntoMatchedClause] ) Optional Non-Matched Clause ( Option[DeltaMergeIntoInsertClause] ) Migrated Schema MergeIntoCommand is created when: PreprocessTableMerge logical resolution rule is executed (on a DeltaMergeInto logical command) Source Data (to Merge From) \u00b6 When created , MergeIntoCommand is given a LogicalPlan for the source data to merge from (referred to internally as source ). The source LogicalPlan is used twice: Firstly, in one of the following: An inner join (in findTouchedFiles ) that is count in web UI A leftanti join (in writeInsertsOnlyWhenNoMatchedClauses ) Secondly, in rightOuter or fullOuter join (in writeAllChanges ) Tip Enable DEBUG logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger to see the inner-workings of writeAllChanges . Target DeltaLog \u00b6 targetDeltaLog : DeltaLog targetDeltaLog is the DeltaLog of the TahoeFileIndex . targetDeltaLog is used for the following: Start a new transaction when executed To access the Data Path when finding files to rewrite Lazy Value targetDeltaLog is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards. Executing Command \u00b6 run ( spark : SparkSession ): Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run requests the target DeltaLog to start a new transaction . With spark.databricks.delta.schema.autoMerge.enabled configuration property enabled, run updates the metadata (of the transaction). run determines Delta actions ( RemoveFile s and AddFile s). Describe deltaActions part With spark.databricks.delta.history.metricsEnabled configuration property enabled, run requests the current transaction to register SQL metrics for the Delta operation . run requests the current transaction to commit (with the Delta actions and Merge operation). run records the Delta event. run posts a SparkListenerDriverAccumUpdates Spark event (with the metrics). In the end, run requests the CacheManager to recacheByPlan . Finding Files to Rewrite \u00b6 findTouchedFiles ( spark : SparkSession , deltaTxn : OptimisticTransaction ): Seq [ AddFile ] Important findTouchedFiles is such a fine piece of art ( a gem ). It uses a custom accumulator, a UDF (to use this accumulator to record touched file names) and input_file_name() standard function for the names of the files read. It is always worth keeping in mind that Delta Lake uses files for data storage and that is why input_file_name() standard function works. It would not work for non-file-based data sources. Example 1: Understanding the Internals of findTouchedFiles The following query writes out a 10-element dataset using the default parquet data source to /tmp/parquet directory: val target = \"/tmp/parquet\" spark . range ( 10 ). write . save ( target ) The number of parquet part files varies based on the number of partitions (CPU cores). The following query loads the parquet dataset back alongside input_file_name() standard function to mimic findTouchedFiles 's behaviour. val FILE_NAME_COL = \"_file_name_\" val dataFiles = spark . read . parquet ( target ). withColumn ( FILE_NAME_COL , input_file_name ()) scala> dataFiles.show(truncate = false) +---+---------------------------------------------------------------------------------------+ |id |_file_name_ | +---+---------------------------------------------------------------------------------------+ |4 |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |0 |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |3 |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |6 |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |1 |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |8 |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |2 |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |7 |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |5 |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |9 |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| +---+---------------------------------------------------------------------------------------+ As you may have thought, not all part files have got data and so they are not included in the dataset. That is when findTouchedFiles uses groupBy operator and count action to calculate match frequency. val counts = dataFiles.groupBy(FILE_NAME_COL).count() scala> counts.show(truncate = false) +---------------------------------------------------------------------------------------+-----+ |_file_name_ |count| +---------------------------------------------------------------------------------------+-----+ |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | +---------------------------------------------------------------------------------------+-----+ Let's load all the part files in the /tmp/parquet directory and find which file(s) have no data. import scala . sys . process . _ val cmd = ( s\"ls $ target \" #| \"grep .parquet\" ). lineStream val allFiles = cmd . toArray . toSeq . toDF ( FILE_NAME_COL ) . select ( concat ( lit ( s\"file:// $ target /\" ), col ( FILE_NAME_COL )) as FILE_NAME_COL ) val joinType = \"left_anti\" // MergeIntoCommand uses inner as it wants data file val noDataFiles = allFiles . join ( dataFiles , Seq ( FILE_NAME_COL ), joinType ) Mind that the data vs non-data datasets could be different, but that should not \"interfere\" with the main reasoning flow. scala> noDataFiles.show(truncate = false) +---------------------------------------------------------------------------------------+ |_file_name_ | +---------------------------------------------------------------------------------------+ |file:///tmp/parquet/part-00000-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| +---------------------------------------------------------------------------------------+ findTouchedFiles registers an accumulator to collect all the distinct files that need to be rewritten ( touched files ). Note The name of the accumulator is internal.metrics.MergeIntoDelta.touchedFiles and internal.metrics part is supposed to hide it from web UI as potentially large (set of file names to be rewritten). findTouchedFiles defines a nondeterministic UDF that adds the file names to the accumulator ( recordTouchedFileName ). findTouchedFiles splits conjunctive predicates ( And binary expressions) in the condition expression and collects the predicates that use the target 's columns ( targetOnlyPredicates ). findTouchedFiles requests the given OptimisticTransaction for the files that match the target-only predicates (and creates a dataSkippedFiles collection of AddFile s). Note This step looks similar to filter predicate pushdown . findTouchedFiles creates one DataFrame for the source data (using Spark SQL utility). findTouchedFiles builds a logical query plan for the files (matching the predicates) and creates another DataFrame for the target data. findTouchedFiles adds two columns to the target dataframe: _row_id_ for monotonically_increasing_id() standard function _file_name_ for input_file_name() standard function findTouchedFiles creates (a DataFrame that is) an INNER JOIN of the source and target DataFrame s using the condition expression. findTouchedFiles takes the joined dataframe and selects _row_id_ column and the recordTouchedFileName UDF on the _file_name_ column as one . The DataFrame is internally known as collectTouchedFiles . findTouchedFiles uses groupBy operator on _row_id_ to calculate a sum of all the values in the one column (as count column) in the two-column collectTouchedFiles dataset. The DataFrame is internally known as matchedRowCounts . Note No Spark job has been submitted yet. findTouchedFiles is still in \"query preparation\" mode. findTouchedFiles uses filter on the count column (in the matchedRowCounts dataset) with values greater than 1 . If there are any, findTouchedFiles throws an UnsupportedOperationException exception: Cannot perform MERGE as multiple source rows matched and attempted to update the same target row in the Delta table. By SQL semantics of merge, when multiple source rows match on the same target row, the update operation is ambiguous as it is unclear which source should be used to update the matching target row. You can preprocess the source table to eliminate the possibility of multiple matches. Note Since findTouchedFiles uses count action there should be a Spark SQL query reported (and possibly Spark jobs) in web UI. findTouchedFiles requests the touchedFilesAccum accumulator for the touched file names. Example 2: Understanding the Internals of findTouchedFiles val TOUCHED_FILES_ACCUM_NAME = \"MergeIntoDelta.touchedFiles\" val touchedFilesAccum = spark . sparkContext . collectionAccumulator [ String ]( TOUCHED_FILES_ACCUM_NAME ) val recordTouchedFileName = udf { ( fileName : String ) => { touchedFilesAccum . add ( fileName ) 1 }}. asNondeterministic () val target = \"/tmp/parquet\" spark . range ( 10 ). write . save ( target ) val FILE_NAME_COL = \"_file_name_\" val dataFiles = spark . read . parquet ( target ). withColumn ( FILE_NAME_COL , input_file_name ()) val collectTouchedFiles = dataFiles . select ( col ( FILE_NAME_COL ), recordTouchedFileName ( col ( FILE_NAME_COL )). as ( \"one\" )) scala> collectTouchedFiles.show(truncate = false) +---------------------------------------------------------------------------------------+---+ |_file_name_ |one| +---------------------------------------------------------------------------------------+---+ |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | +---------------------------------------------------------------------------------------+---+ import scala . collection . JavaConverters . _ val touchedFileNames = touchedFilesAccum . value . asScala . toSeq Use the Stages tab in web UI to review the accumulator values. findTouchedFiles prints out the following TRACE message to the logs: findTouchedFiles: matched files: [touchedFileNames] findTouchedFiles generateCandidateFileMap for the files that match the target-only predicates . findTouchedFiles getTouchedFile for every touched file name. findTouchedFiles updates the following performance metrics: numTargetFilesBeforeSkipping and adds the numOfFiles of the Snapshot of the given OptimisticTransaction numTargetFilesAfterSkipping and adds the number of the files that match the target-only predicates numTargetFilesRemoved and adds the number of the touched files In the end, findTouchedFiles gives the touched files (as AddFile s). Writing All Changes \u00b6 writeAllChanges ( spark : SparkSession , deltaTxn : OptimisticTransaction , filesToRewrite : Seq [ AddFile ]): Seq [ AddFile ] writeAllChanges builds the target output columns (possibly with some null s for the target columns that are not in the current schema). writeAllChanges builds a target logical query plan for the AddFiles . writeAllChanges determines a join type to use ( rightOuter or fullOuter ). writeAllChanges prints out the following DEBUG message to the logs: writeAllChanges using [joinType] join: source.output: [outputSet] target.output: [outputSet] condition: [condition] newTarget.output: [outputSet] writeAllChanges creates a joinedDF DataFrame that is a join of the DataFrames for the source and the new target logical plans with the given join condition and the join type . writeAllChanges creates a JoinedRowProcessor that is then used to map over partitions of the joined DataFrame . writeAllChanges prints out the following DEBUG message to the logs: writeAllChanges: join output plan: [outputDF.queryExecution] writeAllChanges requests the input OptimisticTransaction to writeFiles (possibly repartitioning by the partition columns if table is partitioned and spark.databricks.delta.merge.repartitionBeforeWrite.enabled configuration property is enabled). writeAllChanges is used when MergeIntoCommand is requested to run . Building Target Logical Query Plan for AddFiles \u00b6 buildTargetPlanWithFiles ( deltaTxn : OptimisticTransaction , files : Seq [ AddFile ]): LogicalPlan buildTargetPlanWithFiles creates a DataFrame to represent the given AddFile s to access the analyzed logical query plan. buildTargetPlanWithFiles requests the given OptimisticTransaction for the DeltaLog to create a DataFrame (for the Snapshot and the given AddFile s). In the end, buildTargetPlanWithFiles creates a Project logical operator with Alias expressions so the output columns of the analyzed logical query plan (of the DataFrame of the AddFiles ) reference the target's output columns (by name). Note The output columns of the target delta table are associated with a OptimisticTransaction as the Metadata . deltaTxn . metadata . schema writeInsertsOnlyWhenNoMatchedClauses \u00b6 writeInsertsOnlyWhenNoMatchedClauses ( spark : SparkSession , deltaTxn : OptimisticTransaction ): Seq [ AddFile ] writeInsertsOnlyWhenNoMatchedClauses ...FIXME Exceptions \u00b6 run throws an AnalysisException when the target schema is different than the delta table's (has changed after analysis phase): The schema of your Delta table has changed in an incompatible way since your DataFrame or DeltaTable object was created. Please redefine your DataFrame or DeltaTable object. Changes: [schemaDiff] This check can be turned off by setting the session configuration key spark.databricks.delta.checkLatestSchemaOnRead to false. Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.commands.MergeIntoCommand=ALL Refer to Logging .","title":"MergeIntoCommand"},{"location":"commands/MergeIntoCommand/#mergeintocommand","text":"MergeIntoCommand is a DeltaCommand that represents a DeltaMergeInto logical command at execution. MergeIntoCommand is a RunnableCommand logical operator ( Spark SQL ). Tip Learn more in Demo: Merge Operation .","title":"MergeIntoCommand"},{"location":"commands/MergeIntoCommand/#performance-metrics","text":"Name web UI numSourceRows number of source rows numTargetRowsCopied number of target rows rewritten unmodified numTargetRowsInserted number of inserted rows numTargetRowsUpdated number of updated rows numTargetRowsDeleted number of deleted rows numTargetFilesBeforeSkipping number of target files before skipping numTargetFilesAfterSkipping number of target files after skipping numTargetFilesRemoved number of files removed to target numTargetFilesAdded number of files added to target","title":"Performance Metrics"},{"location":"commands/MergeIntoCommand/#number-of-target-rows-rewritten-unmodified","text":"numTargetRowsCopied performance metric (like the other metrics ) is turned into a non-deterministic user-defined function (UDF). numTargetRowsCopied becomes incrNoopCountExpr UDF. incrNoopCountExpr UDF is resolved on a joined plan and used to create a JoinedRowProcessor for processing partitions of the joined plan Dataset .","title":" number of target rows rewritten unmodified"},{"location":"commands/MergeIntoCommand/#creating-instance","text":"MergeIntoCommand takes the following to be created: Source Data Target Data ( LogicalPlan ) TahoeFileIndex Condition Expression Matched Clauses ( Seq[DeltaMergeIntoMatchedClause] ) Optional Non-Matched Clause ( Option[DeltaMergeIntoInsertClause] ) Migrated Schema MergeIntoCommand is created when: PreprocessTableMerge logical resolution rule is executed (on a DeltaMergeInto logical command)","title":"Creating Instance"},{"location":"commands/MergeIntoCommand/#source-data-to-merge-from","text":"When created , MergeIntoCommand is given a LogicalPlan for the source data to merge from (referred to internally as source ). The source LogicalPlan is used twice: Firstly, in one of the following: An inner join (in findTouchedFiles ) that is count in web UI A leftanti join (in writeInsertsOnlyWhenNoMatchedClauses ) Secondly, in rightOuter or fullOuter join (in writeAllChanges ) Tip Enable DEBUG logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger to see the inner-workings of writeAllChanges .","title":" Source Data (to Merge From)"},{"location":"commands/MergeIntoCommand/#target-deltalog","text":"targetDeltaLog : DeltaLog targetDeltaLog is the DeltaLog of the TahoeFileIndex . targetDeltaLog is used for the following: Start a new transaction when executed To access the Data Path when finding files to rewrite Lazy Value targetDeltaLog is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards.","title":" Target DeltaLog"},{"location":"commands/MergeIntoCommand/#executing-command","text":"run ( spark : SparkSession ): Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run requests the target DeltaLog to start a new transaction . With spark.databricks.delta.schema.autoMerge.enabled configuration property enabled, run updates the metadata (of the transaction). run determines Delta actions ( RemoveFile s and AddFile s). Describe deltaActions part With spark.databricks.delta.history.metricsEnabled configuration property enabled, run requests the current transaction to register SQL metrics for the Delta operation . run requests the current transaction to commit (with the Delta actions and Merge operation). run records the Delta event. run posts a SparkListenerDriverAccumUpdates Spark event (with the metrics). In the end, run requests the CacheManager to recacheByPlan .","title":" Executing Command"},{"location":"commands/MergeIntoCommand/#finding-files-to-rewrite","text":"findTouchedFiles ( spark : SparkSession , deltaTxn : OptimisticTransaction ): Seq [ AddFile ] Important findTouchedFiles is such a fine piece of art ( a gem ). It uses a custom accumulator, a UDF (to use this accumulator to record touched file names) and input_file_name() standard function for the names of the files read. It is always worth keeping in mind that Delta Lake uses files for data storage and that is why input_file_name() standard function works. It would not work for non-file-based data sources. Example 1: Understanding the Internals of findTouchedFiles The following query writes out a 10-element dataset using the default parquet data source to /tmp/parquet directory: val target = \"/tmp/parquet\" spark . range ( 10 ). write . save ( target ) The number of parquet part files varies based on the number of partitions (CPU cores). The following query loads the parquet dataset back alongside input_file_name() standard function to mimic findTouchedFiles 's behaviour. val FILE_NAME_COL = \"_file_name_\" val dataFiles = spark . read . parquet ( target ). withColumn ( FILE_NAME_COL , input_file_name ()) scala> dataFiles.show(truncate = false) +---+---------------------------------------------------------------------------------------+ |id |_file_name_ | +---+---------------------------------------------------------------------------------------+ |4 |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |0 |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |3 |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |6 |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |1 |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |8 |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |2 |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |7 |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |5 |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |9 |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| +---+---------------------------------------------------------------------------------------+ As you may have thought, not all part files have got data and so they are not included in the dataset. That is when findTouchedFiles uses groupBy operator and count action to calculate match frequency. val counts = dataFiles.groupBy(FILE_NAME_COL).count() scala> counts.show(truncate = false) +---------------------------------------------------------------------------------------+-----+ |_file_name_ |count| +---------------------------------------------------------------------------------------+-----+ |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | +---------------------------------------------------------------------------------------+-----+ Let's load all the part files in the /tmp/parquet directory and find which file(s) have no data. import scala . sys . process . _ val cmd = ( s\"ls $ target \" #| \"grep .parquet\" ). lineStream val allFiles = cmd . toArray . toSeq . toDF ( FILE_NAME_COL ) . select ( concat ( lit ( s\"file:// $ target /\" ), col ( FILE_NAME_COL )) as FILE_NAME_COL ) val joinType = \"left_anti\" // MergeIntoCommand uses inner as it wants data file val noDataFiles = allFiles . join ( dataFiles , Seq ( FILE_NAME_COL ), joinType ) Mind that the data vs non-data datasets could be different, but that should not \"interfere\" with the main reasoning flow. scala> noDataFiles.show(truncate = false) +---------------------------------------------------------------------------------------+ |_file_name_ | +---------------------------------------------------------------------------------------+ |file:///tmp/parquet/part-00000-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| +---------------------------------------------------------------------------------------+ findTouchedFiles registers an accumulator to collect all the distinct files that need to be rewritten ( touched files ). Note The name of the accumulator is internal.metrics.MergeIntoDelta.touchedFiles and internal.metrics part is supposed to hide it from web UI as potentially large (set of file names to be rewritten). findTouchedFiles defines a nondeterministic UDF that adds the file names to the accumulator ( recordTouchedFileName ). findTouchedFiles splits conjunctive predicates ( And binary expressions) in the condition expression and collects the predicates that use the target 's columns ( targetOnlyPredicates ). findTouchedFiles requests the given OptimisticTransaction for the files that match the target-only predicates (and creates a dataSkippedFiles collection of AddFile s). Note This step looks similar to filter predicate pushdown . findTouchedFiles creates one DataFrame for the source data (using Spark SQL utility). findTouchedFiles builds a logical query plan for the files (matching the predicates) and creates another DataFrame for the target data. findTouchedFiles adds two columns to the target dataframe: _row_id_ for monotonically_increasing_id() standard function _file_name_ for input_file_name() standard function findTouchedFiles creates (a DataFrame that is) an INNER JOIN of the source and target DataFrame s using the condition expression. findTouchedFiles takes the joined dataframe and selects _row_id_ column and the recordTouchedFileName UDF on the _file_name_ column as one . The DataFrame is internally known as collectTouchedFiles . findTouchedFiles uses groupBy operator on _row_id_ to calculate a sum of all the values in the one column (as count column) in the two-column collectTouchedFiles dataset. The DataFrame is internally known as matchedRowCounts . Note No Spark job has been submitted yet. findTouchedFiles is still in \"query preparation\" mode. findTouchedFiles uses filter on the count column (in the matchedRowCounts dataset) with values greater than 1 . If there are any, findTouchedFiles throws an UnsupportedOperationException exception: Cannot perform MERGE as multiple source rows matched and attempted to update the same target row in the Delta table. By SQL semantics of merge, when multiple source rows match on the same target row, the update operation is ambiguous as it is unclear which source should be used to update the matching target row. You can preprocess the source table to eliminate the possibility of multiple matches. Note Since findTouchedFiles uses count action there should be a Spark SQL query reported (and possibly Spark jobs) in web UI. findTouchedFiles requests the touchedFilesAccum accumulator for the touched file names. Example 2: Understanding the Internals of findTouchedFiles val TOUCHED_FILES_ACCUM_NAME = \"MergeIntoDelta.touchedFiles\" val touchedFilesAccum = spark . sparkContext . collectionAccumulator [ String ]( TOUCHED_FILES_ACCUM_NAME ) val recordTouchedFileName = udf { ( fileName : String ) => { touchedFilesAccum . add ( fileName ) 1 }}. asNondeterministic () val target = \"/tmp/parquet\" spark . range ( 10 ). write . save ( target ) val FILE_NAME_COL = \"_file_name_\" val dataFiles = spark . read . parquet ( target ). withColumn ( FILE_NAME_COL , input_file_name ()) val collectTouchedFiles = dataFiles . select ( col ( FILE_NAME_COL ), recordTouchedFileName ( col ( FILE_NAME_COL )). as ( \"one\" )) scala> collectTouchedFiles.show(truncate = false) +---------------------------------------------------------------------------------------+---+ |_file_name_ |one| +---------------------------------------------------------------------------------------+---+ |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | +---------------------------------------------------------------------------------------+---+ import scala . collection . JavaConverters . _ val touchedFileNames = touchedFilesAccum . value . asScala . toSeq Use the Stages tab in web UI to review the accumulator values. findTouchedFiles prints out the following TRACE message to the logs: findTouchedFiles: matched files: [touchedFileNames] findTouchedFiles generateCandidateFileMap for the files that match the target-only predicates . findTouchedFiles getTouchedFile for every touched file name. findTouchedFiles updates the following performance metrics: numTargetFilesBeforeSkipping and adds the numOfFiles of the Snapshot of the given OptimisticTransaction numTargetFilesAfterSkipping and adds the number of the files that match the target-only predicates numTargetFilesRemoved and adds the number of the touched files In the end, findTouchedFiles gives the touched files (as AddFile s).","title":" Finding Files to Rewrite"},{"location":"commands/MergeIntoCommand/#writing-all-changes","text":"writeAllChanges ( spark : SparkSession , deltaTxn : OptimisticTransaction , filesToRewrite : Seq [ AddFile ]): Seq [ AddFile ] writeAllChanges builds the target output columns (possibly with some null s for the target columns that are not in the current schema). writeAllChanges builds a target logical query plan for the AddFiles . writeAllChanges determines a join type to use ( rightOuter or fullOuter ). writeAllChanges prints out the following DEBUG message to the logs: writeAllChanges using [joinType] join: source.output: [outputSet] target.output: [outputSet] condition: [condition] newTarget.output: [outputSet] writeAllChanges creates a joinedDF DataFrame that is a join of the DataFrames for the source and the new target logical plans with the given join condition and the join type . writeAllChanges creates a JoinedRowProcessor that is then used to map over partitions of the joined DataFrame . writeAllChanges prints out the following DEBUG message to the logs: writeAllChanges: join output plan: [outputDF.queryExecution] writeAllChanges requests the input OptimisticTransaction to writeFiles (possibly repartitioning by the partition columns if table is partitioned and spark.databricks.delta.merge.repartitionBeforeWrite.enabled configuration property is enabled). writeAllChanges is used when MergeIntoCommand is requested to run .","title":" Writing All Changes"},{"location":"commands/MergeIntoCommand/#building-target-logical-query-plan-for-addfiles","text":"buildTargetPlanWithFiles ( deltaTxn : OptimisticTransaction , files : Seq [ AddFile ]): LogicalPlan buildTargetPlanWithFiles creates a DataFrame to represent the given AddFile s to access the analyzed logical query plan. buildTargetPlanWithFiles requests the given OptimisticTransaction for the DeltaLog to create a DataFrame (for the Snapshot and the given AddFile s). In the end, buildTargetPlanWithFiles creates a Project logical operator with Alias expressions so the output columns of the analyzed logical query plan (of the DataFrame of the AddFiles ) reference the target's output columns (by name). Note The output columns of the target delta table are associated with a OptimisticTransaction as the Metadata . deltaTxn . metadata . schema","title":" Building Target Logical Query Plan for AddFiles"},{"location":"commands/MergeIntoCommand/#writeinsertsonlywhennomatchedclauses","text":"writeInsertsOnlyWhenNoMatchedClauses ( spark : SparkSession , deltaTxn : OptimisticTransaction ): Seq [ AddFile ] writeInsertsOnlyWhenNoMatchedClauses ...FIXME","title":" writeInsertsOnlyWhenNoMatchedClauses"},{"location":"commands/MergeIntoCommand/#exceptions","text":"run throws an AnalysisException when the target schema is different than the delta table's (has changed after analysis phase): The schema of your Delta table has changed in an incompatible way since your DataFrame or DeltaTable object was created. Please redefine your DataFrame or DeltaTable object. Changes: [schemaDiff] This check can be turned off by setting the session configuration key spark.databricks.delta.checkLatestSchemaOnRead to false.","title":" Exceptions"},{"location":"commands/MergeIntoCommand/#logging","text":"Enable ALL logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.commands.MergeIntoCommand=ALL Refer to Logging .","title":"Logging"},{"location":"commands/WriteIntoDelta/","text":"WriteIntoDelta Command \u00b6 WriteIntoDelta is a Delta command that can write data(frame) transactionally into a delta table . WriteIntoDelta is a RunnableCommand ( Spark SQL ) logical operator. Creating Instance \u00b6 WriteIntoDelta takes the following to be created: DeltaLog SaveMode DeltaOptions Names of the partition columns Configuration Data ( DataFrame ) WriteIntoDelta is created when: DeltaLog is requested to create an insertable HadoopFsRelation (when DeltaDataSource is requested to create a relation as a CreatableRelationProvider or a RelationProvider ) DeltaCatalog is requested to createDeltaTable WriteIntoDeltaBuilder is requested to buildForV1Write CreateDeltaTableCommand command is executed DeltaDataSource is requested to create a relation (for writing) (as a CreatableRelationProvider ) ImplicitMetadataOperation \u00b6 WriteIntoDelta is an operation that can update metadata (schema and partitioning) of the delta table . Executing Command \u00b6 run ( sparkSession : SparkSession ): Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run requests the DeltaLog to start a new transaction . run writes and requests the OptimisticTransaction to commit (with DeltaOperations.Write operation with the SaveMode , partition columns , replaceWhere and userMetadata ). write \u00b6 write ( txn : OptimisticTransaction , sparkSession : SparkSession ): Seq [ Action ] write checks out whether the write operation is to a delta table that already exists. If so (i.e. the readVersion of the transaction is above -1 ), write branches per the SaveMode : For ErrorIfExists , write throws an AnalysisException . [path] already exists. For Ignore , write does nothing and returns back with no Action s. For Overwrite , write requests the DeltaLog to assert being removable write updateMetadata (with rearrangeOnly option). write ...FIXME write is used when: CreateDeltaTableCommand is executed WriteIntoDelta is executed Demo \u00b6 import org.apache.spark.sql.delta.commands.WriteIntoDelta import org.apache.spark.sql.delta.DeltaLog import org.apache.spark.sql.SaveMode import org.apache.spark.sql.delta.DeltaOptions val tableName = \"/tmp/delta/t1\" val data = spark.range(5).toDF val writeCmd = WriteIntoDelta( deltaLog = DeltaLog.forTable(spark, tableName), mode = SaveMode.Overwrite, options = new DeltaOptions(Map.empty[String, String], spark.sessionState.conf), partitionColumns = Seq.empty[String], configuration = Map.empty[String, String], data) // Review web UI @ http://localhost:4040 writeCmd.run(spark)","title":"WriteIntoDelta"},{"location":"commands/WriteIntoDelta/#writeintodelta-command","text":"WriteIntoDelta is a Delta command that can write data(frame) transactionally into a delta table . WriteIntoDelta is a RunnableCommand ( Spark SQL ) logical operator.","title":"WriteIntoDelta Command"},{"location":"commands/WriteIntoDelta/#creating-instance","text":"WriteIntoDelta takes the following to be created: DeltaLog SaveMode DeltaOptions Names of the partition columns Configuration Data ( DataFrame ) WriteIntoDelta is created when: DeltaLog is requested to create an insertable HadoopFsRelation (when DeltaDataSource is requested to create a relation as a CreatableRelationProvider or a RelationProvider ) DeltaCatalog is requested to createDeltaTable WriteIntoDeltaBuilder is requested to buildForV1Write CreateDeltaTableCommand command is executed DeltaDataSource is requested to create a relation (for writing) (as a CreatableRelationProvider )","title":"Creating Instance"},{"location":"commands/WriteIntoDelta/#implicitmetadataoperation","text":"WriteIntoDelta is an operation that can update metadata (schema and partitioning) of the delta table .","title":"ImplicitMetadataOperation"},{"location":"commands/WriteIntoDelta/#executing-command","text":"run ( sparkSession : SparkSession ): Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run requests the DeltaLog to start a new transaction . run writes and requests the OptimisticTransaction to commit (with DeltaOperations.Write operation with the SaveMode , partition columns , replaceWhere and userMetadata ).","title":" Executing Command"},{"location":"commands/WriteIntoDelta/#write","text":"write ( txn : OptimisticTransaction , sparkSession : SparkSession ): Seq [ Action ] write checks out whether the write operation is to a delta table that already exists. If so (i.e. the readVersion of the transaction is above -1 ), write branches per the SaveMode : For ErrorIfExists , write throws an AnalysisException . [path] already exists. For Ignore , write does nothing and returns back with no Action s. For Overwrite , write requests the DeltaLog to assert being removable write updateMetadata (with rearrangeOnly option). write ...FIXME write is used when: CreateDeltaTableCommand is executed WriteIntoDelta is executed","title":" write"},{"location":"commands/WriteIntoDelta/#demo","text":"import org.apache.spark.sql.delta.commands.WriteIntoDelta import org.apache.spark.sql.delta.DeltaLog import org.apache.spark.sql.SaveMode import org.apache.spark.sql.delta.DeltaOptions val tableName = \"/tmp/delta/t1\" val data = spark.range(5).toDF val writeCmd = WriteIntoDelta( deltaLog = DeltaLog.forTable(spark, tableName), mode = SaveMode.Overwrite, options = new DeltaOptions(Map.empty[String, String], spark.sessionState.conf), partitionColumns = Seq.empty[String], configuration = Map.empty[String, String], data) // Review web UI @ http://localhost:4040 writeCmd.run(spark)","title":"Demo"},{"location":"commands/convert/","text":"Convert to Delta Command \u00b6 Delta Lake supports converting ( importing ) parquet tables to delta format using the following high-level operators: CONVERT TO DELTA SQL command DeltaTable.convertToDelta","title":"Convert to Delta Command"},{"location":"commands/convert/#convert-to-delta-command","text":"Delta Lake supports converting ( importing ) parquet tables to delta format using the following high-level operators: CONVERT TO DELTA SQL command DeltaTable.convertToDelta","title":"Convert to Delta Command"},{"location":"commands/convert/ConvertToDeltaCommand/","text":"ConvertToDeltaCommand \u00b6 ConvertToDeltaCommand is a DeltaCommand that converts a parquet table to delta format ( imports it into Delta). ConvertToDeltaCommand is a RunnableCommand ( Spark SQL ). ConvertToDeltaCommand requires that the partition schema matches the partitions of the parquet table ( or an AnalysisException is thrown ) Creating Instance \u00b6 ConvertToDeltaCommand takes the following to be created: Parquet table ( TableIdentifier ) Partition schema ( Option[StructType] ) Delta Path ( Option[String] ) ConvertToDeltaCommand is created when: CONVERT TO DELTA statement is used (and DeltaSqlAstBuilder is requested to visitConvert ) DeltaTable.convertToDelta utility is used (and DeltaConvert utility is used to executeConvert ) Executing Command \u00b6 run ( spark : SparkSession ): Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) contract. run creates a ConvertProperties from the TableIdentifier (with the given SparkSession ). run makes sure that the (data source) provider (the database part of the TableIdentifier ) is either delta or parquet . For all other data source providers, run throws an AnalysisException : CONVERT TO DELTA only supports parquet tables, but you are trying to convert a [sourceName] source: [ident] For delta data source provider, run simply prints out the following message to standard output and returns. The table you are trying to convert is already a delta table For parquet data source provider, run uses DeltaLog utility to create a DeltaLog . run then requests DeltaLog to update and start a new transaction . In the end, run performConvert . In case the readVersion of the new transaction is greater than -1 , run simply prints out the following message to standard output and returns. The table you are trying to convert is already a delta table performConvert \u00b6 performConvert ( spark : SparkSession , txn : OptimisticTransaction , convertProperties : ConvertTarget ): Seq [ Row ] performConvert makes sure that the directory exists (from the given ConvertProperties which is the table part of the TableIdentifier of the command). performConvert requests the OptimisticTransaction for the DeltaLog that is then requested to ensureLogDirectoryExist . performConvert creates a Dataset to recursively list directories and files in the directory and leaves only files (by filtering out directories using WHERE clause). Note performConvert uses Dataset API to build a distributed computation to query files. performConvert caches the Dataset of file names. performConvert uses spark.databricks.delta.import.batchSize.schemaInference configuration property for the number of files per batch for schema inference. performConvert mergeSchemasInParallel for every batch of files and then mergeSchemas . performConvert constructTableSchema using the inferred table schema and the partitionSchema (if specified). performConvert creates a new Metadata using the table schema and the partitionSchema (if specified). performConvert requests the OptimisticTransaction to update the metadata . performConvert uses spark.databricks.delta.import.batchSize.statsCollection configuration property for the number of files per batch for stats collection. performConvert creates an AddFile (in the data path of the DeltaLog of the OptimisticTransaction ) for every file in a batch. In the end, performConvert streamWrite (with the OptimisticTransaction , the AddFile s, and Convert operation) and unpersists the Dataset of file names. streamWrite \u00b6 streamWrite ( spark : SparkSession , txn : OptimisticTransaction , addFiles : Iterator [ AddFile ], op : DeltaOperations . Operation , numFiles : Long ): Long streamWrite ...FIXME createAddFile \u00b6 createAddFile ( file : SerializableFileStatus , basePath : Path , fs : FileSystem , conf : SQLConf ): AddFile createAddFile creates an AddFile action. Internally, createAddFile ...FIXME createAddFile throws an AnalysisException if the number of fields in the given partition schema does not match the number of partitions found (at partition discovery phase): Expecting [size] partition column(s): [expectedCols], but found [size] partition column(s): [parsedCols] from parsing the file name: [path] mergeSchemasInParallel \u00b6 mergeSchemasInParallel ( sparkSession : SparkSession , filesToTouch : Seq [ FileStatus ], serializedConf : SerializableConfiguration ): Option [ StructType ] mergeSchemasInParallel ...FIXME constructTableSchema \u00b6 constructTableSchema ( spark : SparkSession , dataSchema : StructType , partitionFields : Seq [ StructField ]): StructType constructTableSchema ...FIXME ConvertToDeltaCommandBase \u00b6 ConvertToDeltaCommandBase is the base of ConvertToDeltaCommand -like commands with the only known implementation being ConvertToDeltaCommand itself.","title":"ConvertToDeltaCommand"},{"location":"commands/convert/ConvertToDeltaCommand/#converttodeltacommand","text":"ConvertToDeltaCommand is a DeltaCommand that converts a parquet table to delta format ( imports it into Delta). ConvertToDeltaCommand is a RunnableCommand ( Spark SQL ). ConvertToDeltaCommand requires that the partition schema matches the partitions of the parquet table ( or an AnalysisException is thrown )","title":"ConvertToDeltaCommand"},{"location":"commands/convert/ConvertToDeltaCommand/#creating-instance","text":"ConvertToDeltaCommand takes the following to be created: Parquet table ( TableIdentifier ) Partition schema ( Option[StructType] ) Delta Path ( Option[String] ) ConvertToDeltaCommand is created when: CONVERT TO DELTA statement is used (and DeltaSqlAstBuilder is requested to visitConvert ) DeltaTable.convertToDelta utility is used (and DeltaConvert utility is used to executeConvert )","title":"Creating Instance"},{"location":"commands/convert/ConvertToDeltaCommand/#executing-command","text":"run ( spark : SparkSession ): Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) contract. run creates a ConvertProperties from the TableIdentifier (with the given SparkSession ). run makes sure that the (data source) provider (the database part of the TableIdentifier ) is either delta or parquet . For all other data source providers, run throws an AnalysisException : CONVERT TO DELTA only supports parquet tables, but you are trying to convert a [sourceName] source: [ident] For delta data source provider, run simply prints out the following message to standard output and returns. The table you are trying to convert is already a delta table For parquet data source provider, run uses DeltaLog utility to create a DeltaLog . run then requests DeltaLog to update and start a new transaction . In the end, run performConvert . In case the readVersion of the new transaction is greater than -1 , run simply prints out the following message to standard output and returns. The table you are trying to convert is already a delta table","title":" Executing Command"},{"location":"commands/convert/ConvertToDeltaCommand/#performconvert","text":"performConvert ( spark : SparkSession , txn : OptimisticTransaction , convertProperties : ConvertTarget ): Seq [ Row ] performConvert makes sure that the directory exists (from the given ConvertProperties which is the table part of the TableIdentifier of the command). performConvert requests the OptimisticTransaction for the DeltaLog that is then requested to ensureLogDirectoryExist . performConvert creates a Dataset to recursively list directories and files in the directory and leaves only files (by filtering out directories using WHERE clause). Note performConvert uses Dataset API to build a distributed computation to query files. performConvert caches the Dataset of file names. performConvert uses spark.databricks.delta.import.batchSize.schemaInference configuration property for the number of files per batch for schema inference. performConvert mergeSchemasInParallel for every batch of files and then mergeSchemas . performConvert constructTableSchema using the inferred table schema and the partitionSchema (if specified). performConvert creates a new Metadata using the table schema and the partitionSchema (if specified). performConvert requests the OptimisticTransaction to update the metadata . performConvert uses spark.databricks.delta.import.batchSize.statsCollection configuration property for the number of files per batch for stats collection. performConvert creates an AddFile (in the data path of the DeltaLog of the OptimisticTransaction ) for every file in a batch. In the end, performConvert streamWrite (with the OptimisticTransaction , the AddFile s, and Convert operation) and unpersists the Dataset of file names.","title":" performConvert"},{"location":"commands/convert/ConvertToDeltaCommand/#streamwrite","text":"streamWrite ( spark : SparkSession , txn : OptimisticTransaction , addFiles : Iterator [ AddFile ], op : DeltaOperations . Operation , numFiles : Long ): Long streamWrite ...FIXME","title":" streamWrite"},{"location":"commands/convert/ConvertToDeltaCommand/#createaddfile","text":"createAddFile ( file : SerializableFileStatus , basePath : Path , fs : FileSystem , conf : SQLConf ): AddFile createAddFile creates an AddFile action. Internally, createAddFile ...FIXME createAddFile throws an AnalysisException if the number of fields in the given partition schema does not match the number of partitions found (at partition discovery phase): Expecting [size] partition column(s): [expectedCols], but found [size] partition column(s): [parsedCols] from parsing the file name: [path]","title":" createAddFile"},{"location":"commands/convert/ConvertToDeltaCommand/#mergeschemasinparallel","text":"mergeSchemasInParallel ( sparkSession : SparkSession , filesToTouch : Seq [ FileStatus ], serializedConf : SerializableConfiguration ): Option [ StructType ] mergeSchemasInParallel ...FIXME","title":" mergeSchemasInParallel"},{"location":"commands/convert/ConvertToDeltaCommand/#constructtableschema","text":"constructTableSchema ( spark : SparkSession , dataSchema : StructType , partitionFields : Seq [ StructField ]): StructType constructTableSchema ...FIXME","title":" constructTableSchema"},{"location":"commands/convert/ConvertToDeltaCommand/#converttodeltacommandbase","text":"ConvertToDeltaCommandBase is the base of ConvertToDeltaCommand -like commands with the only known implementation being ConvertToDeltaCommand itself.","title":" ConvertToDeltaCommandBase"},{"location":"commands/convert/DeltaConvert/","text":"DeltaConvert Utility \u00b6 executeConvert \u00b6 executeConvert ( spark : SparkSession , tableIdentifier : TableIdentifier , partitionSchema : Option [ StructType ], deltaPath : Option [ String ]): DeltaTable executeConvert converts a parquet table to a delta table. executeConvert executes a new ConvertToDeltaCommand . In the end, executeConvert creates a DeltaTable . Note executeConvert can convert a Spark table (to Delta) that is registered in a metastore. executeConvert is used when: DeltaTable.convertToDelta utility is used Demo \u00b6 import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) // CONVERT TO DELTA only supports parquet tables // TableIdentifier should be parquet.`users` import org.apache.spark.sql.catalyst.TableIdentifier val table = TableIdentifier(table = \"users\", database = Some(\"parquet\")) import org.apache.spark.sql.types.{StringType, StructField, StructType} val partitionSchema: Option[StructType] = Some( new StructType().add(StructField(\"country\", StringType))) val deltaPath: Option[String] = None // Use web UI to monitor execution, e.g. http://localhost:4040 import io.delta.tables.execution.DeltaConvert DeltaConvert.executeConvert(spark, table, partitionSchema, deltaPath)","title":"DeltaConvert"},{"location":"commands/convert/DeltaConvert/#deltaconvert-utility","text":"","title":"DeltaConvert Utility"},{"location":"commands/convert/DeltaConvert/#executeconvert","text":"executeConvert ( spark : SparkSession , tableIdentifier : TableIdentifier , partitionSchema : Option [ StructType ], deltaPath : Option [ String ]): DeltaTable executeConvert converts a parquet table to a delta table. executeConvert executes a new ConvertToDeltaCommand . In the end, executeConvert creates a DeltaTable . Note executeConvert can convert a Spark table (to Delta) that is registered in a metastore. executeConvert is used when: DeltaTable.convertToDelta utility is used","title":" executeConvert"},{"location":"commands/convert/DeltaConvert/#demo","text":"import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) // CONVERT TO DELTA only supports parquet tables // TableIdentifier should be parquet.`users` import org.apache.spark.sql.catalyst.TableIdentifier val table = TableIdentifier(table = \"users\", database = Some(\"parquet\")) import org.apache.spark.sql.types.{StringType, StructField, StructType} val partitionSchema: Option[StructType] = Some( new StructType().add(StructField(\"country\", StringType))) val deltaPath: Option[String] = None // Use web UI to monitor execution, e.g. http://localhost:4040 import io.delta.tables.execution.DeltaConvert DeltaConvert.executeConvert(spark, table, partitionSchema, deltaPath)","title":"Demo"},{"location":"commands/convert/FileManifest/","text":"FileManifest \u00b6 FileManifest is an abstraction of file manifests for ConvertToDeltaCommand . FileManifest is Closeable ( Java ). Contract \u00b6 basePath \u00b6 basePath : String The base path of a delta table getFiles \u00b6 getFiles : Iterator [ SerializableFileStatus ] The active files of a delta table Used when: ConvertToDeltaCommand is requested to createDeltaActions and performConvert Implementations \u00b6 ManualListingFileManifest MetadataLogFileManifest","title":"FileManifest"},{"location":"commands/convert/FileManifest/#filemanifest","text":"FileManifest is an abstraction of file manifests for ConvertToDeltaCommand . FileManifest is Closeable ( Java ).","title":"FileManifest"},{"location":"commands/convert/FileManifest/#contract","text":"","title":"Contract"},{"location":"commands/convert/FileManifest/#basepath","text":"basePath : String The base path of a delta table","title":" basePath"},{"location":"commands/convert/FileManifest/#getfiles","text":"getFiles : Iterator [ SerializableFileStatus ] The active files of a delta table Used when: ConvertToDeltaCommand is requested to createDeltaActions and performConvert","title":" getFiles"},{"location":"commands/convert/FileManifest/#implementations","text":"ManualListingFileManifest MetadataLogFileManifest","title":"Implementations"},{"location":"commands/convert/ManualListingFileManifest/","text":"ManualListingFileManifest \u00b6 ManualListingFileManifest is a FileManifest . getFiles \u00b6 getFiles : Iterator [ SerializableFileStatus ] getFiles is part of the FileManifest abstraction. getFiles ...FIXME close \u00b6 close close is part of the Closeable ( Java ) abstraction. close ...FIXME HDFS FileStatus List Dataset \u00b6 list : Dataset [ SerializableFileStatus ] list creates a HDFS FileStatus dataset and marks it to be cached (once an action is executed). Scala lazy value list is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged. list is used when: ManualListingFileManifest is requested to getFiles and close doList \u00b6 doList (): Dataset [ SerializableFileStatus ] doList ...FIXME doList is used when: ManualListingFileManifest is requested for file status dataset","title":"ManualListingFileManifest"},{"location":"commands/convert/ManualListingFileManifest/#manuallistingfilemanifest","text":"ManualListingFileManifest is a FileManifest .","title":"ManualListingFileManifest"},{"location":"commands/convert/ManualListingFileManifest/#getfiles","text":"getFiles : Iterator [ SerializableFileStatus ] getFiles is part of the FileManifest abstraction. getFiles ...FIXME","title":" getFiles"},{"location":"commands/convert/ManualListingFileManifest/#close","text":"close close is part of the Closeable ( Java ) abstraction. close ...FIXME","title":" close"},{"location":"commands/convert/ManualListingFileManifest/#hdfs-filestatus-list-dataset","text":"list : Dataset [ SerializableFileStatus ] list creates a HDFS FileStatus dataset and marks it to be cached (once an action is executed). Scala lazy value list is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged. list is used when: ManualListingFileManifest is requested to getFiles and close","title":" HDFS FileStatus List Dataset"},{"location":"commands/convert/ManualListingFileManifest/#dolist","text":"doList (): Dataset [ SerializableFileStatus ] doList ...FIXME doList is used when: ManualListingFileManifest is requested for file status dataset","title":" doList"},{"location":"commands/convert/MetadataLogFileManifest/","text":"MetadataLogFileManifest \u00b6 MetadataLogFileManifest is...FIXME","title":"MetadataLogFileManifest"},{"location":"commands/convert/MetadataLogFileManifest/#metadatalogfilemanifest","text":"MetadataLogFileManifest is...FIXME","title":"MetadataLogFileManifest"},{"location":"commands/delete/","text":"Delete Command \u00b6 Delta Lake supports deleting records from delta tables using the following high-level operators: DELETE FROM SQL command DeltaTable.delete","title":"Delete Command"},{"location":"commands/delete/#delete-command","text":"Delta Lake supports deleting records from delta tables using the following high-level operators: DELETE FROM SQL command DeltaTable.delete","title":"Delete Command"},{"location":"commands/delete/DeleteCommand/","text":"DeleteCommand \u00b6 DeleteCommand is a DeltaCommand that represents DeltaDelete logical command at execution. DeleteCommand is a RunnableCommand ( Spark SQL ) logical operator. Creating Instance \u00b6 DeleteCommand takes the following to be created: TahoeFileIndex Target Data ( LogicalPlan ) Condition ( Expression ) DeleteCommand is created (also using apply factory utility) when: PreprocessTableDelete logical resolution rule is executed (and resolves a DeltaDelete logical command) Performance Metrics \u00b6 Name web UI numRemovedFiles number of files removed. numAddedFiles number of files added. numDeletedRows number of rows deleted. Executing Command \u00b6 run ( sparkSession : SparkSession ): Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run requests the TahoeFileIndex for the DeltaLog (and asserts that the table is removable ). run requests the DeltaLog to start a new transaction for performDelete . In the end, run re-caches all cached plans (incl. this relation itself) by requesting the CacheManager ( Spark SQL ) to recache the target . performDelete \u00b6 performDelete ( sparkSession : SparkSession , deltaLog : DeltaLog , txn : OptimisticTransaction ): Unit Number of Table Files \u00b6 performDelete requests the given DeltaLog for the current Snapshot that is in turn requested for the number of files in the delta table. Finding Delete Actions \u00b6 performDelete branches off based on the optional condition : No condition to delete the whole table Condition defined on metadata only Other conditions Delete Condition Undefined \u00b6 performDelete ...FIXME Metadata-Only Delete Condition \u00b6 performDelete ...FIXME Other Delete Conditions \u00b6 performDelete ...FIXME Delete Actions Available \u00b6 performDelete ...FIXME Creating DeleteCommand \u00b6 apply ( delete : DeltaDelete ): DeleteCommand apply creates a DeleteCommand .","title":"DeleteCommand"},{"location":"commands/delete/DeleteCommand/#deletecommand","text":"DeleteCommand is a DeltaCommand that represents DeltaDelete logical command at execution. DeleteCommand is a RunnableCommand ( Spark SQL ) logical operator.","title":"DeleteCommand"},{"location":"commands/delete/DeleteCommand/#creating-instance","text":"DeleteCommand takes the following to be created: TahoeFileIndex Target Data ( LogicalPlan ) Condition ( Expression ) DeleteCommand is created (also using apply factory utility) when: PreprocessTableDelete logical resolution rule is executed (and resolves a DeltaDelete logical command)","title":"Creating Instance"},{"location":"commands/delete/DeleteCommand/#performance-metrics","text":"Name web UI numRemovedFiles number of files removed. numAddedFiles number of files added. numDeletedRows number of rows deleted.","title":"Performance Metrics"},{"location":"commands/delete/DeleteCommand/#executing-command","text":"run ( sparkSession : SparkSession ): Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run requests the TahoeFileIndex for the DeltaLog (and asserts that the table is removable ). run requests the DeltaLog to start a new transaction for performDelete . In the end, run re-caches all cached plans (incl. this relation itself) by requesting the CacheManager ( Spark SQL ) to recache the target .","title":" Executing Command"},{"location":"commands/delete/DeleteCommand/#performdelete","text":"performDelete ( sparkSession : SparkSession , deltaLog : DeltaLog , txn : OptimisticTransaction ): Unit","title":" performDelete"},{"location":"commands/delete/DeleteCommand/#number-of-table-files","text":"performDelete requests the given DeltaLog for the current Snapshot that is in turn requested for the number of files in the delta table.","title":" Number of Table Files"},{"location":"commands/delete/DeleteCommand/#finding-delete-actions","text":"performDelete branches off based on the optional condition : No condition to delete the whole table Condition defined on metadata only Other conditions","title":" Finding Delete Actions"},{"location":"commands/delete/DeleteCommand/#delete-condition-undefined","text":"performDelete ...FIXME","title":" Delete Condition Undefined"},{"location":"commands/delete/DeleteCommand/#metadata-only-delete-condition","text":"performDelete ...FIXME","title":" Metadata-Only Delete Condition"},{"location":"commands/delete/DeleteCommand/#other-delete-conditions","text":"performDelete ...FIXME","title":" Other Delete Conditions"},{"location":"commands/delete/DeleteCommand/#delete-actions-available","text":"performDelete ...FIXME","title":" Delete Actions Available"},{"location":"commands/delete/DeleteCommand/#creating-deletecommand","text":"apply ( delete : DeltaDelete ): DeleteCommand apply creates a DeleteCommand .","title":" Creating DeleteCommand"},{"location":"commands/delete/DeltaDelete/","text":"DeltaDelete Unary Logical Command \u00b6 DeltaDelete is an unary logical operator ( Spark SQL ) that represents DeleteFromTable s ( Spark SQL ) at execution. As per the comment : Needs to be compatible with DBR 6 and can't use the new class added in Spark 3.0: DeleteFromTable . Creating Instance \u00b6 DeltaDelete takes the following to be created: Child LogicalPlan ( Spark SQL ) Condition Expression ( Spark SQL ) DeltaDelete is created when: DeltaAnalysis logical resolution rule is executed and resolves DeleteFromTable s ( Spark SQL ) Logical Resolution \u00b6 DeltaDelete is resolved to a DeleteCommand when PreprocessTableDelete post-hoc logical resolution rule is executed.","title":"DeltaDelete"},{"location":"commands/delete/DeltaDelete/#deltadelete-unary-logical-command","text":"DeltaDelete is an unary logical operator ( Spark SQL ) that represents DeleteFromTable s ( Spark SQL ) at execution. As per the comment : Needs to be compatible with DBR 6 and can't use the new class added in Spark 3.0: DeleteFromTable .","title":"DeltaDelete Unary Logical Command"},{"location":"commands/delete/DeltaDelete/#creating-instance","text":"DeltaDelete takes the following to be created: Child LogicalPlan ( Spark SQL ) Condition Expression ( Spark SQL ) DeltaDelete is created when: DeltaAnalysis logical resolution rule is executed and resolves DeleteFromTable s ( Spark SQL )","title":"Creating Instance"},{"location":"commands/delete/DeltaDelete/#logical-resolution","text":"DeltaDelete is resolved to a DeleteCommand when PreprocessTableDelete post-hoc logical resolution rule is executed.","title":"Logical Resolution"},{"location":"commands/describe-detail/","text":"DESCRIBE DETAIL Command \u00b6 Delta Lake supports displaying details of delta tables using the following high-level operator: DESCRIBE DETAIL SQL command","title":"DESCRIBE DETAIL Command"},{"location":"commands/describe-detail/#describe-detail-command","text":"Delta Lake supports displaying details of delta tables using the following high-level operator: DESCRIBE DETAIL SQL command","title":"DESCRIBE DETAIL Command"},{"location":"commands/describe-detail/DescribeDeltaDetailCommand/","text":"DescribeDeltaDetailCommand (DescribeDeltaDetailCommandBase) \u00b6 DescribeDeltaDetailCommand is a RunnableCommand ( Spark SQL ) for DESCRIBE DETAIL SQL command. (DESC | DESCRIBE) DETAIL (path | table) Creating Instance \u00b6 DescribeDeltaDetailCommand takes the following to be created: (optional) Table Path (optional) Table identifier DescribeDeltaDetailCommand is created when: DeltaSqlAstBuilder is requested to parse DESCRIBE DETAIL SQL command ) run \u00b6 run ( sparkSession : SparkSession ): Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run ...FIXME Demo \u00b6 val q = sql ( \"DESCRIBE DETAIL '/tmp/delta/users'\" ) scala> q.show +------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+ |format| id|name|description| location| createdAt| lastModified|partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion| +------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+ | delta|3799b291-dbfa-4f8...|null| null|file:/tmp/delta/u...|2020-01-06 17:08:...|2020-01-06 17:12:28| [city, country]| 4| 2581| []| 1| 2| +------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+","title":"DescribeDeltaDetailCommand"},{"location":"commands/describe-detail/DescribeDeltaDetailCommand/#describedeltadetailcommand-describedeltadetailcommandbase","text":"DescribeDeltaDetailCommand is a RunnableCommand ( Spark SQL ) for DESCRIBE DETAIL SQL command. (DESC | DESCRIBE) DETAIL (path | table)","title":"DescribeDeltaDetailCommand (DescribeDeltaDetailCommandBase)"},{"location":"commands/describe-detail/DescribeDeltaDetailCommand/#creating-instance","text":"DescribeDeltaDetailCommand takes the following to be created: (optional) Table Path (optional) Table identifier DescribeDeltaDetailCommand is created when: DeltaSqlAstBuilder is requested to parse DESCRIBE DETAIL SQL command )","title":"Creating Instance"},{"location":"commands/describe-detail/DescribeDeltaDetailCommand/#run","text":"run ( sparkSession : SparkSession ): Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run ...FIXME","title":" run"},{"location":"commands/describe-detail/DescribeDeltaDetailCommand/#demo","text":"val q = sql ( \"DESCRIBE DETAIL '/tmp/delta/users'\" ) scala> q.show +------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+ |format| id|name|description| location| createdAt| lastModified|partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion| +------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+ | delta|3799b291-dbfa-4f8...|null| null|file:/tmp/delta/u...|2020-01-06 17:08:...|2020-01-06 17:12:28| [city, country]| 4| 2581| []| 1| 2| +------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+","title":"Demo"},{"location":"commands/describe-history/","text":"Describe History Command \u00b6 Delta Lake supports displaying versions ( history ) of delta tables using the following high-level operators: DESCRIBE HISTORY SQL command DeltaTable.history","title":"Describe History Command"},{"location":"commands/describe-history/#describe-history-command","text":"Delta Lake supports displaying versions ( history ) of delta tables using the following high-level operators: DESCRIBE HISTORY SQL command DeltaTable.history","title":"Describe History Command"},{"location":"commands/describe-history/DescribeDeltaHistoryCommand/","text":"DescribeDeltaHistoryCommand \u00b6 DescribeDeltaHistoryCommand is a RunnableCommand ( Spark SQL ) that uses DeltaHistoryManager for the commit history of a delta table. DescribeDeltaHistoryCommand is used for DESCRIBE HISTORY SQL command. Creating Instance \u00b6 DescribeDeltaHistoryCommand takes the following to be created: (optional) Directory (optional) TableIdentifier (optional) Number of commits to display Output Attributes (default: CommitInfo ) DescribeDeltaHistoryCommand is created for: DESCRIBE HISTORY SQL command (that uses DeltaSqlAstBuilder to parse DESCRIBE HISTORY SQL command ) Executing Command \u00b6 run ( sparkSession : SparkSession ): Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run creates a Hadoop Path to (the location of) the delta table (based on DeltaTableIdentifier ). run creates a DeltaLog for the delta table. run requests the DeltaLog for the DeltaHistoryManager that is requested for the commit history .","title":"DescribeDeltaHistoryCommand"},{"location":"commands/describe-history/DescribeDeltaHistoryCommand/#describedeltahistorycommand","text":"DescribeDeltaHistoryCommand is a RunnableCommand ( Spark SQL ) that uses DeltaHistoryManager for the commit history of a delta table. DescribeDeltaHistoryCommand is used for DESCRIBE HISTORY SQL command.","title":"DescribeDeltaHistoryCommand"},{"location":"commands/describe-history/DescribeDeltaHistoryCommand/#creating-instance","text":"DescribeDeltaHistoryCommand takes the following to be created: (optional) Directory (optional) TableIdentifier (optional) Number of commits to display Output Attributes (default: CommitInfo ) DescribeDeltaHistoryCommand is created for: DESCRIBE HISTORY SQL command (that uses DeltaSqlAstBuilder to parse DESCRIBE HISTORY SQL command )","title":"Creating Instance"},{"location":"commands/describe-history/DescribeDeltaHistoryCommand/#executing-command","text":"run ( sparkSession : SparkSession ): Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run creates a Hadoop Path to (the location of) the delta table (based on DeltaTableIdentifier ). run creates a DeltaLog for the delta table. run requests the DeltaLog for the DeltaHistoryManager that is requested for the commit history .","title":" Executing Command"},{"location":"commands/generate/","text":"Generate Command \u00b6 Delta Lake supports executing generator functions on delta tables using the following high-level operators: GENERATE SQL command DeltaTable.generate Only symlink_format_manifest mode is supported.","title":"Generate Command"},{"location":"commands/generate/#generate-command","text":"Delta Lake supports executing generator functions on delta tables using the following high-level operators: GENERATE SQL command DeltaTable.generate Only symlink_format_manifest mode is supported.","title":"Generate Command"},{"location":"commands/generate/DeltaGenerateCommand/","text":"DeltaGenerateCommand \u00b6 DeltaGenerateCommand is a RunnableCommand ( Spark SQL ) to execute a generate function on a delta table . DeltaGenerateCommand is used for the following: GENERATE SQL command DeltaTable.generate operation DeltaGenerateCommand supports symlink_format_manifest mode name only. Demo \u00b6 val path = \"/tmp/delta/d01\" val tid = s\"delta.`$path`\" val q = s\"GENERATE symlink_format_manifest FOR TABLE $tid\" sql(q).collect Creating Instance \u00b6 DeltaGenerateCommand takes the following to be created: Mode Name TableIdentifier (Spark SQL) DeltaGenerateCommand is created for: GENERATE SQL command (that uses DeltaSqlAstBuilder to parse GENERATE SQL command ) DeltaTable.generate operator (that uses DeltaTableOperations to executeGenerate ) Generate Mode Name \u00b6 DeltaGenerateCommand is given a mode name when created . DeltaGenerateCommand uses a lookup table of the supported generation functions by mode name (yet supports just symlink_format_manifest ). Mode Name Generation Function symlink_format_manifest generateFullManifest Executing Command \u00b6 run ( sparkSession : SparkSession ): Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run creates a Hadoop Path to (the location of) the delta table (based on DeltaTableIdentifier ). run creates a DeltaLog for the delta table. run executes the generation function for the mode name . run returns no rows (an empty collection). IllegalArgumentException \u00b6 run throws an IllegalArgumentException when executed with an unsupported mode name : Specified mode '[modeName]' is not supported. Supported modes are: [supportedModes] AnalysisException \u00b6 run throws an AnalysisException when executed for a non-delta table: GENERATE is only supported for Delta tables.","title":"DeltaGenerateCommand"},{"location":"commands/generate/DeltaGenerateCommand/#deltageneratecommand","text":"DeltaGenerateCommand is a RunnableCommand ( Spark SQL ) to execute a generate function on a delta table . DeltaGenerateCommand is used for the following: GENERATE SQL command DeltaTable.generate operation DeltaGenerateCommand supports symlink_format_manifest mode name only.","title":"DeltaGenerateCommand"},{"location":"commands/generate/DeltaGenerateCommand/#demo","text":"val path = \"/tmp/delta/d01\" val tid = s\"delta.`$path`\" val q = s\"GENERATE symlink_format_manifest FOR TABLE $tid\" sql(q).collect","title":"Demo"},{"location":"commands/generate/DeltaGenerateCommand/#creating-instance","text":"DeltaGenerateCommand takes the following to be created: Mode Name TableIdentifier (Spark SQL) DeltaGenerateCommand is created for: GENERATE SQL command (that uses DeltaSqlAstBuilder to parse GENERATE SQL command ) DeltaTable.generate operator (that uses DeltaTableOperations to executeGenerate )","title":"Creating Instance"},{"location":"commands/generate/DeltaGenerateCommand/#generate-mode-name","text":"DeltaGenerateCommand is given a mode name when created . DeltaGenerateCommand uses a lookup table of the supported generation functions by mode name (yet supports just symlink_format_manifest ). Mode Name Generation Function symlink_format_manifest generateFullManifest","title":" Generate Mode Name"},{"location":"commands/generate/DeltaGenerateCommand/#executing-command","text":"run ( sparkSession : SparkSession ): Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run creates a Hadoop Path to (the location of) the delta table (based on DeltaTableIdentifier ). run creates a DeltaLog for the delta table. run executes the generation function for the mode name . run returns no rows (an empty collection).","title":" Executing Command"},{"location":"commands/generate/DeltaGenerateCommand/#illegalargumentexception","text":"run throws an IllegalArgumentException when executed with an unsupported mode name : Specified mode '[modeName]' is not supported. Supported modes are: [supportedModes]","title":" IllegalArgumentException"},{"location":"commands/generate/DeltaGenerateCommand/#analysisexception","text":"run throws an AnalysisException when executed for a non-delta table: GENERATE is only supported for Delta tables.","title":" AnalysisException"},{"location":"commands/generate/GenerateSymlinkManifest/","text":"GenerateSymlinkManifest (GenerateSymlinkManifestImpl) \u00b6 GenerateSymlinkManifest is a post-commit hook to generate incremental and full Hive-style manifests for delta tables. GenerateSymlinkManifest is registered when OptimisticTransactionImpl is requested to commit (with delta.compatibility.symlinkFormatManifest.enabled table property enabled). Executing Post-Commit Hook \u00b6 run ( spark : SparkSession , txn : OptimisticTransactionImpl , committedActions : Seq [ Action ]): Unit run is part of the PostCommitHook abstraction. run generates an incremental manifest for the committed action s (the deltaLog and snapshot are from the OptimisticTransactionImpl ). generateIncrementalManifest \u00b6 generateIncrementalManifest ( spark : SparkSession , deltaLog : DeltaLog , txnReadSnapshot : Snapshot , actions : Seq [ Action ]): Unit generateIncrementalManifest ...FIXME generateFullManifest \u00b6 generateFullManifest ( spark : SparkSession , deltaLog : DeltaLog ): Unit generateFullManifest ...FIXME generateFullManifest is used when: GenerateSymlinkManifestImpl is requested to generateIncrementalManifest DeltaGenerateCommand is executed (with symlink_format_manifest mode)","title":"GenerateSymlinkManifest"},{"location":"commands/generate/GenerateSymlinkManifest/#generatesymlinkmanifest-generatesymlinkmanifestimpl","text":"GenerateSymlinkManifest is a post-commit hook to generate incremental and full Hive-style manifests for delta tables. GenerateSymlinkManifest is registered when OptimisticTransactionImpl is requested to commit (with delta.compatibility.symlinkFormatManifest.enabled table property enabled).","title":"GenerateSymlinkManifest (GenerateSymlinkManifestImpl)"},{"location":"commands/generate/GenerateSymlinkManifest/#executing-post-commit-hook","text":"run ( spark : SparkSession , txn : OptimisticTransactionImpl , committedActions : Seq [ Action ]): Unit run is part of the PostCommitHook abstraction. run generates an incremental manifest for the committed action s (the deltaLog and snapshot are from the OptimisticTransactionImpl ).","title":" Executing Post-Commit Hook"},{"location":"commands/generate/GenerateSymlinkManifest/#generateincrementalmanifest","text":"generateIncrementalManifest ( spark : SparkSession , deltaLog : DeltaLog , txnReadSnapshot : Snapshot , actions : Seq [ Action ]): Unit generateIncrementalManifest ...FIXME","title":" generateIncrementalManifest"},{"location":"commands/generate/GenerateSymlinkManifest/#generatefullmanifest","text":"generateFullManifest ( spark : SparkSession , deltaLog : DeltaLog ): Unit generateFullManifest ...FIXME generateFullManifest is used when: GenerateSymlinkManifestImpl is requested to generateIncrementalManifest DeltaGenerateCommand is executed (with symlink_format_manifest mode)","title":" generateFullManifest"},{"location":"commands/update/","text":"Update Command \u00b6 Delta Lake supports updating (records in) delta tables using the following high-level operators: UPDATE TABLE SQL command DeltaTable.update or DeltaTable.updateExpr","title":"Update Command"},{"location":"commands/update/#update-command","text":"Delta Lake supports updating (records in) delta tables using the following high-level operators: UPDATE TABLE SQL command DeltaTable.update or DeltaTable.updateExpr","title":"Update Command"},{"location":"commands/update/DeltaUpdateTable/","text":"DeltaUpdateTable Unary Logical Operator \u00b6 DeltaUpdateTable is an unary logical operator ( Spark SQL ) that represents UpdateTable ( Spark SQL ) at execution. Creating Instance \u00b6 DeltaUpdateTable takes the following to be created: Child LogicalPlan ( Spark SQL ) Update Column Expressions ( Spark SQL ) Update Expressions ( Spark SQL ) (optional) Condition Expression ( Spark SQL ) DeltaUpdateTable is created when: DeltaAnalysis logical resolution rule is executed (on an UpdateTable ) Logical Resolution \u00b6 DeltaUpdateTable is resolved to a UpdateCommand when PreprocessTableUpdate post-hoc logical resolution rule is executed.","title":"DeltaUpdateTable"},{"location":"commands/update/DeltaUpdateTable/#deltaupdatetable-unary-logical-operator","text":"DeltaUpdateTable is an unary logical operator ( Spark SQL ) that represents UpdateTable ( Spark SQL ) at execution.","title":"DeltaUpdateTable Unary Logical Operator"},{"location":"commands/update/DeltaUpdateTable/#creating-instance","text":"DeltaUpdateTable takes the following to be created: Child LogicalPlan ( Spark SQL ) Update Column Expressions ( Spark SQL ) Update Expressions ( Spark SQL ) (optional) Condition Expression ( Spark SQL ) DeltaUpdateTable is created when: DeltaAnalysis logical resolution rule is executed (on an UpdateTable )","title":"Creating Instance"},{"location":"commands/update/DeltaUpdateTable/#logical-resolution","text":"DeltaUpdateTable is resolved to a UpdateCommand when PreprocessTableUpdate post-hoc logical resolution rule is executed.","title":"Logical Resolution"},{"location":"commands/update/UpdateCommand/","text":"UpdateCommand \u00b6 UpdateCommand is a DeltaCommand that represents DeltaUpdateTable logical command at execution. UpdateCommand is a RunnableCommand ( Spark SQL ) logical operator. Creating Instance \u00b6 UpdateCommand takes the following to be created: TahoeFileIndex Target Data ( LogicalPlan ) Update Expressions ( Spark SQL ) (optional) Condition Expression ( Spark SQL ) UpdateCommand is created when: PreprocessTableUpdate logical resolution rule is executed (and resolves a DeltaUpdateTable logical command) Performance Metrics \u00b6 Name web UI numAddedFiles number of files added. numRemovedFiles number of files removed. numUpdatedRows number of rows updated. executionTimeMs time taken to execute the entire operation scanTimeMs time taken to scan the files for matches rewriteTimeMs time taken to rewrite the matched files Executing Command \u00b6 run ( sparkSession : SparkSession ): Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run ...FIXME performUpdate \u00b6 performUpdate ( sparkSession : SparkSession , deltaLog : DeltaLog , txn : OptimisticTransaction ): Unit performUpdate ...FIXME rewriteFiles \u00b6 rewriteFiles ( spark : SparkSession , txn : OptimisticTransaction , rootPath : Path , inputLeafFiles : Seq [ String ], nameToAddFileMap : Map [ String , AddFile ], condition : Expression ): Seq [ FileAction ] rewriteFiles ...FIXME buildUpdatedColumns \u00b6 buildUpdatedColumns ( condition : Expression ): Seq [ Column ] buildUpdatedColumns ...FIXME","title":"UpdateCommand"},{"location":"commands/update/UpdateCommand/#updatecommand","text":"UpdateCommand is a DeltaCommand that represents DeltaUpdateTable logical command at execution. UpdateCommand is a RunnableCommand ( Spark SQL ) logical operator.","title":"UpdateCommand"},{"location":"commands/update/UpdateCommand/#creating-instance","text":"UpdateCommand takes the following to be created: TahoeFileIndex Target Data ( LogicalPlan ) Update Expressions ( Spark SQL ) (optional) Condition Expression ( Spark SQL ) UpdateCommand is created when: PreprocessTableUpdate logical resolution rule is executed (and resolves a DeltaUpdateTable logical command)","title":"Creating Instance"},{"location":"commands/update/UpdateCommand/#performance-metrics","text":"Name web UI numAddedFiles number of files added. numRemovedFiles number of files removed. numUpdatedRows number of rows updated. executionTimeMs time taken to execute the entire operation scanTimeMs time taken to scan the files for matches rewriteTimeMs time taken to rewrite the matched files","title":"Performance Metrics"},{"location":"commands/update/UpdateCommand/#executing-command","text":"run ( sparkSession : SparkSession ): Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run ...FIXME","title":" Executing Command"},{"location":"commands/update/UpdateCommand/#performupdate","text":"performUpdate ( sparkSession : SparkSession , deltaLog : DeltaLog , txn : OptimisticTransaction ): Unit performUpdate ...FIXME","title":" performUpdate"},{"location":"commands/update/UpdateCommand/#rewritefiles","text":"rewriteFiles ( spark : SparkSession , txn : OptimisticTransaction , rootPath : Path , inputLeafFiles : Seq [ String ], nameToAddFileMap : Map [ String , AddFile ], condition : Expression ): Seq [ FileAction ] rewriteFiles ...FIXME","title":" rewriteFiles"},{"location":"commands/update/UpdateCommand/#buildupdatedcolumns","text":"buildUpdatedColumns ( condition : Expression ): Seq [ Column ] buildUpdatedColumns ...FIXME","title":" buildUpdatedColumns"},{"location":"commands/vacuum/","text":"Vacuum Command \u00b6 Vacuum command allows for garbage collection of a delta table . Vacuum command can be executed as VACUUM SQL command or DeltaTable.vacuum operator. Demo \u00b6 VACUUM SQL Command \u00b6 val q = sql ( \"VACUUM delta.`/tmp/delta/t1`\" ) scala> q.show Deleted 0 files and directories in a total of 2 directories. +------------------+ | path| +------------------+ |file:/tmp/delta/t1| +------------------+ DeltaTable.vacuum \u00b6 import io . delta . tables . DeltaTable DeltaTable . forPath ( \"/tmp/delta/t1\" ). vacuum Dry Run \u00b6 Visit Demo: Vacuum .","title":"Vacuum Command"},{"location":"commands/vacuum/#vacuum-command","text":"Vacuum command allows for garbage collection of a delta table . Vacuum command can be executed as VACUUM SQL command or DeltaTable.vacuum operator.","title":"Vacuum Command"},{"location":"commands/vacuum/#demo","text":"","title":"Demo"},{"location":"commands/vacuum/#vacuum-sql-command","text":"val q = sql ( \"VACUUM delta.`/tmp/delta/t1`\" ) scala> q.show Deleted 0 files and directories in a total of 2 directories. +------------------+ | path| +------------------+ |file:/tmp/delta/t1| +------------------+","title":"VACUUM SQL Command"},{"location":"commands/vacuum/#deltatablevacuum","text":"import io . delta . tables . DeltaTable DeltaTable . forPath ( \"/tmp/delta/t1\" ). vacuum","title":"DeltaTable.vacuum"},{"location":"commands/vacuum/#dry-run","text":"Visit Demo: Vacuum .","title":"Dry Run"},{"location":"commands/vacuum/VacuumCommand/","text":"VacuumCommand Utility \u00b6 VacuumCommand is a concrete VacuumCommandImpl for garbage collection of a delta table for the following: DeltaTable.vacuum operator (as DeltaTableOperations is requested to execute vacuum command ) VACUUM SQL command (as VacuumTableCommand is executed) Garbage Collection of Delta Table \u00b6 gc ( spark : SparkSession , deltaLog : DeltaLog , dryRun : Boolean = true , retentionHours : Option [ Double ] = None , clock : Clock = new SystemClock ): DataFrame gc requests the given DeltaLog to update (and hence give the latest Snapshot of the delta table). retentionMillis \u00b6 gc converts the retention hours to milliseconds and checkRetentionPeriodSafety (with deletedFileRetentionDuration table configuration). Timestamp to Delete Files Before \u00b6 gc determines the timestamp to delete files before based on the retentionMillis (if defined) or defaults to minFileRetentionTimestamp table configuration. gc prints out the following INFO message to the logs (with the path of the given DeltaLog ): Starting garbage collection (dryRun = [dryRun]) of untracked files older than [deleteBeforeTimestamp] in [path] Valid Files \u00b6 gc requests the Snapshot for the state dataset and maps over partitions ( Dataset.mapPartitions ) with a map function that does the following (for every Action in a partition of SingleAction s): Skips RemoveFile s with the deletion timestamp after the timestamp to delete files before Adds the path of FileAction s (that live inside the directory of the table) with all subdirectories Skips other actions gc converts the mapped state dataset into a DataFrame with a single path column. Note There is no DataFrame action executed so no processing yet (using Spark). All Files and Directories Dataset \u00b6 gc finds all the files and directories (recursively) in the data path (with spark.sql.sources.parallelPartitionDiscovery.parallelism number of file listing tasks). Caching All Files and Directories Dataset \u00b6 gc caches the allFilesAndDirs dataset. Number of Directories \u00b6 gc counts the number of directories (as the count of the rows with isDir column being true in the allFilesAndDirs dataset). Note This step submits a Spark job for Dataset.count . Paths Dataset \u00b6 gc creates a Spark SQL query to count path s of the allFilesAndDirs with files with the modificationTime ealier than the deleteBeforeTimestamp and the directories ( isDir s). That creates a DataFrame of path and count columns. gc uses left-anti join of the counted path DataFrame with the validFiles on path . gc filters out paths with count more than 1 and selects path . Dry Run \u00b6 gc counts the rows in the paths Dataset for the number of files and directories that are safe to delete ( numFiles ). Note This step submits a Spark job for Dataset.count . gc prints out the following message to the console (with the dirCounts ): Found [numFiles] files and directories in a total of [dirCounts] directories that are safe to delete. In the end, gc converts the paths to Hadoop DFS format and creates a DataFrame with a single path column. Deleting Files and Directories \u00b6 gc prints out the following INFO message to the logs: Deleting untracked files and empty directories in [path] gc deletes the untracked files and empty directories (with parallel delete enabled flag based on spark.databricks.delta.vacuum.parallelDelete.enabled configuration property). gc prints out the following message to standard output (with the dirCounts ): Deleted [filesDeleted] files and directories in a total of [dirCounts] directories. In the end, gc creates a DataFrame with a single path column with just the data path of the delta table to vacuum. Unpersist All Files and Directories Dataset \u00b6 gc unpersists the allFilesAndDirs dataset. checkRetentionPeriodSafety \u00b6 checkRetentionPeriodSafety ( spark : SparkSession , retentionMs : Option [ Long ], configuredRetention : Long ): Unit checkRetentionPeriodSafety ...FIXME Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.delta.commands.VacuumCommand logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.commands.VacuumCommand=ALL Refer to Logging .","title":"VacuumCommand"},{"location":"commands/vacuum/VacuumCommand/#vacuumcommand-utility","text":"VacuumCommand is a concrete VacuumCommandImpl for garbage collection of a delta table for the following: DeltaTable.vacuum operator (as DeltaTableOperations is requested to execute vacuum command ) VACUUM SQL command (as VacuumTableCommand is executed)","title":"VacuumCommand Utility"},{"location":"commands/vacuum/VacuumCommand/#garbage-collection-of-delta-table","text":"gc ( spark : SparkSession , deltaLog : DeltaLog , dryRun : Boolean = true , retentionHours : Option [ Double ] = None , clock : Clock = new SystemClock ): DataFrame gc requests the given DeltaLog to update (and hence give the latest Snapshot of the delta table).","title":" Garbage Collection of Delta Table"},{"location":"commands/vacuum/VacuumCommand/#retentionmillis","text":"gc converts the retention hours to milliseconds and checkRetentionPeriodSafety (with deletedFileRetentionDuration table configuration).","title":" retentionMillis"},{"location":"commands/vacuum/VacuumCommand/#timestamp-to-delete-files-before","text":"gc determines the timestamp to delete files before based on the retentionMillis (if defined) or defaults to minFileRetentionTimestamp table configuration. gc prints out the following INFO message to the logs (with the path of the given DeltaLog ): Starting garbage collection (dryRun = [dryRun]) of untracked files older than [deleteBeforeTimestamp] in [path]","title":" Timestamp to Delete Files Before"},{"location":"commands/vacuum/VacuumCommand/#valid-files","text":"gc requests the Snapshot for the state dataset and maps over partitions ( Dataset.mapPartitions ) with a map function that does the following (for every Action in a partition of SingleAction s): Skips RemoveFile s with the deletion timestamp after the timestamp to delete files before Adds the path of FileAction s (that live inside the directory of the table) with all subdirectories Skips other actions gc converts the mapped state dataset into a DataFrame with a single path column. Note There is no DataFrame action executed so no processing yet (using Spark).","title":" Valid Files"},{"location":"commands/vacuum/VacuumCommand/#all-files-and-directories-dataset","text":"gc finds all the files and directories (recursively) in the data path (with spark.sql.sources.parallelPartitionDiscovery.parallelism number of file listing tasks).","title":" All Files and Directories Dataset"},{"location":"commands/vacuum/VacuumCommand/#caching-all-files-and-directories-dataset","text":"gc caches the allFilesAndDirs dataset.","title":" Caching All Files and Directories Dataset"},{"location":"commands/vacuum/VacuumCommand/#number-of-directories","text":"gc counts the number of directories (as the count of the rows with isDir column being true in the allFilesAndDirs dataset). Note This step submits a Spark job for Dataset.count .","title":" Number of Directories"},{"location":"commands/vacuum/VacuumCommand/#paths-dataset","text":"gc creates a Spark SQL query to count path s of the allFilesAndDirs with files with the modificationTime ealier than the deleteBeforeTimestamp and the directories ( isDir s). That creates a DataFrame of path and count columns. gc uses left-anti join of the counted path DataFrame with the validFiles on path . gc filters out paths with count more than 1 and selects path .","title":" Paths Dataset"},{"location":"commands/vacuum/VacuumCommand/#dry-run","text":"gc counts the rows in the paths Dataset for the number of files and directories that are safe to delete ( numFiles ). Note This step submits a Spark job for Dataset.count . gc prints out the following message to the console (with the dirCounts ): Found [numFiles] files and directories in a total of [dirCounts] directories that are safe to delete. In the end, gc converts the paths to Hadoop DFS format and creates a DataFrame with a single path column.","title":" Dry Run"},{"location":"commands/vacuum/VacuumCommand/#deleting-files-and-directories","text":"gc prints out the following INFO message to the logs: Deleting untracked files and empty directories in [path] gc deletes the untracked files and empty directories (with parallel delete enabled flag based on spark.databricks.delta.vacuum.parallelDelete.enabled configuration property). gc prints out the following message to standard output (with the dirCounts ): Deleted [filesDeleted] files and directories in a total of [dirCounts] directories. In the end, gc creates a DataFrame with a single path column with just the data path of the delta table to vacuum.","title":" Deleting Files and Directories"},{"location":"commands/vacuum/VacuumCommand/#unpersist-all-files-and-directories-dataset","text":"gc unpersists the allFilesAndDirs dataset.","title":" Unpersist All Files and Directories Dataset"},{"location":"commands/vacuum/VacuumCommand/#checkretentionperiodsafety","text":"checkRetentionPeriodSafety ( spark : SparkSession , retentionMs : Option [ Long ], configuredRetention : Long ): Unit checkRetentionPeriodSafety ...FIXME","title":" checkRetentionPeriodSafety"},{"location":"commands/vacuum/VacuumCommand/#logging","text":"Enable ALL logging level for org.apache.spark.sql.delta.commands.VacuumCommand logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.commands.VacuumCommand=ALL Refer to Logging .","title":"Logging"},{"location":"commands/vacuum/VacuumCommandImpl/","text":"VacuumCommandImpl \u00b6 VacuumCommandImpl is a DeltaCommand . Note VacuumCommandImpl is a Scala trait just to let Databricks provide a commercial version of vacuum command. delete \u00b6 delete ( diff : Dataset [ String ], spark : SparkSession , basePath : String , hadoopConf : Broadcast [ SerializableConfiguration ], parallel : Boolean ): Long delete ...FIXME delete is used when: VacuumCommand is requested to gc","title":"VacuumCommandImpl"},{"location":"commands/vacuum/VacuumCommandImpl/#vacuumcommandimpl","text":"VacuumCommandImpl is a DeltaCommand . Note VacuumCommandImpl is a Scala trait just to let Databricks provide a commercial version of vacuum command.","title":"VacuumCommandImpl"},{"location":"commands/vacuum/VacuumCommandImpl/#delete","text":"delete ( diff : Dataset [ String ], spark : SparkSession , basePath : String , hadoopConf : Broadcast [ SerializableConfiguration ], parallel : Boolean ): Long delete ...FIXME delete is used when: VacuumCommand is requested to gc","title":" delete"},{"location":"commands/vacuum/VacuumTableCommand/","text":"VacuumTableCommand \u00b6 VacuumTableCommand is a runnable command ( Spark SQL ) for VACUUM SQL command. Creating Instance \u00b6 VacuumTableCommand takes the following to be created: Path TableIdentifier Optional Horizon Hours dryRun flag VacuumTableCommand requires that either the table or the path is defined and it is the root directory of a delta table. Partition directories are not supported. VacuumTableCommand is created when: DeltaSqlAstBuilder is requested to parse VACUUM SQL command Executing Command \u00b6 run ( sparkSession : SparkSession ): Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run takes the path to vacuum (either the table or the path ) and finds the root directory of the delta table . run creates a DeltaLog instance for the delta table and gc it (passing in the DeltaLog instance, the dryRun and the horizonHours options). run throws an AnalysisException when executed for a non-root directory of a delta table: Please provide the base path ([baseDeltaPath]) when Vacuuming Delta tables. Vacuuming specific partitions is currently not supported. run throws an AnalysisException when executed for a DeltaLog with the snapshot version being -1 : [deltaTableIdentifier] is not a Delta table. VACUUM is only supported for Delta tables. Output Schema \u00b6 The output schema of VacuumTableCommand is a single path column (of type StringType ).","title":"VacuumTableCommand"},{"location":"commands/vacuum/VacuumTableCommand/#vacuumtablecommand","text":"VacuumTableCommand is a runnable command ( Spark SQL ) for VACUUM SQL command.","title":"VacuumTableCommand"},{"location":"commands/vacuum/VacuumTableCommand/#creating-instance","text":"VacuumTableCommand takes the following to be created: Path TableIdentifier Optional Horizon Hours dryRun flag VacuumTableCommand requires that either the table or the path is defined and it is the root directory of a delta table. Partition directories are not supported. VacuumTableCommand is created when: DeltaSqlAstBuilder is requested to parse VACUUM SQL command","title":"Creating Instance"},{"location":"commands/vacuum/VacuumTableCommand/#executing-command","text":"run ( sparkSession : SparkSession ): Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run takes the path to vacuum (either the table or the path ) and finds the root directory of the delta table . run creates a DeltaLog instance for the delta table and gc it (passing in the DeltaLog instance, the dryRun and the horizonHours options). run throws an AnalysisException when executed for a non-root directory of a delta table: Please provide the base path ([baseDeltaPath]) when Vacuuming Delta tables. Vacuuming specific partitions is currently not supported. run throws an AnalysisException when executed for a DeltaLog with the snapshot version being -1 : [deltaTableIdentifier] is not a Delta table. VACUUM is only supported for Delta tables.","title":" Executing Command"},{"location":"commands/vacuum/VacuumTableCommand/#output-schema","text":"The output schema of VacuumTableCommand is a single path column (of type StringType ).","title":" Output Schema"},{"location":"contenders/","text":"Contenders \u00b6 As it happens in the open source software world, Delta Lake is not alone in the area of Data Lakes on top of Apache Spark. The following is a list of some other open source projects that seems to compete or cover the same use cases. Apache Iceberg \u00b6 Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to Presto and Spark that use a high-performance format that works just like a SQL table. Videos \u00b6 ACID ORC, Iceberg, and Delta Lake\u2014An Overview of Table Formats for Large Scale Storage and Analytics Introducing Iceberg Tables designed for object stores Introducing Apache Iceberg: Tables Designed for Object Stores Iceberg: a fast table format for S3 Apache Hudi \u00b6 Apache Hudi ingests and manages storage of large analytical datasets over DFS (HDFS or cloud stores) and provides three logical views for query access. Videos \u00b6 Hoodie: An Open Source Incremental Processing Framework From Uber Powering Uber's global network analytics pipelines in real-time with Apache Hudi","title":"Contenders"},{"location":"contenders/#contenders","text":"As it happens in the open source software world, Delta Lake is not alone in the area of Data Lakes on top of Apache Spark. The following is a list of some other open source projects that seems to compete or cover the same use cases.","title":"Contenders"},{"location":"contenders/#apache-iceberg","text":"Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to Presto and Spark that use a high-performance format that works just like a SQL table.","title":"Apache Iceberg"},{"location":"contenders/#videos","text":"ACID ORC, Iceberg, and Delta Lake\u2014An Overview of Table Formats for Large Scale Storage and Analytics Introducing Iceberg Tables designed for object stores Introducing Apache Iceberg: Tables Designed for Object Stores Iceberg: a fast table format for S3","title":"Videos"},{"location":"contenders/#apache-hudi","text":"Apache Hudi ingests and manages storage of large analytical datasets over DFS (HDFS or cloud stores) and provides three logical views for query access.","title":"Apache Hudi"},{"location":"contenders/#videos_1","text":"Hoodie: An Open Source Incremental Processing Framework From Uber Powering Uber's global network analytics pipelines in real-time with Apache Hudi","title":"Videos"},{"location":"demo/","text":"Demos \u00b6 The following demos are available: Vacuum dataChange replaceWhere Merge Operation Converting Parquet Dataset Into Delta Format Stream Processing of Delta Table Using Delta Lake as Streaming Sink in Structured Streaming Debugging Delta Lake Using IntelliJ IDEA Observing Transaction Retries DeltaTable, DeltaLog And Snapshots Schema Evolution User Metadata for Labelling Commits","title":"Demos"},{"location":"demo/#demos","text":"The following demos are available: Vacuum dataChange replaceWhere Merge Operation Converting Parquet Dataset Into Delta Format Stream Processing of Delta Table Using Delta Lake as Streaming Sink in Structured Streaming Debugging Delta Lake Using IntelliJ IDEA Observing Transaction Retries DeltaTable, DeltaLog And Snapshots Schema Evolution User Metadata for Labelling Commits","title":"Demos"},{"location":"demo/Converting-Parquet-Dataset-Into-Delta-Format/","text":"Demo: Converting Parquet Dataset Into Delta Format \u00b6 /* spark-shell \\ --packages io.delta:delta-core_2.12:1.0.0-SNAPSHOT \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ assert(spark.version.matches(\"2.4.[2-6]\"), \"Delta Lake supports Spark 2.4.2+\") import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val deltaLake = \"/tmp/delta\" // Create parquet table val users = s\"$deltaLake/users\" import spark.implicits._ val data = Seq( (0L, \"Agata\", \"Warsaw\", \"Poland\"), (1L, \"Jacek\", \"Warsaw\", \"Poland\"), (2L, \"Bartosz\", \"Paris\", \"France\") ).toDF(\"id\", \"name\", \"city\", \"country\") data .write .format(\"parquet\") .partitionBy(\"city\", \"country\") .mode(\"overwrite\") .save(users) // TIP: Use git to version the users directory // to track the changes for import // CONVERT TO DELTA only supports parquet tables // TableIdentifier should be parquet.`users` // Use TableIdentifier to refer to the parquet table // The path itself would work too val tableId = s\"parquet.`$users`\" val partitionSchema = \"city STRING, country STRING\" // Import users table into Delta Lake // Well, convert the parquet table into delta table // Use web UI to monitor execution, e.g. http://localhost:4040 import io.delta.tables.DeltaTable val dt = DeltaTable.convertToDelta(spark, tableId, partitionSchema) assert(dt.isInstanceOf[DeltaTable]) // users table is now in delta format assert(DeltaTable.isDeltaTable(users))","title":"Converting Parquet Dataset Into Delta Format"},{"location":"demo/Converting-Parquet-Dataset-Into-Delta-Format/#demo-converting-parquet-dataset-into-delta-format","text":"/* spark-shell \\ --packages io.delta:delta-core_2.12:1.0.0-SNAPSHOT \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ assert(spark.version.matches(\"2.4.[2-6]\"), \"Delta Lake supports Spark 2.4.2+\") import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val deltaLake = \"/tmp/delta\" // Create parquet table val users = s\"$deltaLake/users\" import spark.implicits._ val data = Seq( (0L, \"Agata\", \"Warsaw\", \"Poland\"), (1L, \"Jacek\", \"Warsaw\", \"Poland\"), (2L, \"Bartosz\", \"Paris\", \"France\") ).toDF(\"id\", \"name\", \"city\", \"country\") data .write .format(\"parquet\") .partitionBy(\"city\", \"country\") .mode(\"overwrite\") .save(users) // TIP: Use git to version the users directory // to track the changes for import // CONVERT TO DELTA only supports parquet tables // TableIdentifier should be parquet.`users` // Use TableIdentifier to refer to the parquet table // The path itself would work too val tableId = s\"parquet.`$users`\" val partitionSchema = \"city STRING, country STRING\" // Import users table into Delta Lake // Well, convert the parquet table into delta table // Use web UI to monitor execution, e.g. http://localhost:4040 import io.delta.tables.DeltaTable val dt = DeltaTable.convertToDelta(spark, tableId, partitionSchema) assert(dt.isInstanceOf[DeltaTable]) // users table is now in delta format assert(DeltaTable.isDeltaTable(users))","title":"Demo: Converting Parquet Dataset Into Delta Format"},{"location":"demo/Debugging-Delta-Lake-Using-IntelliJ-IDEA/","text":"Demo: Debugging Delta Lake Using IntelliJ IDEA \u00b6 Import Delta Lake's sources to IntelliJ IDEA. Configure a new Remote debug configuration in IntelliJ IDEA (e.g. Run > Debug > Edit Configurations...) and simply give it a name and save. Tip Use Option+Ctrl+D to access Debug menu on mac OS. Run spark-shell as follows to enable remote JVM for debugging. export SPARK_SUBMIT_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 spark-shell \\ --packages io.delta:delta-core_2.12:1.0.0-SNAPSHOT \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\ --conf spark.databricks.delta.snapshotPartitions=1","title":"Debugging Delta Lake Using IntelliJ IDEA"},{"location":"demo/Debugging-Delta-Lake-Using-IntelliJ-IDEA/#demo-debugging-delta-lake-using-intellij-idea","text":"Import Delta Lake's sources to IntelliJ IDEA. Configure a new Remote debug configuration in IntelliJ IDEA (e.g. Run > Debug > Edit Configurations...) and simply give it a name and save. Tip Use Option+Ctrl+D to access Debug menu on mac OS. Run spark-shell as follows to enable remote JVM for debugging. export SPARK_SUBMIT_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 spark-shell \\ --packages io.delta:delta-core_2.12:1.0.0-SNAPSHOT \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\ --conf spark.databricks.delta.snapshotPartitions=1","title":"Demo: Debugging Delta Lake Using IntelliJ IDEA"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/","text":"Demo: DeltaTable, DeltaLog And Snapshots \u00b6 Create Delta Table \u00b6 import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val name = \"users\" sql(s\"DROP TABLE IF EXISTS $name\") sql(s\"\"\" | CREATE TABLE $name (id bigint, name string, city string, country string) | USING delta \"\"\".stripMargin) Access Transaction Log (DeltaLog) \u00b6 import org.apache.spark.sql.catalyst.TableIdentifier val tid = TableIdentifier(name) import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, tid) Update the state of the delta table to the most recent version. val snapshot = deltaLog.update() assert(snapshot.version == 0) val state = snapshot.state scala> :type state org.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.SingleAction] Review the cached RDD for the state snapshot in the Storage tab of the web UI (e.g. http://localhost:4040/storage/ ). The \"version\" part of Delta Table State name of the cached RDD should match the version of the snapshot // Show the changes (actions) scala> snapshot.state.show +----+----+------+--------------------+--------+----------+ | txn| add|remove| metaData|protocol|commitInfo| +----+----+------+--------------------+--------+----------+ |null|null| null| null| [1, 2]| null| |null|null| null|[5156c9e3-9668-43...| null| null| +----+----+------+--------------------+--------+----------+ DeltaTable as DataFrame \u00b6 import io.delta.tables.DeltaTable val dt = DeltaTable.forName(name) scala> dt.history.select('version, 'operation, 'operationParameters, 'operationMetrics).show(truncate = false) +-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+ |version| timestamp|userId|userName| operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics|userMetadata| +-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+ | 0|2020-09-29 10:31:30| null| null|CREATE TABLE|[isManaged -> tru...|null| null| null| null| null| true| []| null| +-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+ val users = dt.toDF scala> users.show +---+----+----+-------+ | id|name|city|country| +---+----+----+-------+ +---+----+----+-------+ Add new users \u00b6 val newUsers = Seq( (0L, \"Agata\", \"Warsaw\", \"Poland\"), (1L, \"Bartosz\", \"Paris\", \"France\") ).toDF(\"id\", \"name\", \"city\", \"country\") scala> newUsers.show +---+-------+------+-------+ | id| name| city|country| +---+-------+------+-------+ | 0| Agata|Warsaw| Poland| | 1|Bartosz| Paris| France| +---+-------+------+-------+ newUsers.write.format(\"delta\").mode(\"append\").saveAsTable(name) assert(deltaLog.snapshot.version == 1) Review the cached RDD for the state snapshot in the Storage tab of the web UI (e.g. http://localhost:4040/storage/ ). The \"version\" part of Delta Table State name of the cached RDD should match the version of the snapshot scala> users.show +---+-------+------+-------+ | id| name| city|country| +---+-------+------+-------+ | 1|Bartosz| Paris| France| | 0| Agata|Warsaw| Poland| +---+-------+------+-------+ scala> dt.history.select('version, 'operation, 'operationParameters, 'operationMetrics).show(truncate = false) +-------+------------+------------------------------------------------------------------------+-----------------------------------------------------------+ |version|operation |operationParameters |operationMetrics | +-------+------------+------------------------------------------------------------------------+-----------------------------------------------------------+ |1 |WRITE |[mode -> Append, partitionBy -> []] |[numFiles -> 2, numOutputBytes -> 2299, numOutputRows -> 2]| |0 |CREATE TABLE|[isManaged -> true, description ->, partitionBy -> [], properties -> {}]|[] | +-------+------------+------------------------------------------------------------------------+-----------------------------------------------------------+","title":"DeltaTable, DeltaLog And Snapshots"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#demo-deltatable-deltalog-and-snapshots","text":"","title":"Demo: DeltaTable, DeltaLog And Snapshots"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#create-delta-table","text":"import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val name = \"users\" sql(s\"DROP TABLE IF EXISTS $name\") sql(s\"\"\" | CREATE TABLE $name (id bigint, name string, city string, country string) | USING delta \"\"\".stripMargin)","title":"Create Delta Table"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#access-transaction-log-deltalog","text":"import org.apache.spark.sql.catalyst.TableIdentifier val tid = TableIdentifier(name) import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, tid) Update the state of the delta table to the most recent version. val snapshot = deltaLog.update() assert(snapshot.version == 0) val state = snapshot.state scala> :type state org.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.SingleAction] Review the cached RDD for the state snapshot in the Storage tab of the web UI (e.g. http://localhost:4040/storage/ ). The \"version\" part of Delta Table State name of the cached RDD should match the version of the snapshot // Show the changes (actions) scala> snapshot.state.show +----+----+------+--------------------+--------+----------+ | txn| add|remove| metaData|protocol|commitInfo| +----+----+------+--------------------+--------+----------+ |null|null| null| null| [1, 2]| null| |null|null| null|[5156c9e3-9668-43...| null| null| +----+----+------+--------------------+--------+----------+","title":"Access Transaction Log (DeltaLog)"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#deltatable-as-dataframe","text":"import io.delta.tables.DeltaTable val dt = DeltaTable.forName(name) scala> dt.history.select('version, 'operation, 'operationParameters, 'operationMetrics).show(truncate = false) +-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+ |version| timestamp|userId|userName| operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics|userMetadata| +-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+ | 0|2020-09-29 10:31:30| null| null|CREATE TABLE|[isManaged -> tru...|null| null| null| null| null| true| []| null| +-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+ val users = dt.toDF scala> users.show +---+----+----+-------+ | id|name|city|country| +---+----+----+-------+ +---+----+----+-------+","title":"DeltaTable as DataFrame"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#add-new-users","text":"val newUsers = Seq( (0L, \"Agata\", \"Warsaw\", \"Poland\"), (1L, \"Bartosz\", \"Paris\", \"France\") ).toDF(\"id\", \"name\", \"city\", \"country\") scala> newUsers.show +---+-------+------+-------+ | id| name| city|country| +---+-------+------+-------+ | 0| Agata|Warsaw| Poland| | 1|Bartosz| Paris| France| +---+-------+------+-------+ newUsers.write.format(\"delta\").mode(\"append\").saveAsTable(name) assert(deltaLog.snapshot.version == 1) Review the cached RDD for the state snapshot in the Storage tab of the web UI (e.g. http://localhost:4040/storage/ ). The \"version\" part of Delta Table State name of the cached RDD should match the version of the snapshot scala> users.show +---+-------+------+-------+ | id| name| city|country| +---+-------+------+-------+ | 1|Bartosz| Paris| France| | 0| Agata|Warsaw| Poland| +---+-------+------+-------+ scala> dt.history.select('version, 'operation, 'operationParameters, 'operationMetrics).show(truncate = false) +-------+------------+------------------------------------------------------------------------+-----------------------------------------------------------+ |version|operation |operationParameters |operationMetrics | +-------+------------+------------------------------------------------------------------------+-----------------------------------------------------------+ |1 |WRITE |[mode -> Append, partitionBy -> []] |[numFiles -> 2, numOutputBytes -> 2299, numOutputRows -> 2]| |0 |CREATE TABLE|[isManaged -> true, description ->, partitionBy -> [], properties -> {}]|[] | +-------+------------+------------------------------------------------------------------------+-----------------------------------------------------------+","title":"Add new users"},{"location":"demo/Observing-Transaction-Retries/","text":"Demo: Observing Transaction Retries \u00b6 Enable ALL logging level for org.apache.spark.sql.delta.OptimisticTransaction logger. You'll be looking for the following DEBUG message in the logs: Attempting to commit version [version] with 13 actions with Serializable isolation level Start with Debugging Delta Lake Using IntelliJ IDEA and place the following line breakpoints in OptimisticTransactionImpl : . In OptimisticTransactionImpl.doCommit when a transaction is about to deltaLog.store.write (line 388) . In OptimisticTransactionImpl.doCommit when a transaction is about to checkAndRetry after a FileAlreadyExistsException (line 433) . In OptimisticTransactionImpl.checkAndRetry when a transaction calculates nextAttemptVersion (line 453) In order to interfere with a transaction about to be committed, you will use ROOT:WriteIntoDelta.md[WriteIntoDelta] action (it is simple and does the work). Run the command (copy and paste the ROOT:WriteIntoDelta.md#demo[demo code] to spark-shell using paste mode). You should see the following messages in the logs: scala> writeCmd.run(spark) DeltaLog: DELTA: Updating the Delta table's state OptimisticTransaction: Attempting to commit version 6 with 13 actions with Serializable isolation level That's when you \"commit\" another transaction (to simulate two competing transactional writes). Simply create a delta file for the transaction. The commit version in the message above is 6 so the name of the delta file should be 00000000000000000006.json : $ touch /tmp/delta/t1/_delta_log/00000000000000000006.json F9 in IntelliJ IDEA to resume the WriteIntoDelta command. It should stop at checkAndRetry due to FileAlreadyExistsException . Press F9 twice to resume. You should see the following messages in the logs: OptimisticTransaction: No logical conflicts with deltas [6, 7), retrying. OptimisticTransaction: Attempting to commit version 7 with 13 actions with Serializable isolation level Rinse and repeat. You know the drill already. Happy debugging!","title":"Observing Transaction Retries"},{"location":"demo/Observing-Transaction-Retries/#demo-observing-transaction-retries","text":"Enable ALL logging level for org.apache.spark.sql.delta.OptimisticTransaction logger. You'll be looking for the following DEBUG message in the logs: Attempting to commit version [version] with 13 actions with Serializable isolation level Start with Debugging Delta Lake Using IntelliJ IDEA and place the following line breakpoints in OptimisticTransactionImpl : . In OptimisticTransactionImpl.doCommit when a transaction is about to deltaLog.store.write (line 388) . In OptimisticTransactionImpl.doCommit when a transaction is about to checkAndRetry after a FileAlreadyExistsException (line 433) . In OptimisticTransactionImpl.checkAndRetry when a transaction calculates nextAttemptVersion (line 453) In order to interfere with a transaction about to be committed, you will use ROOT:WriteIntoDelta.md[WriteIntoDelta] action (it is simple and does the work). Run the command (copy and paste the ROOT:WriteIntoDelta.md#demo[demo code] to spark-shell using paste mode). You should see the following messages in the logs: scala> writeCmd.run(spark) DeltaLog: DELTA: Updating the Delta table's state OptimisticTransaction: Attempting to commit version 6 with 13 actions with Serializable isolation level That's when you \"commit\" another transaction (to simulate two competing transactional writes). Simply create a delta file for the transaction. The commit version in the message above is 6 so the name of the delta file should be 00000000000000000006.json : $ touch /tmp/delta/t1/_delta_log/00000000000000000006.json F9 in IntelliJ IDEA to resume the WriteIntoDelta command. It should stop at checkAndRetry due to FileAlreadyExistsException . Press F9 twice to resume. You should see the following messages in the logs: OptimisticTransaction: No logical conflicts with deltas [6, 7), retrying. OptimisticTransaction: Attempting to commit version 7 with 13 actions with Serializable isolation level Rinse and repeat. You know the drill already. Happy debugging!","title":"Demo: Observing Transaction Retries"},{"location":"demo/Using-Delta-Lake-as-Streaming-Sink-in-Structured-Streaming/","text":"Demo: Using Delta Lake as Streaming Sink in Structured Streaming \u00b6 assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession]) assert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\") // Input data \"format\" case class User(id: Long, name: String, city: String) // Any streaming data source would work // Using memory data source // Gives control over the input stream implicit val ctx = spark.sqlContext import org.apache.spark.sql.execution.streaming.MemoryStream val usersIn = MemoryStream[User] val users = usersIn.toDF val deltaLake = \"/tmp/delta-lake\" val checkpointLocation = \"/tmp/delta-checkpointLocation\" val path = s\"$deltaLake/users\" val partitionBy = \"city\" // The streaming query that writes out to Delta Lake val sq = users .writeStream .format(\"delta\") .option(\"checkpointLocation\", checkpointLocation) .option(\"path\", path) .partitionBy(partitionBy) .start // TIP: You could use git to version the path directory // and track the changes of every micro-batch // TIP: Use web UI to monitor execution, e.g. http://localhost:4040 // FIXME: Use DESCRIBE HISTORY every micro-batch val batch1 = Seq( User(0, \"Agata\", \"Warsaw\"), User(1, \"Jacek\", \"Warsaw\")) val offset = usersIn.addData(batch1) sq.processAllAvailable() val history = s\"DESCRIBE HISTORY delta.`$path`\" val clmns = Seq($\"version\", $\"timestamp\", $\"operation\", $\"operationParameters\", $\"isBlindAppend\") val h = sql(history).select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ val batch2 = Seq( User(2, \"Bartek\", \"Paris\"), User(3, \"Jacek\", \"Paris\")) val offset = usersIn.addData(batch2) sq.processAllAvailable() // You have to execute the history SQL command again // It materializes immediately with whatever data is available at the time val h = sql(history).select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | |1 |2019-12-06 10:13:27|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 1]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ val batch3 = Seq( User(4, \"Gorazd\", \"Ljubljana\")) val offset = usersIn.addData(batch3) sq.processAllAvailable() // Let's use DeltaTable API instead import io.delta.tables.DeltaTable val usersDT = DeltaTable.forPath(path) val h = usersDT.history.select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | |1 |2019-12-06 10:13:27|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 1]|true | |2 |2019-12-06 10:20:56|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 2]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+","title":"Using Delta Lake as Streaming Sink in Structured Streaming"},{"location":"demo/Using-Delta-Lake-as-Streaming-Sink-in-Structured-Streaming/#demo-using-delta-lake-as-streaming-sink-in-structured-streaming","text":"assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession]) assert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\") // Input data \"format\" case class User(id: Long, name: String, city: String) // Any streaming data source would work // Using memory data source // Gives control over the input stream implicit val ctx = spark.sqlContext import org.apache.spark.sql.execution.streaming.MemoryStream val usersIn = MemoryStream[User] val users = usersIn.toDF val deltaLake = \"/tmp/delta-lake\" val checkpointLocation = \"/tmp/delta-checkpointLocation\" val path = s\"$deltaLake/users\" val partitionBy = \"city\" // The streaming query that writes out to Delta Lake val sq = users .writeStream .format(\"delta\") .option(\"checkpointLocation\", checkpointLocation) .option(\"path\", path) .partitionBy(partitionBy) .start // TIP: You could use git to version the path directory // and track the changes of every micro-batch // TIP: Use web UI to monitor execution, e.g. http://localhost:4040 // FIXME: Use DESCRIBE HISTORY every micro-batch val batch1 = Seq( User(0, \"Agata\", \"Warsaw\"), User(1, \"Jacek\", \"Warsaw\")) val offset = usersIn.addData(batch1) sq.processAllAvailable() val history = s\"DESCRIBE HISTORY delta.`$path`\" val clmns = Seq($\"version\", $\"timestamp\", $\"operation\", $\"operationParameters\", $\"isBlindAppend\") val h = sql(history).select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ val batch2 = Seq( User(2, \"Bartek\", \"Paris\"), User(3, \"Jacek\", \"Paris\")) val offset = usersIn.addData(batch2) sq.processAllAvailable() // You have to execute the history SQL command again // It materializes immediately with whatever data is available at the time val h = sql(history).select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | |1 |2019-12-06 10:13:27|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 1]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ val batch3 = Seq( User(4, \"Gorazd\", \"Ljubljana\")) val offset = usersIn.addData(batch3) sq.processAllAvailable() // Let's use DeltaTable API instead import io.delta.tables.DeltaTable val usersDT = DeltaTable.forPath(path) val h = usersDT.history.select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | |1 |2019-12-06 10:13:27|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 1]|true | |2 |2019-12-06 10:20:56|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 2]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+","title":"Demo: Using Delta Lake as Streaming Sink in Structured Streaming"},{"location":"demo/dataChange/","text":"Demo: dataChange \u00b6 This demo shows dataChange option in action. In combination with Overwrite mode, dataChange option can be used to transactionally rearrange data in a delta table. Start Spark Shell \u00b6 ./bin/spark-shell \\ --packages io.delta:delta-core_2.12:1.0.0-SNAPSHOT \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog Create Delta Table \u00b6 val path = \"/tmp/delta/d01\" Make sure that there is no delta table at the location. Remove it if exists and start over. import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, path) assert(deltaLog.tableExists == false) Create the demo delta table (using SQL). sql(s\"\"\" CREATE TABLE delta.`$path` USING delta VALUES ((0, 'Jacek'), (1, 'Agata')) AS (id, name) \"\"\") Show History (Before) \u00b6 import io . delta . tables . DeltaTable val dt = DeltaTable . forPath ( path ) assert ( dt . history . count == 1 ) Repartition Table \u00b6 The following dataChange example shows a batch query that repartitions a delta table (perhaps while other queries could be using the delta table). Let's check out the number of partitions. spark.read.format(\"delta\").load(path).rdd.getNumPartitions The key items to pay attention to are: The batch query is independent from any other running streaming or batch queries over the delta table The batch query reads from the same delta table it saves data to The save mode is overwrite dataChange option is disabled spark .read .format(\"delta\") .load(path) .repartition(10) .write .format(\"delta\") .mode(\"overwrite\") .option(\"dataChange\", false) .save(path) Let's check out the number of partitions after the repartition. spark.read.format(\"delta\").load(path).rdd.getNumPartitions Show History (After) \u00b6 assert ( dt . history . count == 2 ) dt.history .select('version, 'operation, 'operationParameters, 'operationMetrics) .orderBy('version.asc) .show(truncate = false) +-------+----------------------+------------------------------------------------------------------------------+-----------------------------------------------------------+ |version|operation |operationParameters |operationMetrics | +-------+----------------------+------------------------------------------------------------------------------+-----------------------------------------------------------+ |0 |CREATE TABLE AS SELECT|{isManaged -> false, description -> null, partitionBy -> [], properties -> {}}|{numFiles -> 1, numOutputBytes -> 1273, numOutputRows -> 1}| |1 |WRITE |{mode -> Overwrite, partitionBy -> []} |{numFiles -> 2, numOutputBytes -> 1992, numOutputRows -> 1}| +-------+----------------------+------------------------------------------------------------------------------+-----------------------------------------------------------+","title":"dataChange"},{"location":"demo/dataChange/#demo-datachange","text":"This demo shows dataChange option in action. In combination with Overwrite mode, dataChange option can be used to transactionally rearrange data in a delta table.","title":"Demo: dataChange"},{"location":"demo/dataChange/#start-spark-shell","text":"./bin/spark-shell \\ --packages io.delta:delta-core_2.12:1.0.0-SNAPSHOT \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog","title":"Start Spark Shell"},{"location":"demo/dataChange/#create-delta-table","text":"val path = \"/tmp/delta/d01\" Make sure that there is no delta table at the location. Remove it if exists and start over. import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, path) assert(deltaLog.tableExists == false) Create the demo delta table (using SQL). sql(s\"\"\" CREATE TABLE delta.`$path` USING delta VALUES ((0, 'Jacek'), (1, 'Agata')) AS (id, name) \"\"\")","title":"Create Delta Table"},{"location":"demo/dataChange/#show-history-before","text":"import io . delta . tables . DeltaTable val dt = DeltaTable . forPath ( path ) assert ( dt . history . count == 1 )","title":"Show History (Before)"},{"location":"demo/dataChange/#repartition-table","text":"The following dataChange example shows a batch query that repartitions a delta table (perhaps while other queries could be using the delta table). Let's check out the number of partitions. spark.read.format(\"delta\").load(path).rdd.getNumPartitions The key items to pay attention to are: The batch query is independent from any other running streaming or batch queries over the delta table The batch query reads from the same delta table it saves data to The save mode is overwrite dataChange option is disabled spark .read .format(\"delta\") .load(path) .repartition(10) .write .format(\"delta\") .mode(\"overwrite\") .option(\"dataChange\", false) .save(path) Let's check out the number of partitions after the repartition. spark.read.format(\"delta\").load(path).rdd.getNumPartitions","title":"Repartition Table"},{"location":"demo/dataChange/#show-history-after","text":"assert ( dt . history . count == 2 ) dt.history .select('version, 'operation, 'operationParameters, 'operationMetrics) .orderBy('version.asc) .show(truncate = false) +-------+----------------------+------------------------------------------------------------------------------+-----------------------------------------------------------+ |version|operation |operationParameters |operationMetrics | +-------+----------------------+------------------------------------------------------------------------------+-----------------------------------------------------------+ |0 |CREATE TABLE AS SELECT|{isManaged -> false, description -> null, partitionBy -> [], properties -> {}}|{numFiles -> 1, numOutputBytes -> 1273, numOutputRows -> 1}| |1 |WRITE |{mode -> Overwrite, partitionBy -> []} |{numFiles -> 2, numOutputBytes -> 1992, numOutputRows -> 1}| +-------+----------------------+------------------------------------------------------------------------------+-----------------------------------------------------------+","title":"Show History (After)"},{"location":"demo/merge-operation/","text":"Demo: Merge Operation \u00b6 This demo shows DeltaTable.merge operation (and the underlying MergeIntoCommand ) in action. Tip Enable ALL logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger as described in Logging . Create Delta Table (Target Data) \u00b6 val path = \"/tmp/delta/demo\" val data = spark . range ( 5 ) data . write . format ( \"delta\" ). save ( path ) import io . delta . tables . DeltaTable val target = DeltaTable . forPath ( path ) assert ( target . isInstanceOf [ io . delta . tables . DeltaTable ]) assert ( target . history . count == 1 , \"There must be version 0 only\" ) Source Data \u00b6 case class Person ( id : Long , name : String ) val source = Seq ( Person ( 0 , \"Zero\" ), Person ( 1 , \"One\" )). toDF Note the difference in schemas scala> target.toDF.printSchema root |-- id: long (nullable = true) scala> source.printSchema root |-- id: long (nullable = false) |-- name: string (nullable = true) Merge with Schema Evolution \u00b6 Not only do we update the matching rows, but also update the schema (schema evolution) val mergeBuilder = target . as ( \"to\" ) . merge ( source = source . as ( \"from\" ), condition = $ \"to.id\" === $ \"from.id\" ) assert(mergeBuilder.isInstanceOf[io.delta.tables.DeltaMergeBuilder]) scala> mergeBuilder.execute org.apache.spark.sql.AnalysisException: There must be at least one WHEN clause in a MERGE query; at org.apache.spark.sql.catalyst.plans.logical.DeltaMergeInto$.apply(deltaMerge.scala:217) at io.delta.tables.DeltaMergeBuilder.mergePlan(DeltaMergeBuilder.scala:255) at io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:228) at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:60) at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:48) at io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:121) at io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:225) ... 47 elided val mergeMatchedBuilder = mergeBuilder . whenMatched () assert ( mergeMatchedBuilder . isInstanceOf [ io . delta . tables . DeltaMergeMatchedActionBuilder ]) val mergeBuilderDeleteMatched = mergeMatchedBuilder . delete () assert ( mergeBuilderDeleteMatched . isInstanceOf [ io . delta . tables . DeltaMergeBuilder ]) mergeBuilderDeleteMatched . execute () assert ( target . history . count == 2 , \"There must be two versions only\" ) Update All Columns Except One \u00b6 This demo shows how to update all columns except one on a match. val targetDF = target . toDF . withColumn ( \"num\" , lit ( 1 )) . withColumn ( \"updated\" , lit ( false )) scala> targetDF.sort('id.asc).show +---+---+-------+ | id|num|updated| +---+---+-------+ | 0| 1| false| | 1| 1| false| | 2| 1| false| | 3| 1| false| | 4| 1| false| +---+---+-------+ Write the modified data out to the delta table (that will create a new version with the schema changed). targetDF . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . option ( \"overwriteSchema\" , true ) . save ( path ) Reload the delta table (with the new column changes). val target = DeltaTable . forPath ( path ) val targetDF = target . toDF val sourceDF = Seq ( 0 , 1 , 2 ). toDF ( \"num\" ) Create an update map (with the columns of the target delta table and the new values). val updates = Map ( \"updated\" -> lit ( true )) target . as ( \"to\" ) . merge ( source = sourceDF . as ( \"from\" ), condition = $ \"to.id\" === $ \"from.num\" ) . whenMatched . update ( updates ) . execute () Reload the delta table (with the merge changes). val target = DeltaTable . forPath ( path ) scala> target.toDF.sort('id.asc).show +---+---+-------+ | id|num|updated| +---+---+-------+ | 0| 1| true| | 1| 1| true| | 2| 1| true| | 3| 1| false| | 4| 1| false| +---+---+-------+","title":"Merge Operation"},{"location":"demo/merge-operation/#demo-merge-operation","text":"This demo shows DeltaTable.merge operation (and the underlying MergeIntoCommand ) in action. Tip Enable ALL logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger as described in Logging .","title":"Demo: Merge Operation"},{"location":"demo/merge-operation/#create-delta-table-target-data","text":"val path = \"/tmp/delta/demo\" val data = spark . range ( 5 ) data . write . format ( \"delta\" ). save ( path ) import io . delta . tables . DeltaTable val target = DeltaTable . forPath ( path ) assert ( target . isInstanceOf [ io . delta . tables . DeltaTable ]) assert ( target . history . count == 1 , \"There must be version 0 only\" )","title":"Create Delta Table (Target Data)"},{"location":"demo/merge-operation/#source-data","text":"case class Person ( id : Long , name : String ) val source = Seq ( Person ( 0 , \"Zero\" ), Person ( 1 , \"One\" )). toDF Note the difference in schemas scala> target.toDF.printSchema root |-- id: long (nullable = true) scala> source.printSchema root |-- id: long (nullable = false) |-- name: string (nullable = true)","title":"Source Data"},{"location":"demo/merge-operation/#merge-with-schema-evolution","text":"Not only do we update the matching rows, but also update the schema (schema evolution) val mergeBuilder = target . as ( \"to\" ) . merge ( source = source . as ( \"from\" ), condition = $ \"to.id\" === $ \"from.id\" ) assert(mergeBuilder.isInstanceOf[io.delta.tables.DeltaMergeBuilder]) scala> mergeBuilder.execute org.apache.spark.sql.AnalysisException: There must be at least one WHEN clause in a MERGE query; at org.apache.spark.sql.catalyst.plans.logical.DeltaMergeInto$.apply(deltaMerge.scala:217) at io.delta.tables.DeltaMergeBuilder.mergePlan(DeltaMergeBuilder.scala:255) at io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:228) at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:60) at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:48) at io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:121) at io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:225) ... 47 elided val mergeMatchedBuilder = mergeBuilder . whenMatched () assert ( mergeMatchedBuilder . isInstanceOf [ io . delta . tables . DeltaMergeMatchedActionBuilder ]) val mergeBuilderDeleteMatched = mergeMatchedBuilder . delete () assert ( mergeBuilderDeleteMatched . isInstanceOf [ io . delta . tables . DeltaMergeBuilder ]) mergeBuilderDeleteMatched . execute () assert ( target . history . count == 2 , \"There must be two versions only\" )","title":"Merge with Schema Evolution"},{"location":"demo/merge-operation/#update-all-columns-except-one","text":"This demo shows how to update all columns except one on a match. val targetDF = target . toDF . withColumn ( \"num\" , lit ( 1 )) . withColumn ( \"updated\" , lit ( false )) scala> targetDF.sort('id.asc).show +---+---+-------+ | id|num|updated| +---+---+-------+ | 0| 1| false| | 1| 1| false| | 2| 1| false| | 3| 1| false| | 4| 1| false| +---+---+-------+ Write the modified data out to the delta table (that will create a new version with the schema changed). targetDF . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . option ( \"overwriteSchema\" , true ) . save ( path ) Reload the delta table (with the new column changes). val target = DeltaTable . forPath ( path ) val targetDF = target . toDF val sourceDF = Seq ( 0 , 1 , 2 ). toDF ( \"num\" ) Create an update map (with the columns of the target delta table and the new values). val updates = Map ( \"updated\" -> lit ( true )) target . as ( \"to\" ) . merge ( source = sourceDF . as ( \"from\" ), condition = $ \"to.id\" === $ \"from.num\" ) . whenMatched . update ( updates ) . execute () Reload the delta table (with the merge changes). val target = DeltaTable . forPath ( path ) scala> target.toDF.sort('id.asc).show +---+---+-------+ | id|num|updated| +---+---+-------+ | 0| 1| true| | 1| 1| true| | 2| 1| true| | 3| 1| false| | 4| 1| false| +---+---+-------+","title":"Update All Columns Except One"},{"location":"demo/replaceWhere/","text":"Demo: replaceWhere \u00b6 This demo shows replaceWhere predicate option. In combination with Overwrite mode, a replaceWhere option can be used to transactionally replace data that matches a predicate. Create Delta Table \u00b6 val table = \"d1\" sql ( s\"\"\" CREATE TABLE $ table (`id` LONG, p STRING) USING delta PARTITIONED BY (p) COMMENT 'Delta table' \"\"\" ) spark . catalog . listTables . show +----+--------+-----------+---------+-----------+ |name|database|description|tableType|isTemporary| +----+--------+-----------+---------+-----------+ | d1| default|Delta table| MANAGED| false| +----+--------+-----------+---------+-----------+ import org . apache . spark . sql . delta . DeltaLog val dt = DeltaLog . forTable ( spark , table ) val m = dt . snapshot . metadata val partitionSchema = m . partitionSchema Write Data \u00b6 Seq (( 0L , \"a\" ), ( 1L , \"a\" )) . toDF ( \"id\" , \"p\" ) . write . format ( \"delta\" ) . option ( \"replaceWhere\" , \"p = 'a'\" ) . mode ( \"overwrite\" ) . saveAsTable ( table )","title":"replaceWhere"},{"location":"demo/replaceWhere/#demo-replacewhere","text":"This demo shows replaceWhere predicate option. In combination with Overwrite mode, a replaceWhere option can be used to transactionally replace data that matches a predicate.","title":"Demo: replaceWhere"},{"location":"demo/replaceWhere/#create-delta-table","text":"val table = \"d1\" sql ( s\"\"\" CREATE TABLE $ table (`id` LONG, p STRING) USING delta PARTITIONED BY (p) COMMENT 'Delta table' \"\"\" ) spark . catalog . listTables . show +----+--------+-----------+---------+-----------+ |name|database|description|tableType|isTemporary| +----+--------+-----------+---------+-----------+ | d1| default|Delta table| MANAGED| false| +----+--------+-----------+---------+-----------+ import org . apache . spark . sql . delta . DeltaLog val dt = DeltaLog . forTable ( spark , table ) val m = dt . snapshot . metadata val partitionSchema = m . partitionSchema","title":"Create Delta Table"},{"location":"demo/replaceWhere/#write-data","text":"Seq (( 0L , \"a\" ), ( 1L , \"a\" )) . toDF ( \"id\" , \"p\" ) . write . format ( \"delta\" ) . option ( \"replaceWhere\" , \"p = 'a'\" ) . mode ( \"overwrite\" ) . saveAsTable ( table )","title":"Write Data"},{"location":"demo/schema-evolution/","text":"Demo: Schema Evolution \u00b6 /* spark-shell \\ --packages io.delta:delta-core_2.12:1.0.0-SNAPSHOT \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ case class PersonV1(id: Long, name: String) import org.apache.spark.sql.Encoders val schemaV1 = Encoders.product[PersonV1].schema scala> schemaV1.printTreeString root |-- id: long (nullable = false) |-- name: string (nullable = true) val dataPath = \"/tmp/delta/people\" // Write data Seq(PersonV1(0, \"Zero\"), PersonV1(1, \"One\")) .toDF .write .format(\"delta\") .mode(\"overwrite\") .option(\"overwriteSchema\", \"true\") .save(dataPath) // Committed delta #0 to file:/tmp/delta/people/_delta_log import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, dataPath) assert(deltaLog.snapshot.version == 0) scala> deltaLog.snapshot.dataSchema.printTreeString root |-- id: long (nullable = true) |-- name: string (nullable = true) import io.delta.tables.DeltaTable val dt = DeltaTable.forPath(dataPath) scala> dt.toDF.show +---+----+ | id|name| +---+----+ | 0|Zero| | 1| One| +---+----+ val main = dt.as(\"main\") case class PersonV2(id: Long, name: String, newField: Boolean) val schemaV2 = Encoders.product[PersonV2].schema scala> schemaV2.printTreeString root |-- id: long (nullable = false) |-- name: string (nullable = true) |-- newField: boolean (nullable = false) val updates = Seq( PersonV2(0, \"ZERO\", newField = true), PersonV2(2, \"TWO\", newField = false)).toDF // Merge two datasets and create a new version // Schema evolution in play main.merge( source = updates.as(\"updates\"), condition = $\"main.id\" === $\"updates.id\") .whenMatched.updateAll .execute val latestPeople = spark .read .format(\"delta\") .load(dataPath) scala> latestPeople.show +---+----+ | id|name| +---+----+ | 0|ZERO| | 1| One| +---+----+","title":"Schema Evolution"},{"location":"demo/schema-evolution/#demo-schema-evolution","text":"/* spark-shell \\ --packages io.delta:delta-core_2.12:1.0.0-SNAPSHOT \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ case class PersonV1(id: Long, name: String) import org.apache.spark.sql.Encoders val schemaV1 = Encoders.product[PersonV1].schema scala> schemaV1.printTreeString root |-- id: long (nullable = false) |-- name: string (nullable = true) val dataPath = \"/tmp/delta/people\" // Write data Seq(PersonV1(0, \"Zero\"), PersonV1(1, \"One\")) .toDF .write .format(\"delta\") .mode(\"overwrite\") .option(\"overwriteSchema\", \"true\") .save(dataPath) // Committed delta #0 to file:/tmp/delta/people/_delta_log import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, dataPath) assert(deltaLog.snapshot.version == 0) scala> deltaLog.snapshot.dataSchema.printTreeString root |-- id: long (nullable = true) |-- name: string (nullable = true) import io.delta.tables.DeltaTable val dt = DeltaTable.forPath(dataPath) scala> dt.toDF.show +---+----+ | id|name| +---+----+ | 0|Zero| | 1| One| +---+----+ val main = dt.as(\"main\") case class PersonV2(id: Long, name: String, newField: Boolean) val schemaV2 = Encoders.product[PersonV2].schema scala> schemaV2.printTreeString root |-- id: long (nullable = false) |-- name: string (nullable = true) |-- newField: boolean (nullable = false) val updates = Seq( PersonV2(0, \"ZERO\", newField = true), PersonV2(2, \"TWO\", newField = false)).toDF // Merge two datasets and create a new version // Schema evolution in play main.merge( source = updates.as(\"updates\"), condition = $\"main.id\" === $\"updates.id\") .whenMatched.updateAll .execute val latestPeople = spark .read .format(\"delta\") .load(dataPath) scala> latestPeople.show +---+----+ | id|name| +---+----+ | 0|ZERO| | 1| One| +---+----+","title":"Demo: Schema Evolution"},{"location":"demo/stream-processing-of-delta-table/","text":"Demo: Stream Processing of Delta Table \u00b6 /* spark-shell \\ --packages io.delta:delta-core_2.12:1.0.0-SNAPSHOT \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ assert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\") import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val deltaTableDir = \"/tmp/delta/users\" val checkpointLocation = \"/tmp/checkpointLocation\" // Initialize the delta table // - No data // - Schema only case class Person(id: Long, name: String, city: String) spark.emptyDataset[Person].write.format(\"delta\").save(deltaTableDir) import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"delta\") .option(\"maxFilesPerTrigger\", 1) // Maximum number of files to scan per micro-batch .load(deltaTableDir) .writeStream .format(\"console\") .option(\"truncate\", false) .option(\"checkpointLocation\", checkpointLocation) .trigger(Trigger.ProcessingTime(10.seconds)) // Useful for debugging .start // The streaming query over delta table // should display the 0th version as Batch 0 ------------------------------------------- Batch: 0 ------------------------------------------- +---+----+----+ |id |name|city| +---+----+----+ +---+----+----+ // Let's write to the delta table val users = Seq( Person(0, \"Jacek\", \"Warsaw\"), Person(1, \"Agata\", \"Warsaw\"), Person(2, \"Jacek\", \"Paris\"), Person(3, \"Domas\", \"Vilnius\")).toDF // More partitions are more file added // And per maxFilesPerTrigger as 1 file addition per micro-batch // You should see more micro-batches scala> println(users.rdd.getNumPartitions) 4 // Change the default SaveMode.ErrorIfExists to more meaningful save mode import org.apache.spark.sql.SaveMode users .write .format(\"delta\") .mode(SaveMode.Append) // Appending rows .save(deltaTableDir) // Immediately after the above write finishes // New batches should be printed out to the console // Per the number of partitions of users dataset // And per maxFilesPerTrigger as 1 file addition // You should see as many micro-batches as files ------------------------------------------- Batch: 1 ------------------------------------------- +---+-----+------+ |id |name |city | +---+-----+------+ |0 |Jacek|Warsaw| +---+-----+------+ ------------------------------------------- Batch: 2 ------------------------------------------- +---+-----+------+ |id |name |city | +---+-----+------+ |1 |Agata|Warsaw| +---+-----+------+ ------------------------------------------- Batch: 3 ------------------------------------------- +---+-----+-----+ |id |name |city | +---+-----+-----+ |2 |Jacek|Paris| +---+-----+-----+ ------------------------------------------- Batch: 4 ------------------------------------------- +---+-----+-------+ |id |name |city | +---+-----+-------+ |3 |Domas|Vilnius| +---+-----+-------+","title":"Stream Processing of Delta Table"},{"location":"demo/stream-processing-of-delta-table/#demo-stream-processing-of-delta-table","text":"/* spark-shell \\ --packages io.delta:delta-core_2.12:1.0.0-SNAPSHOT \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ assert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\") import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val deltaTableDir = \"/tmp/delta/users\" val checkpointLocation = \"/tmp/checkpointLocation\" // Initialize the delta table // - No data // - Schema only case class Person(id: Long, name: String, city: String) spark.emptyDataset[Person].write.format(\"delta\").save(deltaTableDir) import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"delta\") .option(\"maxFilesPerTrigger\", 1) // Maximum number of files to scan per micro-batch .load(deltaTableDir) .writeStream .format(\"console\") .option(\"truncate\", false) .option(\"checkpointLocation\", checkpointLocation) .trigger(Trigger.ProcessingTime(10.seconds)) // Useful for debugging .start // The streaming query over delta table // should display the 0th version as Batch 0 ------------------------------------------- Batch: 0 ------------------------------------------- +---+----+----+ |id |name|city| +---+----+----+ +---+----+----+ // Let's write to the delta table val users = Seq( Person(0, \"Jacek\", \"Warsaw\"), Person(1, \"Agata\", \"Warsaw\"), Person(2, \"Jacek\", \"Paris\"), Person(3, \"Domas\", \"Vilnius\")).toDF // More partitions are more file added // And per maxFilesPerTrigger as 1 file addition per micro-batch // You should see more micro-batches scala> println(users.rdd.getNumPartitions) 4 // Change the default SaveMode.ErrorIfExists to more meaningful save mode import org.apache.spark.sql.SaveMode users .write .format(\"delta\") .mode(SaveMode.Append) // Appending rows .save(deltaTableDir) // Immediately after the above write finishes // New batches should be printed out to the console // Per the number of partitions of users dataset // And per maxFilesPerTrigger as 1 file addition // You should see as many micro-batches as files ------------------------------------------- Batch: 1 ------------------------------------------- +---+-----+------+ |id |name |city | +---+-----+------+ |0 |Jacek|Warsaw| +---+-----+------+ ------------------------------------------- Batch: 2 ------------------------------------------- +---+-----+------+ |id |name |city | +---+-----+------+ |1 |Agata|Warsaw| +---+-----+------+ ------------------------------------------- Batch: 3 ------------------------------------------- +---+-----+-----+ |id |name |city | +---+-----+-----+ |2 |Jacek|Paris| +---+-----+-----+ ------------------------------------------- Batch: 4 ------------------------------------------- +---+-----+-------+ |id |name |city | +---+-----+-------+ |3 |Domas|Vilnius| +---+-----+-------+","title":"Demo: Stream Processing of Delta Table"},{"location":"demo/user-metadata-for-labelling-commits/","text":"Demo: User Metadata for Labelling Commits \u00b6 The demo shows how to differentiate commits of a write batch query using userMetadata option. Tip A fine example could be for distinguishing between two or more separate streaming write queries. Creating Delta Table \u00b6 val tableName = \"/tmp/delta-demo-userMetadata\" spark.range(5) .write .format(\"delta\") .save(tableName) Describing History \u00b6 import io . delta . tables . DeltaTable val d = DeltaTable . forPath ( tableName ) We are interested in a subset of the available history metadata. d . history . select ( 'version , 'operation , 'operationParameters , 'userMetadata ) . show ( truncate = false ) +-------+---------+------------------------------------------+------------+ |version|operation|operationParameters |userMetadata| +-------+---------+------------------------------------------+------------+ |0 |WRITE |[mode -> ErrorIfExists, partitionBy -> []]|null | +-------+---------+------------------------------------------+------------+ Appending Data \u00b6 In this step, you're going to append new data to the existing Delta table. You're going to use userMetadata option for a custom user-defined historical marker (e.g. to know when this extra append happended in the life of the Delta table). val userMetadata = \"two more rows for demo\" Since you're appending new rows, it is required to use Append mode. import org . apache . spark . sql . SaveMode . Append The whole append write is as follows: spark . range ( start = 5 , end = 7 ) . write . format ( \"delta\" ) . option ( \"userMetadata\" , userMetadata ) . mode ( Append ) . save ( tableName ) That write query creates another version of the Delta table. Listing Versions with userMetadata \u00b6 For the sake of the demo, you are going to show the versions of the Delta table with userMetadata defined. d . history . select ( 'version , 'operation , 'operationParameters , 'userMetadata ) . where ( 'userMetadata . isNotNull ) . show ( truncate = false ) +-------+---------+-----------------------------------+----------------------+ |version|operation|operationParameters |userMetadata | +-------+---------+-----------------------------------+----------------------+ |1 |WRITE |[mode -> Append, partitionBy -> []]|two more rows for demo| +-------+---------+-----------------------------------+----------------------+","title":"User Metadata for Labelling Commits"},{"location":"demo/user-metadata-for-labelling-commits/#demo-user-metadata-for-labelling-commits","text":"The demo shows how to differentiate commits of a write batch query using userMetadata option. Tip A fine example could be for distinguishing between two or more separate streaming write queries.","title":"Demo: User Metadata for Labelling Commits"},{"location":"demo/user-metadata-for-labelling-commits/#creating-delta-table","text":"val tableName = \"/tmp/delta-demo-userMetadata\" spark.range(5) .write .format(\"delta\") .save(tableName)","title":"Creating Delta Table"},{"location":"demo/user-metadata-for-labelling-commits/#describing-history","text":"import io . delta . tables . DeltaTable val d = DeltaTable . forPath ( tableName ) We are interested in a subset of the available history metadata. d . history . select ( 'version , 'operation , 'operationParameters , 'userMetadata ) . show ( truncate = false ) +-------+---------+------------------------------------------+------------+ |version|operation|operationParameters |userMetadata| +-------+---------+------------------------------------------+------------+ |0 |WRITE |[mode -> ErrorIfExists, partitionBy -> []]|null | +-------+---------+------------------------------------------+------------+","title":"Describing History"},{"location":"demo/user-metadata-for-labelling-commits/#appending-data","text":"In this step, you're going to append new data to the existing Delta table. You're going to use userMetadata option for a custom user-defined historical marker (e.g. to know when this extra append happended in the life of the Delta table). val userMetadata = \"two more rows for demo\" Since you're appending new rows, it is required to use Append mode. import org . apache . spark . sql . SaveMode . Append The whole append write is as follows: spark . range ( start = 5 , end = 7 ) . write . format ( \"delta\" ) . option ( \"userMetadata\" , userMetadata ) . mode ( Append ) . save ( tableName ) That write query creates another version of the Delta table.","title":"Appending Data"},{"location":"demo/user-metadata-for-labelling-commits/#listing-versions-with-usermetadata","text":"For the sake of the demo, you are going to show the versions of the Delta table with userMetadata defined. d . history . select ( 'version , 'operation , 'operationParameters , 'userMetadata ) . where ( 'userMetadata . isNotNull ) . show ( truncate = false ) +-------+---------+-----------------------------------+----------------------+ |version|operation|operationParameters |userMetadata | +-------+---------+-----------------------------------+----------------------+ |1 |WRITE |[mode -> Append, partitionBy -> []]|two more rows for demo| +-------+---------+-----------------------------------+----------------------+","title":"Listing Versions with userMetadata"},{"location":"demo/vacuum/","text":"Demo: Vacuum \u00b6 This demo shows vacuum command in action. Start Spark Shell \u00b6 ./bin/spark-shell \\ --packages io.delta:delta-core_2.12:1.0.0-SNAPSHOT \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog Create Delta Table \u00b6 val path = \"/tmp/delta/t1\" Make sure that there is no delta table at the location. Remove it if exists and start over. import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, path) assert(deltaLog.tableExists == false) Create a demo delta table (using Scala API). Write some data to the delta table, effectively creating the first version. spark . range ( 4 ) . withColumn ( \"p\" , 'id % 2 ) . write . format ( \"delta\" ) . partitionBy ( \"p\" ) . save ( path ) Display the available versions of the delta table. import io . delta . tables . DeltaTable val dt = DeltaTable . forPath ( path ) val history = dt . history . select ( 'version , 'operation , 'operationMetrics ) history . show ( truncate = false ) +-------+---------+-----------------------------------------------------------+ |version|operation|operationMetrics | +-------+---------+-----------------------------------------------------------+ |0 |WRITE |[numFiles -> 4, numOutputBytes -> 1852, numOutputRows -> 4]| +-------+---------+-----------------------------------------------------------+ Delete All \u00b6 Delete all data in the delta table, effectively creating the second version. import io . delta . tables . DeltaTable DeltaTable . forPath ( path ). delete Display the available versions of the delta table. val history = dt . history . select ( 'version , 'operation , 'operationMetrics ) history . show ( truncate = false ) +-------+---------+-----------------------------------------------------------+ |version|operation|operationMetrics | +-------+---------+-----------------------------------------------------------+ |1 |DELETE |[numRemovedFiles -> 4] | |0 |WRITE |[numFiles -> 4, numOutputBytes -> 1852, numOutputRows -> 4]| +-------+---------+-----------------------------------------------------------+ Vacuum DRY RUN (IllegalArgumentException) \u00b6 Let's vacuum the delta table (in DRY RUN mode). sql ( s\"VACUUM delta.` $ path ` RETAIN 0 HOURS DRY RUN\" ) java.lang.IllegalArgumentException: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have writers that are currently writing to this table, there is a risk that you may corrupt the state of your Delta table. If you are certain that there are no operations being performed on this table, such as insert/upsert/delete/optimize, then you may turn off this check by setting: spark.databricks.delta.retentionDurationCheck.enabled = false If you are not sure, please use a value not less than \"168 hours\". Attempting to vacuum the delta table (even with DRY RUN ) gives an IllegalArgumentException because of the default values of the following: spark.databricks.delta.retentionDurationCheck.enabled configuration property deletedFileRetentionDuration table property Vacuum DRY RUN \u00b6 retentionDurationCheck.enabled Configuration Property \u00b6 Turn the spark.databricks.delta.retentionDurationCheck.enabled configuration property off and give the VACUUM command a go again. spark . conf . set ( \"spark.databricks.delta.retentionDurationCheck.enabled\" , false ) val q = sql ( s\"VACUUM delta.` $ path ` RETAIN 0 HOURS DRY RUN\" ) You should see the following message in the console: Found 4 files and directories in a total of 3 directories that are safe to delete. The result DataFrame is the paths that are safe to delete which are all of the data files in the delta table. q . show ( truncate = false ) +------------------------------------------------------------------------------------------+ |path | +------------------------------------------------------------------------------------------+ |file:/tmp/delta/t1/p=0/part-00011-40983cc5-18bf-4d91-8b7b-eb805b8c862d.c000.snappy.parquet| |file:/tmp/delta/t1/p=1/part-00015-9576ff56-28f7-410e-8ea3-e43352a5083c.c000.snappy.parquet| |file:/tmp/delta/t1/p=0/part-00003-8e300857-ad6a-4889-b944-d31624a8024f.c000.snappy.parquet| |file:/tmp/delta/t1/p=1/part-00007-4fba265a-0884-44c3-84ed-4e9aee524e3a.c000.snappy.parquet| +------------------------------------------------------------------------------------------+ deletedFileRetentionDuration Table Property \u00b6 Let's DESCRIBE DETAIL to review the current table properties (incl. deletedFileRetentionDuration ). val tid = s\"delta.` $ path `\" val detail = sql ( s\"DESCRIBE DETAIL $ tid \" ). select ( 'format , 'location , 'properties ) detail . show ( truncate = false ) +------+------------------+----------+ |format|location |properties| +------+------------------+----------+ |delta |file:/tmp/delta/t1|[] | +------+------------------+----------+ Prefix the deletedFileRetentionDuration table property with delta. for ALTER TABLE to accept it as a Delta property. sql ( s\"ALTER TABLE $ tid SET TBLPROPERTIES (delta.deletedFileRetentionDuration = '0 hours')\" ) val detail = sql ( s\"DESCRIBE DETAIL $ tid \" ). select ( 'format , 'location , 'properties ) detail . show ( truncate = false ) +------+------------------+-----------------------------------------------+ |format|location |properties | +------+------------------+-----------------------------------------------+ |delta |file:/tmp/delta/t1|[delta.deletedFileRetentionDuration -> 0 hours]| +------+------------------+-----------------------------------------------+ Display the available versions of the delta table and note that ALTER TABLE gives a new version. This time you include operationParameters column (not operationMetrics as less important). val history = dt . history . select ( 'version , 'operation , 'operationParameters ) history . show ( truncate = false ) +-------+-----------------+----------------------------------------------------------------+ |version|operation |operationParameters | +-------+-----------------+----------------------------------------------------------------+ |2 |SET TBLPROPERTIES|[properties -> {\"delta.deletedFileRetentionDuration\":\"0 hours\"}]| |1 |DELETE |[predicate -> []] | |0 |WRITE |[mode -> ErrorIfExists, partitionBy -> [\"p\"]] | +-------+-----------------+----------------------------------------------------------------+ You can access the table properties ( table configuration ) using DeltaLog Scala API. import org . apache . spark . sql . delta . DeltaLog val log = DeltaLog . forTable ( spark , path ) log . snapshot . metadata . configuration Let's revert the latest change to spark.databricks.delta.retentionDurationCheck.enabled and turn it on back. spark . conf . set ( \"spark.databricks.delta.retentionDurationCheck.enabled\" , true ) val q = sql ( s\"VACUUM delta.` $ path ` RETAIN 0 HOURS DRY RUN\" ) You should see the following message in the console: Found 4 files and directories in a total of 3 directories that are safe to delete. The result DataFrame is the paths that are safe to delete which are all of the data files in the delta table. q . show ( truncate = false ) +------------------------------------------------------------------------------------------+ |path | +------------------------------------------------------------------------------------------+ |file:/tmp/delta/t1/p=0/part-00011-40983cc5-18bf-4d91-8b7b-eb805b8c862d.c000.snappy.parquet| |file:/tmp/delta/t1/p=1/part-00015-9576ff56-28f7-410e-8ea3-e43352a5083c.c000.snappy.parquet| |file:/tmp/delta/t1/p=0/part-00003-8e300857-ad6a-4889-b944-d31624a8024f.c000.snappy.parquet| |file:/tmp/delta/t1/p=1/part-00007-4fba265a-0884-44c3-84ed-4e9aee524e3a.c000.snappy.parquet| +------------------------------------------------------------------------------------------+ Tree Delta Table Directory \u00b6 In a terminal (outside spark-shell ) run tree or a similar command to review what the directory of the delta table looks like. tree /tmp/delta/t1 /tmp/delta/t1 \u251c\u2500\u2500 _delta_log \u2502 \u251c\u2500\u2500 00000000000000000000.json \u2502 \u251c\u2500\u2500 00000000000000000001.json \u2502 \u2514\u2500\u2500 00000000000000000002.json \u251c\u2500\u2500 p=0 \u2502 \u251c\u2500\u2500 part-00003-8e300857-ad6a-4889-b944-d31624a8024f.c000.snappy.parquet \u2502 \u2514\u2500\u2500 part-00011-40983cc5-18bf-4d91-8b7b-eb805b8c862d.c000.snappy.parquet \u2514\u2500\u2500 p=1 \u251c\u2500\u2500 part-00007-4fba265a-0884-44c3-84ed-4e9aee524e3a.c000.snappy.parquet \u2514\u2500\u2500 part-00015-9576ff56-28f7-410e-8ea3-e43352a5083c.c000.snappy.parquet 3 directories, 7 files Retain 0 Hours \u00b6 Let's clean up ( vacuum ) the delta table entirely, effectively deleting all the data files physically from disk. Back in spark-shell , run VACUUM SQL command. Note that you're going to use it with no DRY RUN . sql ( s\"VACUUM delta.` $ path ` RETAIN 0 HOURS\" ). show ( truncate = false ) Deleted 4 files and directories in a total of 3 directories. +------------------+ |path | +------------------+ |file:/tmp/delta/t1| +------------------+ In a terminal (outside spark-shell ), run tree or a similar command to review what the directory of the delta table looks like. tree /tmp/delta/t1 /tmp/delta/t1 \u251c\u2500\u2500 _delta_log \u2502 \u251c\u2500\u2500 00000000000000000000.json \u2502 \u251c\u2500\u2500 00000000000000000001.json \u2502 \u2514\u2500\u2500 00000000000000000002.json \u251c\u2500\u2500 p=0 \u2514\u2500\u2500 p=1 3 directories, 3 files Switch to spark-shell and display the available versions of the delta table. There should really be no change compared to the last time you executed it. val history = dt . history . select ( 'version , 'operation , 'operationParameters ) history . show ( truncate = false ) +-------+-----------------+----------------------------------------------------------------+ |version|operation |operationParameters | +-------+-----------------+----------------------------------------------------------------+ |2 |SET TBLPROPERTIES|[properties -> {\"delta.deletedFileRetentionDuration\":\"0 hours\"}]| |1 |DELETE |[predicate -> []] | |0 |WRITE |[mode -> ErrorIfExists, partitionBy -> [\"p\"]] | +-------+-----------------+----------------------------------------------------------------+","title":"Vacuum"},{"location":"demo/vacuum/#demo-vacuum","text":"This demo shows vacuum command in action.","title":"Demo: Vacuum"},{"location":"demo/vacuum/#start-spark-shell","text":"./bin/spark-shell \\ --packages io.delta:delta-core_2.12:1.0.0-SNAPSHOT \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog","title":"Start Spark Shell"},{"location":"demo/vacuum/#create-delta-table","text":"val path = \"/tmp/delta/t1\" Make sure that there is no delta table at the location. Remove it if exists and start over. import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, path) assert(deltaLog.tableExists == false) Create a demo delta table (using Scala API). Write some data to the delta table, effectively creating the first version. spark . range ( 4 ) . withColumn ( \"p\" , 'id % 2 ) . write . format ( \"delta\" ) . partitionBy ( \"p\" ) . save ( path ) Display the available versions of the delta table. import io . delta . tables . DeltaTable val dt = DeltaTable . forPath ( path ) val history = dt . history . select ( 'version , 'operation , 'operationMetrics ) history . show ( truncate = false ) +-------+---------+-----------------------------------------------------------+ |version|operation|operationMetrics | +-------+---------+-----------------------------------------------------------+ |0 |WRITE |[numFiles -> 4, numOutputBytes -> 1852, numOutputRows -> 4]| +-------+---------+-----------------------------------------------------------+","title":"Create Delta Table"},{"location":"demo/vacuum/#delete-all","text":"Delete all data in the delta table, effectively creating the second version. import io . delta . tables . DeltaTable DeltaTable . forPath ( path ). delete Display the available versions of the delta table. val history = dt . history . select ( 'version , 'operation , 'operationMetrics ) history . show ( truncate = false ) +-------+---------+-----------------------------------------------------------+ |version|operation|operationMetrics | +-------+---------+-----------------------------------------------------------+ |1 |DELETE |[numRemovedFiles -> 4] | |0 |WRITE |[numFiles -> 4, numOutputBytes -> 1852, numOutputRows -> 4]| +-------+---------+-----------------------------------------------------------+","title":"Delete All"},{"location":"demo/vacuum/#vacuum-dry-run-illegalargumentexception","text":"Let's vacuum the delta table (in DRY RUN mode). sql ( s\"VACUUM delta.` $ path ` RETAIN 0 HOURS DRY RUN\" ) java.lang.IllegalArgumentException: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have writers that are currently writing to this table, there is a risk that you may corrupt the state of your Delta table. If you are certain that there are no operations being performed on this table, such as insert/upsert/delete/optimize, then you may turn off this check by setting: spark.databricks.delta.retentionDurationCheck.enabled = false If you are not sure, please use a value not less than \"168 hours\". Attempting to vacuum the delta table (even with DRY RUN ) gives an IllegalArgumentException because of the default values of the following: spark.databricks.delta.retentionDurationCheck.enabled configuration property deletedFileRetentionDuration table property","title":"Vacuum DRY RUN (IllegalArgumentException)"},{"location":"demo/vacuum/#vacuum-dry-run","text":"","title":"Vacuum DRY RUN"},{"location":"demo/vacuum/#retentiondurationcheckenabled-configuration-property","text":"Turn the spark.databricks.delta.retentionDurationCheck.enabled configuration property off and give the VACUUM command a go again. spark . conf . set ( \"spark.databricks.delta.retentionDurationCheck.enabled\" , false ) val q = sql ( s\"VACUUM delta.` $ path ` RETAIN 0 HOURS DRY RUN\" ) You should see the following message in the console: Found 4 files and directories in a total of 3 directories that are safe to delete. The result DataFrame is the paths that are safe to delete which are all of the data files in the delta table. q . show ( truncate = false ) +------------------------------------------------------------------------------------------+ |path | +------------------------------------------------------------------------------------------+ |file:/tmp/delta/t1/p=0/part-00011-40983cc5-18bf-4d91-8b7b-eb805b8c862d.c000.snappy.parquet| |file:/tmp/delta/t1/p=1/part-00015-9576ff56-28f7-410e-8ea3-e43352a5083c.c000.snappy.parquet| |file:/tmp/delta/t1/p=0/part-00003-8e300857-ad6a-4889-b944-d31624a8024f.c000.snappy.parquet| |file:/tmp/delta/t1/p=1/part-00007-4fba265a-0884-44c3-84ed-4e9aee524e3a.c000.snappy.parquet| +------------------------------------------------------------------------------------------+","title":"retentionDurationCheck.enabled Configuration Property"},{"location":"demo/vacuum/#deletedfileretentionduration-table-property","text":"Let's DESCRIBE DETAIL to review the current table properties (incl. deletedFileRetentionDuration ). val tid = s\"delta.` $ path `\" val detail = sql ( s\"DESCRIBE DETAIL $ tid \" ). select ( 'format , 'location , 'properties ) detail . show ( truncate = false ) +------+------------------+----------+ |format|location |properties| +------+------------------+----------+ |delta |file:/tmp/delta/t1|[] | +------+------------------+----------+ Prefix the deletedFileRetentionDuration table property with delta. for ALTER TABLE to accept it as a Delta property. sql ( s\"ALTER TABLE $ tid SET TBLPROPERTIES (delta.deletedFileRetentionDuration = '0 hours')\" ) val detail = sql ( s\"DESCRIBE DETAIL $ tid \" ). select ( 'format , 'location , 'properties ) detail . show ( truncate = false ) +------+------------------+-----------------------------------------------+ |format|location |properties | +------+------------------+-----------------------------------------------+ |delta |file:/tmp/delta/t1|[delta.deletedFileRetentionDuration -> 0 hours]| +------+------------------+-----------------------------------------------+ Display the available versions of the delta table and note that ALTER TABLE gives a new version. This time you include operationParameters column (not operationMetrics as less important). val history = dt . history . select ( 'version , 'operation , 'operationParameters ) history . show ( truncate = false ) +-------+-----------------+----------------------------------------------------------------+ |version|operation |operationParameters | +-------+-----------------+----------------------------------------------------------------+ |2 |SET TBLPROPERTIES|[properties -> {\"delta.deletedFileRetentionDuration\":\"0 hours\"}]| |1 |DELETE |[predicate -> []] | |0 |WRITE |[mode -> ErrorIfExists, partitionBy -> [\"p\"]] | +-------+-----------------+----------------------------------------------------------------+ You can access the table properties ( table configuration ) using DeltaLog Scala API. import org . apache . spark . sql . delta . DeltaLog val log = DeltaLog . forTable ( spark , path ) log . snapshot . metadata . configuration Let's revert the latest change to spark.databricks.delta.retentionDurationCheck.enabled and turn it on back. spark . conf . set ( \"spark.databricks.delta.retentionDurationCheck.enabled\" , true ) val q = sql ( s\"VACUUM delta.` $ path ` RETAIN 0 HOURS DRY RUN\" ) You should see the following message in the console: Found 4 files and directories in a total of 3 directories that are safe to delete. The result DataFrame is the paths that are safe to delete which are all of the data files in the delta table. q . show ( truncate = false ) +------------------------------------------------------------------------------------------+ |path | +------------------------------------------------------------------------------------------+ |file:/tmp/delta/t1/p=0/part-00011-40983cc5-18bf-4d91-8b7b-eb805b8c862d.c000.snappy.parquet| |file:/tmp/delta/t1/p=1/part-00015-9576ff56-28f7-410e-8ea3-e43352a5083c.c000.snappy.parquet| |file:/tmp/delta/t1/p=0/part-00003-8e300857-ad6a-4889-b944-d31624a8024f.c000.snappy.parquet| |file:/tmp/delta/t1/p=1/part-00007-4fba265a-0884-44c3-84ed-4e9aee524e3a.c000.snappy.parquet| +------------------------------------------------------------------------------------------+","title":"deletedFileRetentionDuration Table Property"},{"location":"demo/vacuum/#tree-delta-table-directory","text":"In a terminal (outside spark-shell ) run tree or a similar command to review what the directory of the delta table looks like. tree /tmp/delta/t1 /tmp/delta/t1 \u251c\u2500\u2500 _delta_log \u2502 \u251c\u2500\u2500 00000000000000000000.json \u2502 \u251c\u2500\u2500 00000000000000000001.json \u2502 \u2514\u2500\u2500 00000000000000000002.json \u251c\u2500\u2500 p=0 \u2502 \u251c\u2500\u2500 part-00003-8e300857-ad6a-4889-b944-d31624a8024f.c000.snappy.parquet \u2502 \u2514\u2500\u2500 part-00011-40983cc5-18bf-4d91-8b7b-eb805b8c862d.c000.snappy.parquet \u2514\u2500\u2500 p=1 \u251c\u2500\u2500 part-00007-4fba265a-0884-44c3-84ed-4e9aee524e3a.c000.snappy.parquet \u2514\u2500\u2500 part-00015-9576ff56-28f7-410e-8ea3-e43352a5083c.c000.snappy.parquet 3 directories, 7 files","title":"Tree Delta Table Directory"},{"location":"demo/vacuum/#retain-0-hours","text":"Let's clean up ( vacuum ) the delta table entirely, effectively deleting all the data files physically from disk. Back in spark-shell , run VACUUM SQL command. Note that you're going to use it with no DRY RUN . sql ( s\"VACUUM delta.` $ path ` RETAIN 0 HOURS\" ). show ( truncate = false ) Deleted 4 files and directories in a total of 3 directories. +------------------+ |path | +------------------+ |file:/tmp/delta/t1| +------------------+ In a terminal (outside spark-shell ), run tree or a similar command to review what the directory of the delta table looks like. tree /tmp/delta/t1 /tmp/delta/t1 \u251c\u2500\u2500 _delta_log \u2502 \u251c\u2500\u2500 00000000000000000000.json \u2502 \u251c\u2500\u2500 00000000000000000001.json \u2502 \u2514\u2500\u2500 00000000000000000002.json \u251c\u2500\u2500 p=0 \u2514\u2500\u2500 p=1 3 directories, 3 files Switch to spark-shell and display the available versions of the delta table. There should really be no change compared to the last time you executed it. val history = dt . history . select ( 'version , 'operation , 'operationParameters ) history . show ( truncate = false ) +-------+-----------------+----------------------------------------------------------------+ |version|operation |operationParameters | +-------+-----------------+----------------------------------------------------------------+ |2 |SET TBLPROPERTIES|[properties -> {\"delta.deletedFileRetentionDuration\":\"0 hours\"}]| |1 |DELETE |[predicate -> []] | |0 |WRITE |[mode -> ErrorIfExists, partitionBy -> [\"p\"]] | +-------+-----------------+----------------------------------------------------------------+","title":"Retain 0 Hours"},{"location":"sql/","text":"Delta SQL \u00b6 Delta Lake registers custom SQL statements (using DeltaSparkSessionExtension to inject DeltaSqlParser with DeltaSqlAstBuilder ). The SQL statements support table of the format delta.`path` (with backticks), e.g. delta.`/tmp/delta/t1` while path is between single quotes, e.g. '/tmp/delta/t1' . The SQL statements can also refer to a table registered in a metastore. ALTER TABLE ADD CONSTRAINT \u00b6 ALTER TABLE table ADD CONSTRAINT name constraint ALTER TABLE DROP CONSTRAINT \u00b6 ALTER TABLE table DROP CONSTRAINT (IF EXISTS)? name CONVERT TO DELTA \u00b6 CONVERT TO DELTA table (PARTITIONED BY '(' colTypeList ')')? Executes ConvertToDeltaCommand DESCRIBE DETAIL \u00b6 (DESC | DESCRIBE) DETAIL (path | table) Executes DescribeDeltaDetailCommand DESCRIBE HISTORY \u00b6 (DESC | DESCRIBE) HISTORY (path | table) (LIMIT limit)? Executes DescribeDeltaHistoryCommand GENERATE \u00b6 GENERATE modeName FOR TABLE table Executes DeltaGenerateCommand VACUUM \u00b6 VACUUM (path | table) (RETAIN number HOURS)? (DRY RUN)? Executes VacuumTableCommand","title":"Delta SQL"},{"location":"sql/#delta-sql","text":"Delta Lake registers custom SQL statements (using DeltaSparkSessionExtension to inject DeltaSqlParser with DeltaSqlAstBuilder ). The SQL statements support table of the format delta.`path` (with backticks), e.g. delta.`/tmp/delta/t1` while path is between single quotes, e.g. '/tmp/delta/t1' . The SQL statements can also refer to a table registered in a metastore.","title":"Delta SQL"},{"location":"sql/#alter-table-add-constraint","text":"ALTER TABLE table ADD CONSTRAINT name constraint","title":" ALTER TABLE ADD CONSTRAINT"},{"location":"sql/#alter-table-drop-constraint","text":"ALTER TABLE table DROP CONSTRAINT (IF EXISTS)? name","title":" ALTER TABLE DROP CONSTRAINT"},{"location":"sql/#convert-to-delta","text":"CONVERT TO DELTA table (PARTITIONED BY '(' colTypeList ')')? Executes ConvertToDeltaCommand","title":" CONVERT TO DELTA"},{"location":"sql/#describe-detail","text":"(DESC | DESCRIBE) DETAIL (path | table) Executes DescribeDeltaDetailCommand","title":" DESCRIBE DETAIL"},{"location":"sql/#describe-history","text":"(DESC | DESCRIBE) HISTORY (path | table) (LIMIT limit)? Executes DescribeDeltaHistoryCommand","title":" DESCRIBE HISTORY"},{"location":"sql/#generate","text":"GENERATE modeName FOR TABLE table Executes DeltaGenerateCommand","title":" GENERATE"},{"location":"sql/#vacuum","text":"VACUUM (path | table) (RETAIN number HOURS)? (DRY RUN)? Executes VacuumTableCommand","title":" VACUUM"},{"location":"sql/DeltaSqlAstBuilder/","text":"DeltaSqlAstBuilder \u00b6 DeltaSqlAstBuilder is a command builder for the Delta SQL statements (described in DeltaSqlBase.g4 ANTLR grammar). DeltaSqlParser is used by DeltaSqlParser . SQL Statement Logical Command ALTER TABLE ADD CONSTRAINT AlterTableAddConstraintStatement ALTER TABLE DROP CONSTRAINT AlterTableDropConstraintStatement CONVERT TO DELTA ConvertToDeltaCommand DESCRIBE DETAIL DescribeDeltaDetailCommand DESCRIBE HISTORY DescribeDeltaHistoryCommand GENERATE DeltaGenerateCommand VACUUM VacuumTableCommand","title":"DeltaSqlAstBuilder"},{"location":"sql/DeltaSqlAstBuilder/#deltasqlastbuilder","text":"DeltaSqlAstBuilder is a command builder for the Delta SQL statements (described in DeltaSqlBase.g4 ANTLR grammar). DeltaSqlParser is used by DeltaSqlParser . SQL Statement Logical Command ALTER TABLE ADD CONSTRAINT AlterTableAddConstraintStatement ALTER TABLE DROP CONSTRAINT AlterTableDropConstraintStatement CONVERT TO DELTA ConvertToDeltaCommand DESCRIBE DETAIL DescribeDeltaDetailCommand DESCRIBE HISTORY DescribeDeltaHistoryCommand GENERATE DeltaGenerateCommand VACUUM VacuumTableCommand","title":"DeltaSqlAstBuilder"},{"location":"sql/DeltaSqlParser/","text":"DeltaSqlParser \u00b6 DeltaSqlParser is a SQL parser (Spark SQL's ParserInterface ) for Delta SQL . DeltaSqlParser is registered in a Spark SQL application using DeltaSparkSessionExtension . Creating Instance \u00b6 DeltaSqlParser takes the following to be created: ParserInterface (to fall back to for unsupported SQL) DeltaSqlParser is created when DeltaSparkSessionExtension is requested to register Delta SQL support . DeltaSqlAstBuilder \u00b6 DeltaSqlParser uses DeltaSqlAstBuilder to convert SQL statements to their runtime representation (as a LogicalPlan ). In case an AST could not be converted to a LogicalPlan , DeltaSqlAstBuilder requests the delegate ParserInterface to parse it.","title":"DeltaSqlParser"},{"location":"sql/DeltaSqlParser/#deltasqlparser","text":"DeltaSqlParser is a SQL parser (Spark SQL's ParserInterface ) for Delta SQL . DeltaSqlParser is registered in a Spark SQL application using DeltaSparkSessionExtension .","title":"DeltaSqlParser"},{"location":"sql/DeltaSqlParser/#creating-instance","text":"DeltaSqlParser takes the following to be created: ParserInterface (to fall back to for unsupported SQL) DeltaSqlParser is created when DeltaSparkSessionExtension is requested to register Delta SQL support .","title":"Creating Instance"},{"location":"sql/DeltaSqlParser/#deltasqlastbuilder","text":"DeltaSqlParser uses DeltaSqlAstBuilder to convert SQL statements to their runtime representation (as a LogicalPlan ). In case an AST could not be converted to a LogicalPlan , DeltaSqlAstBuilder requests the delegate ParserInterface to parse it.","title":" DeltaSqlAstBuilder"}]}