{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Delta Lake 0.8.0 \u00b6 Welcome to The Internals of Delta Lake online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Delta Lake as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's take a deep dive into Delta Lake \ud83d\udd25","title":"Home"},{"location":"#the-internals-of-delta-lake-080","text":"Welcome to The Internals of Delta Lake online book! \ud83e\udd19 I'm Jacek Laskowski , an IT freelancer specializing in Apache Spark , Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB , mostly during Warsaw Data Engineering meetups). I'm very excited to have you here and hope you will enjoy exploring the internals of Delta Lake as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let's take a deep dive into Delta Lake \ud83d\udd25","title":"The Internals of Delta Lake 0.8.0"},{"location":"Action/","text":"Action \u00b6 Action is an < > of < > of a change to (the state of) a Delta table. Action can be converted ( serialized ) to < > format for...FIXME [[logSchema]] Action object defines logSchema that is a schema ( StructType ) based on the < > case class. [source, scala] \u00b6 import org.apache.spark.sql.delta.actions.Action.logSchema scala> logSchema.printTreeString root |-- txn: struct (nullable = true) | |-- appId: string (nullable = true) | |-- version: long (nullable = false) | |-- lastUpdated: long (nullable = true) |-- add: struct (nullable = true) | |-- path: string (nullable = true) | |-- partitionValues: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- size: long (nullable = false) | |-- modificationTime: long (nullable = false) | |-- dataChange: boolean (nullable = false) | |-- stats: string (nullable = true) | |-- tags: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) |-- remove: struct (nullable = true) | |-- path: string (nullable = true) | |-- deletionTimestamp: long (nullable = true) | |-- dataChange: boolean (nullable = false) |-- metaData: struct (nullable = true) | |-- id: string (nullable = true) | |-- name: string (nullable = true) | |-- description: string (nullable = true) | |-- format: struct (nullable = true) | | |-- provider: string (nullable = true) | | |-- options: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) | |-- schemaString: string (nullable = true) | |-- partitionColumns: array (nullable = true) | | |-- element: string (containsNull = true) | |-- configuration: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- createdTime: long (nullable = true) |-- protocol: struct (nullable = true) | |-- minReaderVersion: integer (nullable = false) | |-- minWriterVersion: integer (nullable = false) |-- commitInfo: struct (nullable = true) | |-- version: long (nullable = true) | |-- timestamp: timestamp (nullable = true) | |-- userId: string (nullable = true) | |-- userName: string (nullable = true) | |-- operation: string (nullable = true) | |-- operationParameters: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- job: struct (nullable = true) | | |-- jobId: string (nullable = true) | | |-- jobName: string (nullable = true) | | |-- runId: string (nullable = true) | | |-- jobOwnerId: string (nullable = true) | | |-- triggerType: string (nullable = true) | |-- notebook: struct (nullable = true) | | |-- notebookId: string (nullable = true) | |-- clusterId: string (nullable = true) | |-- readVersion: long (nullable = true) | |-- isolationLevel: string (nullable = true) | |-- isBlindAppend: boolean (nullable = true) [[contract]] .Action Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | wrap a| [[wrap]] [source, scala] \u00b6 wrap: SingleAction \u00b6 Wraps the action into a < > for serialization Used when: Snapshot is created (and initializes the < > for the < > of a delta table) Action is requested to < > |=== [[implementations]] [[extensions]] .Actions (Direct Implementations and Extensions Only) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Action | Description | < > | [[CommitInfo]] | < > | [[FileAction]] | < > | [[Metadata]] | < > | [[Protocol]] | < > | [[SetTransaction]] |=== NOTE: Action is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file). == [[json]] Serializing to JSON Format -- json Method [source, scala] \u00b6 json: String \u00b6 json simply serializes ( converts ) the < > to JSON format. Note json uses Jackson library (with jackson-module-scala ) as the JSON processor. [NOTE] \u00b6 json is used when: OptimisticTransactionImpl is requested to < > * ConvertToDeltaCommand is requested to < > \u00b6 == [[fromJson]] Deserializing Action (From JSON Format) -- fromJson Utility [source, scala] \u00b6 fromJson( json: String): Action fromJson ...FIXME [NOTE] \u00b6 fromJson is used when: DeltaHistoryManager utility is requested for the < > DeltaLog is requested for the < > * OptimisticTransactionImpl is requested to < > \u00b6","title":"Action"},{"location":"Action/#action","text":"Action is an < > of < > of a change to (the state of) a Delta table. Action can be converted ( serialized ) to < > format for...FIXME [[logSchema]] Action object defines logSchema that is a schema ( StructType ) based on the < > case class.","title":"Action"},{"location":"Action/#source-scala","text":"import org.apache.spark.sql.delta.actions.Action.logSchema scala> logSchema.printTreeString root |-- txn: struct (nullable = true) | |-- appId: string (nullable = true) | |-- version: long (nullable = false) | |-- lastUpdated: long (nullable = true) |-- add: struct (nullable = true) | |-- path: string (nullable = true) | |-- partitionValues: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- size: long (nullable = false) | |-- modificationTime: long (nullable = false) | |-- dataChange: boolean (nullable = false) | |-- stats: string (nullable = true) | |-- tags: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) |-- remove: struct (nullable = true) | |-- path: string (nullable = true) | |-- deletionTimestamp: long (nullable = true) | |-- dataChange: boolean (nullable = false) |-- metaData: struct (nullable = true) | |-- id: string (nullable = true) | |-- name: string (nullable = true) | |-- description: string (nullable = true) | |-- format: struct (nullable = true) | | |-- provider: string (nullable = true) | | |-- options: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) | |-- schemaString: string (nullable = true) | |-- partitionColumns: array (nullable = true) | | |-- element: string (containsNull = true) | |-- configuration: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- createdTime: long (nullable = true) |-- protocol: struct (nullable = true) | |-- minReaderVersion: integer (nullable = false) | |-- minWriterVersion: integer (nullable = false) |-- commitInfo: struct (nullable = true) | |-- version: long (nullable = true) | |-- timestamp: timestamp (nullable = true) | |-- userId: string (nullable = true) | |-- userName: string (nullable = true) | |-- operation: string (nullable = true) | |-- operationParameters: map (nullable = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) | |-- job: struct (nullable = true) | | |-- jobId: string (nullable = true) | | |-- jobName: string (nullable = true) | | |-- runId: string (nullable = true) | | |-- jobOwnerId: string (nullable = true) | | |-- triggerType: string (nullable = true) | |-- notebook: struct (nullable = true) | | |-- notebookId: string (nullable = true) | |-- clusterId: string (nullable = true) | |-- readVersion: long (nullable = true) | |-- isolationLevel: string (nullable = true) | |-- isBlindAppend: boolean (nullable = true) [[contract]] .Action Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | wrap a| [[wrap]]","title":"[source, scala]"},{"location":"Action/#source-scala_1","text":"","title":"[source, scala]"},{"location":"Action/#wrap-singleaction","text":"Wraps the action into a < > for serialization Used when: Snapshot is created (and initializes the < > for the < > of a delta table) Action is requested to < > |=== [[implementations]] [[extensions]] .Actions (Direct Implementations and Extensions Only) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Action | Description | < > | [[CommitInfo]] | < > | [[FileAction]] | < > | [[Metadata]] | < > | [[Protocol]] | < > | [[SetTransaction]] |=== NOTE: Action is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file). == [[json]] Serializing to JSON Format -- json Method","title":"wrap: SingleAction"},{"location":"Action/#source-scala_2","text":"","title":"[source, scala]"},{"location":"Action/#json-string","text":"json simply serializes ( converts ) the < > to JSON format. Note json uses Jackson library (with jackson-module-scala ) as the JSON processor.","title":"json: String"},{"location":"Action/#note","text":"json is used when: OptimisticTransactionImpl is requested to < >","title":"[NOTE]"},{"location":"Action/#converttodeltacommand-is-requested-to","text":"== [[fromJson]] Deserializing Action (From JSON Format) -- fromJson Utility","title":"* ConvertToDeltaCommand is requested to &lt;&gt;"},{"location":"Action/#source-scala_3","text":"fromJson( json: String): Action fromJson ...FIXME","title":"[source, scala]"},{"location":"Action/#note_1","text":"fromJson is used when: DeltaHistoryManager utility is requested for the < > DeltaLog is requested for the < >","title":"[NOTE]"},{"location":"Action/#optimistictransactionimpl-is-requested-to","text":"","title":"* OptimisticTransactionImpl is requested to &lt;&gt;"},{"location":"ActiveOptimisticTransactionRule/","text":"ActiveOptimisticTransactionRule Logical Optimization Rule \u00b6 ActiveOptimisticTransactionRule is a logical optimization rule ( Spark SQL ) to transform logical query plans. Creating Instance \u00b6 ActiveOptimisticTransactionRule takes the following to be created: SparkSession ( Spark SQL ) ActiveOptimisticTransactionRule is created when: DeltaSparkSessionExtension is requested to inject extensions Executing Rule \u00b6 apply ( plan : LogicalPlan ) : LogicalPlan apply is part of the Rule ( Spark SQL ) abstraction. apply ...FIXME","title":"ActiveOptimisticTransactionRule"},{"location":"ActiveOptimisticTransactionRule/#activeoptimistictransactionrule-logical-optimization-rule","text":"ActiveOptimisticTransactionRule is a logical optimization rule ( Spark SQL ) to transform logical query plans.","title":"ActiveOptimisticTransactionRule Logical Optimization Rule"},{"location":"ActiveOptimisticTransactionRule/#creating-instance","text":"ActiveOptimisticTransactionRule takes the following to be created: SparkSession ( Spark SQL ) ActiveOptimisticTransactionRule is created when: DeltaSparkSessionExtension is requested to inject extensions","title":"Creating Instance"},{"location":"ActiveOptimisticTransactionRule/#executing-rule","text":"apply ( plan : LogicalPlan ) : LogicalPlan apply is part of the Rule ( Spark SQL ) abstraction. apply ...FIXME","title":" Executing Rule"},{"location":"AddCDCFile/","text":"AddCDCFile \u00b6 AddCDCFile is a FileAction . Creating Instance \u00b6 AddCDCFile takes the following to be created: Path Partition values ( Map[String, String] ) Size (in bytes) Tags (default: null ) AddCDCFile does not seem to be created ever. dataChange \u00b6 dataChange : Boolean dataChange is part of the FileAction abstraction. dataChange is always turned off ( false ). Converting to SingleAction \u00b6 wrap : SingleAction wrap is part of the Action abstraction. wrap creates a new SingleAction with the cdc field set to this AddCDCFile .","title":"AddCDCFile"},{"location":"AddCDCFile/#addcdcfile","text":"AddCDCFile is a FileAction .","title":"AddCDCFile"},{"location":"AddCDCFile/#creating-instance","text":"AddCDCFile takes the following to be created: Path Partition values ( Map[String, String] ) Size (in bytes) Tags (default: null ) AddCDCFile does not seem to be created ever.","title":"Creating Instance"},{"location":"AddCDCFile/#datachange","text":"dataChange : Boolean dataChange is part of the FileAction abstraction. dataChange is always turned off ( false ).","title":" dataChange"},{"location":"AddCDCFile/#converting-to-singleaction","text":"wrap : SingleAction wrap is part of the Action abstraction. wrap creates a new SingleAction with the cdc field set to this AddCDCFile .","title":" Converting to SingleAction"},{"location":"AddFile/","text":"AddFile \u00b6 AddFile is a FileAction that represents an action of adding a file to a delta table . Creating Instance \u00b6 AddFile takes the following to be created: Path Partition values ( Map[String, String] ) Size (in bytes) Modification time dataChange flag Stats (default: null ) Tags ( Map[String, String] ) (default: null ) AddFile is created when: ConvertToDeltaCommand is executed (for every data file to import ) DelayedCommitProtocol is requested to commit a task (after successful write) (for optimistic transactional writers ) Converting to SingleAction \u00b6 wrap : SingleAction wrap is part of the Action abstraction. wrap creates a new SingleAction with the add field set to this AddFile . Converting to RemoveFile with Defaults \u00b6 remove : RemoveFile remove simply creates a RemoveFile for the path (with the current time and dataChange flag enabled). remove is used when: MergeIntoCommand is executed WriteIntoDelta is requested to write (with Overwrite mode) DeltaSink is requested to add a streaming micro-batch (for Complete output mode) Converting to RemoveFile \u00b6 removeWithTimestamp ( timestamp : Long = System . currentTimeMillis (), dataChange : Boolean = true ) : RemoveFile remove creates a new RemoveFile for the path with the given timestamp and dataChange flag. removeWithTimestamp is used when: AddFile is requested to create a RemoveFile action with the defaults CreateDeltaTableCommand , DeleteCommand and UpdateCommand commands are executed DeltaCommand is requested to removeFilesFromPaths","title":"AddFile"},{"location":"AddFile/#addfile","text":"AddFile is a FileAction that represents an action of adding a file to a delta table .","title":"AddFile"},{"location":"AddFile/#creating-instance","text":"AddFile takes the following to be created: Path Partition values ( Map[String, String] ) Size (in bytes) Modification time dataChange flag Stats (default: null ) Tags ( Map[String, String] ) (default: null ) AddFile is created when: ConvertToDeltaCommand is executed (for every data file to import ) DelayedCommitProtocol is requested to commit a task (after successful write) (for optimistic transactional writers )","title":"Creating Instance"},{"location":"AddFile/#converting-to-singleaction","text":"wrap : SingleAction wrap is part of the Action abstraction. wrap creates a new SingleAction with the add field set to this AddFile .","title":" Converting to SingleAction"},{"location":"AddFile/#converting-to-removefile-with-defaults","text":"remove : RemoveFile remove simply creates a RemoveFile for the path (with the current time and dataChange flag enabled). remove is used when: MergeIntoCommand is executed WriteIntoDelta is requested to write (with Overwrite mode) DeltaSink is requested to add a streaming micro-batch (for Complete output mode)","title":" Converting to RemoveFile with Defaults"},{"location":"AddFile/#converting-to-removefile","text":"removeWithTimestamp ( timestamp : Long = System . currentTimeMillis (), dataChange : Boolean = true ) : RemoveFile remove creates a new RemoveFile for the path with the given timestamp and dataChange flag. removeWithTimestamp is used when: AddFile is requested to create a RemoveFile action with the defaults CreateDeltaTableCommand , DeleteCommand and UpdateCommand commands are executed DeltaCommand is requested to removeFilesFromPaths","title":" Converting to RemoveFile"},{"location":"AlterTableAddConstraintStatement/","text":"AlterTableAddConstraintStatement \u00b6 AlterTableAddConstraintStatement is a ParsedStatement ( Spark SQL ) for ALTER TABLE ADD CONSTRAINT SQL statement. Creating Instance \u00b6 AlterTableAddConstraintStatement takes the following to be created: Table Name Constraint Name Expression Analysis Phase \u00b6 AlterTableAddConstraintStatement is resolved by DeltaAnalysis logical resolution rule.","title":"AlterTableAddConstraintStatement"},{"location":"AlterTableAddConstraintStatement/#altertableaddconstraintstatement","text":"AlterTableAddConstraintStatement is a ParsedStatement ( Spark SQL ) for ALTER TABLE ADD CONSTRAINT SQL statement.","title":"AlterTableAddConstraintStatement"},{"location":"AlterTableAddConstraintStatement/#creating-instance","text":"AlterTableAddConstraintStatement takes the following to be created: Table Name Constraint Name Expression","title":"Creating Instance"},{"location":"AlterTableAddConstraintStatement/#analysis-phase","text":"AlterTableAddConstraintStatement is resolved by DeltaAnalysis logical resolution rule.","title":"Analysis Phase"},{"location":"AlterTableDropConstraintStatement/","text":"AlterTableDropConstraintStatement \u00b6 AlterTableDropConstraintStatement is...FIXME","title":"AlterTableDropConstraintStatement"},{"location":"AlterTableDropConstraintStatement/#altertabledropconstraintstatement","text":"AlterTableDropConstraintStatement is...FIXME","title":"AlterTableDropConstraintStatement"},{"location":"CachedDS/","text":"CachedDS \u2014 Cached Delta State \u00b6 CachedDS is used when StateCache is requested to cacheDS . When created, CachedDS immediately initializes the cachedDs internal registry that requests the Dataset to generate a RDD[InternalRow] and associates the RDD with the given name : Delta Table State for Snapshot Delta Source Snapshot for DeltaSourceSnapshot The RDD is marked to be persisted using StorageLevel.MEMORY_AND_DISK_SER storage level. Note CachedDS is an internal class of StateCache and has access to its internals. Creating Instance \u00b6 CachedDS takes the following to be created: Dataset[A] Name CachedDS is created when StateCache is requested to cacheDS . getDS Method \u00b6 getDS : Dataset [ A ] getDS ...FIXME getDS is used when: Snapshot is requested to state DeltaSourceSnapshot is requested to initialFiles","title":"CachedDS"},{"location":"CachedDS/#cachedds-cached-delta-state","text":"CachedDS is used when StateCache is requested to cacheDS . When created, CachedDS immediately initializes the cachedDs internal registry that requests the Dataset to generate a RDD[InternalRow] and associates the RDD with the given name : Delta Table State for Snapshot Delta Source Snapshot for DeltaSourceSnapshot The RDD is marked to be persisted using StorageLevel.MEMORY_AND_DISK_SER storage level. Note CachedDS is an internal class of StateCache and has access to its internals.","title":"CachedDS &mdash; Cached Delta State"},{"location":"CachedDS/#creating-instance","text":"CachedDS takes the following to be created: Dataset[A] Name CachedDS is created when StateCache is requested to cacheDS .","title":"Creating Instance"},{"location":"CachedDS/#getds-method","text":"getDS : Dataset [ A ] getDS ...FIXME getDS is used when: Snapshot is requested to state DeltaSourceSnapshot is requested to initialFiles","title":" getDS Method"},{"location":"Checkpoints/","text":"Checkpoints \u00b6 Checkpoints is an < > of < > that can < > the current state of a delta table (represented by the < >). [[contract]] .Checkpoints Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | logPath a| [[logPath]] [source, scala] \u00b6 logPath: Path \u00b6 Used when...FIXME | dataPath a| [[dataPath]] [source, scala] \u00b6 dataPath: Path \u00b6 Used when...FIXME | snapshot a| [[snapshot]] [source, scala] \u00b6 snapshot: Snapshot \u00b6 Used when...FIXME | store a| [[store]] [source, scala] \u00b6 store: LogStore \u00b6 Used when...FIXME | metadata a| [[metadata]] [source, scala] \u00b6 metadata: Metadata \u00b6 < > of (the current state of) the < > Used when...FIXME | doLogCleanup a| [[doLogCleanup]] [source, scala] \u00b6 doLogCleanup(): Unit \u00b6 Used when...FIXME |=== [[implementations]][[self]] NOTE: < > is the default and only known Checkpoints in Delta Lake. == [[LAST_CHECKPOINT]][[_last_checkpoint]] _last_checkpoint Metadata File Checkpoints uses _last_checkpoint metadata file (under the < >) for the following: < > < > == [[checkpoint]] Checkpointing -- checkpoint Method [source, scala] \u00b6 checkpoint(): Unit checkpoint( snapshotToCheckpoint: Snapshot): CheckpointMetaData checkpoint ...FIXME NOTE: checkpoint is used when...FIXME Loading Latest Checkpoint Metadata \u00b6 lastCheckpoint : Option [ CheckpointMetaData ] lastCheckpoint simply loadMetadataFromFile (allowing for 3 retries). lastCheckpoint is used when: SnapshotManagement is requested to load the latest snapshot MetadataCleanup is requested to listExpiredDeltaLogs loadMetadataFromFile Helper Method \u00b6 loadMetadataFromFile ( tries : Int ) : Option [ CheckpointMetaData ] loadMetadataFromFile loads the _last_checkpoint file (in JSON format) and converts it to CheckpointMetaData (with a version, size and parts). loadMetadataFromFile uses the LogStore to read the _last_checkpoint file. In case the _last_checkpoint file is corrupted, loadMetadataFromFile ...FIXME == [[manuallyLoadCheckpoint]] manuallyLoadCheckpoint Method [source, scala] \u00b6 manuallyLoadCheckpoint(cv: CheckpointInstance): CheckpointMetaData \u00b6 manuallyLoadCheckpoint ...FIXME NOTE: manuallyLoadCheckpoint is used when...FIXME == [[findLastCompleteCheckpoint]] findLastCompleteCheckpoint Method [source, scala] \u00b6 findLastCompleteCheckpoint(cv: CheckpointInstance): Option[CheckpointInstance] \u00b6 findLastCompleteCheckpoint ...FIXME NOTE: findLastCompleteCheckpoint is used when...FIXME == [[getLatestCompleteCheckpointFromList]] getLatestCompleteCheckpointFromList Method [source, scala] \u00b6 getLatestCompleteCheckpointFromList( instances: Array[CheckpointInstance], notLaterThan: CheckpointInstance): Option[CheckpointInstance] getLatestCompleteCheckpointFromList ...FIXME NOTE: getLatestCompleteCheckpointFromList is used when...FIXME == [[writeCheckpoint]] writeCheckpoint Utility [source, scala] \u00b6 writeCheckpoint( spark: SparkSession, deltaLog: DeltaLog, snapshot: Snapshot): CheckpointMetaData writeCheckpoint ...FIXME NOTE: writeCheckpoint is used when Checkpoints is requested to < >.","title":"Checkpoints"},{"location":"Checkpoints/#checkpoints","text":"Checkpoints is an < > of < > that can < > the current state of a delta table (represented by the < >). [[contract]] .Checkpoints Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | logPath a| [[logPath]]","title":"Checkpoints"},{"location":"Checkpoints/#source-scala","text":"","title":"[source, scala]"},{"location":"Checkpoints/#logpath-path","text":"Used when...FIXME | dataPath a| [[dataPath]]","title":"logPath: Path"},{"location":"Checkpoints/#source-scala_1","text":"","title":"[source, scala]"},{"location":"Checkpoints/#datapath-path","text":"Used when...FIXME | snapshot a| [[snapshot]]","title":"dataPath: Path"},{"location":"Checkpoints/#source-scala_2","text":"","title":"[source, scala]"},{"location":"Checkpoints/#snapshot-snapshot","text":"Used when...FIXME | store a| [[store]]","title":"snapshot: Snapshot"},{"location":"Checkpoints/#source-scala_3","text":"","title":"[source, scala]"},{"location":"Checkpoints/#store-logstore","text":"Used when...FIXME | metadata a| [[metadata]]","title":"store: LogStore"},{"location":"Checkpoints/#source-scala_4","text":"","title":"[source, scala]"},{"location":"Checkpoints/#metadata-metadata","text":"< > of (the current state of) the < > Used when...FIXME | doLogCleanup a| [[doLogCleanup]]","title":"metadata: Metadata"},{"location":"Checkpoints/#source-scala_5","text":"","title":"[source, scala]"},{"location":"Checkpoints/#dologcleanup-unit","text":"Used when...FIXME |=== [[implementations]][[self]] NOTE: < > is the default and only known Checkpoints in Delta Lake. == [[LAST_CHECKPOINT]][[_last_checkpoint]] _last_checkpoint Metadata File Checkpoints uses _last_checkpoint metadata file (under the < >) for the following: < > < > == [[checkpoint]] Checkpointing -- checkpoint Method","title":"doLogCleanup(): Unit"},{"location":"Checkpoints/#source-scala_6","text":"checkpoint(): Unit checkpoint( snapshotToCheckpoint: Snapshot): CheckpointMetaData checkpoint ...FIXME NOTE: checkpoint is used when...FIXME","title":"[source, scala]"},{"location":"Checkpoints/#loading-latest-checkpoint-metadata","text":"lastCheckpoint : Option [ CheckpointMetaData ] lastCheckpoint simply loadMetadataFromFile (allowing for 3 retries). lastCheckpoint is used when: SnapshotManagement is requested to load the latest snapshot MetadataCleanup is requested to listExpiredDeltaLogs","title":" Loading Latest Checkpoint Metadata"},{"location":"Checkpoints/#loadmetadatafromfile-helper-method","text":"loadMetadataFromFile ( tries : Int ) : Option [ CheckpointMetaData ] loadMetadataFromFile loads the _last_checkpoint file (in JSON format) and converts it to CheckpointMetaData (with a version, size and parts). loadMetadataFromFile uses the LogStore to read the _last_checkpoint file. In case the _last_checkpoint file is corrupted, loadMetadataFromFile ...FIXME == [[manuallyLoadCheckpoint]] manuallyLoadCheckpoint Method","title":" loadMetadataFromFile Helper Method"},{"location":"Checkpoints/#source-scala_7","text":"","title":"[source, scala]"},{"location":"Checkpoints/#manuallyloadcheckpointcv-checkpointinstance-checkpointmetadata","text":"manuallyLoadCheckpoint ...FIXME NOTE: manuallyLoadCheckpoint is used when...FIXME == [[findLastCompleteCheckpoint]] findLastCompleteCheckpoint Method","title":"manuallyLoadCheckpoint(cv: CheckpointInstance): CheckpointMetaData"},{"location":"Checkpoints/#source-scala_8","text":"","title":"[source, scala]"},{"location":"Checkpoints/#findlastcompletecheckpointcv-checkpointinstance-optioncheckpointinstance","text":"findLastCompleteCheckpoint ...FIXME NOTE: findLastCompleteCheckpoint is used when...FIXME == [[getLatestCompleteCheckpointFromList]] getLatestCompleteCheckpointFromList Method","title":"findLastCompleteCheckpoint(cv: CheckpointInstance): Option[CheckpointInstance]"},{"location":"Checkpoints/#source-scala_9","text":"getLatestCompleteCheckpointFromList( instances: Array[CheckpointInstance], notLaterThan: CheckpointInstance): Option[CheckpointInstance] getLatestCompleteCheckpointFromList ...FIXME NOTE: getLatestCompleteCheckpointFromList is used when...FIXME == [[writeCheckpoint]] writeCheckpoint Utility","title":"[source, scala]"},{"location":"Checkpoints/#source-scala_10","text":"writeCheckpoint( spark: SparkSession, deltaLog: DeltaLog, snapshot: Snapshot): CheckpointMetaData writeCheckpoint ...FIXME NOTE: writeCheckpoint is used when Checkpoints is requested to < >.","title":"[source, scala]"},{"location":"CommitInfo/","text":"= CommitInfo CommitInfo is an Action.md[] with the following: [[version]] Version [[timestamp]] Timestamp [[userId]] User ID [[userName]] User Name [[operation]] Operation.md#name[Name of the operation] [[operationParameters]] Operation.md#parameters[Parameters of the operation] [[job]] JobInfo [[notebook]] NotebookInfo [[clusterId]] Cluster ID [[readVersion]] Read Version [[isolationLevel]] Isolation Level [[isBlindAppend]] isBlindAppend flag (to indicate whether a commit has blindly appended without caring about existing files) [[operationMetrics]] Metrics of the operation [[userMetadata]] User metadata CommitInfo is created (using < > utility) when: OptimisticTransactionImpl is requested to OptimisticTransactionImpl.md#commit[commit] ConvertToDeltaCommand command is requested to ConvertToDeltaCommand.md#streamWrite[streamWrite] (when executed) CommitInfo is used in OptimisticTransactionImpl.md#commitInfo[OptimisticTransactionImpl] and CommitStats. CommitInfo is added ( logged ) to a Delta log only for DeltaSQLConf.md#commitInfo.enabled[spark.databricks.delta.commitInfo.enabled] configuration enabled. == [[apply]] apply Utility [source,scala] \u00b6 apply( time: Long, operation: String, operationParameters: Map[String, String], commandContext: Map[String, String], readVersion: Option[Long], isolationLevel: Option[String], isBlindAppend: Option[Boolean], operationMetrics: Option[Map[String, String]], userMetadata: Option[String]): CommitInfo apply creates a CommitInfo (for the given arguments and based on the given commandContext for the user ID, user name, job, notebook, cluster). NOTE: commandContext is always empty, but could be customized using ConvertToDeltaCommand.md#ConvertToDeltaCommandBase[ConvertToDeltaCommandBase]. apply is used when: OptimisticTransactionImpl is requested to OptimisticTransactionImpl.md#commit[commit] ConvertToDeltaCommand command is requested to ConvertToDeltaCommand.md#streamWrite[streamWrite] (when executed)","title":"CommitInfo"},{"location":"CommitInfo/#sourcescala","text":"apply( time: Long, operation: String, operationParameters: Map[String, String], commandContext: Map[String, String], readVersion: Option[Long], isolationLevel: Option[String], isBlindAppend: Option[Boolean], operationMetrics: Option[Map[String, String]], userMetadata: Option[String]): CommitInfo apply creates a CommitInfo (for the given arguments and based on the given commandContext for the user ID, user name, job, notebook, cluster). NOTE: commandContext is always empty, but could be customized using ConvertToDeltaCommand.md#ConvertToDeltaCommandBase[ConvertToDeltaCommandBase]. apply is used when: OptimisticTransactionImpl is requested to OptimisticTransactionImpl.md#commit[commit] ConvertToDeltaCommand command is requested to ConvertToDeltaCommand.md#streamWrite[streamWrite] (when executed)","title":"[source,scala]"},{"location":"DelayedCommitProtocol/","text":"DelayedCommitProtocol \u00b6 DelayedCommitProtocol is used to model a distributed write that is orchestrated by the Spark driver with the write itself happening on executors. DelayedCommitProtocol is a concrete FileCommitProtocol (Spark Core) to write out a result of a structured query to a < > and return a < >. FileCommitProtocol ( Apache Spark ) allows to track a write job (with a write task per partition) and inform the driver when all the write tasks finished successfully (and were < >) to consider the write job < >. TaskCommitMessage (Spark Core) allows to \"transfer\" the files added (written out) on the executors to the driver for the < >. DelayedCommitProtocol is < > exclusively when TransactionalWrite is requested for a < > to < > to the < >. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.delta.files.DelayedCommitProtocol logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.files.DelayedCommitProtocol=ALL Refer to Logging . \u00b6 == [[creating-instance]] Creating DelayedCommitProtocol Instance DelayedCommitProtocol takes the following to be created: [[jobId]] Job ID (seems always < >) [[path]] Directory (to write files to) [[randomPrefixLength]] Optional length of a random prefix (seems always < >) DelayedCommitProtocol initializes the < >. == [[setupTask]] setupTask Method [source, scala] \u00b6 setupTask( taskContext: TaskAttemptContext): Unit NOTE: setupTask is part of the FileCommitProtocol contract to set up a task for a writing job. setupTask simply initializes the < > internal registry to be empty. == [[newTaskTempFile]] newTaskTempFile Method [source, scala] \u00b6 newTaskTempFile( taskContext: TaskAttemptContext, dir: Option[String], ext: String): String NOTE: newTaskTempFile is part of the FileCommitProtocol contract to inform the committer to add a new file. newTaskTempFile < > for the given TaskAttemptContext and ext . newTaskTempFile tries to < > with the given dir or falls back to an empty partitionValues . NOTE: The given dir defines a partition directory if the streaming query (and hence the write) is partitioned. newTaskTempFile builds a path (based on the given randomPrefixLength and the dir , or uses the file name directly). NOTE: FIXME When would the optional dir and the < > be defined? newTaskTempFile adds the partition values and the relative path to the < > internal registry. In the end, newTaskTempFile returns the absolute path of the (relative) path in the < >. == [[commitTask]] Committing Task (After Successful Write) -- commitTask Method [source, scala] \u00b6 commitTask( taskContext: TaskAttemptContext): TaskCommitMessage NOTE: commitTask is part of the FileCommitProtocol contract to commit a task after the writes succeed. commitTask simply creates a TaskCommitMessage with an < > for every < > if there were any. Otherwise, the TaskCommitMessage is empty. NOTE: A file is added (to < > internal registry) when DelayedCommitProtocol is requested for a < >. == [[commitJob]] Committing Spark Job (After Successful Write) -- commitJob Method [source, scala] \u00b6 commitJob( jobContext: JobContext, taskCommits: Seq[TaskCommitMessage]): Unit NOTE: commitJob is part of the FileCommitProtocol contract to commit a job after the writes succeed. commitJob simply adds the < > (from the given taskCommits from every < >) to the < > internal registry. == [[parsePartitions]] parsePartitions Method [source, scala] \u00b6 parsePartitions( dir: String): Map[String, String] parsePartitions ...FIXME NOTE: parsePartitions is used exclusively when DelayedCommitProtocol is requested to < >. == [[setupJob]] setupJob Method [source, scala] \u00b6 setupJob( jobContext: JobContext): Unit NOTE: setupJob is part of the FileCommitProtocol contract to set up a Spark job. setupJob does nothing. == [[abortJob]] abortJob Method [source, scala] \u00b6 abortJob( jobContext: JobContext): Unit NOTE: abortJob is part of the FileCommitProtocol contract to abort a Spark job. abortJob does nothing. == [[getFileName]] getFileName Method [source, scala] \u00b6 getFileName( taskContext: TaskAttemptContext, ext: String): String getFileName takes the task ID from the given TaskAttemptContext (for the split part below). getFileName generates a random UUID (for the uuid part below). In the end, getFileName returns a file name of the format: part-[split]%05d-[uuid][ext] NOTE: getFileName is used exclusively when DelayedCommitProtocol is requested to < >. == [[addedFiles]] addedFiles Internal Registry [source, scala] \u00b6 addedFiles: ArrayBuffer[(Map[String, String], String)] \u00b6 addedFiles tracks the files < > (that runs on an executor). addedFiles is initialized (as an empty collection) in < >. NOTE: addedFiles is used when DelayedCommitProtocol is requested to < > (on an executor and create a TaskCommitMessage with the files added while a task was writing out a partition of a streaming query). == [[addedStatuses]] addedStatuses Internal Registry [source, scala] \u00b6 addedStatuses = new ArrayBuffer[AddFile] \u00b6 addedStatuses is the files that were added by < > (on executors) once all they finish successfully and the < > (on a driver). [NOTE] \u00b6 addedStatuses is used when: DelayedCommitProtocol is requested to < > (on a driver) * TransactionalWrite is requested to < > \u00b6","title":"DelayedCommitProtocol"},{"location":"DelayedCommitProtocol/#delayedcommitprotocol","text":"DelayedCommitProtocol is used to model a distributed write that is orchestrated by the Spark driver with the write itself happening on executors. DelayedCommitProtocol is a concrete FileCommitProtocol (Spark Core) to write out a result of a structured query to a < > and return a < >. FileCommitProtocol ( Apache Spark ) allows to track a write job (with a write task per partition) and inform the driver when all the write tasks finished successfully (and were < >) to consider the write job < >. TaskCommitMessage (Spark Core) allows to \"transfer\" the files added (written out) on the executors to the driver for the < >. DelayedCommitProtocol is < > exclusively when TransactionalWrite is requested for a < > to < > to the < >. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.delta.files.DelayedCommitProtocol logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.files.DelayedCommitProtocol=ALL","title":"DelayedCommitProtocol"},{"location":"DelayedCommitProtocol/#refer-to-logging","text":"== [[creating-instance]] Creating DelayedCommitProtocol Instance DelayedCommitProtocol takes the following to be created: [[jobId]] Job ID (seems always < >) [[path]] Directory (to write files to) [[randomPrefixLength]] Optional length of a random prefix (seems always < >) DelayedCommitProtocol initializes the < >. == [[setupTask]] setupTask Method","title":"Refer to Logging."},{"location":"DelayedCommitProtocol/#source-scala","text":"setupTask( taskContext: TaskAttemptContext): Unit NOTE: setupTask is part of the FileCommitProtocol contract to set up a task for a writing job. setupTask simply initializes the < > internal registry to be empty. == [[newTaskTempFile]] newTaskTempFile Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_1","text":"newTaskTempFile( taskContext: TaskAttemptContext, dir: Option[String], ext: String): String NOTE: newTaskTempFile is part of the FileCommitProtocol contract to inform the committer to add a new file. newTaskTempFile < > for the given TaskAttemptContext and ext . newTaskTempFile tries to < > with the given dir or falls back to an empty partitionValues . NOTE: The given dir defines a partition directory if the streaming query (and hence the write) is partitioned. newTaskTempFile builds a path (based on the given randomPrefixLength and the dir , or uses the file name directly). NOTE: FIXME When would the optional dir and the < > be defined? newTaskTempFile adds the partition values and the relative path to the < > internal registry. In the end, newTaskTempFile returns the absolute path of the (relative) path in the < >. == [[commitTask]] Committing Task (After Successful Write) -- commitTask Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_2","text":"commitTask( taskContext: TaskAttemptContext): TaskCommitMessage NOTE: commitTask is part of the FileCommitProtocol contract to commit a task after the writes succeed. commitTask simply creates a TaskCommitMessage with an < > for every < > if there were any. Otherwise, the TaskCommitMessage is empty. NOTE: A file is added (to < > internal registry) when DelayedCommitProtocol is requested for a < >. == [[commitJob]] Committing Spark Job (After Successful Write) -- commitJob Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_3","text":"commitJob( jobContext: JobContext, taskCommits: Seq[TaskCommitMessage]): Unit NOTE: commitJob is part of the FileCommitProtocol contract to commit a job after the writes succeed. commitJob simply adds the < > (from the given taskCommits from every < >) to the < > internal registry. == [[parsePartitions]] parsePartitions Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_4","text":"parsePartitions( dir: String): Map[String, String] parsePartitions ...FIXME NOTE: parsePartitions is used exclusively when DelayedCommitProtocol is requested to < >. == [[setupJob]] setupJob Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_5","text":"setupJob( jobContext: JobContext): Unit NOTE: setupJob is part of the FileCommitProtocol contract to set up a Spark job. setupJob does nothing. == [[abortJob]] abortJob Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_6","text":"abortJob( jobContext: JobContext): Unit NOTE: abortJob is part of the FileCommitProtocol contract to abort a Spark job. abortJob does nothing. == [[getFileName]] getFileName Method","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_7","text":"getFileName( taskContext: TaskAttemptContext, ext: String): String getFileName takes the task ID from the given TaskAttemptContext (for the split part below). getFileName generates a random UUID (for the uuid part below). In the end, getFileName returns a file name of the format: part-[split]%05d-[uuid][ext] NOTE: getFileName is used exclusively when DelayedCommitProtocol is requested to < >. == [[addedFiles]] addedFiles Internal Registry","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#source-scala_8","text":"","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#addedfiles-arraybuffermapstring-string-string","text":"addedFiles tracks the files < > (that runs on an executor). addedFiles is initialized (as an empty collection) in < >. NOTE: addedFiles is used when DelayedCommitProtocol is requested to < > (on an executor and create a TaskCommitMessage with the files added while a task was writing out a partition of a streaming query). == [[addedStatuses]] addedStatuses Internal Registry","title":"addedFiles: ArrayBuffer[(Map[String, String], String)]"},{"location":"DelayedCommitProtocol/#source-scala_9","text":"","title":"[source, scala]"},{"location":"DelayedCommitProtocol/#addedstatuses-new-arraybufferaddfile","text":"addedStatuses is the files that were added by < > (on executors) once all they finish successfully and the < > (on a driver).","title":"addedStatuses = new ArrayBuffer[AddFile]"},{"location":"DelayedCommitProtocol/#note","text":"addedStatuses is used when: DelayedCommitProtocol is requested to < > (on a driver)","title":"[NOTE]"},{"location":"DelayedCommitProtocol/#transactionalwrite-is-requested-to","text":"","title":"* TransactionalWrite is requested to &lt;&gt;"},{"location":"DeltaAnalysis/","text":"DeltaAnalysis Logical Resolution Rule \u00b6 DeltaAnalysis is a logical resolution rule ( Spark SQL ). Creating Instance \u00b6 DeltaAnalysis takes the following to be created: SparkSession SQLConf DeltaAnalysis is created when: DeltaSparkSessionExtension is requested to inject Delta extensions Executing Rule \u00b6 apply ( plan : LogicalPlan ) : LogicalPlan apply is part of the Rule ( Spark SQL ) abstraction. apply resolves logical operators. AlterTableAddConstraintStatement \u00b6 apply creates an AlterTable ( Spark SQL ) logical command with an AddConstraint table change. AlterTableDropConstraintStatement \u00b6 apply creates an AlterTable ( Spark SQL ) logical command with an DropConstraint table change. AppendDelta \u00b6 DataSourceV2Relation \u00b6 DeleteFromTable \u00b6 DeltaTable \u00b6 MergeIntoTable \u00b6 OverwriteDelta \u00b6 UpdateTable \u00b6","title":"DeltaAnalysis"},{"location":"DeltaAnalysis/#deltaanalysis-logical-resolution-rule","text":"DeltaAnalysis is a logical resolution rule ( Spark SQL ).","title":"DeltaAnalysis Logical Resolution Rule"},{"location":"DeltaAnalysis/#creating-instance","text":"DeltaAnalysis takes the following to be created: SparkSession SQLConf DeltaAnalysis is created when: DeltaSparkSessionExtension is requested to inject Delta extensions","title":"Creating Instance"},{"location":"DeltaAnalysis/#executing-rule","text":"apply ( plan : LogicalPlan ) : LogicalPlan apply is part of the Rule ( Spark SQL ) abstraction. apply resolves logical operators.","title":"Executing Rule"},{"location":"DeltaAnalysis/#altertableaddconstraintstatement","text":"apply creates an AlterTable ( Spark SQL ) logical command with an AddConstraint table change.","title":" AlterTableAddConstraintStatement"},{"location":"DeltaAnalysis/#altertabledropconstraintstatement","text":"apply creates an AlterTable ( Spark SQL ) logical command with an DropConstraint table change.","title":" AlterTableDropConstraintStatement"},{"location":"DeltaAnalysis/#appenddelta","text":"","title":" AppendDelta"},{"location":"DeltaAnalysis/#datasourcev2relation","text":"","title":" DataSourceV2Relation"},{"location":"DeltaAnalysis/#deletefromtable","text":"","title":" DeleteFromTable"},{"location":"DeltaAnalysis/#deltatable","text":"","title":" DeltaTable"},{"location":"DeltaAnalysis/#mergeintotable","text":"","title":" MergeIntoTable"},{"location":"DeltaAnalysis/#overwritedelta","text":"","title":" OverwriteDelta"},{"location":"DeltaAnalysis/#updatetable","text":"","title":" UpdateTable"},{"location":"DeltaCatalog/","text":"DeltaCatalog \u00b6 DeltaCatalog is a DelegatingCatalogExtension ( Spark SQL ) and a StagingTableCatalog ( Spark SQL ). DeltaCatalog is registered using spark.sql.catalog.spark_catalog configuration property (while creating a SparkSession in a Spark application). Altering Table \u00b6 alterTable ( ident : Identifier , changes : TableChange* ) : Table alterTable ...FIXME alterTable is part of the TableCatalog ( Spark SQL ) abstraction. Creating Table \u00b6 createTable ( ident : Identifier , schema : StructType , partitions : Array [ Transform ], properties : util.Map [ String , String ]) : Table createTable ...FIXME createTable is part of the TableCatalog ( Spark SQL ) abstraction. Loading Table \u00b6 loadTable ( ident : Identifier ) : Table loadTable loads a table by the given identifier from a catalog. If found and the table is a delta table (Spark SQL's V1Table with delta provider), loadTable creates a DeltaTableV2 . loadTable is part of the TableCatalog ( Spark SQL ) abstraction. Creating Delta Table \u00b6 createDeltaTable ( ident : Identifier , schema : StructType , partitions : Array [ Transform ], properties : util.Map [ String , String ], sourceQuery : Option [ LogicalPlan ], operation : TableCreationModes.CreationMode ) : Table createDeltaTable ...FIXME createDeltaTable is used when: DeltaCatalog is requested to createTable StagedDeltaTableV2 is requested to commitStagedChanges","title":"DeltaCatalog"},{"location":"DeltaCatalog/#deltacatalog","text":"DeltaCatalog is a DelegatingCatalogExtension ( Spark SQL ) and a StagingTableCatalog ( Spark SQL ). DeltaCatalog is registered using spark.sql.catalog.spark_catalog configuration property (while creating a SparkSession in a Spark application).","title":"DeltaCatalog"},{"location":"DeltaCatalog/#altering-table","text":"alterTable ( ident : Identifier , changes : TableChange* ) : Table alterTable ...FIXME alterTable is part of the TableCatalog ( Spark SQL ) abstraction.","title":" Altering Table"},{"location":"DeltaCatalog/#creating-table","text":"createTable ( ident : Identifier , schema : StructType , partitions : Array [ Transform ], properties : util.Map [ String , String ]) : Table createTable ...FIXME createTable is part of the TableCatalog ( Spark SQL ) abstraction.","title":" Creating Table"},{"location":"DeltaCatalog/#loading-table","text":"loadTable ( ident : Identifier ) : Table loadTable loads a table by the given identifier from a catalog. If found and the table is a delta table (Spark SQL's V1Table with delta provider), loadTable creates a DeltaTableV2 . loadTable is part of the TableCatalog ( Spark SQL ) abstraction.","title":" Loading Table"},{"location":"DeltaCatalog/#creating-delta-table","text":"createDeltaTable ( ident : Identifier , schema : StructType , partitions : Array [ Transform ], properties : util.Map [ String , String ], sourceQuery : Option [ LogicalPlan ], operation : TableCreationModes.CreationMode ) : Table createDeltaTable ...FIXME createDeltaTable is used when: DeltaCatalog is requested to createTable StagedDeltaTableV2 is requested to commitStagedChanges","title":" Creating Delta Table"},{"location":"DeltaConfig/","text":"DeltaConfig \u00b6 DeltaConfig (of type T ) represents a named configuration property of a delta table with values (of type T ). Creating Instance \u00b6 DeltaConfig takes the following to be created: Configuration Key Default Value Conversion function (from text representation of the DeltaConfig to the T type, i.e. String => T ) Validation function (that guards from incorrect values, i.e. T => Boolean ) Help message (optional) Minimum version of protocol supported (default: undefined) DeltaConfig is created when: DeltaConfigs utility is used to build a DeltaConfig Reading Configuration Property From Metadata \u00b6 fromMetaData ( metadata : Metadata ) : T fromMetaData looks up the key in the configuration of the given Metadata . If not found, fromMetaData gives the default value . In the end, fromMetaData converts the text representation to the proper type using fromString conversion function. fromMetaData is used when: Checkpoints utility is used to buildCheckpoint DeltaErrors utility is used to logFileNotFoundException DeltaLog is requested for checkpointInterval and deletedFileRetentionDuration table properties, and to assert a table is not read-only MetadataCleanup is requested for the enableExpiredLogCleanup and the deltaRetentionMillis OptimisticTransactionImpl is requested to commit Snapshot is requested for the numIndexedCols","title":"DeltaConfig"},{"location":"DeltaConfig/#deltaconfig","text":"DeltaConfig (of type T ) represents a named configuration property of a delta table with values (of type T ).","title":"DeltaConfig"},{"location":"DeltaConfig/#creating-instance","text":"DeltaConfig takes the following to be created: Configuration Key Default Value Conversion function (from text representation of the DeltaConfig to the T type, i.e. String => T ) Validation function (that guards from incorrect values, i.e. T => Boolean ) Help message (optional) Minimum version of protocol supported (default: undefined) DeltaConfig is created when: DeltaConfigs utility is used to build a DeltaConfig","title":"Creating Instance"},{"location":"DeltaConfig/#reading-configuration-property-from-metadata","text":"fromMetaData ( metadata : Metadata ) : T fromMetaData looks up the key in the configuration of the given Metadata . If not found, fromMetaData gives the default value . In the end, fromMetaData converts the text representation to the proper type using fromString conversion function. fromMetaData is used when: Checkpoints utility is used to buildCheckpoint DeltaErrors utility is used to logFileNotFoundException DeltaLog is requested for checkpointInterval and deletedFileRetentionDuration table properties, and to assert a table is not read-only MetadataCleanup is requested for the enableExpiredLogCleanup and the deltaRetentionMillis OptimisticTransactionImpl is requested to commit Snapshot is requested for the numIndexedCols","title":" Reading Configuration Property From Metadata"},{"location":"DeltaConfigs/","text":"DeltaConfigs \u00b6 DeltaConfigs is the configuration properties of a delta table. Configuration Properties \u00b6 appendOnly \u00b6 Whether a delta table is append-only ( true ) or not ( false ). When enabled, a table allows appends only and no updates or deletes. Default: false Used when: DeltaLog is requested to assertRemovable (that in turn uses DeltaErrors utility to modifyAppendOnlyTableException ) Protocol utility is used to requiredMinimumProtocol autoOptimize \u00b6 Whether this delta table will automagically optimize the layout of files during writes. Default: false checkpointInterval \u00b6 How often to checkpoint the state of a delta table Default: 10 checkpointRetentionDuration \u00b6 How long to keep checkpoint files around before deleting them Default: interval 2 days The most recent checkpoint is never deleted. It is acceptable to keep checkpoint files beyond this duration until the next calendar day. checkpoint.writeStatsAsJson \u00b6 Controls whether to write file statistics in the checkpoint in JSON format as the stats column. Default: true checkpoint.writeStatsAsStruct \u00b6 Controls whether to write file statistics in the checkpoint in the struct format in the stats_parsed column and partition values as a struct as partitionValues_parsed Default: undefined ( Option[Boolean] ) compatibility.symlinkFormatManifest.enabled \u00b6 Whether to register the GenerateSymlinkManifest post-commit hook while committing a transaction or not Default: false dataSkippingNumIndexedCols \u00b6 The number of columns to collect stats on for data skipping. -1 means collecting stats for all columns. Default: 32 deletedFileRetentionDuration \u00b6 How long to keep logically deleted data files around before deleting them physically (to prevent failures in stale readers after compactions or partition overwrites) Default: interval 1 week enableExpiredLogCleanup \u00b6 Whether to clean up expired log files and checkpoints Default: true enableFullRetentionRollback \u00b6 Controls whether or not a delta table can be rolled back to any point within logRetentionDuration . When disabled, the table can be rolled back checkpointRetentionDuration only. Default: true logRetentionDuration \u00b6 How long to keep obsolete logs around before deleting them. Delta can keep logs beyond the duration until the next calendar day to avoid constantly creating checkpoints. Default: interval 30 days ( CalendarInterval ) minReaderVersion \u00b6 The protocol reader version Default: 1 This property is not stored as a table property in the Metadata action. It is stored as its own action. Having it modelled as a table property makes it easier to upgrade, and view the version. minWriterVersion \u00b6 The protocol reader version Default: 3 This property is not stored as a table property in the Metadata action. It is stored as its own action. Having it modelled as a table property makes it easier to upgrade, and view the version. randomizeFilePrefixes \u00b6 Whether to use a random prefix in a file path instead of partition information (may be required for very high volume S3 calls to better be partitioned across S3 servers) Default: false randomPrefixLength \u00b6 The length of the random prefix in a file path for randomizeFilePrefixes Default: 2 sampleRetentionDuration \u00b6 How long to keep delta sample files around before deleting them Default: interval 7 days Building Configuration \u00b6 buildConfig [ T ]( key : String , defaultValue : String , fromString : String => T , validationFunction : T => Boolean , helpMessage : String , minimumProtocolVersion : Option [ Protocol ] = None ) : DeltaConfig [ T ] buildConfig creates a DeltaConfig for the given key (with delta prefix added) and adds it to the entries internal registry. buildConfig is used to define all of the configuration properties in a type-safe way and (as a side effect) register them with the system-wide entries internal registry. System-Wide Configuration Entries Registry \u00b6 entries : HashMap [ String , DeltaConfig [ _ ]] DeltaConfigs utility (a Scala object) uses entries internal registry of DeltaConfig s by their key. New entries are added in buildConfig . entries is used when: validateConfigurations mergeGlobalConfigs normalizeConfigKey and normalizeConfigKeys mergeGlobalConfigs Utility \u00b6 mergeGlobalConfigs ( sqlConfs : SQLConf , tableConf : Map [ String , String ], protocol : Protocol ) : Map [ String , String ] mergeGlobalConfigs finds all spark.databricks.delta.properties.defaults -prefixed configuration properties among the entries . mergeGlobalConfigs is used when: OptimisticTransactionImpl is requested to withGlobalConfigDefaults InitialSnapshot is created validateConfigurations Utility \u00b6 validateConfigurations ( configurations : Map [ String , String ]) : Map [ String , String ] validateConfigurations ...FIXME validateConfigurations is used when: DeltaCatalog is requested to verifyTableAndSolidify and alterTable normalizeConfigKeys Utility \u00b6 normalizeConfigKeys ( propKeys : Seq [ String ]) : Seq [ String ] normalizeConfigKeys ...FIXME normalizeConfigKeys is used when: AlterTableUnsetPropertiesDeltaCommand is executed ALTER TABLE SQL Command \u00b6 Table properties can be set a value or unset using ALTER TABLE SQL command: ALTER TABLE < table_name > SET TBLPROPERTIES ( < key >=< value > ) ALTER TABLE table1 UNSET TBLPROPERTIES [ IF EXISTS ] ( 'key1' , 'key2' , ...); spark.databricks.delta.properties.defaults Prefix \u00b6 DeltaConfigs uses spark.databricks.delta.properties.defaults prefix for global configuration properties .","title":"DeltaConfigs"},{"location":"DeltaConfigs/#deltaconfigs","text":"DeltaConfigs is the configuration properties of a delta table.","title":"DeltaConfigs"},{"location":"DeltaConfigs/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"DeltaConfigs/#appendonly","text":"Whether a delta table is append-only ( true ) or not ( false ). When enabled, a table allows appends only and no updates or deletes. Default: false Used when: DeltaLog is requested to assertRemovable (that in turn uses DeltaErrors utility to modifyAppendOnlyTableException ) Protocol utility is used to requiredMinimumProtocol","title":" appendOnly"},{"location":"DeltaConfigs/#autooptimize","text":"Whether this delta table will automagically optimize the layout of files during writes. Default: false","title":" autoOptimize"},{"location":"DeltaConfigs/#checkpointinterval","text":"How often to checkpoint the state of a delta table Default: 10","title":" checkpointInterval"},{"location":"DeltaConfigs/#checkpointretentionduration","text":"How long to keep checkpoint files around before deleting them Default: interval 2 days The most recent checkpoint is never deleted. It is acceptable to keep checkpoint files beyond this duration until the next calendar day.","title":" checkpointRetentionDuration"},{"location":"DeltaConfigs/#checkpointwritestatsasjson","text":"Controls whether to write file statistics in the checkpoint in JSON format as the stats column. Default: true","title":" checkpoint.writeStatsAsJson"},{"location":"DeltaConfigs/#checkpointwritestatsasstruct","text":"Controls whether to write file statistics in the checkpoint in the struct format in the stats_parsed column and partition values as a struct as partitionValues_parsed Default: undefined ( Option[Boolean] )","title":" checkpoint.writeStatsAsStruct"},{"location":"DeltaConfigs/#compatibilitysymlinkformatmanifestenabled","text":"Whether to register the GenerateSymlinkManifest post-commit hook while committing a transaction or not Default: false","title":" compatibility.symlinkFormatManifest.enabled"},{"location":"DeltaConfigs/#dataskippingnumindexedcols","text":"The number of columns to collect stats on for data skipping. -1 means collecting stats for all columns. Default: 32","title":" dataSkippingNumIndexedCols"},{"location":"DeltaConfigs/#deletedfileretentionduration","text":"How long to keep logically deleted data files around before deleting them physically (to prevent failures in stale readers after compactions or partition overwrites) Default: interval 1 week","title":" deletedFileRetentionDuration"},{"location":"DeltaConfigs/#enableexpiredlogcleanup","text":"Whether to clean up expired log files and checkpoints Default: true","title":" enableExpiredLogCleanup"},{"location":"DeltaConfigs/#enablefullretentionrollback","text":"Controls whether or not a delta table can be rolled back to any point within logRetentionDuration . When disabled, the table can be rolled back checkpointRetentionDuration only. Default: true","title":" enableFullRetentionRollback"},{"location":"DeltaConfigs/#logretentionduration","text":"How long to keep obsolete logs around before deleting them. Delta can keep logs beyond the duration until the next calendar day to avoid constantly creating checkpoints. Default: interval 30 days ( CalendarInterval )","title":" logRetentionDuration"},{"location":"DeltaConfigs/#minreaderversion","text":"The protocol reader version Default: 1 This property is not stored as a table property in the Metadata action. It is stored as its own action. Having it modelled as a table property makes it easier to upgrade, and view the version.","title":" minReaderVersion"},{"location":"DeltaConfigs/#minwriterversion","text":"The protocol reader version Default: 3 This property is not stored as a table property in the Metadata action. It is stored as its own action. Having it modelled as a table property makes it easier to upgrade, and view the version.","title":" minWriterVersion"},{"location":"DeltaConfigs/#randomizefileprefixes","text":"Whether to use a random prefix in a file path instead of partition information (may be required for very high volume S3 calls to better be partitioned across S3 servers) Default: false","title":" randomizeFilePrefixes"},{"location":"DeltaConfigs/#randomprefixlength","text":"The length of the random prefix in a file path for randomizeFilePrefixes Default: 2","title":" randomPrefixLength"},{"location":"DeltaConfigs/#sampleretentionduration","text":"How long to keep delta sample files around before deleting them Default: interval 7 days","title":" sampleRetentionDuration"},{"location":"DeltaConfigs/#building-configuration","text":"buildConfig [ T ]( key : String , defaultValue : String , fromString : String => T , validationFunction : T => Boolean , helpMessage : String , minimumProtocolVersion : Option [ Protocol ] = None ) : DeltaConfig [ T ] buildConfig creates a DeltaConfig for the given key (with delta prefix added) and adds it to the entries internal registry. buildConfig is used to define all of the configuration properties in a type-safe way and (as a side effect) register them with the system-wide entries internal registry.","title":" Building Configuration"},{"location":"DeltaConfigs/#system-wide-configuration-entries-registry","text":"entries : HashMap [ String , DeltaConfig [ _ ]] DeltaConfigs utility (a Scala object) uses entries internal registry of DeltaConfig s by their key. New entries are added in buildConfig . entries is used when: validateConfigurations mergeGlobalConfigs normalizeConfigKey and normalizeConfigKeys","title":" System-Wide Configuration Entries Registry"},{"location":"DeltaConfigs/#mergeglobalconfigs-utility","text":"mergeGlobalConfigs ( sqlConfs : SQLConf , tableConf : Map [ String , String ], protocol : Protocol ) : Map [ String , String ] mergeGlobalConfigs finds all spark.databricks.delta.properties.defaults -prefixed configuration properties among the entries . mergeGlobalConfigs is used when: OptimisticTransactionImpl is requested to withGlobalConfigDefaults InitialSnapshot is created","title":" mergeGlobalConfigs Utility"},{"location":"DeltaConfigs/#validateconfigurations-utility","text":"validateConfigurations ( configurations : Map [ String , String ]) : Map [ String , String ] validateConfigurations ...FIXME validateConfigurations is used when: DeltaCatalog is requested to verifyTableAndSolidify and alterTable","title":" validateConfigurations Utility"},{"location":"DeltaConfigs/#normalizeconfigkeys-utility","text":"normalizeConfigKeys ( propKeys : Seq [ String ]) : Seq [ String ] normalizeConfigKeys ...FIXME normalizeConfigKeys is used when: AlterTableUnsetPropertiesDeltaCommand is executed","title":" normalizeConfigKeys Utility"},{"location":"DeltaConfigs/#alter-table-sql-command","text":"Table properties can be set a value or unset using ALTER TABLE SQL command: ALTER TABLE < table_name > SET TBLPROPERTIES ( < key >=< value > ) ALTER TABLE table1 UNSET TBLPROPERTIES [ IF EXISTS ] ( 'key1' , 'key2' , ...);","title":"ALTER TABLE SQL Command"},{"location":"DeltaConfigs/#sparkdatabricksdeltapropertiesdefaults-prefix","text":"DeltaConfigs uses spark.databricks.delta.properties.defaults prefix for global configuration properties .","title":" spark.databricks.delta.properties.defaults Prefix"},{"location":"DeltaConvert/","text":"DeltaConvert Utility \u00b6 DeltaConvert utility is used exclusively for < >. DeltaConvert utility can be used directly or indirectly via < > utility. import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) // CONVERT TO DELTA only supports parquet tables // TableIdentifier should be parquet.`users` import org.apache.spark.sql.catalyst.TableIdentifier val table = TableIdentifier(table = \"users\", database = Some(\"parquet\")) import org.apache.spark.sql.types.{StringType, StructField, StructType} val partitionSchema: Option[StructType] = Some( new StructType().add(StructField(\"country\", StringType))) val deltaPath: Option[String] = None // Use web UI to monitor execution, e.g. http://localhost:4040 import io.delta.tables.execution.DeltaConvert DeltaConvert.executeConvert(spark, table, partitionSchema, deltaPath) DeltaConvert utility is a concrete DeltaConvertBase . Importing Parquet Table Into Delta Lake (Converting Parquet Table To Delta Format) \u00b6 executeConvert ( spark : SparkSession , tableIdentifier : TableIdentifier , partitionSchema : Option [ StructType ], deltaPath : Option [ String ]) : DeltaTable executeConvert creates a new ConvertToDeltaCommand and executes it. In the end, executeConvert creates a DeltaTable . Note executeConvert can convert a Spark table (to Delta) that is registered in a metastore. executeConvert is used when DeltaTable utility is requested to convert a parquet table to delta format (DeltaTable.convertToDelta) .","title":"DeltaConvert"},{"location":"DeltaConvert/#deltaconvert-utility","text":"DeltaConvert utility is used exclusively for < >. DeltaConvert utility can be used directly or indirectly via < > utility. import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) // CONVERT TO DELTA only supports parquet tables // TableIdentifier should be parquet.`users` import org.apache.spark.sql.catalyst.TableIdentifier val table = TableIdentifier(table = \"users\", database = Some(\"parquet\")) import org.apache.spark.sql.types.{StringType, StructField, StructType} val partitionSchema: Option[StructType] = Some( new StructType().add(StructField(\"country\", StringType))) val deltaPath: Option[String] = None // Use web UI to monitor execution, e.g. http://localhost:4040 import io.delta.tables.execution.DeltaConvert DeltaConvert.executeConvert(spark, table, partitionSchema, deltaPath) DeltaConvert utility is a concrete DeltaConvertBase .","title":"DeltaConvert Utility"},{"location":"DeltaConvert/#importing-parquet-table-into-delta-lake-converting-parquet-table-to-delta-format","text":"executeConvert ( spark : SparkSession , tableIdentifier : TableIdentifier , partitionSchema : Option [ StructType ], deltaPath : Option [ String ]) : DeltaTable executeConvert creates a new ConvertToDeltaCommand and executes it. In the end, executeConvert creates a DeltaTable . Note executeConvert can convert a Spark table (to Delta) that is registered in a metastore. executeConvert is used when DeltaTable utility is requested to convert a parquet table to delta format (DeltaTable.convertToDelta) .","title":" Importing Parquet Table Into Delta Lake (Converting Parquet Table To Delta Format)"},{"location":"DeltaDataSource/","text":"DeltaDataSource \u00b6 DeltaDataSource is a DataSourceRegister and is the entry point to all the features provided by delta data source. DeltaDataSource is a RelationProvider . DeltaDataSource is a StreamSinkProvider for a streaming sink for streaming queries (Structured Streaming). DataSourceRegister and delta Alias \u00b6 DeltaDataSource is a DataSourceRegister ( Spark SQL ) and registers delta alias. DeltaDataSource is registered using META-INF/services/org.apache.spark.sql.sources.DataSourceRegister : org.apache.spark.sql.delta.sources.DeltaDataSource RelationProvider for Batch Queries \u00b6 DeltaDataSource is a RelationProvider ( Spark SQL ) for reading ( loading ) data from a delta table in a structured query. createRelation ( sqlContext : SQLContext , parameters : Map [ String , String ]) : BaseRelation createRelation reads the path option from the given parameters. createRelation verifies the given parameters . createRelation extracts time travel specification from the given parameters. In the end, createRelation creates a DeltaTableV2 (for the path option and the time travel specification) and requests it for an insertable HadoopFsRelation . createRelation throws an IllegalArgumentException when path option is not specified: 'path' is not specified Source Schema \u00b6 sourceSchema ( sqlContext : SQLContext , schema : Option [ StructType ], providerName : String , parameters : Map [ String , String ]) : ( String , StructType ) sourceSchema creates a DeltaLog for a Delta table in the directory specified by the required path option (in the parameters) and returns the delta name with the schema (of the Delta table). sourceSchema throws an IllegalArgumentException when the path option has not been specified: 'path' is not specified sourceSchema throws an AnalysisException when the path option uses time travel : Cannot time travel views, subqueries or streams. sourceSchema is part of the StreamSourceProvider abstraction ( Spark Structured Streaming ). CreatableRelationProvider \u00b6 DeltaDataSource is a CreatableRelationProvider ( Spark SQL ) for writing out the result of a structured query. Creating Streaming Source \u00b6 DeltaDataSource is a StreamSourceProvider ( Spark Structured Streaming ) for a streaming source in streaming queries. Creating Streaming Sink \u00b6 DeltaDataSource is a StreamSinkProvider ( Spark Structured Streaming ) for a streaming sink in streaming queries. DeltaDataSource supports Append and Complete output modes only. In the end, DeltaDataSource creates a DeltaSink . Tip Consult the demo Using Delta Lake (as Streaming Sink) in Streaming Queries . Loading Table \u00b6 getTable ( schema : StructType , partitioning : Array [ Transform ], properties : java.util.Map [ String , String ]) : Table getTable ...FIXME getTable is part of the TableProvider (Spark SQL 3.0.0) abstraction. Utilities \u00b6 getTimeTravelVersion \u00b6 getTimeTravelVersion ( parameters : Map [ String , String ]) : Option [ DeltaTimeTravelSpec ] getTimeTravelVersion ...FIXME getTimeTravelVersion is used when DeltaDataSource is requested to create a relation (as a RelationProvider) . parsePathIdentifier \u00b6 parsePathIdentifier ( spark : SparkSession , userPath : String ) : ( Path , Seq [( String , String )], Option [ DeltaTimeTravelSpec ]) parsePathIdentifier ...FIXME parsePathIdentifier is used when DeltaTableV2 is requested for metadata (for a non-catalog table).","title":"DeltaDataSource"},{"location":"DeltaDataSource/#deltadatasource","text":"DeltaDataSource is a DataSourceRegister and is the entry point to all the features provided by delta data source. DeltaDataSource is a RelationProvider . DeltaDataSource is a StreamSinkProvider for a streaming sink for streaming queries (Structured Streaming).","title":"DeltaDataSource"},{"location":"DeltaDataSource/#datasourceregister-and-delta-alias","text":"DeltaDataSource is a DataSourceRegister ( Spark SQL ) and registers delta alias. DeltaDataSource is registered using META-INF/services/org.apache.spark.sql.sources.DataSourceRegister : org.apache.spark.sql.delta.sources.DeltaDataSource","title":" DataSourceRegister and delta Alias"},{"location":"DeltaDataSource/#relationprovider-for-batch-queries","text":"DeltaDataSource is a RelationProvider ( Spark SQL ) for reading ( loading ) data from a delta table in a structured query. createRelation ( sqlContext : SQLContext , parameters : Map [ String , String ]) : BaseRelation createRelation reads the path option from the given parameters. createRelation verifies the given parameters . createRelation extracts time travel specification from the given parameters. In the end, createRelation creates a DeltaTableV2 (for the path option and the time travel specification) and requests it for an insertable HadoopFsRelation . createRelation throws an IllegalArgumentException when path option is not specified: 'path' is not specified","title":" RelationProvider for Batch Queries"},{"location":"DeltaDataSource/#source-schema","text":"sourceSchema ( sqlContext : SQLContext , schema : Option [ StructType ], providerName : String , parameters : Map [ String , String ]) : ( String , StructType ) sourceSchema creates a DeltaLog for a Delta table in the directory specified by the required path option (in the parameters) and returns the delta name with the schema (of the Delta table). sourceSchema throws an IllegalArgumentException when the path option has not been specified: 'path' is not specified sourceSchema throws an AnalysisException when the path option uses time travel : Cannot time travel views, subqueries or streams. sourceSchema is part of the StreamSourceProvider abstraction ( Spark Structured Streaming ).","title":" Source Schema"},{"location":"DeltaDataSource/#creatablerelationprovider","text":"DeltaDataSource is a CreatableRelationProvider ( Spark SQL ) for writing out the result of a structured query.","title":" CreatableRelationProvider"},{"location":"DeltaDataSource/#creating-streaming-source","text":"DeltaDataSource is a StreamSourceProvider ( Spark Structured Streaming ) for a streaming source in streaming queries.","title":" Creating Streaming Source"},{"location":"DeltaDataSource/#creating-streaming-sink","text":"DeltaDataSource is a StreamSinkProvider ( Spark Structured Streaming ) for a streaming sink in streaming queries. DeltaDataSource supports Append and Complete output modes only. In the end, DeltaDataSource creates a DeltaSink . Tip Consult the demo Using Delta Lake (as Streaming Sink) in Streaming Queries .","title":" Creating Streaming Sink"},{"location":"DeltaDataSource/#loading-table","text":"getTable ( schema : StructType , partitioning : Array [ Transform ], properties : java.util.Map [ String , String ]) : Table getTable ...FIXME getTable is part of the TableProvider (Spark SQL 3.0.0) abstraction.","title":" Loading Table"},{"location":"DeltaDataSource/#utilities","text":"","title":"Utilities"},{"location":"DeltaDataSource/#gettimetravelversion","text":"getTimeTravelVersion ( parameters : Map [ String , String ]) : Option [ DeltaTimeTravelSpec ] getTimeTravelVersion ...FIXME getTimeTravelVersion is used when DeltaDataSource is requested to create a relation (as a RelationProvider) .","title":" getTimeTravelVersion"},{"location":"DeltaDataSource/#parsepathidentifier","text":"parsePathIdentifier ( spark : SparkSession , userPath : String ) : ( Path , Seq [( String , String )], Option [ DeltaTimeTravelSpec ]) parsePathIdentifier ...FIXME parsePathIdentifier is used when DeltaTableV2 is requested for metadata (for a non-catalog table).","title":" parsePathIdentifier"},{"location":"DeltaErrors/","text":"DeltaErrors Utility \u00b6 modifyAppendOnlyTableException \u00b6 modifyAppendOnlyTableException : Throwable modifyAppendOnlyTableException throws an UnsupportedOperationException : This table is configured to only allow appends. If you would like to permit updates or deletes, use 'ALTER TABLE <table_name> SET TBLPROPERTIES (appendOnly=false)'. modifyAppendOnlyTableException is used when: DeltaLog is requested to assertRemovable Reporting Post-Commit Hook Failure \u00b6 postCommitHookFailedException ( failedHook : PostCommitHook , failedOnCommitVersion : Long , extraErrorMessage : String , error : Throwable ) : Throwable postCommitHookFailedException throws a RuntimeException : Committing to the Delta table version [failedOnCommitVersion] succeeded but error while executing post-commit hook [failedHook]: [extraErrorMessage] postCommitHookFailedException is used when: GenerateSymlinkManifestImpl is requested to handleError","title":"DeltaErrors"},{"location":"DeltaErrors/#deltaerrors-utility","text":"","title":"DeltaErrors Utility"},{"location":"DeltaErrors/#modifyappendonlytableexception","text":"modifyAppendOnlyTableException : Throwable modifyAppendOnlyTableException throws an UnsupportedOperationException : This table is configured to only allow appends. If you would like to permit updates or deletes, use 'ALTER TABLE <table_name> SET TBLPROPERTIES (appendOnly=false)'. modifyAppendOnlyTableException is used when: DeltaLog is requested to assertRemovable","title":" modifyAppendOnlyTableException"},{"location":"DeltaErrors/#reporting-post-commit-hook-failure","text":"postCommitHookFailedException ( failedHook : PostCommitHook , failedOnCommitVersion : Long , extraErrorMessage : String , error : Throwable ) : Throwable postCommitHookFailedException throws a RuntimeException : Committing to the Delta table version [failedOnCommitVersion] succeeded but error while executing post-commit hook [failedHook]: [extraErrorMessage] postCommitHookFailedException is used when: GenerateSymlinkManifestImpl is requested to handleError","title":" Reporting Post-Commit Hook Failure"},{"location":"DeltaFileFormat/","text":"= [[DeltaFileFormat]] DeltaFileFormat Contract -- Spark FileFormat Of Delta Table DeltaFileFormat is the < > of < > that can < >. [[contract]] .DeltaFileFormat Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | fileFormat a| [[fileFormat]] [source, scala] \u00b6 fileFormat: FileFormat \u00b6 Spark SQL's FileFormat of a delta table Default: ParquetFileFormat Used when: DeltaLog is requested for a < > (in batch queries) and < > DeltaCommand is requested for a < > TransactionalWrite is requested to < > |=== [[implementations]] NOTE: < > is the only known DeltaFileFormat .","title":"DeltaFileFormat"},{"location":"DeltaFileFormat/#source-scala","text":"","title":"[source, scala]"},{"location":"DeltaFileFormat/#fileformat-fileformat","text":"Spark SQL's FileFormat of a delta table Default: ParquetFileFormat Used when: DeltaLog is requested for a < > (in batch queries) and < > DeltaCommand is requested for a < > TransactionalWrite is requested to < > |=== [[implementations]] NOTE: < > is the only known DeltaFileFormat .","title":"fileFormat: FileFormat"},{"location":"DeltaFileOperations/","text":"= DeltaFileOperations Utilities :navtitle: DeltaFileOperations == [[utilities]] Utilities === [[recursiveListDirs]] recursiveListDirs [source, scala] \u00b6 recursiveListDirs( spark: SparkSession, subDirs: Seq[String], hadoopConf: Broadcast[SerializableConfiguration], hiddenFileNameFilter: String => Boolean = defaultHiddenFileFilter, fileListingParallelism: Option[Int] = None): Dataset[SerializableFileStatus] recursiveListDirs...FIXME recursiveListDirs is used when: ManualListingFileManifest (of ConvertToDeltaCommandBase) is requested to doList VacuumCommand utility is used to VacuumCommand.md#gc[gc] === [[tryDeleteNonRecursive]] tryDeleteNonRecursive [source,scala] \u00b6 tryDeleteNonRecursive( fs: FileSystem, path: Path, tries: Int = 3): Boolean tryDeleteNonRecursive...FIXME tryDeleteNonRecursive is used when VacuumCommandImpl is requested to VacuumCommandImpl.md#delete[delete] == [[internal-methods]] Internal Methods === [[recurseDirectories]] recurseDirectories [source,scala] \u00b6 recurseDirectories( logStore: LogStore, filesAndDirs: Iterator[SerializableFileStatus], hiddenFileNameFilter: String => Boolean): Iterator[SerializableFileStatus] recurseDirectories...FIXME recurseDirectories is used when DeltaFileOperations is requested to < > and < >. === [[listUsingLogStore]] listUsingLogStore [source,scala] \u00b6 listUsingLogStore( logStore: LogStore, subDirs: Iterator[String], recurse: Boolean, hiddenFileNameFilter: String => Boolean): Iterator[SerializableFileStatus] listUsingLogStore...FIXME listUsingLogStore is used when DeltaFileOperations is requested to < > and < >. === [[isThrottlingError]] isThrottlingError [source,scala] \u00b6 isThrottlingError( t: Throwable): Boolean isThrottlingError returns true when the Throwable contains slow down . isThrottlingError is used when DeltaFileOperations is requested to < > and < >.","title":"DeltaFileOperations"},{"location":"DeltaFileOperations/#source-scala","text":"recursiveListDirs( spark: SparkSession, subDirs: Seq[String], hadoopConf: Broadcast[SerializableConfiguration], hiddenFileNameFilter: String => Boolean = defaultHiddenFileFilter, fileListingParallelism: Option[Int] = None): Dataset[SerializableFileStatus] recursiveListDirs...FIXME recursiveListDirs is used when: ManualListingFileManifest (of ConvertToDeltaCommandBase) is requested to doList VacuumCommand utility is used to VacuumCommand.md#gc[gc] === [[tryDeleteNonRecursive]] tryDeleteNonRecursive","title":"[source, scala]"},{"location":"DeltaFileOperations/#sourcescala","text":"tryDeleteNonRecursive( fs: FileSystem, path: Path, tries: Int = 3): Boolean tryDeleteNonRecursive...FIXME tryDeleteNonRecursive is used when VacuumCommandImpl is requested to VacuumCommandImpl.md#delete[delete] == [[internal-methods]] Internal Methods === [[recurseDirectories]] recurseDirectories","title":"[source,scala]"},{"location":"DeltaFileOperations/#sourcescala_1","text":"recurseDirectories( logStore: LogStore, filesAndDirs: Iterator[SerializableFileStatus], hiddenFileNameFilter: String => Boolean): Iterator[SerializableFileStatus] recurseDirectories...FIXME recurseDirectories is used when DeltaFileOperations is requested to < > and < >. === [[listUsingLogStore]] listUsingLogStore","title":"[source,scala]"},{"location":"DeltaFileOperations/#sourcescala_2","text":"listUsingLogStore( logStore: LogStore, subDirs: Iterator[String], recurse: Boolean, hiddenFileNameFilter: String => Boolean): Iterator[SerializableFileStatus] listUsingLogStore...FIXME listUsingLogStore is used when DeltaFileOperations is requested to < > and < >. === [[isThrottlingError]] isThrottlingError","title":"[source,scala]"},{"location":"DeltaFileOperations/#sourcescala_3","text":"isThrottlingError( t: Throwable): Boolean isThrottlingError returns true when the Throwable contains slow down . isThrottlingError is used when DeltaFileOperations is requested to < > and < >.","title":"[source,scala]"},{"location":"DeltaHistoryManager/","text":"= DeltaHistoryManager DeltaHistoryManager is...FIXME == [[getHistory]] getHistory Method [source, scala] \u00b6 getHistory( limitOpt: Option[Int]): Seq[CommitInfo] getHistory( start: Long, end: Option[Long]): Seq[CommitInfo] getHistory ...FIXME NOTE: getHistory is used when...FIXME == [[getActiveCommitAtTime]] getActiveCommitAtTime Method [source, scala] \u00b6 getActiveCommitAtTime( timestamp: Timestamp, canReturnLastCommit: Boolean, mustBeRecreatable: Boolean = true): Commit getActiveCommitAtTime ...FIXME NOTE: getActiveCommitAtTime is used exclusively when DeltaTableUtils utility is requested to < >. == [[checkVersionExists]] checkVersionExists Method [source, scala] \u00b6 checkVersionExists(version: Long): Unit \u00b6 checkVersionExists ...FIXME NOTE: checkVersionExists is used when...FIXME == [[parallelSearch]] parallelSearch Internal Method [source, scala] \u00b6 parallelSearch( time: Long, start: Long, end: Long): Commit parallelSearch ...FIXME NOTE: parallelSearch is used exclusively when DeltaHistoryManager is requested to < >. == [[parallelSearch0]] parallelSearch0 Internal Utility [source, scala] \u00b6 parallelSearch0( spark: SparkSession, conf: SerializableConfiguration, logPath: String, time: Long, start: Long, end: Long, step: Long): Commit parallelSearch0 ...FIXME NOTE: parallelSearch0 is used exclusively when DeltaHistoryManager is requested to < >. == [[getCommitInfo]] CommitInfo Of Delta File -- getCommitInfo Internal Utility [source, scala] \u00b6 getCommitInfo( logStore: LogStore, basePath: Path, version: Long): CommitInfo getCommitInfo ...FIXME NOTE: getCommitInfo is used when...FIXME","title":"DeltaHistoryManager"},{"location":"DeltaHistoryManager/#source-scala","text":"getHistory( limitOpt: Option[Int]): Seq[CommitInfo] getHistory( start: Long, end: Option[Long]): Seq[CommitInfo] getHistory ...FIXME NOTE: getHistory is used when...FIXME == [[getActiveCommitAtTime]] getActiveCommitAtTime Method","title":"[source, scala]"},{"location":"DeltaHistoryManager/#source-scala_1","text":"getActiveCommitAtTime( timestamp: Timestamp, canReturnLastCommit: Boolean, mustBeRecreatable: Boolean = true): Commit getActiveCommitAtTime ...FIXME NOTE: getActiveCommitAtTime is used exclusively when DeltaTableUtils utility is requested to < >. == [[checkVersionExists]] checkVersionExists Method","title":"[source, scala]"},{"location":"DeltaHistoryManager/#source-scala_2","text":"","title":"[source, scala]"},{"location":"DeltaHistoryManager/#checkversionexistsversion-long-unit","text":"checkVersionExists ...FIXME NOTE: checkVersionExists is used when...FIXME == [[parallelSearch]] parallelSearch Internal Method","title":"checkVersionExists(version: Long): Unit"},{"location":"DeltaHistoryManager/#source-scala_3","text":"parallelSearch( time: Long, start: Long, end: Long): Commit parallelSearch ...FIXME NOTE: parallelSearch is used exclusively when DeltaHistoryManager is requested to < >. == [[parallelSearch0]] parallelSearch0 Internal Utility","title":"[source, scala]"},{"location":"DeltaHistoryManager/#source-scala_4","text":"parallelSearch0( spark: SparkSession, conf: SerializableConfiguration, logPath: String, time: Long, start: Long, end: Long, step: Long): Commit parallelSearch0 ...FIXME NOTE: parallelSearch0 is used exclusively when DeltaHistoryManager is requested to < >. == [[getCommitInfo]] CommitInfo Of Delta File -- getCommitInfo Internal Utility","title":"[source, scala]"},{"location":"DeltaHistoryManager/#source-scala_5","text":"getCommitInfo( logStore: LogStore, basePath: Path, version: Long): CommitInfo getCommitInfo ...FIXME NOTE: getCommitInfo is used when...FIXME","title":"[source, scala]"},{"location":"DeltaInvariantCheckerExec/","text":"= [[DeltaInvariantCheckerExec]] DeltaInvariantCheckerExec Unary Physical Operator DeltaInvariantCheckerExec is...FIXME","title":"DeltaInvariantCheckerExec"},{"location":"DeltaLog/","text":"DeltaLog \u00b6 DeltaLog is a transaction log ( change log ) of changes to the state of a Delta table (in the given data directory ). Creating Instance \u00b6 DeltaLog takes the following to be created: Log directory (Hadoop Path ) Data directory (Hadoop Path ) Clock DeltaLog is created (indirectly via DeltaLog.apply utility) when: DeltaLog.forTable utility is used _delta_log Metadata Directory \u00b6 DeltaLog uses _delta_log metadata directory for the transaction log of a Delta table. The _delta_log directory is in the given data path directory (when created using DeltaLog.forTable utility). The _delta_log directory is resolved (in the DeltaLog.apply utility) using the application-wide Hadoop Configuration . Once resolved and turned into a qualified path, the _delta_log directory is cached . DeltaLog.forTable Utility \u00b6 forTable ( spark : SparkSession , table : CatalogTable ) : DeltaLog forTable ( spark : SparkSession , table : CatalogTable , clock : Clock ) : DeltaLog forTable ( spark : SparkSession , deltaTable : DeltaTableIdentifier ) : DeltaLog forTable ( spark : SparkSession , dataPath : File ) : DeltaLog forTable ( spark : SparkSession , dataPath : File , clock : Clock ) : DeltaLog forTable ( spark : SparkSession , dataPath : Path ) : DeltaLog forTable ( spark : SparkSession , dataPath : Path , clock : Clock ) : DeltaLog forTable ( spark : SparkSession , dataPath : String ) : DeltaLog forTable ( spark : SparkSession , dataPath : String , clock : Clock ) : DeltaLog forTable ( spark : SparkSession , tableName : TableIdentifier ) : DeltaLog forTable ( spark : SparkSession , tableName : TableIdentifier , clock : Clock ) : DeltaLog forTable creates a DeltaLog with _delta_log directory (in the given dataPath directory). forTable is used when: AlterTableSetLocationDeltaCommand , ConvertToDeltaCommand , VacuumTableCommand , CreateDeltaTableCommand , DeltaGenerateCommand , DescribeDeltaDetailCommand , DescribeDeltaHistoryCommand commands are executed DeltaDataSource is requested for the source schema , a source , and a relation DeltaTable.isDeltaTable utility is used DeltaTableUtils.combineWithCatalogMetadata utility is used DeltaTableIdentifier is requested to getDeltaLog DeltaCatalog is requested to createDeltaTable DeltaTableV2 is requested for the DeltaLog DeltaSink is created Looking Up Or Creating DeltaLog Instance \u00b6 apply ( spark : SparkSession , rawPath : Path , clock : Clock = new SystemClock ) : DeltaLog Note rawPath is a Hadoop Path to the _delta_log directory at the root of the data of a delta table. apply ...FIXME tableExists \u00b6 tableExists : Boolean tableExists requests the current Snapshot for the version and checks out whether it is 0 or higher. is used when: DeltaTable utility is used to isDeltaTable DeltaUnsupportedOperationsCheck logical check rule is executed DeltaTableV2 is requested to toBaseRelation Demo: Creating DeltaLog \u00b6 import org.apache.spark.sql.SparkSession assert ( spark . isInstanceOf [ SparkSession ]) val dataPath = \"/tmp/delta/t1\" import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog . forTable ( spark , dataPath ) import org.apache.hadoop.fs.Path val expected = new Path ( s\"file: $dataPath /_delta_log/_last_checkpoint\" ) assert ( deltaLog . LAST_CHECKPOINT == expected ) Accessing Current Version \u00b6 A common idiom (if not the only way) to know the current version of the delta table is to request the DeltaLog for the current state (snapshot) and then for the version . import org.apache.spark.sql.delta.DeltaLog assert(deltaLog.isInstanceOf[DeltaLog]) val deltaVersion = deltaLog.snapshot.version scala> println(deltaVersion) 5 Initialization \u00b6 When created, DeltaLog does the following: Creates the LogStore based on spark.delta.logStore.class configuration property Initializes the current snapshot Updates state of the delta table when there is no metadata checkpoint (e.g. the version of the state is -1 ) In other words, the version of (the DeltaLog of) a delta table is at version 0 at the very minimum. assert ( deltaLog . snapshot . version >= 0 ) filterFileList Utility \u00b6 filterFileList ( partitionSchema : StructType , files : DataFrame , partitionFilters : Seq [ Expression ], partitionColumnPrefixes : Seq [ String ] = Nil ) : DataFrame filterFileList ...FIXME filterFileList is used when: OptimisticTransactionImpl is requested to checkAndRetry PartitionFiltering is requested to filesForScan WriteIntoDelta is requested to write SnapshotIterator is requested to iterator TahoeBatchFileIndex is requested to matchingFiles DeltaDataSource utility is requested to verifyAndCreatePartitionFilters FileFormats \u00b6 DeltaLog defines two FileFormat s ( Spark SQL ): ParquetFileFormat for indices of delta files JsonFileFormat for indices of checkpoint files These FileFormat s are used to create DeltaLogFileIndex es for Snapshots that in turn used them for stateReconstruction . LogStore \u00b6 DeltaLog uses a LogStore for...FIXME Transaction Logs (DeltaLogs) per Fully-Qualified Path \u00b6 deltaLogCache : Cache [ Path , DeltaLog ] deltaLogCache is part of DeltaLog Scala object which makes it an application-wide cache \"for free\". Once used, deltaLogCache will only be one until the application that uses it stops. deltaLogCache is a registry of DeltaLogs by their fully-qualified _delta_log directories. A new instance of DeltaLog is added when DeltaLog.apply utility is used and the instance hasn't been created before for a path. deltaLogCache is invalidated: For a delta table using DeltaLog.invalidateCache utility For all delta tables using DeltaLog.clearCache utility Executing Single-Threaded Operation in New Transaction \u00b6 withNewTransaction [ T ]( thunk : OptimisticTransaction => T ) : T withNewTransaction starts a new transaction (that is active for the whole thread) and executes the given thunk block. In the end, withNewTransaction makes the transaction no longer active . withNewTransaction is used when: DeleteCommand , MergeIntoCommand , UpdateCommand , and WriteIntoDelta commands are executed DeltaSink is requested to add a streaming micro-batch Starting New Transaction \u00b6 startTransaction () : OptimisticTransaction startTransaction updates and creates a new OptimisticTransaction (for this DeltaLog ). Note startTransaction is a \"subset\" of withNewTransaction . startTransaction is used when: DeltaLog is requested to upgradeProtocol AlterDeltaTableCommand is requested to startTransaction ConvertToDeltaCommand and CreateDeltaTableCommand are executed Throwing UnsupportedOperationException For appendOnly Table Property Enabled \u00b6 assertRemovable () : Unit assertRemovable throws an UnsupportedOperationException for the appendOnly table property ( in the Metadata ) enabled ( true ): This table is configured to only allow appends. If you would like to permit updates or deletes, use 'ALTER TABLE <table_name> SET TBLPROPERTIES (appendOnly=false)'. assertRemovable is used when: OptimisticTransactionImpl is requested to prepareCommit DeleteCommand , UpdateCommand , WriteIntoDelta (with Overwrite mode) are executed DeltaSink is requested to addBatch (with Complete output mode) metadata \u00b6 metadata : Metadata metadata is part of the Checkpoints abstraction. metadata requests the current Snapshot for the metadata or creates a new one (if the current Snapshot is not initialized). update \u00b6 update ( stalenessAcceptable : Boolean = false ) : Snapshot update branches off based on a combination of flags: the given stalenessAcceptable and isSnapshotStale flags. For the stalenessAcceptable not acceptable (default) and the snapshot not stale , update simply acquires the deltaLogLock lock and updateInternal (with isAsync flag off). For all other cases, update ...FIXME update is used when: DeltaHistoryManager is requested to getHistory , getActiveCommitAtTime , and checkVersionExists DeltaLog is created (with no checkpoint created), and requested to startTransaction and withNewTransaction OptimisticTransactionImpl is requested to doCommit and checkAndRetry ConvertToDeltaCommand is requested to run and streamWrite VacuumCommand utility is used to gc TahoeLogFileIndex is requested for the (historical or latest) snapshot DeltaDataSource is requested for a relation tryUpdate \u00b6 tryUpdate ( isAsync : Boolean = false ) : Snapshot tryUpdate ...FIXME Current State Snapshot \u00b6 snapshot : Snapshot snapshot returns the current snapshot . snapshot is used when: OptimisticTransaction is created Checkpoints is requested to checkpoint DeltaLog is requested for the metadata , to upgradeProtocol , getSnapshotAt , createRelation OptimisticTransactionImpl is requested to getNextAttemptVersion DeleteCommand , DeltaGenerateCommand , DescribeDeltaDetailCommand , UpdateCommand commands are executed GenerateSymlinkManifest is executed DeltaCommand is requested to buildBaseRelation TahoeFileIndex is requested for the table version , partitionSchema TahoeLogFileIndex is requested for the table size DeltaDataSource is requested for the schema of the streaming delta source DeltaSource is created and requested for the getStartingOffset , getBatch Current State Snapshot \u00b6 currentSnapshot : Snapshot currentSnapshot is a Snapshot based on the metadata checkpoint if available or a new Snapshot instance (with version being -1 ). Note For a new Snapshot instance (with version being -1 ) DeltaLog immediately updates the state . Internally, currentSnapshot ...FIXME currentSnapshot is available using snapshot method. currentSnapshot is used when: DeltaLog is requested to updateInternal , update and tryUpdate Creating Insertable HadoopFsRelation For Batch Queries \u00b6 createRelation ( partitionFilters : Seq [ Expression ] = Nil , snapshotToUseOpt : Option [ Snapshot ] = None , isTimeTravelQuery : Boolean = false , cdcOptions : CaseInsensitiveStringMap = CaseInsensitiveStringMap . empty ) : BaseRelation createRelation ...FIXME createRelation creates a TahoeLogFileIndex for the data path , the given partitionFilters and a version (if defined). createRelation ...FIXME In the end, createRelation creates a HadoopFsRelation for the TahoeLogFileIndex and...FIXME. The HadoopFsRelation is also an InsertableRelation . createRelation is used when: DeltaTableV2 is requested to toBaseRelation WriteIntoDeltaBuilder is requested to buildForV1Write DeltaDataSource is requested for a writable relation insert \u00b6 insert ( data : DataFrame , overwrite : Boolean ) : Unit insert ...FIXME insert is part of the InsertableRelation ( Spark SQL ) abstraction. Retrieving State Of Delta Table At Given Version \u00b6 getSnapshotAt ( version : Long , commitTimestamp : Option [ Long ] = None , lastCheckpointHint : Option [ CheckpointInstance ] = None ) : Snapshot getSnapshotAt ...FIXME getSnapshotAt is used when: DeltaLog is requested for a relation , and to updateInternal DeltaSource is requested to getSnapshotAt TahoeLogFileIndex is requested for historicalSnapshotOpt checkpointInterval \u00b6 checkpointInterval : Int checkpointInterval gives the value of checkpointInterval table property ( from the Metadata ). checkpointInterval is used when...FIXME Changes (Actions) Of Delta Version And Later \u00b6 getChanges ( startVersion : Long ) : Iterator [( Long , Seq [ Action ])] getChanges gives all action s ( changes ) per delta log file for the given startVersion of a delta table and later. val dataPath = \"/tmp/delta/users\" import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog . forTable ( spark , dataPath ) assert ( deltaLog . isInstanceOf [ DeltaLog ]) val changesPerVersion = deltaLog . getChanges ( startVersion = 0 ) Internally, getChanges requests the LogStore for files that are lexicographically greater or equal to the delta log file for the given startVersion (in the logPath ) and leaves only delta log files (e.g. files with numbers only as file name and .json file extension). For every delta file, getChanges requests the LogStore to read the JSON content (every line is an action ), and then deserializes it to an action . getChanges is used when: DeltaSource is requested for the indexed file additions (FileAdd actions) Creating DataFrame For Given AddFiles \u00b6 createDataFrame ( snapshot : Snapshot , addFiles : Seq [ AddFile ], isStreaming : Boolean = false , actionTypeOpt : Option [ String ] = None ) : DataFrame createDataFrame uses the action type based on the optional action type (if defined) or uses the following based on the isStreaming flag: streaming when isStreaming flag is enabled ( true ) batch when isStreaming flag is disabled ( false ) Note actionTypeOpt seems not to be defined ever. createDataFrame creates a new TahoeBatchFileIndex (for the action type, and the given AddFile s and Snapshot ). createDataFrame creates a HadoopFsRelation ( Spark SQL ) with the TahoeBatchFileIndex and the other properties based on the given Snapshot (and the associated Metadata ). In the end, createDataFrame creates a DataFrame with a logical query plan with a LogicalRelation ( Spark SQL ) over the HadoopFsRelation . createDataFrame is used when: MergeIntoCommand is executed DeltaSource is requested for a DataFrame for data between start and end offsets minFileRetentionTimestamp Method \u00b6 minFileRetentionTimestamp : Long minFileRetentionTimestamp is the timestamp that is tombstoneRetentionMillis before the current time (per the given Clock ). minFileRetentionTimestamp is used when: DeltaLog is requested for the currentSnapshot , to updateInternal , and to getSnapshotAt VacuumCommand is requested for garbage collecting of a delta table tombstoneRetentionMillis Method \u00b6 tombstoneRetentionMillis : Long tombstoneRetentionMillis gives the value of deletedFileRetentionDuration table property ( from the Metadata ). tombstoneRetentionMillis is used when: DeltaLog is requested for minFileRetentionTimestamp VacuumCommand is requested for garbage collecting of a delta table updateInternal Internal Method \u00b6 updateInternal ( isAsync : Boolean ) : Snapshot updateInternal ...FIXME updateInternal is used when: DeltaLog is requested to update (directly or via tryUpdate ) Invalidating Cached DeltaLog Instance By Path \u00b6 invalidateCache ( spark : SparkSession , dataPath : Path ) : Unit invalidateCache ...FIXME invalidateCache is a public API and does not seem to be used at all. protocolRead \u00b6 protocolRead ( protocol : Protocol ) : Unit protocolRead ...FIXME protocolRead is used when: OptimisticTransactionImpl is requested to validate and retry a commit Snapshot is created DeltaSource is requested to verifyStreamHygieneAndFilterAddFiles upgradeProtocol \u00b6 upgradeProtocol ( newVersion : Protocol = Protocol ()) : Unit upgradeProtocol ...FIXME upgradeProtocol is used when: DeltaTable is requested to upgradeTableProtocol LogStoreProvider \u00b6 DeltaLog is a LogStoreProvider . Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.delta.DeltaLog logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.DeltaLog=ALL Refer to Logging .","title":"DeltaLog"},{"location":"DeltaLog/#deltalog","text":"DeltaLog is a transaction log ( change log ) of changes to the state of a Delta table (in the given data directory ).","title":"DeltaLog"},{"location":"DeltaLog/#creating-instance","text":"DeltaLog takes the following to be created: Log directory (Hadoop Path ) Data directory (Hadoop Path ) Clock DeltaLog is created (indirectly via DeltaLog.apply utility) when: DeltaLog.forTable utility is used","title":"Creating Instance"},{"location":"DeltaLog/#_delta_log-metadata-directory","text":"DeltaLog uses _delta_log metadata directory for the transaction log of a Delta table. The _delta_log directory is in the given data path directory (when created using DeltaLog.forTable utility). The _delta_log directory is resolved (in the DeltaLog.apply utility) using the application-wide Hadoop Configuration . Once resolved and turned into a qualified path, the _delta_log directory is cached .","title":" _delta_log Metadata Directory"},{"location":"DeltaLog/#deltalogfortable-utility","text":"forTable ( spark : SparkSession , table : CatalogTable ) : DeltaLog forTable ( spark : SparkSession , table : CatalogTable , clock : Clock ) : DeltaLog forTable ( spark : SparkSession , deltaTable : DeltaTableIdentifier ) : DeltaLog forTable ( spark : SparkSession , dataPath : File ) : DeltaLog forTable ( spark : SparkSession , dataPath : File , clock : Clock ) : DeltaLog forTable ( spark : SparkSession , dataPath : Path ) : DeltaLog forTable ( spark : SparkSession , dataPath : Path , clock : Clock ) : DeltaLog forTable ( spark : SparkSession , dataPath : String ) : DeltaLog forTable ( spark : SparkSession , dataPath : String , clock : Clock ) : DeltaLog forTable ( spark : SparkSession , tableName : TableIdentifier ) : DeltaLog forTable ( spark : SparkSession , tableName : TableIdentifier , clock : Clock ) : DeltaLog forTable creates a DeltaLog with _delta_log directory (in the given dataPath directory). forTable is used when: AlterTableSetLocationDeltaCommand , ConvertToDeltaCommand , VacuumTableCommand , CreateDeltaTableCommand , DeltaGenerateCommand , DescribeDeltaDetailCommand , DescribeDeltaHistoryCommand commands are executed DeltaDataSource is requested for the source schema , a source , and a relation DeltaTable.isDeltaTable utility is used DeltaTableUtils.combineWithCatalogMetadata utility is used DeltaTableIdentifier is requested to getDeltaLog DeltaCatalog is requested to createDeltaTable DeltaTableV2 is requested for the DeltaLog DeltaSink is created","title":" DeltaLog.forTable Utility"},{"location":"DeltaLog/#looking-up-or-creating-deltalog-instance","text":"apply ( spark : SparkSession , rawPath : Path , clock : Clock = new SystemClock ) : DeltaLog Note rawPath is a Hadoop Path to the _delta_log directory at the root of the data of a delta table. apply ...FIXME","title":" Looking Up Or Creating DeltaLog Instance"},{"location":"DeltaLog/#tableexists","text":"tableExists : Boolean tableExists requests the current Snapshot for the version and checks out whether it is 0 or higher. is used when: DeltaTable utility is used to isDeltaTable DeltaUnsupportedOperationsCheck logical check rule is executed DeltaTableV2 is requested to toBaseRelation","title":" tableExists"},{"location":"DeltaLog/#demo-creating-deltalog","text":"import org.apache.spark.sql.SparkSession assert ( spark . isInstanceOf [ SparkSession ]) val dataPath = \"/tmp/delta/t1\" import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog . forTable ( spark , dataPath ) import org.apache.hadoop.fs.Path val expected = new Path ( s\"file: $dataPath /_delta_log/_last_checkpoint\" ) assert ( deltaLog . LAST_CHECKPOINT == expected )","title":"Demo: Creating DeltaLog"},{"location":"DeltaLog/#accessing-current-version","text":"A common idiom (if not the only way) to know the current version of the delta table is to request the DeltaLog for the current state (snapshot) and then for the version . import org.apache.spark.sql.delta.DeltaLog assert(deltaLog.isInstanceOf[DeltaLog]) val deltaVersion = deltaLog.snapshot.version scala> println(deltaVersion) 5","title":"Accessing Current Version"},{"location":"DeltaLog/#initialization","text":"When created, DeltaLog does the following: Creates the LogStore based on spark.delta.logStore.class configuration property Initializes the current snapshot Updates state of the delta table when there is no metadata checkpoint (e.g. the version of the state is -1 ) In other words, the version of (the DeltaLog of) a delta table is at version 0 at the very minimum. assert ( deltaLog . snapshot . version >= 0 )","title":"Initialization"},{"location":"DeltaLog/#filterfilelist-utility","text":"filterFileList ( partitionSchema : StructType , files : DataFrame , partitionFilters : Seq [ Expression ], partitionColumnPrefixes : Seq [ String ] = Nil ) : DataFrame filterFileList ...FIXME filterFileList is used when: OptimisticTransactionImpl is requested to checkAndRetry PartitionFiltering is requested to filesForScan WriteIntoDelta is requested to write SnapshotIterator is requested to iterator TahoeBatchFileIndex is requested to matchingFiles DeltaDataSource utility is requested to verifyAndCreatePartitionFilters","title":" filterFileList Utility"},{"location":"DeltaLog/#fileformats","text":"DeltaLog defines two FileFormat s ( Spark SQL ): ParquetFileFormat for indices of delta files JsonFileFormat for indices of checkpoint files These FileFormat s are used to create DeltaLogFileIndex es for Snapshots that in turn used them for stateReconstruction .","title":"FileFormats"},{"location":"DeltaLog/#logstore","text":"DeltaLog uses a LogStore for...FIXME","title":" LogStore"},{"location":"DeltaLog/#transaction-logs-deltalogs-per-fully-qualified-path","text":"deltaLogCache : Cache [ Path , DeltaLog ] deltaLogCache is part of DeltaLog Scala object which makes it an application-wide cache \"for free\". Once used, deltaLogCache will only be one until the application that uses it stops. deltaLogCache is a registry of DeltaLogs by their fully-qualified _delta_log directories. A new instance of DeltaLog is added when DeltaLog.apply utility is used and the instance hasn't been created before for a path. deltaLogCache is invalidated: For a delta table using DeltaLog.invalidateCache utility For all delta tables using DeltaLog.clearCache utility","title":" Transaction Logs (DeltaLogs) per Fully-Qualified Path"},{"location":"DeltaLog/#executing-single-threaded-operation-in-new-transaction","text":"withNewTransaction [ T ]( thunk : OptimisticTransaction => T ) : T withNewTransaction starts a new transaction (that is active for the whole thread) and executes the given thunk block. In the end, withNewTransaction makes the transaction no longer active . withNewTransaction is used when: DeleteCommand , MergeIntoCommand , UpdateCommand , and WriteIntoDelta commands are executed DeltaSink is requested to add a streaming micro-batch","title":" Executing Single-Threaded Operation in New Transaction"},{"location":"DeltaLog/#starting-new-transaction","text":"startTransaction () : OptimisticTransaction startTransaction updates and creates a new OptimisticTransaction (for this DeltaLog ). Note startTransaction is a \"subset\" of withNewTransaction . startTransaction is used when: DeltaLog is requested to upgradeProtocol AlterDeltaTableCommand is requested to startTransaction ConvertToDeltaCommand and CreateDeltaTableCommand are executed","title":" Starting New Transaction"},{"location":"DeltaLog/#throwing-unsupportedoperationexception-for-appendonly-table-property-enabled","text":"assertRemovable () : Unit assertRemovable throws an UnsupportedOperationException for the appendOnly table property ( in the Metadata ) enabled ( true ): This table is configured to only allow appends. If you would like to permit updates or deletes, use 'ALTER TABLE <table_name> SET TBLPROPERTIES (appendOnly=false)'. assertRemovable is used when: OptimisticTransactionImpl is requested to prepareCommit DeleteCommand , UpdateCommand , WriteIntoDelta (with Overwrite mode) are executed DeltaSink is requested to addBatch (with Complete output mode)","title":" Throwing UnsupportedOperationException For appendOnly Table Property Enabled"},{"location":"DeltaLog/#metadata","text":"metadata : Metadata metadata is part of the Checkpoints abstraction. metadata requests the current Snapshot for the metadata or creates a new one (if the current Snapshot is not initialized).","title":" metadata"},{"location":"DeltaLog/#update","text":"update ( stalenessAcceptable : Boolean = false ) : Snapshot update branches off based on a combination of flags: the given stalenessAcceptable and isSnapshotStale flags. For the stalenessAcceptable not acceptable (default) and the snapshot not stale , update simply acquires the deltaLogLock lock and updateInternal (with isAsync flag off). For all other cases, update ...FIXME update is used when: DeltaHistoryManager is requested to getHistory , getActiveCommitAtTime , and checkVersionExists DeltaLog is created (with no checkpoint created), and requested to startTransaction and withNewTransaction OptimisticTransactionImpl is requested to doCommit and checkAndRetry ConvertToDeltaCommand is requested to run and streamWrite VacuumCommand utility is used to gc TahoeLogFileIndex is requested for the (historical or latest) snapshot DeltaDataSource is requested for a relation","title":" update"},{"location":"DeltaLog/#tryupdate","text":"tryUpdate ( isAsync : Boolean = false ) : Snapshot tryUpdate ...FIXME","title":" tryUpdate"},{"location":"DeltaLog/#current-state-snapshot","text":"snapshot : Snapshot snapshot returns the current snapshot . snapshot is used when: OptimisticTransaction is created Checkpoints is requested to checkpoint DeltaLog is requested for the metadata , to upgradeProtocol , getSnapshotAt , createRelation OptimisticTransactionImpl is requested to getNextAttemptVersion DeleteCommand , DeltaGenerateCommand , DescribeDeltaDetailCommand , UpdateCommand commands are executed GenerateSymlinkManifest is executed DeltaCommand is requested to buildBaseRelation TahoeFileIndex is requested for the table version , partitionSchema TahoeLogFileIndex is requested for the table size DeltaDataSource is requested for the schema of the streaming delta source DeltaSource is created and requested for the getStartingOffset , getBatch","title":" Current State Snapshot"},{"location":"DeltaLog/#current-state-snapshot_1","text":"currentSnapshot : Snapshot currentSnapshot is a Snapshot based on the metadata checkpoint if available or a new Snapshot instance (with version being -1 ). Note For a new Snapshot instance (with version being -1 ) DeltaLog immediately updates the state . Internally, currentSnapshot ...FIXME currentSnapshot is available using snapshot method. currentSnapshot is used when: DeltaLog is requested to updateInternal , update and tryUpdate","title":" Current State Snapshot"},{"location":"DeltaLog/#creating-insertable-hadoopfsrelation-for-batch-queries","text":"createRelation ( partitionFilters : Seq [ Expression ] = Nil , snapshotToUseOpt : Option [ Snapshot ] = None , isTimeTravelQuery : Boolean = false , cdcOptions : CaseInsensitiveStringMap = CaseInsensitiveStringMap . empty ) : BaseRelation createRelation ...FIXME createRelation creates a TahoeLogFileIndex for the data path , the given partitionFilters and a version (if defined). createRelation ...FIXME In the end, createRelation creates a HadoopFsRelation for the TahoeLogFileIndex and...FIXME. The HadoopFsRelation is also an InsertableRelation . createRelation is used when: DeltaTableV2 is requested to toBaseRelation WriteIntoDeltaBuilder is requested to buildForV1Write DeltaDataSource is requested for a writable relation","title":" Creating Insertable HadoopFsRelation For Batch Queries"},{"location":"DeltaLog/#insert","text":"insert ( data : DataFrame , overwrite : Boolean ) : Unit insert ...FIXME insert is part of the InsertableRelation ( Spark SQL ) abstraction.","title":" insert"},{"location":"DeltaLog/#retrieving-state-of-delta-table-at-given-version","text":"getSnapshotAt ( version : Long , commitTimestamp : Option [ Long ] = None , lastCheckpointHint : Option [ CheckpointInstance ] = None ) : Snapshot getSnapshotAt ...FIXME getSnapshotAt is used when: DeltaLog is requested for a relation , and to updateInternal DeltaSource is requested to getSnapshotAt TahoeLogFileIndex is requested for historicalSnapshotOpt","title":" Retrieving State Of Delta Table At Given Version"},{"location":"DeltaLog/#checkpointinterval","text":"checkpointInterval : Int checkpointInterval gives the value of checkpointInterval table property ( from the Metadata ). checkpointInterval is used when...FIXME","title":" checkpointInterval"},{"location":"DeltaLog/#changes-actions-of-delta-version-and-later","text":"getChanges ( startVersion : Long ) : Iterator [( Long , Seq [ Action ])] getChanges gives all action s ( changes ) per delta log file for the given startVersion of a delta table and later. val dataPath = \"/tmp/delta/users\" import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog . forTable ( spark , dataPath ) assert ( deltaLog . isInstanceOf [ DeltaLog ]) val changesPerVersion = deltaLog . getChanges ( startVersion = 0 ) Internally, getChanges requests the LogStore for files that are lexicographically greater or equal to the delta log file for the given startVersion (in the logPath ) and leaves only delta log files (e.g. files with numbers only as file name and .json file extension). For every delta file, getChanges requests the LogStore to read the JSON content (every line is an action ), and then deserializes it to an action . getChanges is used when: DeltaSource is requested for the indexed file additions (FileAdd actions)","title":" Changes (Actions) Of Delta Version And Later"},{"location":"DeltaLog/#creating-dataframe-for-given-addfiles","text":"createDataFrame ( snapshot : Snapshot , addFiles : Seq [ AddFile ], isStreaming : Boolean = false , actionTypeOpt : Option [ String ] = None ) : DataFrame createDataFrame uses the action type based on the optional action type (if defined) or uses the following based on the isStreaming flag: streaming when isStreaming flag is enabled ( true ) batch when isStreaming flag is disabled ( false ) Note actionTypeOpt seems not to be defined ever. createDataFrame creates a new TahoeBatchFileIndex (for the action type, and the given AddFile s and Snapshot ). createDataFrame creates a HadoopFsRelation ( Spark SQL ) with the TahoeBatchFileIndex and the other properties based on the given Snapshot (and the associated Metadata ). In the end, createDataFrame creates a DataFrame with a logical query plan with a LogicalRelation ( Spark SQL ) over the HadoopFsRelation . createDataFrame is used when: MergeIntoCommand is executed DeltaSource is requested for a DataFrame for data between start and end offsets","title":" Creating DataFrame For Given AddFiles"},{"location":"DeltaLog/#minfileretentiontimestamp-method","text":"minFileRetentionTimestamp : Long minFileRetentionTimestamp is the timestamp that is tombstoneRetentionMillis before the current time (per the given Clock ). minFileRetentionTimestamp is used when: DeltaLog is requested for the currentSnapshot , to updateInternal , and to getSnapshotAt VacuumCommand is requested for garbage collecting of a delta table","title":" minFileRetentionTimestamp Method"},{"location":"DeltaLog/#tombstoneretentionmillis-method","text":"tombstoneRetentionMillis : Long tombstoneRetentionMillis gives the value of deletedFileRetentionDuration table property ( from the Metadata ). tombstoneRetentionMillis is used when: DeltaLog is requested for minFileRetentionTimestamp VacuumCommand is requested for garbage collecting of a delta table","title":" tombstoneRetentionMillis Method"},{"location":"DeltaLog/#updateinternal-internal-method","text":"updateInternal ( isAsync : Boolean ) : Snapshot updateInternal ...FIXME updateInternal is used when: DeltaLog is requested to update (directly or via tryUpdate )","title":" updateInternal Internal Method"},{"location":"DeltaLog/#invalidating-cached-deltalog-instance-by-path","text":"invalidateCache ( spark : SparkSession , dataPath : Path ) : Unit invalidateCache ...FIXME invalidateCache is a public API and does not seem to be used at all.","title":" Invalidating Cached DeltaLog Instance By Path"},{"location":"DeltaLog/#protocolread","text":"protocolRead ( protocol : Protocol ) : Unit protocolRead ...FIXME protocolRead is used when: OptimisticTransactionImpl is requested to validate and retry a commit Snapshot is created DeltaSource is requested to verifyStreamHygieneAndFilterAddFiles","title":" protocolRead"},{"location":"DeltaLog/#upgradeprotocol","text":"upgradeProtocol ( newVersion : Protocol = Protocol ()) : Unit upgradeProtocol ...FIXME upgradeProtocol is used when: DeltaTable is requested to upgradeTableProtocol","title":" upgradeProtocol"},{"location":"DeltaLog/#logstoreprovider","text":"DeltaLog is a LogStoreProvider .","title":"LogStoreProvider"},{"location":"DeltaLog/#logging","text":"Enable ALL logging level for org.apache.spark.sql.delta.DeltaLog logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.DeltaLog=ALL Refer to Logging .","title":"Logging"},{"location":"DeltaLogFileIndex/","text":"DeltaLogFileIndex \u00b6 DeltaLogFileIndex is a FileIndex for Snapshot (for the commit and checkpoint files). Note Learn more on FileIndex in The Internals of Spark SQL online book. Creating Instance \u00b6 DeltaLogFileIndex takes the following to be created: FileFormat Files (as Hadoop FileStatus es) While being created, DeltaLogFileIndex prints out the following INFO message to the logs: Created [this] DeltaLogFileIndex is created (indirectly using apply utility) when Snapshot is requested for DeltaLogFileIndex for commit or checkpoint files. FileFormat \u00b6 DeltaLogFileIndex is given a FileFormat ( Spark SQL ) when created : JsonFileFormat ( Spark SQL ) for commit files ParquetFileFormat ( Spark SQL ) for checkpoint files Text Representation \u00b6 toString : String toString returns the following (using the given FileFormat , the number of files and their estimated size): DeltaLogFileIndex([format], numFilesInSegment: [files], totalFileSize: [sizeInBytes]) Creating DeltaLogFileIndex \u00b6 apply ( format : FileFormat , files : Seq [ FileStatus ]) : Option [ DeltaLogFileIndex ] apply creates a new DeltaLogFileIndex (for a non-empty collection of files). apply is used when Snapshot is requested for DeltaLogFileIndex for commit or checkpoint files. Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.delta.DeltaLogFileIndex logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.DeltaLogFileIndex=ALL Refer to Logging .","title":"DeltaLogFileIndex"},{"location":"DeltaLogFileIndex/#deltalogfileindex","text":"DeltaLogFileIndex is a FileIndex for Snapshot (for the commit and checkpoint files). Note Learn more on FileIndex in The Internals of Spark SQL online book.","title":"DeltaLogFileIndex"},{"location":"DeltaLogFileIndex/#creating-instance","text":"DeltaLogFileIndex takes the following to be created: FileFormat Files (as Hadoop FileStatus es) While being created, DeltaLogFileIndex prints out the following INFO message to the logs: Created [this] DeltaLogFileIndex is created (indirectly using apply utility) when Snapshot is requested for DeltaLogFileIndex for commit or checkpoint files.","title":"Creating Instance"},{"location":"DeltaLogFileIndex/#fileformat","text":"DeltaLogFileIndex is given a FileFormat ( Spark SQL ) when created : JsonFileFormat ( Spark SQL ) for commit files ParquetFileFormat ( Spark SQL ) for checkpoint files","title":" FileFormat"},{"location":"DeltaLogFileIndex/#text-representation","text":"toString : String toString returns the following (using the given FileFormat , the number of files and their estimated size): DeltaLogFileIndex([format], numFilesInSegment: [files], totalFileSize: [sizeInBytes])","title":" Text Representation"},{"location":"DeltaLogFileIndex/#creating-deltalogfileindex","text":"apply ( format : FileFormat , files : Seq [ FileStatus ]) : Option [ DeltaLogFileIndex ] apply creates a new DeltaLogFileIndex (for a non-empty collection of files). apply is used when Snapshot is requested for DeltaLogFileIndex for commit or checkpoint files.","title":" Creating DeltaLogFileIndex"},{"location":"DeltaLogFileIndex/#logging","text":"Enable ALL logging level for org.apache.spark.sql.delta.DeltaLogFileIndex logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.DeltaLogFileIndex=ALL Refer to Logging .","title":"Logging"},{"location":"DeltaOptionParser/","text":"DeltaOptionParser \u00b6 DeltaOptionParser is an abstraction of options for reading from and writing to delta tables. Contract \u00b6 SQLConf \u00b6 sqlConf : SQLConf Used when: DeltaWriteOptionsImpl is requested for canMergeSchema Options \u00b6 options : CaseInsensitiveMap [ String ] Implementations \u00b6 DeltaReadOptions DeltaWriteOptions DeltaWriteOptionsImpl","title":"DeltaOptionParser"},{"location":"DeltaOptionParser/#deltaoptionparser","text":"DeltaOptionParser is an abstraction of options for reading from and writing to delta tables.","title":"DeltaOptionParser"},{"location":"DeltaOptionParser/#contract","text":"","title":"Contract"},{"location":"DeltaOptionParser/#sqlconf","text":"sqlConf : SQLConf Used when: DeltaWriteOptionsImpl is requested for canMergeSchema","title":" SQLConf"},{"location":"DeltaOptionParser/#options","text":"options : CaseInsensitiveMap [ String ]","title":" Options"},{"location":"DeltaOptionParser/#implementations","text":"DeltaReadOptions DeltaWriteOptions DeltaWriteOptionsImpl","title":"Implementations"},{"location":"DeltaOptions/","text":"DeltaOptions \u00b6 DeltaOptions is a DeltaWriteOptions and DeltaReadOptions . DeltaOptions is a type-safe abstraction of write and read options. DeltaOptions is used to create WriteIntoDelta command, DeltaSink , and DeltaSource . DeltaOptions is a Serializable ( Java ) (so it can be used in Spark tasks). Creating Instance \u00b6 DeltaOptions takes the following to be created: Case-Insensitive Options SQLConf ( Spark SQL ) When created, DeltaOptions verifies the input options. DeltaOptions is created when: DeltaLog is requested for a relation (for DeltaDataSource as a CreatableRelationProvider and a RelationProvider ) DeltaCatalog is requested to createDeltaTable WriteIntoDeltaBuilder is requested to buildForV1Write CreateDeltaTableCommand is executed DeltaDataSource is requested for a streaming source (to create a DeltaSource for Structured Streaming), a streaming sink (to create a DeltaSink for Structured Streaming), and for an insertable HadoopFsRelation Verifying Options \u00b6 verifyOptions ( options : CaseInsensitiveMap [ String ]) : Unit verifyOptions finds invalid options among the input options . Note In the open-source version verifyOptions does nothing. The underlying objects ( recordDeltaEvent and the others) are no-ops. verifyOptions is used when: DeltaOptions is created DeltaDataSource is requested for a relation (for loading data in batch queries)","title":"DeltaOptions"},{"location":"DeltaOptions/#deltaoptions","text":"DeltaOptions is a DeltaWriteOptions and DeltaReadOptions . DeltaOptions is a type-safe abstraction of write and read options. DeltaOptions is used to create WriteIntoDelta command, DeltaSink , and DeltaSource . DeltaOptions is a Serializable ( Java ) (so it can be used in Spark tasks).","title":"DeltaOptions"},{"location":"DeltaOptions/#creating-instance","text":"DeltaOptions takes the following to be created: Case-Insensitive Options SQLConf ( Spark SQL ) When created, DeltaOptions verifies the input options. DeltaOptions is created when: DeltaLog is requested for a relation (for DeltaDataSource as a CreatableRelationProvider and a RelationProvider ) DeltaCatalog is requested to createDeltaTable WriteIntoDeltaBuilder is requested to buildForV1Write CreateDeltaTableCommand is executed DeltaDataSource is requested for a streaming source (to create a DeltaSource for Structured Streaming), a streaming sink (to create a DeltaSink for Structured Streaming), and for an insertable HadoopFsRelation","title":"Creating Instance"},{"location":"DeltaOptions/#verifying-options","text":"verifyOptions ( options : CaseInsensitiveMap [ String ]) : Unit verifyOptions finds invalid options among the input options . Note In the open-source version verifyOptions does nothing. The underlying objects ( recordDeltaEvent and the others) are no-ops. verifyOptions is used when: DeltaOptions is created DeltaDataSource is requested for a relation (for loading data in batch queries)","title":" Verifying Options"},{"location":"DeltaReadOptions/","text":"DeltaReadOptions \u00b6 DeltaReadOptions is...FIXME","title":"DeltaReadOptions"},{"location":"DeltaReadOptions/#deltareadoptions","text":"DeltaReadOptions is...FIXME","title":"DeltaReadOptions"},{"location":"DeltaSQLConf/","text":"DeltaSQLConf \u2014 spark.databricks.delta Configuration Properties \u00b6 DeltaSQLConf contains spark.databricks.delta -prefixed configuration properties to configure behaviour of Delta Lake. alterLocation.bypassSchemaCheck \u00b6 spark.databricks.delta.alterLocation.bypassSchemaCheck enables Alter Table Set Location on Delta to go through even if the Delta table in the new location has a different schema from the original Delta table Default: false checkLatestSchemaOnRead \u00b6 spark.databricks.delta.checkLatestSchemaOnRead (internal) enables a check that ensures that users won't read corrupt data if the source schema changes in an incompatible way. Default: true In Delta, we always try to give users the latest version of their data without having to call REFRESH TABLE or redefine their DataFrames when used in the context of streaming. There is a possibility that the schema of the latest version of the table may be incompatible with the schema at the time of DataFrame creation. checkpoint.partSize \u00b6 spark.databricks.delta.checkpoint.partSize (internal) is the limit at which we will start parallelizing the checkpoint. We will attempt to write maximum of this many actions per checkpoint. Default: 5000000 commitInfo.enabled \u00b6 spark.databricks.delta.commitInfo.enabled controls whether to log commit information into a Delta log . Default: true commitInfo.userMetadata \u00b6 spark.databricks.delta.commitInfo.userMetadata is an arbitrary user-defined metadata to include in CommitInfo (requires spark.databricks.delta.commitInfo.enabled ). Default: (empty) commitValidation.enabled \u00b6 spark.databricks.delta.commitValidation.enabled (internal) controls whether to perform validation checks before commit or not Default: true convert.metadataCheck.enabled \u00b6 spark.databricks.delta.convert.metadataCheck.enabled enables validation during convert to delta, if there is a difference between the catalog table's properties and the Delta table's configuration, we should error. If disabled, merge the two configurations with the same semantics as update and merge Default: true dummyFileManager.numOfFiles \u00b6 spark.databricks.delta.dummyFileManager.numOfFiles (internal) controls how many dummy files to write in DummyFileManager Default: 3 dummyFileManager.prefix \u00b6 spark.databricks.delta.dummyFileManager.prefix (internal) is the file prefix to use in DummyFileManager Default: .s3-optimization- history.maxKeysPerList \u00b6 spark.databricks.delta.history.maxKeysPerList (internal) controls how many commits to list when performing a parallel search. The default is the maximum keys returned by S3 per list call. Azure can return 5000, therefore we choose 1000. Default: 1000 history.metricsEnabled \u00b6 spark.databricks.delta.history.metricsEnabled enables Metrics reporting in Describe History. CommitInfo will now record the Operation Metrics. Default: true Used when: OptimisticTransactionImpl is requested to getOperationMetrics ConvertToDeltaCommand is requested to streamWrite SQLMetricsReporting is requested to registerSQLMetrics TransactionalWrite is requested to writeFiles import.batchSize.schemaInference \u00b6 spark.databricks.delta.import.batchSize.schemaInference (internal) is the number of files per batch for schema inference during import . Default: 1000000 import.batchSize.statsCollection \u00b6 spark.databricks.delta.import.batchSize.statsCollection (internal) is the number of files per batch for stats collection during import . Default: 50000 maxSnapshotLineageLength \u00b6 spark.databricks.delta.maxSnapshotLineageLength (internal) is the maximum lineage length of a Snapshot before Delta forces to build a Snapshot from scratch Default: 50 merge.maxInsertCount \u00b6 spark.databricks.delta.merge.maxInsertCount (internal) is the maximum row count of inserts in each MERGE execution Default: 10000L merge.optimizeInsertOnlyMerge.enabled \u00b6 spark.databricks.delta.merge.optimizeInsertOnlyMerge.enabled (internal) controls merge without any matched clause (i.e., insert-only merge) will be optimized by avoiding rewriting old files and just inserting new files Default: true merge.optimizeMatchedOnlyMerge.enabled \u00b6 spark.databricks.delta.merge.optimizeMatchedOnlyMerge.enabled (internal) controls merge without 'when not matched' clause will be optimized to use a right outer join instead of a full outer join Default: true merge.repartitionBeforeWrite.enabled \u00b6 spark.databricks.delta.merge.repartitionBeforeWrite.enabled (internal) controls merge will repartition the output by the table's partition columns before writing the files Default: false partitionColumnValidity.enabled \u00b6 spark.databricks.delta.partitionColumnValidity.enabled (internal) enables validation of the partition column names (just like the data columns) Default: true properties.defaults.minReaderVersion \u00b6 spark.databricks.delta.properties.defaults.minReaderVersion is the default reader protocol version to create new tables with, unless a feature that requires a higher version for correctness is enabled. Default: 1 Available values: 1 Used when: Protocol utility is used to create a Protocol properties.defaults.minWriterVersion \u00b6 spark.databricks.delta.properties.defaults.minWriterVersion is the default writer protocol version to create new tables with, unless a feature that requires a higher version for correctness is enabled. Default: 2 Available values: 1 , 2 , 3 Used when: Protocol utility is used to create a Protocol retentionDurationCheck.enabled \u00b6 spark.databricks.delta.retentionDurationCheck.enabled adds a check preventing users from running vacuum with a very short retention period, which may end up corrupting a Delta log. Default: true sampling.enabled \u00b6 spark.databricks.delta.sampling.enabled (internal) enables sample-based estimation Default: false schema.autoMerge.enabled \u00b6 spark.databricks.delta.schema.autoMerge.enabled enables schema merging on appends and overwrites. Default: false Equivalent DataFrame option: mergeSchema snapshotIsolation.enabled \u00b6 spark.databricks.delta.snapshotIsolation.enabled (internal) controls whether queries on Delta tables are guaranteed to have snapshot isolation Default: true snapshotPartitions \u00b6 spark.databricks.delta.snapshotPartitions (internal) is the number of partitions to use for state reconstruction (when building a snapshot of a Delta table). Default: 50 stalenessLimit \u00b6 spark.databricks.delta.stalenessLimit (in millis) allows you to query the last loaded state of the Delta table without blocking on a table update. You can use this configuration to reduce the latency on queries when up-to-date results are not a requirement. Table updates will be scheduled on a separate scheduler pool in a FIFO queue, and will share cluster resources fairly with your query. If a table hasn't updated past this time limit, we will block on a synchronous state update before running the query. Default: 0 (no tables can be stale) state.corruptionIsFatal \u00b6 spark.databricks.delta.state.corruptionIsFatal (internal) throws a fatal error when the recreated Delta State doesn't match committed checksum file Default: true stateReconstructionValidation.enabled \u00b6 spark.databricks.delta.stateReconstructionValidation.enabled (internal) controls whether to perform validation checks on the reconstructed state Default: true stats.collect \u00b6 spark.databricks.delta.stats.collect (internal) enables statistics to be collected while writing files into a Delta table Default: true stats.limitPushdown.enabled \u00b6 spark.databricks.delta.stats.limitPushdown.enabled (internal) enables using the limit clause and file statistics to prune files before they are collected to the driver Default: true stats.localCache.maxNumFiles \u00b6 spark.databricks.delta.stats.localCache.maxNumFiles (internal) is the maximum number of files for a table to be considered a delta small table . Some metadata operations (such as using data skipping) are optimized for small tables using driver local caching and local execution. Default: 2000 stats.skipping \u00b6 spark.databricks.delta.stats.skipping (internal) enables statistics used for skipping Default: true timeTravel.resolveOnIdentifier.enabled \u00b6 spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled (internal) controls whether to resolve patterns as @v123 in identifiers as time travel nodes. Default: true","title":"Configuration Properties"},{"location":"DeltaSQLConf/#deltasqlconf-sparkdatabricksdelta-configuration-properties","text":"DeltaSQLConf contains spark.databricks.delta -prefixed configuration properties to configure behaviour of Delta Lake.","title":"DeltaSQLConf &mdash; spark.databricks.delta Configuration Properties"},{"location":"DeltaSQLConf/#alterlocationbypassschemacheck","text":"spark.databricks.delta.alterLocation.bypassSchemaCheck enables Alter Table Set Location on Delta to go through even if the Delta table in the new location has a different schema from the original Delta table Default: false","title":" alterLocation.bypassSchemaCheck"},{"location":"DeltaSQLConf/#checklatestschemaonread","text":"spark.databricks.delta.checkLatestSchemaOnRead (internal) enables a check that ensures that users won't read corrupt data if the source schema changes in an incompatible way. Default: true In Delta, we always try to give users the latest version of their data without having to call REFRESH TABLE or redefine their DataFrames when used in the context of streaming. There is a possibility that the schema of the latest version of the table may be incompatible with the schema at the time of DataFrame creation.","title":" checkLatestSchemaOnRead"},{"location":"DeltaSQLConf/#checkpointpartsize","text":"spark.databricks.delta.checkpoint.partSize (internal) is the limit at which we will start parallelizing the checkpoint. We will attempt to write maximum of this many actions per checkpoint. Default: 5000000","title":" checkpoint.partSize"},{"location":"DeltaSQLConf/#commitinfoenabled","text":"spark.databricks.delta.commitInfo.enabled controls whether to log commit information into a Delta log . Default: true","title":" commitInfo.enabled"},{"location":"DeltaSQLConf/#commitinfousermetadata","text":"spark.databricks.delta.commitInfo.userMetadata is an arbitrary user-defined metadata to include in CommitInfo (requires spark.databricks.delta.commitInfo.enabled ). Default: (empty)","title":" commitInfo.userMetadata"},{"location":"DeltaSQLConf/#commitvalidationenabled","text":"spark.databricks.delta.commitValidation.enabled (internal) controls whether to perform validation checks before commit or not Default: true","title":" commitValidation.enabled"},{"location":"DeltaSQLConf/#convertmetadatacheckenabled","text":"spark.databricks.delta.convert.metadataCheck.enabled enables validation during convert to delta, if there is a difference between the catalog table's properties and the Delta table's configuration, we should error. If disabled, merge the two configurations with the same semantics as update and merge Default: true","title":" convert.metadataCheck.enabled"},{"location":"DeltaSQLConf/#dummyfilemanagernumoffiles","text":"spark.databricks.delta.dummyFileManager.numOfFiles (internal) controls how many dummy files to write in DummyFileManager Default: 3","title":" dummyFileManager.numOfFiles"},{"location":"DeltaSQLConf/#dummyfilemanagerprefix","text":"spark.databricks.delta.dummyFileManager.prefix (internal) is the file prefix to use in DummyFileManager Default: .s3-optimization-","title":" dummyFileManager.prefix"},{"location":"DeltaSQLConf/#historymaxkeysperlist","text":"spark.databricks.delta.history.maxKeysPerList (internal) controls how many commits to list when performing a parallel search. The default is the maximum keys returned by S3 per list call. Azure can return 5000, therefore we choose 1000. Default: 1000","title":" history.maxKeysPerList"},{"location":"DeltaSQLConf/#historymetricsenabled","text":"spark.databricks.delta.history.metricsEnabled enables Metrics reporting in Describe History. CommitInfo will now record the Operation Metrics. Default: true Used when: OptimisticTransactionImpl is requested to getOperationMetrics ConvertToDeltaCommand is requested to streamWrite SQLMetricsReporting is requested to registerSQLMetrics TransactionalWrite is requested to writeFiles","title":" history.metricsEnabled"},{"location":"DeltaSQLConf/#importbatchsizeschemainference","text":"spark.databricks.delta.import.batchSize.schemaInference (internal) is the number of files per batch for schema inference during import . Default: 1000000","title":" import.batchSize.schemaInference"},{"location":"DeltaSQLConf/#importbatchsizestatscollection","text":"spark.databricks.delta.import.batchSize.statsCollection (internal) is the number of files per batch for stats collection during import . Default: 50000","title":" import.batchSize.statsCollection"},{"location":"DeltaSQLConf/#maxsnapshotlineagelength","text":"spark.databricks.delta.maxSnapshotLineageLength (internal) is the maximum lineage length of a Snapshot before Delta forces to build a Snapshot from scratch Default: 50","title":" maxSnapshotLineageLength"},{"location":"DeltaSQLConf/#mergemaxinsertcount","text":"spark.databricks.delta.merge.maxInsertCount (internal) is the maximum row count of inserts in each MERGE execution Default: 10000L","title":" merge.maxInsertCount"},{"location":"DeltaSQLConf/#mergeoptimizeinsertonlymergeenabled","text":"spark.databricks.delta.merge.optimizeInsertOnlyMerge.enabled (internal) controls merge without any matched clause (i.e., insert-only merge) will be optimized by avoiding rewriting old files and just inserting new files Default: true","title":" merge.optimizeInsertOnlyMerge.enabled"},{"location":"DeltaSQLConf/#mergeoptimizematchedonlymergeenabled","text":"spark.databricks.delta.merge.optimizeMatchedOnlyMerge.enabled (internal) controls merge without 'when not matched' clause will be optimized to use a right outer join instead of a full outer join Default: true","title":" merge.optimizeMatchedOnlyMerge.enabled"},{"location":"DeltaSQLConf/#mergerepartitionbeforewriteenabled","text":"spark.databricks.delta.merge.repartitionBeforeWrite.enabled (internal) controls merge will repartition the output by the table's partition columns before writing the files Default: false","title":" merge.repartitionBeforeWrite.enabled"},{"location":"DeltaSQLConf/#partitioncolumnvalidityenabled","text":"spark.databricks.delta.partitionColumnValidity.enabled (internal) enables validation of the partition column names (just like the data columns) Default: true","title":" partitionColumnValidity.enabled"},{"location":"DeltaSQLConf/#propertiesdefaultsminreaderversion","text":"spark.databricks.delta.properties.defaults.minReaderVersion is the default reader protocol version to create new tables with, unless a feature that requires a higher version for correctness is enabled. Default: 1 Available values: 1 Used when: Protocol utility is used to create a Protocol","title":" properties.defaults.minReaderVersion"},{"location":"DeltaSQLConf/#propertiesdefaultsminwriterversion","text":"spark.databricks.delta.properties.defaults.minWriterVersion is the default writer protocol version to create new tables with, unless a feature that requires a higher version for correctness is enabled. Default: 2 Available values: 1 , 2 , 3 Used when: Protocol utility is used to create a Protocol","title":" properties.defaults.minWriterVersion"},{"location":"DeltaSQLConf/#retentiondurationcheckenabled","text":"spark.databricks.delta.retentionDurationCheck.enabled adds a check preventing users from running vacuum with a very short retention period, which may end up corrupting a Delta log. Default: true","title":" retentionDurationCheck.enabled"},{"location":"DeltaSQLConf/#samplingenabled","text":"spark.databricks.delta.sampling.enabled (internal) enables sample-based estimation Default: false","title":" sampling.enabled"},{"location":"DeltaSQLConf/#schemaautomergeenabled","text":"spark.databricks.delta.schema.autoMerge.enabled enables schema merging on appends and overwrites. Default: false Equivalent DataFrame option: mergeSchema","title":" schema.autoMerge.enabled"},{"location":"DeltaSQLConf/#snapshotisolationenabled","text":"spark.databricks.delta.snapshotIsolation.enabled (internal) controls whether queries on Delta tables are guaranteed to have snapshot isolation Default: true","title":" snapshotIsolation.enabled"},{"location":"DeltaSQLConf/#snapshotpartitions","text":"spark.databricks.delta.snapshotPartitions (internal) is the number of partitions to use for state reconstruction (when building a snapshot of a Delta table). Default: 50","title":" snapshotPartitions"},{"location":"DeltaSQLConf/#stalenesslimit","text":"spark.databricks.delta.stalenessLimit (in millis) allows you to query the last loaded state of the Delta table without blocking on a table update. You can use this configuration to reduce the latency on queries when up-to-date results are not a requirement. Table updates will be scheduled on a separate scheduler pool in a FIFO queue, and will share cluster resources fairly with your query. If a table hasn't updated past this time limit, we will block on a synchronous state update before running the query. Default: 0 (no tables can be stale)","title":" stalenessLimit"},{"location":"DeltaSQLConf/#statecorruptionisfatal","text":"spark.databricks.delta.state.corruptionIsFatal (internal) throws a fatal error when the recreated Delta State doesn't match committed checksum file Default: true","title":" state.corruptionIsFatal"},{"location":"DeltaSQLConf/#statereconstructionvalidationenabled","text":"spark.databricks.delta.stateReconstructionValidation.enabled (internal) controls whether to perform validation checks on the reconstructed state Default: true","title":" stateReconstructionValidation.enabled"},{"location":"DeltaSQLConf/#statscollect","text":"spark.databricks.delta.stats.collect (internal) enables statistics to be collected while writing files into a Delta table Default: true","title":" stats.collect"},{"location":"DeltaSQLConf/#statslimitpushdownenabled","text":"spark.databricks.delta.stats.limitPushdown.enabled (internal) enables using the limit clause and file statistics to prune files before they are collected to the driver Default: true","title":" stats.limitPushdown.enabled"},{"location":"DeltaSQLConf/#statslocalcachemaxnumfiles","text":"spark.databricks.delta.stats.localCache.maxNumFiles (internal) is the maximum number of files for a table to be considered a delta small table . Some metadata operations (such as using data skipping) are optimized for small tables using driver local caching and local execution. Default: 2000","title":" stats.localCache.maxNumFiles"},{"location":"DeltaSQLConf/#statsskipping","text":"spark.databricks.delta.stats.skipping (internal) enables statistics used for skipping Default: true","title":" stats.skipping"},{"location":"DeltaSQLConf/#timetravelresolveonidentifierenabled","text":"spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled (internal) controls whether to resolve patterns as @v123 in identifiers as time travel nodes. Default: true","title":" timeTravel.resolveOnIdentifier.enabled"},{"location":"DeltaSink/","text":"DeltaSink \u00b6 DeltaSink is the Sink ( Spark Structured Streaming ) of the delta data source for streaming queries. Creating Instance \u00b6 DeltaSink takes the following to be created: SQLContext Hadoop Path of the delta table (to write data to as configured by the path option) Names of the partition columns OutputMode ( Spark Structured Streaming ) DeltaOptions DeltaSink is created when: DeltaDataSource is requested for a streaming sink DeltaLog \u00b6 deltaLog : DeltaLog deltaLog is a DeltaLog that is created for the delta table when DeltaSink is created . deltaLog is used when: DeltaSink is requested to add a streaming micro-batch Adding Streaming Micro-Batch \u00b6 addBatch ( batchId : Long , data : DataFrame ) : Unit addBatch is part of the Sink ( Spark Structured Streaming ) abstraction. addBatch requests the DeltaLog to start a new transaction . addBatch ...FIXME In the end, addBatch requests the OptimisticTransaction to commit . Text Representation \u00b6 toString () : String DeltaSink uses the following text representation (with the path ): DeltaSink[path] ImplicitMetadataOperation \u00b6 DeltaSink is an ImplicitMetadataOperation .","title":"DeltaSink"},{"location":"DeltaSink/#deltasink","text":"DeltaSink is the Sink ( Spark Structured Streaming ) of the delta data source for streaming queries.","title":"DeltaSink"},{"location":"DeltaSink/#creating-instance","text":"DeltaSink takes the following to be created: SQLContext Hadoop Path of the delta table (to write data to as configured by the path option) Names of the partition columns OutputMode ( Spark Structured Streaming ) DeltaOptions DeltaSink is created when: DeltaDataSource is requested for a streaming sink","title":"Creating Instance"},{"location":"DeltaSink/#deltalog","text":"deltaLog : DeltaLog deltaLog is a DeltaLog that is created for the delta table when DeltaSink is created . deltaLog is used when: DeltaSink is requested to add a streaming micro-batch","title":" DeltaLog"},{"location":"DeltaSink/#adding-streaming-micro-batch","text":"addBatch ( batchId : Long , data : DataFrame ) : Unit addBatch is part of the Sink ( Spark Structured Streaming ) abstraction. addBatch requests the DeltaLog to start a new transaction . addBatch ...FIXME In the end, addBatch requests the OptimisticTransaction to commit .","title":" Adding Streaming Micro-Batch"},{"location":"DeltaSink/#text-representation","text":"toString () : String DeltaSink uses the following text representation (with the path ): DeltaSink[path]","title":" Text Representation"},{"location":"DeltaSink/#implicitmetadataoperation","text":"DeltaSink is an ImplicitMetadataOperation .","title":" ImplicitMetadataOperation"},{"location":"DeltaSource/","text":"DeltaSource \u00b6 DeltaSource is the Source ( Spark Structured Streaming ) of the delta data source for streaming queries. Creating Instance \u00b6 DeltaSource takes the following to be created: SparkSession ( Spark SQL ) DeltaLog DeltaOptions Filters (default: empty) DeltaSource is created when: DeltaDataSource is requested for a streaming source Demo \u00b6 val q = spark .readStream // Creating a streaming query .format(\"delta\") // Using delta data source .load(\"/tmp/delta/users\") // Over data in a delta table .writeStream .format(\"memory\") .option(\"queryName\", \"demo\") .start import org.apache.spark.sql.execution.streaming.{MicroBatchExecution, StreamingQueryWrapper} val plan = q.asInstanceOf[StreamingQueryWrapper] .streamingQuery .asInstanceOf[MicroBatchExecution] .logicalPlan import org.apache.spark.sql.execution.streaming.StreamingExecutionRelation val relation = plan.collect { case r: StreamingExecutionRelation => r }.head import org.apache.spark.sql.delta.sources.DeltaSource assert(relation.source.asInstanceOf[DeltaSource]) scala> println(relation.source) DeltaSource[file:/tmp/delta/users] Micro-Batch With Data Between Start And End Offsets \u00b6 getBatch ( start : Option [ Offset ], end : Offset ) : DataFrame getBatch is part of the Source ( Spark Structured Streaming ) abstraction. getBatch creates an DeltaSourceOffset for the tableId (aka reservoirId ) and the given end offset. getBatch gets the changes ...FIXME Latest Available Offset \u00b6 getOffset : Option [ Offset ] getOffset is part of the Source ( Spark Structured Streaming ) abstraction. getOffset calculates the latest offset (that a streaming query can use for the data of the next micro-batch) based on the ending offset of the last micro-batch . For no ending offset of the latest micro-batch , getOffset simply retrieves the starting offset (based on the latest version of the delta table). When the ending offset of the latest micro-batch is defined (which means that the DeltaSource is requested for another micro-batch), getOffset takes the last indexed AddFile from getChangesWithRateLimit for the previous ending offset . getOffset returns the previous ending offset when the last element was not available. With the ending offset of the latest micro-batch and the last indexed AddFile both available, getOffset creates a new DeltaSourceOffset for the version, index, and isLast flag from the last indexed AddFile . Note isStartingVersion local value is enabled ( true ) when the following holds: Version of the last indexed AddFile is equal to the reservoirVersion of the previous ending offset isStartingVersion flag of the previous ending offset is enabled ( true ) In the end, getOffset prints out the following DEBUG message to the logs (using the previousOffset internal registry): previousOffset -> currentOffset: [previousOffset] -> [currentOffset] Retrieving Starting Offset \u00b6 getStartingOffset () : Option [ Offset ] getStartingOffset requests the DeltaLog for the version of the delta table (by requesting for the current state (snapshot) and then for the version ). getStartingOffset takes the last file from the files added (with rate limit) for the version of the delta table, -1L as the fromIndex , and the isStartingVersion flag enabled ( true ). getStartingOffset returns a new DeltaSourceOffset for the tableId , the version and the index of the last file added, and whether the last file added is the last file of its version. getStartingOffset returns None ( offset not available ) when either happens: the version of the delta table is negative (below 0 ) no files were added in the version getStartingOffset throws an AssertionError when the version of the last file added is smaller than the delta table's version: assertion failed: getChangesWithRateLimit returns an invalid version: [v] (expected: >= [version]) Stopping \u00b6 stop () : Unit stop is part of the Source ( Spark Structured Streaming ) abstraction. stop simply cleanUpSnapshotResources . Retrieving Last Element From Iterator \u00b6 iteratorLast [ T ]( iter : Iterator [ T ]) : Option [ T ] iteratorLast simply returns the last element in the given Iterator or None . iteratorLast is used when: DeltaSource is requested to getStartingOffset and getOffset DeltaSourceSnapshot \u00b6 DeltaSourceSnapshot Initially uninitialized ( null ). Changes (along with the initialStateVersion ) when DeltaSource is requested for the snapshot at a given version (only when the versions are different) Used when DeltaSource is requested for the snapshot at a given version Closed and dereferenced ( null ) when DeltaSource is requested to cleanUpSnapshotResources Initial Version \u00b6 Version of the delta table Initially -1L and changes (along with the initialState ) to the version requested when DeltaSource is requested for the snapshot at a given version (only when the versions are different) Used when DeltaSource is requested to cleanUpSnapshotResources (and unpersist the current snapshot) Previous Offset \u00b6 Ending DeltaSourceOffset of the latest micro-batch Starts uninitialized ( null ). Used when DeltaSource is requested for the latest available offset . Table ID \u00b6 Table ID Retrieving File Additions (With Rate Limit) \u00b6 getChangesWithRateLimit ( fromVersion : Long , fromIndex : Long , isStartingVersion : Boolean ) : Iterator [ IndexedFile ] getChangesWithRateLimit gets the changes (as indexed AddFile s) for the given fromVersion , fromIndex , and isStartingVersion flag. getChangesWithRateLimit is used when: DeltaSource is requested for the latest available offset Retrieving State of Delta Table at Given Version \u00b6 getSnapshotAt ( version : Long ) : Iterator [ IndexedFile ] getSnapshotAt requests the DeltaSourceSnapshot for the data files (as indexed AddFile s). In case the DeltaSourceSnapshot hasn't been initialized yet ( null ) or the requested version is different from the initialStateVersion , getSnapshotAt does the following: cleanUpSnapshotResources Requests the DeltaLog for the state (snapshot) of the delta table at the version Creates a new DeltaSourceSnapshot for the state (snapshot) as the current DeltaSourceSnapshot Changes the initialStateVersion internal registry to the requested version getSnapshotAt is used when: DeltaSource is requested to getChanges (with isStartingVersion flag enabled) getChanges \u00b6 getChanges ( fromVersion : Long , fromIndex : Long , isStartingVersion : Boolean ) : Iterator [ IndexedFile ] getChanges branches based on isStartingVersion flag (enabled or not): For isStartingVersion flag enabled ( true ), getChanges gets the state (snapshot) for the given fromVersion followed by (filtered out) indexed AddFiles for the next version after the given fromVersion For isStartingVersion flag disabled ( false ), getChanges simply gives (filtered out) indexed AddFiles for the given fromVersion Note isStartingVersion flag simply adds the state (snapshot) before (filtered out) indexed AddFiles when enabled ( true ). isStartingVersion flag is enabled when DeltaSource is requested for the following: Micro-batch with data between start and end offsets and the start offset is not given or is for the starting version Latest available offset with no end offset of the latest micro-batch or the end offset of the latest micro-batch for the starting version In the end, getChanges filters out ( excludes ) indexed AddFile s that are not with the version later than the given fromVersion or the index greater than the given fromIndex . getChanges is used when: DeltaSource is requested for the latest available offset (when requested for the files added (with rate limit) ) and getBatch filterAndIndexDeltaLogs \u00b6 filterAndIndexDeltaLogs ( startVersion : Long ) : Iterator [ IndexedFile ] filterAndIndexDeltaLogs ...FIXME Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.delta.sources.DeltaSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.sources.DeltaSource=ALL Refer to Logging .","title":"DeltaSource"},{"location":"DeltaSource/#deltasource","text":"DeltaSource is the Source ( Spark Structured Streaming ) of the delta data source for streaming queries.","title":"DeltaSource"},{"location":"DeltaSource/#creating-instance","text":"DeltaSource takes the following to be created: SparkSession ( Spark SQL ) DeltaLog DeltaOptions Filters (default: empty) DeltaSource is created when: DeltaDataSource is requested for a streaming source","title":"Creating Instance"},{"location":"DeltaSource/#demo","text":"val q = spark .readStream // Creating a streaming query .format(\"delta\") // Using delta data source .load(\"/tmp/delta/users\") // Over data in a delta table .writeStream .format(\"memory\") .option(\"queryName\", \"demo\") .start import org.apache.spark.sql.execution.streaming.{MicroBatchExecution, StreamingQueryWrapper} val plan = q.asInstanceOf[StreamingQueryWrapper] .streamingQuery .asInstanceOf[MicroBatchExecution] .logicalPlan import org.apache.spark.sql.execution.streaming.StreamingExecutionRelation val relation = plan.collect { case r: StreamingExecutionRelation => r }.head import org.apache.spark.sql.delta.sources.DeltaSource assert(relation.source.asInstanceOf[DeltaSource]) scala> println(relation.source) DeltaSource[file:/tmp/delta/users]","title":"Demo"},{"location":"DeltaSource/#micro-batch-with-data-between-start-and-end-offsets","text":"getBatch ( start : Option [ Offset ], end : Offset ) : DataFrame getBatch is part of the Source ( Spark Structured Streaming ) abstraction. getBatch creates an DeltaSourceOffset for the tableId (aka reservoirId ) and the given end offset. getBatch gets the changes ...FIXME","title":" Micro-Batch With Data Between Start And End Offsets"},{"location":"DeltaSource/#latest-available-offset","text":"getOffset : Option [ Offset ] getOffset is part of the Source ( Spark Structured Streaming ) abstraction. getOffset calculates the latest offset (that a streaming query can use for the data of the next micro-batch) based on the ending offset of the last micro-batch . For no ending offset of the latest micro-batch , getOffset simply retrieves the starting offset (based on the latest version of the delta table). When the ending offset of the latest micro-batch is defined (which means that the DeltaSource is requested for another micro-batch), getOffset takes the last indexed AddFile from getChangesWithRateLimit for the previous ending offset . getOffset returns the previous ending offset when the last element was not available. With the ending offset of the latest micro-batch and the last indexed AddFile both available, getOffset creates a new DeltaSourceOffset for the version, index, and isLast flag from the last indexed AddFile . Note isStartingVersion local value is enabled ( true ) when the following holds: Version of the last indexed AddFile is equal to the reservoirVersion of the previous ending offset isStartingVersion flag of the previous ending offset is enabled ( true ) In the end, getOffset prints out the following DEBUG message to the logs (using the previousOffset internal registry): previousOffset -> currentOffset: [previousOffset] -> [currentOffset]","title":" Latest Available Offset"},{"location":"DeltaSource/#retrieving-starting-offset","text":"getStartingOffset () : Option [ Offset ] getStartingOffset requests the DeltaLog for the version of the delta table (by requesting for the current state (snapshot) and then for the version ). getStartingOffset takes the last file from the files added (with rate limit) for the version of the delta table, -1L as the fromIndex , and the isStartingVersion flag enabled ( true ). getStartingOffset returns a new DeltaSourceOffset for the tableId , the version and the index of the last file added, and whether the last file added is the last file of its version. getStartingOffset returns None ( offset not available ) when either happens: the version of the delta table is negative (below 0 ) no files were added in the version getStartingOffset throws an AssertionError when the version of the last file added is smaller than the delta table's version: assertion failed: getChangesWithRateLimit returns an invalid version: [v] (expected: >= [version])","title":" Retrieving Starting Offset"},{"location":"DeltaSource/#stopping","text":"stop () : Unit stop is part of the Source ( Spark Structured Streaming ) abstraction. stop simply cleanUpSnapshotResources .","title":" Stopping"},{"location":"DeltaSource/#retrieving-last-element-from-iterator","text":"iteratorLast [ T ]( iter : Iterator [ T ]) : Option [ T ] iteratorLast simply returns the last element in the given Iterator or None . iteratorLast is used when: DeltaSource is requested to getStartingOffset and getOffset","title":" Retrieving Last Element From Iterator"},{"location":"DeltaSource/#deltasourcesnapshot","text":"DeltaSourceSnapshot Initially uninitialized ( null ). Changes (along with the initialStateVersion ) when DeltaSource is requested for the snapshot at a given version (only when the versions are different) Used when DeltaSource is requested for the snapshot at a given version Closed and dereferenced ( null ) when DeltaSource is requested to cleanUpSnapshotResources","title":" DeltaSourceSnapshot"},{"location":"DeltaSource/#initial-version","text":"Version of the delta table Initially -1L and changes (along with the initialState ) to the version requested when DeltaSource is requested for the snapshot at a given version (only when the versions are different) Used when DeltaSource is requested to cleanUpSnapshotResources (and unpersist the current snapshot)","title":" Initial Version"},{"location":"DeltaSource/#previous-offset","text":"Ending DeltaSourceOffset of the latest micro-batch Starts uninitialized ( null ). Used when DeltaSource is requested for the latest available offset .","title":" Previous Offset"},{"location":"DeltaSource/#table-id","text":"Table ID","title":" Table ID"},{"location":"DeltaSource/#retrieving-file-additions-with-rate-limit","text":"getChangesWithRateLimit ( fromVersion : Long , fromIndex : Long , isStartingVersion : Boolean ) : Iterator [ IndexedFile ] getChangesWithRateLimit gets the changes (as indexed AddFile s) for the given fromVersion , fromIndex , and isStartingVersion flag. getChangesWithRateLimit is used when: DeltaSource is requested for the latest available offset","title":" Retrieving File Additions (With Rate Limit)"},{"location":"DeltaSource/#retrieving-state-of-delta-table-at-given-version","text":"getSnapshotAt ( version : Long ) : Iterator [ IndexedFile ] getSnapshotAt requests the DeltaSourceSnapshot for the data files (as indexed AddFile s). In case the DeltaSourceSnapshot hasn't been initialized yet ( null ) or the requested version is different from the initialStateVersion , getSnapshotAt does the following: cleanUpSnapshotResources Requests the DeltaLog for the state (snapshot) of the delta table at the version Creates a new DeltaSourceSnapshot for the state (snapshot) as the current DeltaSourceSnapshot Changes the initialStateVersion internal registry to the requested version getSnapshotAt is used when: DeltaSource is requested to getChanges (with isStartingVersion flag enabled)","title":" Retrieving State of Delta Table at Given Version"},{"location":"DeltaSource/#getchanges","text":"getChanges ( fromVersion : Long , fromIndex : Long , isStartingVersion : Boolean ) : Iterator [ IndexedFile ] getChanges branches based on isStartingVersion flag (enabled or not): For isStartingVersion flag enabled ( true ), getChanges gets the state (snapshot) for the given fromVersion followed by (filtered out) indexed AddFiles for the next version after the given fromVersion For isStartingVersion flag disabled ( false ), getChanges simply gives (filtered out) indexed AddFiles for the given fromVersion Note isStartingVersion flag simply adds the state (snapshot) before (filtered out) indexed AddFiles when enabled ( true ). isStartingVersion flag is enabled when DeltaSource is requested for the following: Micro-batch with data between start and end offsets and the start offset is not given or is for the starting version Latest available offset with no end offset of the latest micro-batch or the end offset of the latest micro-batch for the starting version In the end, getChanges filters out ( excludes ) indexed AddFile s that are not with the version later than the given fromVersion or the index greater than the given fromIndex . getChanges is used when: DeltaSource is requested for the latest available offset (when requested for the files added (with rate limit) ) and getBatch","title":" getChanges"},{"location":"DeltaSource/#filterandindexdeltalogs","text":"filterAndIndexDeltaLogs ( startVersion : Long ) : Iterator [ IndexedFile ] filterAndIndexDeltaLogs ...FIXME","title":" filterAndIndexDeltaLogs"},{"location":"DeltaSource/#logging","text":"Enable ALL logging level for org.apache.spark.sql.delta.sources.DeltaSource logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.sources.DeltaSource=ALL Refer to Logging .","title":"Logging"},{"location":"DeltaSourceOffset/","text":"DeltaSourceOffset \u00b6 DeltaSourceOffset is a streaming Offset ( Spark Structured Streaming ) for DeltaSource . DeltaSourceOffset is < > (via < > utility) when DeltaSource is requested for the DeltaSource.md#getOffset[latest offset] and a DeltaSource.md#getBatch[batch (for the given starting and ending offsets)]. [[VERSION]] DeltaSourceOffset uses the version 1 . == [[creating-instance]] Creating DeltaSourceOffset Instance DeltaSourceOffset takes the following to be created: [[sourceVersion]] Source Version (always < >) [[reservoirId]] Reservoir ID (aka DeltaSource.md#tableId[Table ID]) [[reservoirVersion]] Reservoir Version [[index]] Index [[isStartingVersion]] isStartingVersion flag == [[apply]] Creating DeltaSourceOffset Instance -- apply Utility [source, scala] \u00b6 apply( reservoirId: String, reservoirVersion: Long, index: Long, isStartingVersion: Boolean): DeltaSourceOffset apply( reservoirId: String, offset: Offset): DeltaSourceOffset apply creates a new DeltaSourceOffset (for the < > and the given arguments) or converts the given Offset to a DeltaSourceOffset . NOTE: apply is used when DeltaSource is requested for the DeltaSource.md#getOffset[latest offset] and a DeltaSource.md#getBatch[batch (for the given starting and ending offsets)]. == [[json]] json Method [source, scala] \u00b6 json: String \u00b6 NOTE: json is part of the Offset contract to serialize an offset to JSON. json ...FIXME == [[validateSourceVersion]] validateSourceVersion Internal Utility [source, scala] \u00b6 validateSourceVersion( json: String): Unit validateSourceVersion ...FIXME NOTE: validateSourceVersion is used when...FIXME","title":"DeltaSourceOffset"},{"location":"DeltaSourceOffset/#deltasourceoffset","text":"DeltaSourceOffset is a streaming Offset ( Spark Structured Streaming ) for DeltaSource . DeltaSourceOffset is < > (via < > utility) when DeltaSource is requested for the DeltaSource.md#getOffset[latest offset] and a DeltaSource.md#getBatch[batch (for the given starting and ending offsets)]. [[VERSION]] DeltaSourceOffset uses the version 1 . == [[creating-instance]] Creating DeltaSourceOffset Instance DeltaSourceOffset takes the following to be created: [[sourceVersion]] Source Version (always < >) [[reservoirId]] Reservoir ID (aka DeltaSource.md#tableId[Table ID]) [[reservoirVersion]] Reservoir Version [[index]] Index [[isStartingVersion]] isStartingVersion flag == [[apply]] Creating DeltaSourceOffset Instance -- apply Utility","title":"DeltaSourceOffset"},{"location":"DeltaSourceOffset/#source-scala","text":"apply( reservoirId: String, reservoirVersion: Long, index: Long, isStartingVersion: Boolean): DeltaSourceOffset apply( reservoirId: String, offset: Offset): DeltaSourceOffset apply creates a new DeltaSourceOffset (for the < > and the given arguments) or converts the given Offset to a DeltaSourceOffset . NOTE: apply is used when DeltaSource is requested for the DeltaSource.md#getOffset[latest offset] and a DeltaSource.md#getBatch[batch (for the given starting and ending offsets)]. == [[json]] json Method","title":"[source, scala]"},{"location":"DeltaSourceOffset/#source-scala_1","text":"","title":"[source, scala]"},{"location":"DeltaSourceOffset/#json-string","text":"NOTE: json is part of the Offset contract to serialize an offset to JSON. json ...FIXME == [[validateSourceVersion]] validateSourceVersion Internal Utility","title":"json: String"},{"location":"DeltaSourceOffset/#source-scala_2","text":"validateSourceVersion( json: String): Unit validateSourceVersion ...FIXME NOTE: validateSourceVersion is used when...FIXME","title":"[source, scala]"},{"location":"DeltaSourceSnapshot/","text":"= DeltaSourceSnapshot [[SnapshotIterator]][[StateCache]] DeltaSourceSnapshot is a < > with < > DeltaSourceSnapshot is < > when DeltaSource is requested for the < >. [[version]] When < >, DeltaSourceSnapshot requests the < > for the < > that it uses for the < > (a new column and the name of the cached RDD). == [[creating-instance]] Creating DeltaSourceSnapshot Instance DeltaSourceSnapshot takes the following to be created: [[spark]] SparkSession [[snapshot]] < > [[filters]] Filter expressions ( Seq[Expression] ) == [[initialFiles]] Initial Files (Indexed AddFiles) -- initialFiles Method [source, scala] \u00b6 initialFiles: Dataset[IndexedFile] \u00b6 initialFiles requests the < > for < > ( Dataset[AddFile] ) and sorts them by < > and < > in ascending order. initialFiles zips the < > with indices (using RDD.zipWithIndex operator), adds two new columns with the < > and isLast as false , and finally creates a Dataset[IndexedFile] . In the end, initialFiles < > with the following name (with the < > and the < > of the < >) Delta Source Snapshot #[version] - [redactedPath] NOTE: initialFiles is used exclusively when SnapshotIterator is requested for a < >.","title":"DeltaSourceSnapshot"},{"location":"DeltaSourceSnapshot/#source-scala","text":"","title":"[source, scala]"},{"location":"DeltaSourceSnapshot/#initialfiles-datasetindexedfile","text":"initialFiles requests the < > for < > ( Dataset[AddFile] ) and sorts them by < > and < > in ascending order. initialFiles zips the < > with indices (using RDD.zipWithIndex operator), adds two new columns with the < > and isLast as false , and finally creates a Dataset[IndexedFile] . In the end, initialFiles < > with the following name (with the < > and the < > of the < >) Delta Source Snapshot #[version] - [redactedPath] NOTE: initialFiles is used exclusively when SnapshotIterator is requested for a < >.","title":"initialFiles: Dataset[IndexedFile]"},{"location":"DeltaSparkSessionExtension/","text":"DeltaSparkSessionExtension \u00b6 DeltaSparkSessionExtension is used to register ( inject ) the following extensions to a SparkSession : Delta SQL support (using DeltaSqlParser ) DeltaAnalysis logical resolution rule DeltaUnsupportedOperationsCheck PreprocessTableUpdate logical resolution rule PreprocessTableMerge logical resolution rule PreprocessTableDelete logical resolution rule ActiveOptimisticTransactionRule logical optimization rule DeltaSparkSessionExtension is registered using spark.sql.extensions configuration property (while creating a SparkSession in a Spark application).","title":"DeltaSparkSessionExtension"},{"location":"DeltaSparkSessionExtension/#deltasparksessionextension","text":"DeltaSparkSessionExtension is used to register ( inject ) the following extensions to a SparkSession : Delta SQL support (using DeltaSqlParser ) DeltaAnalysis logical resolution rule DeltaUnsupportedOperationsCheck PreprocessTableUpdate logical resolution rule PreprocessTableMerge logical resolution rule PreprocessTableDelete logical resolution rule ActiveOptimisticTransactionRule logical optimization rule DeltaSparkSessionExtension is registered using spark.sql.extensions configuration property (while creating a SparkSession in a Spark application).","title":"DeltaSparkSessionExtension"},{"location":"DeltaTable/","text":"DeltaTable \u00b6 DeltaTable is the management interface of a delta table. DeltaTable is created using utilities (e.g. DeltaTable.forName , DeltaTable.convertToDelta ). io.delta.tables Package \u00b6 DeltaTable belongs to io.delta.tables package. import io.delta.tables.DeltaTable DeltaLog \u00b6 deltaLog : DeltaLog deltaLog is a DeltaLog of the DeltaTableV2 . Utilities (Static Methods) \u00b6 convertToDelta \u00b6 convertToDelta ( spark : SparkSession , identifier : String ) : DeltaTable convertToDelta ( spark : SparkSession , identifier : String , partitionSchema : String ) : DeltaTable convertToDelta ( spark : SparkSession , identifier : String , partitionSchema : StructType ) : DeltaTable convertToDelta converts a parquet table to delta format (and makes the table available in Delta Lake). Note Refer to Demo: Converting Parquet Dataset Into Delta Format for a demo of DeltaTable.convertToDelta . Internally, convertToDelta requests the SparkSession for the SQL parser ( ParserInterface ) that is in turn requested to parse the given table identifier (to get a TableIdentifier ). Tip Read up on ParserInterface in The Internals of Spark SQL online book. In the end, convertToDelta uses the DeltaConvert utility to convert the parquet table to delta format and creates a DeltaTable . forName \u00b6 forName ( sparkSession : SparkSession , tableName : String ) : DeltaTable forName ( tableOrViewName : String ) : DeltaTable forName uses ParserInterface (of the given SparkSession ) to parse the given table name. forName checks whether the given table name is of a Delta table and, if so, creates a DeltaTable with the following: Dataset that represents loading data from the specified table name (using SparkSession.table operator) DeltaLog of the specified table forName throws an AnalysisException when the given table name is for non-Delta table: [deltaTableIdentifier] is not a Delta table. forName is used internally when DeltaConvert utility is used to executeConvert . forPath \u00b6 forPath ( sparkSession : SparkSession , path : String ) : DeltaTable forPath ( path : String ) : DeltaTable forPath creates a DeltaTable instance for data in the given directory ( path ) when the given directory is part of a delta table already (as the root or a child directory). assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession]) val tableId = \"/tmp/delta-table/users\" import io.delta.tables.DeltaTable assert(DeltaTable.isDeltaTable(tableId), s\"$tableId should be a Delta table\") val dt = DeltaTable.forPath(\"delta-table\") forPath throws an AnalysisException when the given path does not belong to a delta table: [deltaTableIdentifier] is not a Delta table. Internally, forPath creates a new DeltaTable with the following: Dataset that represents loading data from the specified path using delta data source DeltaLog for the (transaction log in) the specified path forPath is used internally in DeltaTable.convertToDelta (via DeltaConvert utility). isDeltaTable \u00b6 isDeltaTable ( sparkSession : SparkSession , identifier : String ) : Boolean isDeltaTable ( identifier : String ) : Boolean isDeltaTable checks whether the provided identifier string is a file path that points to the root of a Delta table or one of the subdirectories. Internally, isDeltaTable simply relays to DeltaTableUtils.isDeltaTable utility. Creating Instance \u00b6 DeltaTable takes the following to be created: Table Data ( Dataset[Row] ) DeltaTableV2 DeltaTable is created using DeltaTable.forPath or DeltaTable.forName utilities. Operators \u00b6 alias \u00b6 alias ( alias : String ) : DeltaTable Applies an alias to the DeltaTable (equivalent to as ) as \u00b6 as ( alias : String ) : DeltaTable Applies an alias to the DeltaTable delete \u00b6 delete () : Unit delete ( condition : Column ) : Unit delete ( condition : String ) : Unit Deletes data from the DeltaTable that matches the given condition . generate \u00b6 generate ( mode : String ) : Unit Generates a manifest for the delta table generate executes the DeltaGenerateCommand with the table ID of the format delta.`path` (where the path is the data directory of the DeltaLog ) and the given mode. history \u00b6 history () : DataFrame history ( limit : Int ) : DataFrame Gets available commits ( history ) of the DeltaTable merge \u00b6 merge ( source : DataFrame , condition : Column ) : DeltaMergeBuilder merge ( source : DataFrame , condition : String ) : DeltaMergeBuilder Creates a DeltaMergeBuilder toDF \u00b6 toDF : Dataset [ Row ] Returns the DataFrame representation of the DeltaTable update \u00b6 update ( condition : Column , set : Map [ String , Column ]) : Unit update ( set : Map [ String , Column ]) : Unit Updates data in the DeltaTable on the rows that match the given condition based on the rules defined by set updateExpr \u00b6 updateExpr ( set : Map [ String , String ]) : Unit updateExpr ( condition : String , set : Map [ String , String ]) : Unit Updates data in the DeltaTable on the rows that match the given condition based on the rules defined by set upgradeTableProtocol \u00b6 upgradeTableProtocol ( readerVersion : Int , writerVersion : Int ) : Unit Updates the protocol version of the table to leverage new features. Upgrading the reader version will prevent all clients that have an older version of Delta Lake from accessing this table. Upgrading the writer version will prevent older versions of Delta Lake to write to this table. The reader or writer version cannot be downgraded. Internally, upgradeTableProtocol creates a new Protocol (with the given versions) and requests the DeltaLog to upgradeProtocol . [SC-44271][DELTA] Introduce default protocol version for Delta tables upgradeTableProtocol was introduced in [SC-44271][DELTA] Introduce default protocol version for Delta tables commit. vacuum \u00b6 vacuum () : DataFrame vacuum ( retentionHours : Double ) : DataFrame Deletes files and directories (recursively) in the DeltaTable that are not needed by the table (and maintains older versions up to the given retention threshold). vacuum executes vacuum command .","title":"DeltaTable"},{"location":"DeltaTable/#deltatable","text":"DeltaTable is the management interface of a delta table. DeltaTable is created using utilities (e.g. DeltaTable.forName , DeltaTable.convertToDelta ).","title":"DeltaTable"},{"location":"DeltaTable/#iodeltatables-package","text":"DeltaTable belongs to io.delta.tables package. import io.delta.tables.DeltaTable","title":"io.delta.tables Package"},{"location":"DeltaTable/#deltalog","text":"deltaLog : DeltaLog deltaLog is a DeltaLog of the DeltaTableV2 .","title":" DeltaLog"},{"location":"DeltaTable/#utilities-static-methods","text":"","title":" Utilities (Static Methods)"},{"location":"DeltaTable/#converttodelta","text":"convertToDelta ( spark : SparkSession , identifier : String ) : DeltaTable convertToDelta ( spark : SparkSession , identifier : String , partitionSchema : String ) : DeltaTable convertToDelta ( spark : SparkSession , identifier : String , partitionSchema : StructType ) : DeltaTable convertToDelta converts a parquet table to delta format (and makes the table available in Delta Lake). Note Refer to Demo: Converting Parquet Dataset Into Delta Format for a demo of DeltaTable.convertToDelta . Internally, convertToDelta requests the SparkSession for the SQL parser ( ParserInterface ) that is in turn requested to parse the given table identifier (to get a TableIdentifier ). Tip Read up on ParserInterface in The Internals of Spark SQL online book. In the end, convertToDelta uses the DeltaConvert utility to convert the parquet table to delta format and creates a DeltaTable .","title":" convertToDelta"},{"location":"DeltaTable/#forname","text":"forName ( sparkSession : SparkSession , tableName : String ) : DeltaTable forName ( tableOrViewName : String ) : DeltaTable forName uses ParserInterface (of the given SparkSession ) to parse the given table name. forName checks whether the given table name is of a Delta table and, if so, creates a DeltaTable with the following: Dataset that represents loading data from the specified table name (using SparkSession.table operator) DeltaLog of the specified table forName throws an AnalysisException when the given table name is for non-Delta table: [deltaTableIdentifier] is not a Delta table. forName is used internally when DeltaConvert utility is used to executeConvert .","title":" forName"},{"location":"DeltaTable/#forpath","text":"forPath ( sparkSession : SparkSession , path : String ) : DeltaTable forPath ( path : String ) : DeltaTable forPath creates a DeltaTable instance for data in the given directory ( path ) when the given directory is part of a delta table already (as the root or a child directory). assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession]) val tableId = \"/tmp/delta-table/users\" import io.delta.tables.DeltaTable assert(DeltaTable.isDeltaTable(tableId), s\"$tableId should be a Delta table\") val dt = DeltaTable.forPath(\"delta-table\") forPath throws an AnalysisException when the given path does not belong to a delta table: [deltaTableIdentifier] is not a Delta table. Internally, forPath creates a new DeltaTable with the following: Dataset that represents loading data from the specified path using delta data source DeltaLog for the (transaction log in) the specified path forPath is used internally in DeltaTable.convertToDelta (via DeltaConvert utility).","title":" forPath"},{"location":"DeltaTable/#isdeltatable","text":"isDeltaTable ( sparkSession : SparkSession , identifier : String ) : Boolean isDeltaTable ( identifier : String ) : Boolean isDeltaTable checks whether the provided identifier string is a file path that points to the root of a Delta table or one of the subdirectories. Internally, isDeltaTable simply relays to DeltaTableUtils.isDeltaTable utility.","title":" isDeltaTable"},{"location":"DeltaTable/#creating-instance","text":"DeltaTable takes the following to be created: Table Data ( Dataset[Row] ) DeltaTableV2 DeltaTable is created using DeltaTable.forPath or DeltaTable.forName utilities.","title":"Creating Instance"},{"location":"DeltaTable/#operators","text":"","title":"Operators"},{"location":"DeltaTable/#alias","text":"alias ( alias : String ) : DeltaTable Applies an alias to the DeltaTable (equivalent to as )","title":" alias"},{"location":"DeltaTable/#as","text":"as ( alias : String ) : DeltaTable Applies an alias to the DeltaTable","title":" as"},{"location":"DeltaTable/#delete","text":"delete () : Unit delete ( condition : Column ) : Unit delete ( condition : String ) : Unit Deletes data from the DeltaTable that matches the given condition .","title":" delete"},{"location":"DeltaTable/#generate","text":"generate ( mode : String ) : Unit Generates a manifest for the delta table generate executes the DeltaGenerateCommand with the table ID of the format delta.`path` (where the path is the data directory of the DeltaLog ) and the given mode.","title":" generate"},{"location":"DeltaTable/#history","text":"history () : DataFrame history ( limit : Int ) : DataFrame Gets available commits ( history ) of the DeltaTable","title":" history"},{"location":"DeltaTable/#merge","text":"merge ( source : DataFrame , condition : Column ) : DeltaMergeBuilder merge ( source : DataFrame , condition : String ) : DeltaMergeBuilder Creates a DeltaMergeBuilder","title":" merge"},{"location":"DeltaTable/#todf","text":"toDF : Dataset [ Row ] Returns the DataFrame representation of the DeltaTable","title":" toDF"},{"location":"DeltaTable/#update","text":"update ( condition : Column , set : Map [ String , Column ]) : Unit update ( set : Map [ String , Column ]) : Unit Updates data in the DeltaTable on the rows that match the given condition based on the rules defined by set","title":" update"},{"location":"DeltaTable/#updateexpr","text":"updateExpr ( set : Map [ String , String ]) : Unit updateExpr ( condition : String , set : Map [ String , String ]) : Unit Updates data in the DeltaTable on the rows that match the given condition based on the rules defined by set","title":" updateExpr"},{"location":"DeltaTable/#upgradetableprotocol","text":"upgradeTableProtocol ( readerVersion : Int , writerVersion : Int ) : Unit Updates the protocol version of the table to leverage new features. Upgrading the reader version will prevent all clients that have an older version of Delta Lake from accessing this table. Upgrading the writer version will prevent older versions of Delta Lake to write to this table. The reader or writer version cannot be downgraded. Internally, upgradeTableProtocol creates a new Protocol (with the given versions) and requests the DeltaLog to upgradeProtocol . [SC-44271][DELTA] Introduce default protocol version for Delta tables upgradeTableProtocol was introduced in [SC-44271][DELTA] Introduce default protocol version for Delta tables commit.","title":" upgradeTableProtocol"},{"location":"DeltaTable/#vacuum","text":"vacuum () : DataFrame vacuum ( retentionHours : Double ) : DataFrame Deletes files and directories (recursively) in the DeltaTable that are not needed by the table (and maintains older versions up to the given retention threshold). vacuum executes vacuum command .","title":" vacuum"},{"location":"DeltaTableIdentifier/","text":"DeltaTableIdentifier \u00b6 DeltaTableIdentifier is an identifier of a delta table by directory or TableIdentifier depending whether it is a catalog table or not (and living non-cataloged). Creating Instance \u00b6 DeltaTableIdentifier takes the following to be created: Directory (default: undefined) TableIdentifier (default: undefined) Creating DeltaTableIdentifier \u00b6 apply ( spark : SparkSession , identifier : TableIdentifier ) : Option [ DeltaTableIdentifier ] apply creates a new DeltaTableIdentifier for the given TableIdentifier if the specified table identifier represents a Delta table or None . apply is used when: VacuumTableCommand , DeltaGenerateCommand , DescribeDeltaDetailCommand and DescribeDeltaHistoryCommand are executed Creating DeltaLog \u00b6 getDeltaLog ( spark : SparkSession ) : DeltaLog getDeltaLog creates a DeltaLog (for the location ). Note getDeltaLog does not seem to be used. Location Path \u00b6 getPath ( spark : SparkSession ) : Path getPath creates a Hadoop Path ) for the path if defined or requests SessionCatalog ( Spark SQL ) for the table metadata and uses the locationUri .","title":"DeltaTableIdentifier"},{"location":"DeltaTableIdentifier/#deltatableidentifier","text":"DeltaTableIdentifier is an identifier of a delta table by directory or TableIdentifier depending whether it is a catalog table or not (and living non-cataloged).","title":"DeltaTableIdentifier"},{"location":"DeltaTableIdentifier/#creating-instance","text":"DeltaTableIdentifier takes the following to be created: Directory (default: undefined) TableIdentifier (default: undefined)","title":"Creating Instance"},{"location":"DeltaTableIdentifier/#creating-deltatableidentifier","text":"apply ( spark : SparkSession , identifier : TableIdentifier ) : Option [ DeltaTableIdentifier ] apply creates a new DeltaTableIdentifier for the given TableIdentifier if the specified table identifier represents a Delta table or None . apply is used when: VacuumTableCommand , DeltaGenerateCommand , DescribeDeltaDetailCommand and DescribeDeltaHistoryCommand are executed","title":" Creating DeltaTableIdentifier"},{"location":"DeltaTableIdentifier/#creating-deltalog","text":"getDeltaLog ( spark : SparkSession ) : DeltaLog getDeltaLog creates a DeltaLog (for the location ). Note getDeltaLog does not seem to be used.","title":" Creating DeltaLog"},{"location":"DeltaTableIdentifier/#location-path","text":"getPath ( spark : SparkSession ) : Path getPath creates a Hadoop Path ) for the path if defined or requests SessionCatalog ( Spark SQL ) for the table metadata and uses the locationUri .","title":" Location Path"},{"location":"DeltaTableOperations/","text":"DeltaTableOperations \u2014 Delta DML Operations \u00b6 DeltaTableOperations is an abstraction of management services for executing delete , generate , history , update , and vacuum operations ( commands ). DeltaTableOperations is assumed to be associated with a DeltaTable . Implementations \u00b6 DeltaTable Executing DeleteFromTable Command \u00b6 executeDelete ( condition : Option [ Expression ]) : Unit executeDelete creates a DataFrame for a DeleteFromTable logical operator (from Spark SQL) with (the analyzed logical plan of the DeltaTable and the given condition ). Note DeleteFromTable is a Command (Spark SQL) that represents DELETE FROM SQL statement for v2 tables. As a Command it is executed eagerly. executeDelete is used for DeltaTable.delete operator. Executing DeltaGenerateCommand Command \u00b6 executeGenerate ( tblIdentifier : String , mode : String ) : Unit executeGenerate requests the SQL parser (of the SparkSession ) to parse the given table identifier, creates a DeltaGenerateCommand and runs it. executeGenerate is used for DeltaTable.generate operator. Executing History Command \u00b6 executeHistory ( deltaLog : DeltaLog , limit : Option [ Int ]) : DataFrame executeHistory creates DeltaHistoryManager (for the given DeltaLog ) and requests it for the number of commits to match the limit . In the end, executeHistory creates a DataFrame for the commits. executeHistory is used for DeltaTable.history operator. Executing UpdateTable Command \u00b6 executeUpdate ( set : Map [ String , Column ], condition : Option [ Column ]) : Unit executeUpdate creates a DataFrame for an UpdateTable ( Spark SQL ) logical operator with the analyzed logical plan of the DeltaTable and the given condition . Note UpdateTable represents UPDATE TABLE SQL statement for v2 tables and is executed eagerly. executeUpdate is used for DeltaTable.update and DeltaTable.updateExpr operators. Executing VacuumCommand \u00b6 executeVacuum ( deltaLog : DeltaLog , retentionHours : Option [ Double ]) : DataFrame executeVacuum uses the VacuumCommand utility to gc (with the dryRun flag off and the given retentionHours ). In the end, executeVacuum returns an empty DataFrame (not the one from VacuumCommand.gc ). executeVacuum is used for DeltaTable.vacuum operator.","title":"DeltaTableOperations"},{"location":"DeltaTableOperations/#deltatableoperations-delta-dml-operations","text":"DeltaTableOperations is an abstraction of management services for executing delete , generate , history , update , and vacuum operations ( commands ). DeltaTableOperations is assumed to be associated with a DeltaTable .","title":"DeltaTableOperations &mdash; Delta DML Operations"},{"location":"DeltaTableOperations/#implementations","text":"DeltaTable","title":"Implementations"},{"location":"DeltaTableOperations/#executing-deletefromtable-command","text":"executeDelete ( condition : Option [ Expression ]) : Unit executeDelete creates a DataFrame for a DeleteFromTable logical operator (from Spark SQL) with (the analyzed logical plan of the DeltaTable and the given condition ). Note DeleteFromTable is a Command (Spark SQL) that represents DELETE FROM SQL statement for v2 tables. As a Command it is executed eagerly. executeDelete is used for DeltaTable.delete operator.","title":" Executing DeleteFromTable Command"},{"location":"DeltaTableOperations/#executing-deltageneratecommand-command","text":"executeGenerate ( tblIdentifier : String , mode : String ) : Unit executeGenerate requests the SQL parser (of the SparkSession ) to parse the given table identifier, creates a DeltaGenerateCommand and runs it. executeGenerate is used for DeltaTable.generate operator.","title":" Executing DeltaGenerateCommand Command"},{"location":"DeltaTableOperations/#executing-history-command","text":"executeHistory ( deltaLog : DeltaLog , limit : Option [ Int ]) : DataFrame executeHistory creates DeltaHistoryManager (for the given DeltaLog ) and requests it for the number of commits to match the limit . In the end, executeHistory creates a DataFrame for the commits. executeHistory is used for DeltaTable.history operator.","title":" Executing History Command"},{"location":"DeltaTableOperations/#executing-updatetable-command","text":"executeUpdate ( set : Map [ String , Column ], condition : Option [ Column ]) : Unit executeUpdate creates a DataFrame for an UpdateTable ( Spark SQL ) logical operator with the analyzed logical plan of the DeltaTable and the given condition . Note UpdateTable represents UPDATE TABLE SQL statement for v2 tables and is executed eagerly. executeUpdate is used for DeltaTable.update and DeltaTable.updateExpr operators.","title":" Executing UpdateTable Command"},{"location":"DeltaTableOperations/#executing-vacuumcommand","text":"executeVacuum ( deltaLog : DeltaLog , retentionHours : Option [ Double ]) : DataFrame executeVacuum uses the VacuumCommand utility to gc (with the dryRun flag off and the given retentionHours ). In the end, executeVacuum returns an empty DataFrame (not the one from VacuumCommand.gc ). executeVacuum is used for DeltaTable.vacuum operator.","title":" Executing VacuumCommand"},{"location":"DeltaTableUtils/","text":"DeltaTableUtils Utility \u00b6 extractIfPathContainsTimeTravel \u00b6 extractIfPathContainsTimeTravel ( session : SparkSession , path : String ) : ( String , Option [ DeltaTimeTravelSpec ]) extractIfPathContainsTimeTravel ...FIXME extractIfPathContainsTimeTravel is used when: DeltaDataSource is requested to sourceSchema and parsePathIdentifier resolveTimeTravelVersion Utility \u00b6 resolveTimeTravelVersion ( conf : SQLConf , deltaLog : DeltaLog , tt : DeltaTimeTravelSpec ) : ( Long , String ) resolveTimeTravelVersion ...FIXME resolveTimeTravelVersion is used when: DeltaLog is requested to create a relation (per partition filters and time travel) DeltaTableV2 is requested for a Snapshot == [[isDeltaTable]] isDeltaTable Utility [source, scala] \u00b6 isDeltaTable( spark: SparkSession, path: Path): Boolean isDeltaTable tries to < > for the given path and returns whether it was successful or not. NOTE: isDeltaTable is used when DeltaTable utility is used to < > or < >. == [[findDeltaTableRoot]] findDeltaTableRoot Utility [source, scala] \u00b6 findDeltaTableRoot( spark: SparkSession, path: Path): Option[Path] findDeltaTableRoot traverses the Hadoop DFS-compliant path upwards (to the root directory of the file system) until _delta_log or _samples directories are found, or the root directory is reached. For _delta_log or _samples directories, findDeltaTableRoot returns the parent directory. [NOTE] \u00b6 findDeltaTableRoot is used when: < > is executed DeltaTableUtils utility is used to < > * DeltaDataSource is requested to < > \u00b6 == [[splitMetadataAndDataPredicates]] splitMetadataAndDataPredicates Utility [source, scala] \u00b6 splitMetadataAndDataPredicates( condition: Expression, partitionColumns: Seq[String], spark: SparkSession): (Seq[Expression], Seq[Expression]) splitMetadataAndDataPredicates ...FIXME [NOTE] \u00b6 splitMetadataAndDataPredicates is used when: PartitionFiltering is requested to PartitionFiltering.md#filesForScan[filesForScan] * DeleteCommand.md[DeleteCommand] and UpdateCommand.md[UpdateCommand] are executed \u00b6 == [[isPredicatePartitionColumnsOnly]] isPredicatePartitionColumnsOnly Utility [source, scala] \u00b6 isPredicatePartitionColumnsOnly( condition: Expression, partitionColumns: Seq[String], spark: SparkSession): Boolean isPredicatePartitionColumnsOnly ...FIXME [NOTE] \u00b6 isPredicatePartitionColumnsOnly is used when...FIXME \u00b6 == [[combineWithCatalogMetadata]] combineWithCatalogMetadata Utility [source, scala] \u00b6 combineWithCatalogMetadata( sparkSession: SparkSession, table: CatalogTable): CatalogTable combineWithCatalogMetadata ...FIXME NOTE: combineWithCatalogMetadata seems unused.","title":"DeltaTableUtils"},{"location":"DeltaTableUtils/#deltatableutils-utility","text":"","title":"DeltaTableUtils Utility"},{"location":"DeltaTableUtils/#extractifpathcontainstimetravel","text":"extractIfPathContainsTimeTravel ( session : SparkSession , path : String ) : ( String , Option [ DeltaTimeTravelSpec ]) extractIfPathContainsTimeTravel ...FIXME extractIfPathContainsTimeTravel is used when: DeltaDataSource is requested to sourceSchema and parsePathIdentifier","title":" extractIfPathContainsTimeTravel"},{"location":"DeltaTableUtils/#resolvetimetravelversion-utility","text":"resolveTimeTravelVersion ( conf : SQLConf , deltaLog : DeltaLog , tt : DeltaTimeTravelSpec ) : ( Long , String ) resolveTimeTravelVersion ...FIXME resolveTimeTravelVersion is used when: DeltaLog is requested to create a relation (per partition filters and time travel) DeltaTableV2 is requested for a Snapshot == [[isDeltaTable]] isDeltaTable Utility","title":" resolveTimeTravelVersion Utility"},{"location":"DeltaTableUtils/#source-scala","text":"isDeltaTable( spark: SparkSession, path: Path): Boolean isDeltaTable tries to < > for the given path and returns whether it was successful or not. NOTE: isDeltaTable is used when DeltaTable utility is used to < > or < >. == [[findDeltaTableRoot]] findDeltaTableRoot Utility","title":"[source, scala]"},{"location":"DeltaTableUtils/#source-scala_1","text":"findDeltaTableRoot( spark: SparkSession, path: Path): Option[Path] findDeltaTableRoot traverses the Hadoop DFS-compliant path upwards (to the root directory of the file system) until _delta_log or _samples directories are found, or the root directory is reached. For _delta_log or _samples directories, findDeltaTableRoot returns the parent directory.","title":"[source, scala]"},{"location":"DeltaTableUtils/#note","text":"findDeltaTableRoot is used when: < > is executed DeltaTableUtils utility is used to < >","title":"[NOTE]"},{"location":"DeltaTableUtils/#deltadatasource-is-requested-to","text":"== [[splitMetadataAndDataPredicates]] splitMetadataAndDataPredicates Utility","title":"* DeltaDataSource is requested to &lt;&gt;"},{"location":"DeltaTableUtils/#source-scala_2","text":"splitMetadataAndDataPredicates( condition: Expression, partitionColumns: Seq[String], spark: SparkSession): (Seq[Expression], Seq[Expression]) splitMetadataAndDataPredicates ...FIXME","title":"[source, scala]"},{"location":"DeltaTableUtils/#note_1","text":"splitMetadataAndDataPredicates is used when: PartitionFiltering is requested to PartitionFiltering.md#filesForScan[filesForScan]","title":"[NOTE]"},{"location":"DeltaTableUtils/#deletecommandmddeletecommand-and-updatecommandmdupdatecommand-are-executed","text":"== [[isPredicatePartitionColumnsOnly]] isPredicatePartitionColumnsOnly Utility","title":"* DeleteCommand.md[DeleteCommand] and UpdateCommand.md[UpdateCommand] are executed"},{"location":"DeltaTableUtils/#source-scala_3","text":"isPredicatePartitionColumnsOnly( condition: Expression, partitionColumns: Seq[String], spark: SparkSession): Boolean isPredicatePartitionColumnsOnly ...FIXME","title":"[source, scala]"},{"location":"DeltaTableUtils/#note_2","text":"","title":"[NOTE]"},{"location":"DeltaTableUtils/#ispredicatepartitioncolumnsonly-is-used-whenfixme","text":"== [[combineWithCatalogMetadata]] combineWithCatalogMetadata Utility","title":"isPredicatePartitionColumnsOnly is used when...FIXME"},{"location":"DeltaTableUtils/#source-scala_4","text":"combineWithCatalogMetadata( sparkSession: SparkSession, table: CatalogTable): CatalogTable combineWithCatalogMetadata ...FIXME NOTE: combineWithCatalogMetadata seems unused.","title":"[source, scala]"},{"location":"DeltaTableV2/","text":"DeltaTableV2 \u00b6 DeltaTableV2 is a logical representation of a writable Delta table. In Spark SQL 3's terms, DeltaTableV2 is a Table ( Spark SQL ) that SupportsWrite ( Spark SQL ). Creating Instance \u00b6 DeltaTableV2 takes the following to be created: SparkSession Hadoop Path Optional Catalog Metadata ( Option[CatalogTable] ) Optional Table ID ( Option[String] ) Optional DeltaTimeTravelSpec DeltaTableV2 is created when: DeltaCatalog is requested to load a table DeltaDataSource is requested to load a table or create a table relation DeltaTimeTravelSpec \u00b6 DeltaTableV2 may be given a DeltaTimeTravelSpec to be created . DeltaTimeTravelSpec is assumed not to be defined. DeltaTableV2 is given a DeltaTimeTravelSpec only when DeltaDataSource is requested to create a BaseRelation . DeltaTimeTravelSpec is used for timeTravelSpec . Snapshot \u00b6 snapshot : Snapshot DeltaTableV2 has a Snapshot . In other words, DeltaTableV2 represents a Delta table at a specific version. Scala lazy value snapshot is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged. DeltaTableV2 uses the DeltaLog to load it at a given version (based on the optional timeTravelSpec ) or update to the latest version . snapshot is used when DeltaTableV2 is requested for the schema , partitioning and properties . DeltaTimeTravelSpec \u00b6 timeTravelSpec : Option [ DeltaTimeTravelSpec ] DeltaTableV2 may have a DeltaTimeTravelSpec specified that is either given or timeTravelByPath . timeTravelSpec throws an AnalysisException when timeTravelOpt and timeTravelByPath are both defined: Cannot specify time travel in multiple formats. timeTravelSpec is used when DeltaTableV2 is requested for a Snapshot and BaseRelation . DeltaTimeTravelSpec by Path \u00b6 timeTravelByPath : Option [ DeltaTimeTravelSpec ] Scala lazy value timeTravelByPath is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged. timeTravelByPath is undefined when CatalogTable is defined. With no CatalogTable defined, DeltaTableV2 parses the given Path for the timeTravelByPath (that resolvePath under the covers). Converting to Insertable HadoopFsRelation \u00b6 toBaseRelation : BaseRelation toBaseRelation verifyAndCreatePartitionFilters for the Path , the current Snapshot and partitionFilters . In the end, toBaseRelation requests the DeltaLog for an insertable HadoopFsRelation . toBaseRelation is used when: DeltaDataSource is requested to createRelation DeltaRelation utility is used to fromV2Relation","title":"DeltaTableV2"},{"location":"DeltaTableV2/#deltatablev2","text":"DeltaTableV2 is a logical representation of a writable Delta table. In Spark SQL 3's terms, DeltaTableV2 is a Table ( Spark SQL ) that SupportsWrite ( Spark SQL ).","title":"DeltaTableV2"},{"location":"DeltaTableV2/#creating-instance","text":"DeltaTableV2 takes the following to be created: SparkSession Hadoop Path Optional Catalog Metadata ( Option[CatalogTable] ) Optional Table ID ( Option[String] ) Optional DeltaTimeTravelSpec DeltaTableV2 is created when: DeltaCatalog is requested to load a table DeltaDataSource is requested to load a table or create a table relation","title":"Creating Instance"},{"location":"DeltaTableV2/#deltatimetravelspec","text":"DeltaTableV2 may be given a DeltaTimeTravelSpec to be created . DeltaTimeTravelSpec is assumed not to be defined. DeltaTableV2 is given a DeltaTimeTravelSpec only when DeltaDataSource is requested to create a BaseRelation . DeltaTimeTravelSpec is used for timeTravelSpec .","title":" DeltaTimeTravelSpec"},{"location":"DeltaTableV2/#snapshot","text":"snapshot : Snapshot DeltaTableV2 has a Snapshot . In other words, DeltaTableV2 represents a Delta table at a specific version. Scala lazy value snapshot is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged. DeltaTableV2 uses the DeltaLog to load it at a given version (based on the optional timeTravelSpec ) or update to the latest version . snapshot is used when DeltaTableV2 is requested for the schema , partitioning and properties .","title":" Snapshot"},{"location":"DeltaTableV2/#deltatimetravelspec_1","text":"timeTravelSpec : Option [ DeltaTimeTravelSpec ] DeltaTableV2 may have a DeltaTimeTravelSpec specified that is either given or timeTravelByPath . timeTravelSpec throws an AnalysisException when timeTravelOpt and timeTravelByPath are both defined: Cannot specify time travel in multiple formats. timeTravelSpec is used when DeltaTableV2 is requested for a Snapshot and BaseRelation .","title":" DeltaTimeTravelSpec"},{"location":"DeltaTableV2/#deltatimetravelspec-by-path","text":"timeTravelByPath : Option [ DeltaTimeTravelSpec ] Scala lazy value timeTravelByPath is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged. timeTravelByPath is undefined when CatalogTable is defined. With no CatalogTable defined, DeltaTableV2 parses the given Path for the timeTravelByPath (that resolvePath under the covers).","title":" DeltaTimeTravelSpec by Path"},{"location":"DeltaTableV2/#converting-to-insertable-hadoopfsrelation","text":"toBaseRelation : BaseRelation toBaseRelation verifyAndCreatePartitionFilters for the Path , the current Snapshot and partitionFilters . In the end, toBaseRelation requests the DeltaLog for an insertable HadoopFsRelation . toBaseRelation is used when: DeltaDataSource is requested to createRelation DeltaRelation utility is used to fromV2Relation","title":" Converting to Insertable HadoopFsRelation"},{"location":"DeltaTimeTravelSpec/","text":"DeltaTimeTravelSpec \u00b6 Time Travel Patterns \u00b6 DeltaTimeTravelSpec defines regular expressions for timestamp- and version-based time travel identifiers: Version URI: (path)@[vV](some numbers) Timestamp URI: (path)@(yyyyMMddHHmmssSSS) Creating Instance \u00b6 DeltaTimeTravelSpec takes the following to be created: Timestamp Version creationSource identifier DeltaTimeTravelSpec asserts that either version or timestamp is provided (and throws an AssertionError ). DeltaTimeTravelSpec is created when: DeltaTimeTravelSpec utility is used to resolve a path DeltaDataSource utility is used to getTimeTravelVersion Resolving Path \u00b6 resolvePath ( conf : SQLConf , identifier : String ) : ( DeltaTimeTravelSpec , String ) resolvePath ...FIXME resolvePath is used when DeltaTableUtils utility is used to extractIfPathContainsTimeTravel . getTimestamp \u00b6 getTimestamp ( timeZone : String ) : Timestamp getTimestamp ...FIXME getTimestamp is used when DeltaTableUtils utility is used to resolveTimeTravelVersion . isApplicable \u00b6 isApplicable ( conf : SQLConf , identifier : String ) : Boolean isApplicable is true when all of the following hold: spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled is true identifierContainsTimeTravel is true isApplicable is used when DeltaTableUtils utility is used to extractIfPathContainsTimeTravel . identifierContainsTimeTravel \u00b6 identifierContainsTimeTravel ( identifier : String ) : Boolean identifierContainsTimeTravel is true when the given identifier is either timestamp or version time travel pattern.","title":"DeltaTimeTravelSpec"},{"location":"DeltaTimeTravelSpec/#deltatimetravelspec","text":"","title":"DeltaTimeTravelSpec"},{"location":"DeltaTimeTravelSpec/#time-travel-patterns","text":"DeltaTimeTravelSpec defines regular expressions for timestamp- and version-based time travel identifiers: Version URI: (path)@[vV](some numbers) Timestamp URI: (path)@(yyyyMMddHHmmssSSS)","title":" Time Travel Patterns"},{"location":"DeltaTimeTravelSpec/#creating-instance","text":"DeltaTimeTravelSpec takes the following to be created: Timestamp Version creationSource identifier DeltaTimeTravelSpec asserts that either version or timestamp is provided (and throws an AssertionError ). DeltaTimeTravelSpec is created when: DeltaTimeTravelSpec utility is used to resolve a path DeltaDataSource utility is used to getTimeTravelVersion","title":"Creating Instance"},{"location":"DeltaTimeTravelSpec/#resolving-path","text":"resolvePath ( conf : SQLConf , identifier : String ) : ( DeltaTimeTravelSpec , String ) resolvePath ...FIXME resolvePath is used when DeltaTableUtils utility is used to extractIfPathContainsTimeTravel .","title":" Resolving Path"},{"location":"DeltaTimeTravelSpec/#gettimestamp","text":"getTimestamp ( timeZone : String ) : Timestamp getTimestamp ...FIXME getTimestamp is used when DeltaTableUtils utility is used to resolveTimeTravelVersion .","title":" getTimestamp"},{"location":"DeltaTimeTravelSpec/#isapplicable","text":"isApplicable ( conf : SQLConf , identifier : String ) : Boolean isApplicable is true when all of the following hold: spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled is true identifierContainsTimeTravel is true isApplicable is used when DeltaTableUtils utility is used to extractIfPathContainsTimeTravel .","title":" isApplicable"},{"location":"DeltaTimeTravelSpec/#identifiercontainstimetravel","text":"identifierContainsTimeTravel ( identifier : String ) : Boolean identifierContainsTimeTravel is true when the given identifier is either timestamp or version time travel pattern.","title":" identifierContainsTimeTravel"},{"location":"DeltaUnsupportedOperationsCheck/","text":"DeltaUnsupportedOperationsCheck \u00b6 DeltaUnsupportedOperationsCheck is a logical check rule that adds helpful error messages when Delta is being used with unsupported Hive operations or if an unsupported operation is executed.","title":"DeltaUnsupportedOperationsCheck"},{"location":"DeltaUnsupportedOperationsCheck/#deltaunsupportedoperationscheck","text":"DeltaUnsupportedOperationsCheck is a logical check rule that adds helpful error messages when Delta is being used with unsupported Hive operations or if an unsupported operation is executed.","title":"DeltaUnsupportedOperationsCheck"},{"location":"DeltaWriteOptions/","text":"DeltaWriteOptions \u00b6 DeltaWriteOptions is a type-safe abstraction of the write-related DeltaOptions . DeltaWriteOptions is DeltaWriteOptionsImpl and DeltaOptionParser . replaceWhere \u00b6 replaceWhere : Option [ String ] replaceWhere is the value of replaceWhere option. replaceWhere is used when: CreateDeltaTableCommand command is requested to getOperation WriteIntoDelta command is created , executed , and requested to write userMetadata \u00b6 userMetadata : Option [ String ] userMetadata is the value of userMetadata option. optimizeWrite \u00b6 optimizeWrite : Option [ Boolean ] optimizeWrite is the value of optimizeWrite option.","title":"DeltaWriteOptions"},{"location":"DeltaWriteOptions/#deltawriteoptions","text":"DeltaWriteOptions is a type-safe abstraction of the write-related DeltaOptions . DeltaWriteOptions is DeltaWriteOptionsImpl and DeltaOptionParser .","title":"DeltaWriteOptions"},{"location":"DeltaWriteOptions/#replacewhere","text":"replaceWhere : Option [ String ] replaceWhere is the value of replaceWhere option. replaceWhere is used when: CreateDeltaTableCommand command is requested to getOperation WriteIntoDelta command is created , executed , and requested to write","title":" replaceWhere"},{"location":"DeltaWriteOptions/#usermetadata","text":"userMetadata : Option [ String ] userMetadata is the value of userMetadata option.","title":" userMetadata"},{"location":"DeltaWriteOptions/#optimizewrite","text":"optimizeWrite : Option [ Boolean ] optimizeWrite is the value of optimizeWrite option.","title":" optimizeWrite"},{"location":"DeltaWriteOptionsImpl/","text":"DeltaWriteOptionsImpl \u00b6 DeltaWriteOptionsImpl is a DeltaOptionParser . canMergeSchema \u00b6 canMergeSchema : Boolean canMergeSchema is the value of mergeSchema option (if defined) or spark.databricks.delta.schema.autoMerge.enabled configuration property. canMergeSchema is used when: WriteIntoDelta is created DeltaSink is created canOverwriteSchema \u00b6 canOverwriteSchema : Boolean canOverwriteSchema is the value of overwriteSchema option. canOverwriteSchema is used when: CreateDeltaTableCommand is requested to replaceMetadataIfNecessary WriteIntoDelta is created DeltaSink is created rearrangeOnly \u00b6 rearrangeOnly : Boolean rearrangeOnly is the value of dataChange option. rearrangeOnly is used when: WriteIntoDelta is requested to write","title":"DeltaWriteOptionsImpl"},{"location":"DeltaWriteOptionsImpl/#deltawriteoptionsimpl","text":"DeltaWriteOptionsImpl is a DeltaOptionParser .","title":"DeltaWriteOptionsImpl"},{"location":"DeltaWriteOptionsImpl/#canmergeschema","text":"canMergeSchema : Boolean canMergeSchema is the value of mergeSchema option (if defined) or spark.databricks.delta.schema.autoMerge.enabled configuration property. canMergeSchema is used when: WriteIntoDelta is created DeltaSink is created","title":" canMergeSchema"},{"location":"DeltaWriteOptionsImpl/#canoverwriteschema","text":"canOverwriteSchema : Boolean canOverwriteSchema is the value of overwriteSchema option. canOverwriteSchema is used when: CreateDeltaTableCommand is requested to replaceMetadataIfNecessary WriteIntoDelta is created DeltaSink is created","title":" canOverwriteSchema"},{"location":"DeltaWriteOptionsImpl/#rearrangeonly","text":"rearrangeOnly : Boolean rearrangeOnly is the value of dataChange option. rearrangeOnly is used when: WriteIntoDelta is requested to write","title":" rearrangeOnly"},{"location":"FileAction/","text":"FileAction \u00b6 FileAction is an extension of the Action abstraction for actions that can add or remove files. Contract \u00b6 Path \u00b6 path : String dataChange \u00b6 dataChange : Boolean Implementations \u00b6 AddCDCFile AddFile RemoveFile Sealed Trait FileAction is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).","title":"FileAction"},{"location":"FileAction/#fileaction","text":"FileAction is an extension of the Action abstraction for actions that can add or remove files.","title":"FileAction"},{"location":"FileAction/#contract","text":"","title":"Contract"},{"location":"FileAction/#path","text":"path : String","title":" Path"},{"location":"FileAction/#datachange","text":"dataChange : Boolean","title":" dataChange"},{"location":"FileAction/#implementations","text":"AddCDCFile AddFile RemoveFile Sealed Trait FileAction is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).","title":"Implementations"},{"location":"FileNames/","text":"= FileNames Utility [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | checkpointPrefix a| [[checkpointPrefix]] Creates a Hadoop Path for a file name with a given version : [version][%020d].checkpoint E.g. 00000000000000000005.checkpoint | isCheckpointFile a| [[isCheckpointFile]] | isDeltaFile a| [[isDeltaFile]] |=== == [[deltaFile]] Creating Hadoop Path To Delta File -- deltaFile Utility [source, scala] \u00b6 deltaFile( path: Path, version: Long): Path deltaFile creates a Hadoop Path to a file of the format [version][%020d].json in the path directory, e.g. 00000000000000000001.json . NOTE: deltaFile is used when...FIXME","title":"FileNames"},{"location":"FileNames/#source-scala","text":"deltaFile( path: Path, version: Long): Path deltaFile creates a Hadoop Path to a file of the format [version][%020d].json in the path directory, e.g. 00000000000000000001.json . NOTE: deltaFile is used when...FIXME","title":"[source, scala]"},{"location":"GenerateSymlinkManifest/","text":"GenerateSymlinkManifest (And GenerateSymlinkManifestImpl) \u00b6 [[GenerateSymlinkManifest]] GenerateSymlinkManifest is a concrete < > to generate < > and < > Hive-style manifests for delta tables. NOTE: You can generate a < > Hive-style manifest for delta tables using < > SQL command or < > operator. [[GenerateSymlinkManifestImpl]] GenerateSymlinkManifestImpl is a < > that...FIXME == [[generateFullManifest]] generateFullManifest Method [source, scala] \u00b6 generateFullManifest( spark: SparkSession, deltaLog: DeltaLog): Unit generateFullManifest ...FIXME NOTE: generateFullManifest is used when...FIXME == [[generateIncrementalManifest]] generateIncrementalManifest Method [source, scala] \u00b6 generateIncrementalManifest( spark: SparkSession, deltaLog: DeltaLog, txnReadSnapshot: Snapshot, actions: Seq[Action]): Unit generateIncrementalManifest ...FIXME NOTE: generateIncrementalManifest is used when...FIXME == [[run]] Running Post-Commit Hook -- run Method [source, scala] \u00b6 run( spark: SparkSession, txn: OptimisticTransactionImpl, committedActions: Seq[Action]): Unit NOTE: run is part of the < > to execute a post-commit hook. run simply < > for the < > and < > of the delta table (of the given < >) and the < >. == [[handleError]] Handling Errors -- handleError Method [source, scala] \u00b6 handleError( error: Throwable, version: Long): Unit NOTE: handleError is part of the < > to handle errors while < > handleError ...FIXME","title":"GenerateSymlinkManifest"},{"location":"GenerateSymlinkManifest/#generatesymlinkmanifest-and-generatesymlinkmanifestimpl","text":"[[GenerateSymlinkManifest]] GenerateSymlinkManifest is a concrete < > to generate < > and < > Hive-style manifests for delta tables. NOTE: You can generate a < > Hive-style manifest for delta tables using < > SQL command or < > operator. [[GenerateSymlinkManifestImpl]] GenerateSymlinkManifestImpl is a < > that...FIXME == [[generateFullManifest]] generateFullManifest Method","title":"GenerateSymlinkManifest (And GenerateSymlinkManifestImpl)"},{"location":"GenerateSymlinkManifest/#source-scala","text":"generateFullManifest( spark: SparkSession, deltaLog: DeltaLog): Unit generateFullManifest ...FIXME NOTE: generateFullManifest is used when...FIXME == [[generateIncrementalManifest]] generateIncrementalManifest Method","title":"[source, scala]"},{"location":"GenerateSymlinkManifest/#source-scala_1","text":"generateIncrementalManifest( spark: SparkSession, deltaLog: DeltaLog, txnReadSnapshot: Snapshot, actions: Seq[Action]): Unit generateIncrementalManifest ...FIXME NOTE: generateIncrementalManifest is used when...FIXME == [[run]] Running Post-Commit Hook -- run Method","title":"[source, scala]"},{"location":"GenerateSymlinkManifest/#source-scala_2","text":"run( spark: SparkSession, txn: OptimisticTransactionImpl, committedActions: Seq[Action]): Unit NOTE: run is part of the < > to execute a post-commit hook. run simply < > for the < > and < > of the delta table (of the given < >) and the < >. == [[handleError]] Handling Errors -- handleError Method","title":"[source, scala]"},{"location":"GenerateSymlinkManifest/#source-scala_3","text":"handleError( error: Throwable, version: Long): Unit NOTE: handleError is part of the < > to handle errors while < > handleError ...FIXME","title":"[source, scala]"},{"location":"HDFSLogStore/","text":"= HDFSLogStore HDFSLogStore is...FIXME","title":"HDFSLogStore"},{"location":"HadoopFileSystemLogStore/","text":"= HadoopFileSystemLogStore HadoopFileSystemLogStore is...FIXME","title":"HadoopFileSystemLogStore"},{"location":"ImplicitMetadataOperation/","text":"ImplicitMetadataOperation \u00b6 ImplicitMetadataOperation is an abstraction of operations that can update metadata of a delta table (while writing out a new data). ImplicitMetadataOperation operations can update schema by merging and overwriting schema. Contract \u00b6 canMergeSchema \u00b6 canMergeSchema : Boolean Used when: MergeIntoCommand command is executed ImplicitMetadataOperation is requested to updateMetadata canOverwriteSchema \u00b6 canOverwriteSchema : Boolean Used when: ImplicitMetadataOperation is requested to updateMetadata Implementations \u00b6 DeltaSink MergeIntoCommand WriteIntoDelta Updating Metadata \u00b6 updateMetadata ( // (1) txn : OptimisticTransaction , data : Dataset [ _ ], partitionColumns : Seq [ String ], configuration : Map [ String , String ], isOverwriteMode : Boolean , rearrangeOnly : Boolean = false ) : Unit updateMetadata ( spark : SparkSession , txn : OptimisticTransaction , schema : StructType , partitionColumns : Seq [ String ], configuration : Map [ String , String ], isOverwriteMode : Boolean , rearrangeOnly : Boolean ) : Unit Uses the SparkSession and the schema of the given Dataset and assumes the rearrangeOnly to be off updateMetadata ...FIXME updateMetadata is used when: WriteIntoDelta command is executed MergeIntoCommand command is executed DeltaSink is requested to add a streaming micro-batch Normalizing Partition Columns \u00b6 normalizePartitionColumns ( spark : SparkSession , partitionCols : Seq [ String ], schema : StructType ) : Seq [ String ] normalizePartitionColumns ...FIXME","title":"ImplicitMetadataOperation"},{"location":"ImplicitMetadataOperation/#implicitmetadataoperation","text":"ImplicitMetadataOperation is an abstraction of operations that can update metadata of a delta table (while writing out a new data). ImplicitMetadataOperation operations can update schema by merging and overwriting schema.","title":"ImplicitMetadataOperation"},{"location":"ImplicitMetadataOperation/#contract","text":"","title":"Contract"},{"location":"ImplicitMetadataOperation/#canmergeschema","text":"canMergeSchema : Boolean Used when: MergeIntoCommand command is executed ImplicitMetadataOperation is requested to updateMetadata","title":" canMergeSchema"},{"location":"ImplicitMetadataOperation/#canoverwriteschema","text":"canOverwriteSchema : Boolean Used when: ImplicitMetadataOperation is requested to updateMetadata","title":" canOverwriteSchema"},{"location":"ImplicitMetadataOperation/#implementations","text":"DeltaSink MergeIntoCommand WriteIntoDelta","title":"Implementations"},{"location":"ImplicitMetadataOperation/#updating-metadata","text":"updateMetadata ( // (1) txn : OptimisticTransaction , data : Dataset [ _ ], partitionColumns : Seq [ String ], configuration : Map [ String , String ], isOverwriteMode : Boolean , rearrangeOnly : Boolean = false ) : Unit updateMetadata ( spark : SparkSession , txn : OptimisticTransaction , schema : StructType , partitionColumns : Seq [ String ], configuration : Map [ String , String ], isOverwriteMode : Boolean , rearrangeOnly : Boolean ) : Unit Uses the SparkSession and the schema of the given Dataset and assumes the rearrangeOnly to be off updateMetadata ...FIXME updateMetadata is used when: WriteIntoDelta command is executed MergeIntoCommand command is executed DeltaSink is requested to add a streaming micro-batch","title":" Updating Metadata"},{"location":"ImplicitMetadataOperation/#normalizing-partition-columns","text":"normalizePartitionColumns ( spark : SparkSession , partitionCols : Seq [ String ], schema : StructType ) : Seq [ String ] normalizePartitionColumns ...FIXME","title":" Normalizing Partition Columns"},{"location":"InMemoryLogReplay/","text":"InMemoryLogReplay \u2014 Delta Log Replay \u00b6 InMemoryLogReplay is used at the very last phase of < > (of a < >). InMemoryLogReplay is < > for every partition of the < > dataset ( Dataset[SingleAction] ) that is based on the < > configuration property (default: 50 ). The lifecycle of InMemoryLogReplay is as follows: . < > (with < >) . < > (with all < > of a partition) . < > == [[creating-instance]] Creating InMemoryLogReplay Instance InMemoryLogReplay takes the following to be created: [[minFileRetentionTimestamp]] minFileRetentionTimestamp (that is exactly < >) InMemoryLogReplay initializes the < >. == [[append]] Appending Actions -- append Method [source, scala] \u00b6 append( version: Long, actions: Iterator[Action]): Unit append sets the < > as the given version . append adds < > to respective registries: Every < > is registered in the < > by < > < > is registered as the < > < > is registered as the < > Every < > is registered as follows: ** Added to < > by pathAsUri (with dataChange flag turned off) ** Removed from < > by pathAsUri Every < > is registered as follows: ** Removed from < > by pathAsUri ** Added to < > by pathAsUri (with dataChange flag turned off) < > are ignored append throws an AssertionError when the < > is neither -1 (the default) nor one before the given version : Attempted to replay version [version], but state is at [currentVersion] NOTE: append is used when Snapshot is created (and initializes the < > for the < >). == [[checkpoint]] Current State Of Delta Table -- checkpoint Method [source, scala] \u00b6 checkpoint: Iterator[Action] \u00b6 checkpoint simply builds a sequence ( Iterator[Action] ) of the following (in that order): < > if defined (non- null ) < > if defined (non- null ) < > < > and < > (after the < >) sorted by < > (lexicographically) NOTE: checkpoint is used when Snapshot is created (and initializes the < > for the < >). == [[getTombstones]] getTombstones Internal Method [source, scala] \u00b6 getTombstones: Iterable[FileAction] \u00b6 getTombstones returns < > (from the < >) with their delTimestamp after the < >. NOTE: getTombstones is used when InMemoryLogReplay is requested to < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | currentProtocolVersion a| [[currentProtocolVersion]] < > (default: null ) Used when...FIXME | currentVersion a| [[currentVersion]] Version (default: -1 ) Used when...FIXME | currentMetaData a| [[currentMetaData]] < > (default: null ) Used when...FIXME | transactions a| [[transactions]] < > per ID ( HashMap[String, SetTransaction] ) Used when...FIXME | activeFiles a| [[activeFiles]] < > per URI ( HashMap[URI, AddFile] ) Used when...FIXME | tombstones a| [[tombstones]] < > per URI ( HashMap[URI, RemoveFile] ) Used when...FIXME |===","title":"InMemoryLogReplay"},{"location":"InMemoryLogReplay/#inmemorylogreplay-delta-log-replay","text":"InMemoryLogReplay is used at the very last phase of < > (of a < >). InMemoryLogReplay is < > for every partition of the < > dataset ( Dataset[SingleAction] ) that is based on the < > configuration property (default: 50 ). The lifecycle of InMemoryLogReplay is as follows: . < > (with < >) . < > (with all < > of a partition) . < > == [[creating-instance]] Creating InMemoryLogReplay Instance InMemoryLogReplay takes the following to be created: [[minFileRetentionTimestamp]] minFileRetentionTimestamp (that is exactly < >) InMemoryLogReplay initializes the < >. == [[append]] Appending Actions -- append Method","title":"InMemoryLogReplay &mdash; Delta Log Replay"},{"location":"InMemoryLogReplay/#source-scala","text":"append( version: Long, actions: Iterator[Action]): Unit append sets the < > as the given version . append adds < > to respective registries: Every < > is registered in the < > by < > < > is registered as the < > < > is registered as the < > Every < > is registered as follows: ** Added to < > by pathAsUri (with dataChange flag turned off) ** Removed from < > by pathAsUri Every < > is registered as follows: ** Removed from < > by pathAsUri ** Added to < > by pathAsUri (with dataChange flag turned off) < > are ignored append throws an AssertionError when the < > is neither -1 (the default) nor one before the given version : Attempted to replay version [version], but state is at [currentVersion] NOTE: append is used when Snapshot is created (and initializes the < > for the < >). == [[checkpoint]] Current State Of Delta Table -- checkpoint Method","title":"[source, scala]"},{"location":"InMemoryLogReplay/#source-scala_1","text":"","title":"[source, scala]"},{"location":"InMemoryLogReplay/#checkpoint-iteratoraction","text":"checkpoint simply builds a sequence ( Iterator[Action] ) of the following (in that order): < > if defined (non- null ) < > if defined (non- null ) < > < > and < > (after the < >) sorted by < > (lexicographically) NOTE: checkpoint is used when Snapshot is created (and initializes the < > for the < >). == [[getTombstones]] getTombstones Internal Method","title":"checkpoint: Iterator[Action]"},{"location":"InMemoryLogReplay/#source-scala_2","text":"","title":"[source, scala]"},{"location":"InMemoryLogReplay/#gettombstones-iterablefileaction","text":"getTombstones returns < > (from the < >) with their delTimestamp after the < >. NOTE: getTombstones is used when InMemoryLogReplay is requested to < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | currentProtocolVersion a| [[currentProtocolVersion]] < > (default: null ) Used when...FIXME | currentVersion a| [[currentVersion]] Version (default: -1 ) Used when...FIXME | currentMetaData a| [[currentMetaData]] < > (default: null ) Used when...FIXME | transactions a| [[transactions]] < > per ID ( HashMap[String, SetTransaction] ) Used when...FIXME | activeFiles a| [[activeFiles]] < > per URI ( HashMap[URI, AddFile] ) Used when...FIXME | tombstones a| [[tombstones]] < > per URI ( HashMap[URI, RemoveFile] ) Used when...FIXME |===","title":"getTombstones: Iterable[FileAction]"},{"location":"Invariants/","text":"= [[Invariants]] Invariants Invariants is...FIXME","title":"Invariants"},{"location":"LogStore/","text":"= LogStore LogStore is an < > of < > that can < > and < > actions to a directory (among other things). [[contract]] .LogStore Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | invalidateCache a| [[invalidateCache]] [source, scala] \u00b6 invalidateCache(): Unit \u00b6 Used when...FIXME | isPartialWriteVisible a| [[isPartialWriteVisible]] [source, scala] \u00b6 isPartialWriteVisible(path: Path): Boolean = true \u00b6 Used when...FIXME | listFrom a| [[listFrom]] [source, scala] \u00b6 listFrom( path: Path): Iterator[FileStatus] listFrom( path: String): Iterator[FileStatus] Used when...FIXME | read a| [[read]] [source, scala] \u00b6 read(path: String): Seq[String] read(path: Path): Seq[String] Used when: Checkpoints is requested to < > DeltaHistoryManager utility is requested to < > DeltaLog is requested to < > VerifyChecksum is requested to < > OptimisticTransactionImpl is requested to < > | write a| [[write]] [source, scala] \u00b6 write( path: Path, actions: Iterator[String], overwrite: Boolean = false): Unit write( path: String, actions: Iterator[String]): Unit Writes the actions out to the given path (with or without overwrite as indicated). Used when: Checkpoints is requested to < > ConvertToDeltaCommand is < > (and does < >) OptimisticTransactionImpl is requested to < > |=== [[implementations]] .LogStores (Direct Implementations and Extensions Only) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | LogStore | Description | < > | [[HDFSLogStore]] | < > | [[HadoopFileSystemLogStore]] |=== == [[resolvePathOnPhysicalStorage]] resolvePathOnPhysicalStorage Method [source, scala] \u00b6 resolvePathOnPhysicalStorage(path: Path): Path \u00b6 resolvePathOnPhysicalStorage ...FIXME NOTE: resolvePathOnPhysicalStorage is used when...FIXME == [[apply]] Creating LogStore -- apply Utility [source, scala] \u00b6 apply( sc: SparkContext): LogStore apply( sparkConf: SparkConf, hadoopConf: Configuration): LogStore apply ...FIXME [NOTE] \u00b6 apply is used when: DeltaHistoryManager is requested to < > and < > * DeltaFileOperations utility is used to < > \u00b6","title":"LogStore"},{"location":"LogStore/#source-scala","text":"","title":"[source, scala]"},{"location":"LogStore/#invalidatecache-unit","text":"Used when...FIXME | isPartialWriteVisible a| [[isPartialWriteVisible]]","title":"invalidateCache(): Unit"},{"location":"LogStore/#source-scala_1","text":"","title":"[source, scala]"},{"location":"LogStore/#ispartialwritevisiblepath-path-boolean-true","text":"Used when...FIXME | listFrom a| [[listFrom]]","title":"isPartialWriteVisible(path: Path): Boolean = true"},{"location":"LogStore/#source-scala_2","text":"listFrom( path: Path): Iterator[FileStatus] listFrom( path: String): Iterator[FileStatus] Used when...FIXME | read a| [[read]]","title":"[source, scala]"},{"location":"LogStore/#source-scala_3","text":"read(path: String): Seq[String] read(path: Path): Seq[String] Used when: Checkpoints is requested to < > DeltaHistoryManager utility is requested to < > DeltaLog is requested to < > VerifyChecksum is requested to < > OptimisticTransactionImpl is requested to < > | write a| [[write]]","title":"[source, scala]"},{"location":"LogStore/#source-scala_4","text":"write( path: Path, actions: Iterator[String], overwrite: Boolean = false): Unit write( path: String, actions: Iterator[String]): Unit Writes the actions out to the given path (with or without overwrite as indicated). Used when: Checkpoints is requested to < > ConvertToDeltaCommand is < > (and does < >) OptimisticTransactionImpl is requested to < > |=== [[implementations]] .LogStores (Direct Implementations and Extensions Only) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | LogStore | Description | < > | [[HDFSLogStore]] | < > | [[HadoopFileSystemLogStore]] |=== == [[resolvePathOnPhysicalStorage]] resolvePathOnPhysicalStorage Method","title":"[source, scala]"},{"location":"LogStore/#source-scala_5","text":"","title":"[source, scala]"},{"location":"LogStore/#resolvepathonphysicalstoragepath-path-path","text":"resolvePathOnPhysicalStorage ...FIXME NOTE: resolvePathOnPhysicalStorage is used when...FIXME == [[apply]] Creating LogStore -- apply Utility","title":"resolvePathOnPhysicalStorage(path: Path): Path"},{"location":"LogStore/#source-scala_6","text":"apply( sc: SparkContext): LogStore apply( sparkConf: SparkConf, hadoopConf: Configuration): LogStore apply ...FIXME","title":"[source, scala]"},{"location":"LogStore/#note","text":"apply is used when: DeltaHistoryManager is requested to < > and < >","title":"[NOTE]"},{"location":"LogStore/#deltafileoperations-utility-is-used-to","text":"","title":"* DeltaFileOperations utility is used to &lt;&gt;"},{"location":"LogStoreProvider/","text":"LogStoreProvider \u00b6 LogStoreProvider is an abstraction of providers of LogStores . spark.delta.logStore.class Configuration Property \u00b6 LogStoreProvider uses the spark.delta.logStore.class configuration property for the fully-qualified class name of the LogStore to create (for a DeltaLog , a DeltaHistoryManager , and DeltaFileOperations ). Creating LogStore \u00b6 createLogStore ( spark : SparkSession ) : LogStore createLogStore ( sparkConf : SparkConf , hadoopConf : Configuration ) : LogStore createLogStore ...FIXME createLogStore is used when: DeltaLog is created LogStore.apply utility is used","title":"LogStoreProvider"},{"location":"LogStoreProvider/#logstoreprovider","text":"LogStoreProvider is an abstraction of providers of LogStores .","title":"LogStoreProvider"},{"location":"LogStoreProvider/#sparkdeltalogstoreclass-configuration-property","text":"LogStoreProvider uses the spark.delta.logStore.class configuration property for the fully-qualified class name of the LogStore to create (for a DeltaLog , a DeltaHistoryManager , and DeltaFileOperations ).","title":" spark.delta.logStore.class Configuration Property"},{"location":"LogStoreProvider/#creating-logstore","text":"createLogStore ( spark : SparkSession ) : LogStore createLogStore ( sparkConf : SparkConf , hadoopConf : Configuration ) : LogStore createLogStore ...FIXME createLogStore is used when: DeltaLog is created LogStore.apply utility is used","title":" Creating LogStore"},{"location":"Metadata/","text":"Metadata \u00b6 Metadata is an < > that describes metadata (change) of a < > (indirectly via < >). import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, \"/tmp/delta/users\") scala> :type deltaLog.snapshot.metadata org.apache.spark.sql.delta.actions.Metadata Metadata contains all the non-data information ( metadata ) like < >, < >, < >, < >, < >, < > and < >. These can be changed (e.g., schema evolution). TIP: Use < > to review the metadata of a delta table. Metadata uses < > to uniquely identify a delta table. The ID is never going to change through the history of the table (unless the entire directory, along with the transaction log is deleted). It is known as tableId or < >. [NOTE] \u00b6 When I asked the question https://groups.google.com/forum/#!topic/delta-users/5OKEFvVKiew[tableId and reservoirId - Why two different names for metadata ID?] on delta-users mailing list, Tathagata Das wrote: Any reference to \"reservoir\" is just legacy code. In the early days of this project, the project was called \"Tahoe\" and each table is called a \"reservoir\" (Tahoe is one of the 2 nd deepest lake in US, and is a very large reservoir of water ;) ). So you may still find those two terms all around the codebase. In some cases, like DeltaSourceOffset, the term reservoirId is in the json that is written to the streaming checkpoint directory. So we cannot change that for backward compatibility. ==== Metadata can be < > in a OptimisticTransactionImpl.md[transaction] once (and only when created for an uninitialized table, when < > is -1 ). [source,scala] \u00b6 txn.metadata \u00b6 Metadata is < > when: DeltaLog is requested for the < > OptimisticTransactionImpl is requested for the < > ConvertToDeltaCommand is requested to < > ImplicitMetadataOperation is requested to < > == [[creating-instance]] Creating Metadata Instance Metadata takes the following to be created: [[id]] Table ID (default: a random UUID) [[name]] Name of the delta table (default: null ) [[description]] Description (default: null ) [[format]] Format [[schemaString]] Schema (default: null ) [[partitionColumns]] Partition columns (default: Nil ) [[configuration]] Configuration (default: empty ) [[createdTime]] Created time (in millis since the epoch) == [[wrap]] wrap Method [source, scala] \u00b6 wrap: SingleAction \u00b6 NOTE: wrap is part of the < > contract to wrap the action into a < >. wrap simply creates a new < > with the Metadata field set to this Metadata. == [[partitionSchema]] partitionSchema (Lazy) Property [source, scala] \u00b6 partitionSchema: StructType \u00b6 partitionSchema is the < > as StructFields (and defined in the < >). NOTE: partitionSchema throws an IllegalArgumentException for undefined fields that were used for the < > but not defined in the < >. NOTE: partitionSchema is used when...FIXME == [[dataSchema]] dataSchema (Lazy) Property [source, scala] \u00b6 dataSchema: StructType \u00b6 dataSchema ...FIXME NOTE: dataSchema is used when...FIXME == [[schema]] schema (Lazy) Property [source, scala] \u00b6 schema: StructType \u00b6 schema is a deserialized < > (from JSON format) to StructType . [NOTE] \u00b6 schema is used when: Metadata is requested for the schema of the < > and the < > DeltaLog is requested for an DeltaLog.md#createRelation[insertable HadoopFsRelation for batch queries] (for the data schema), to DeltaLog.md#upgradeProtocol[upgrade protocol], a DeltaLog.md#createDataFrame[DataFrame for given AddFiles] DeltaTableUtils utility is used to DeltaTableUtils.md#combineWithCatalogMetadata[combineWithCatalogMetadata] OptimisticTransactionImpl is requested to OptimisticTransactionImpl.md#verifyNewMetadata[verifyNewMetadata] * ...FIXME (there are other uses) \u00b6","title":"Metadata"},{"location":"Metadata/#metadata","text":"Metadata is an < > that describes metadata (change) of a < > (indirectly via < >). import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, \"/tmp/delta/users\") scala> :type deltaLog.snapshot.metadata org.apache.spark.sql.delta.actions.Metadata Metadata contains all the non-data information ( metadata ) like < >, < >, < >, < >, < >, < > and < >. These can be changed (e.g., schema evolution). TIP: Use < > to review the metadata of a delta table. Metadata uses < > to uniquely identify a delta table. The ID is never going to change through the history of the table (unless the entire directory, along with the transaction log is deleted). It is known as tableId or < >.","title":"Metadata"},{"location":"Metadata/#note","text":"When I asked the question https://groups.google.com/forum/#!topic/delta-users/5OKEFvVKiew[tableId and reservoirId - Why two different names for metadata ID?] on delta-users mailing list, Tathagata Das wrote: Any reference to \"reservoir\" is just legacy code. In the early days of this project, the project was called \"Tahoe\" and each table is called a \"reservoir\" (Tahoe is one of the 2 nd deepest lake in US, and is a very large reservoir of water ;) ). So you may still find those two terms all around the codebase. In some cases, like DeltaSourceOffset, the term reservoirId is in the json that is written to the streaming checkpoint directory. So we cannot change that for backward compatibility. ==== Metadata can be < > in a OptimisticTransactionImpl.md[transaction] once (and only when created for an uninitialized table, when < > is -1 ).","title":"[NOTE]"},{"location":"Metadata/#sourcescala","text":"","title":"[source,scala]"},{"location":"Metadata/#txnmetadata","text":"Metadata is < > when: DeltaLog is requested for the < > OptimisticTransactionImpl is requested for the < > ConvertToDeltaCommand is requested to < > ImplicitMetadataOperation is requested to < > == [[creating-instance]] Creating Metadata Instance Metadata takes the following to be created: [[id]] Table ID (default: a random UUID) [[name]] Name of the delta table (default: null ) [[description]] Description (default: null ) [[format]] Format [[schemaString]] Schema (default: null ) [[partitionColumns]] Partition columns (default: Nil ) [[configuration]] Configuration (default: empty ) [[createdTime]] Created time (in millis since the epoch) == [[wrap]] wrap Method","title":"txn.metadata"},{"location":"Metadata/#source-scala","text":"","title":"[source, scala]"},{"location":"Metadata/#wrap-singleaction","text":"NOTE: wrap is part of the < > contract to wrap the action into a < >. wrap simply creates a new < > with the Metadata field set to this Metadata. == [[partitionSchema]] partitionSchema (Lazy) Property","title":"wrap: SingleAction"},{"location":"Metadata/#source-scala_1","text":"","title":"[source, scala]"},{"location":"Metadata/#partitionschema-structtype","text":"partitionSchema is the < > as StructFields (and defined in the < >). NOTE: partitionSchema throws an IllegalArgumentException for undefined fields that were used for the < > but not defined in the < >. NOTE: partitionSchema is used when...FIXME == [[dataSchema]] dataSchema (Lazy) Property","title":"partitionSchema: StructType"},{"location":"Metadata/#source-scala_2","text":"","title":"[source, scala]"},{"location":"Metadata/#dataschema-structtype","text":"dataSchema ...FIXME NOTE: dataSchema is used when...FIXME == [[schema]] schema (Lazy) Property","title":"dataSchema: StructType"},{"location":"Metadata/#source-scala_3","text":"","title":"[source, scala]"},{"location":"Metadata/#schema-structtype","text":"schema is a deserialized < > (from JSON format) to StructType .","title":"schema: StructType"},{"location":"Metadata/#note_1","text":"schema is used when: Metadata is requested for the schema of the < > and the < > DeltaLog is requested for an DeltaLog.md#createRelation[insertable HadoopFsRelation for batch queries] (for the data schema), to DeltaLog.md#upgradeProtocol[upgrade protocol], a DeltaLog.md#createDataFrame[DataFrame for given AddFiles] DeltaTableUtils utility is used to DeltaTableUtils.md#combineWithCatalogMetadata[combineWithCatalogMetadata] OptimisticTransactionImpl is requested to OptimisticTransactionImpl.md#verifyNewMetadata[verifyNewMetadata]","title":"[NOTE]"},{"location":"Metadata/#fixme-there-are-other-uses","text":"","title":"* ...FIXME (there are other uses)"},{"location":"MetadataCleanup/","text":"MetadataCleanup \u00b6 MetadataCleanup is an abstraction of < > that can < > the < >. [[implementations]][[self]] NOTE: < > is the default and only known MetadataCleanup in Delta Lake. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.delta.MetadataCleanup logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.MetadataCleanup=ALL Refer to Logging .. \u00b6 == [[doLogCleanup]] doLogCleanup Method [source, scala] \u00b6 doLogCleanup(): Unit \u00b6 [NOTE] \u00b6 doLogCleanup is part of the < > to...FIXME. Interestingly, this MetadataCleanup and < > abstractions require to be used with < > only. \u00b6 doLogCleanup < > when the < > table property is enabled. == [[enableExpiredLogCleanup]] enableExpiredLogCleanup Table Property -- enableExpiredLogCleanup Method [source, scala] \u00b6 enableExpiredLogCleanup: Boolean \u00b6 enableExpiredLogCleanup gives the value of < > table property (< > the < >). NOTE: enableExpiredLogCleanup is used exclusively when MetadataCleanup is requested to < >. == [[deltaRetentionMillis]] logRetentionDuration Table Property -- deltaRetentionMillis Method [source, scala] \u00b6 deltaRetentionMillis: Long \u00b6 deltaRetentionMillis gives the value of < > table property (< > the < >). NOTE: deltaRetentionMillis is used when...FIXME == [[cleanUpExpiredLogs]] cleanUpExpiredLogs Internal Method [source, scala] \u00b6 cleanUpExpiredLogs(): Unit \u00b6 cleanUpExpiredLogs calculates a so-called fileCutOffTime based on the < > and the < > table property. cleanUpExpiredLogs prints out the following INFO message to the logs: Starting the deletion of log files older than [date] cleanUpExpiredLogs < > (based on the fileCutOffTime ) and deletes the files (using Hadoop's FileSystem.delete non-recursively). In the end, cleanUpExpiredLogs prints out the following INFO message to the logs: Deleted numDeleted log files older than [date] NOTE: cleanUpExpiredLogs is used exclusively when MetadataCleanup is requested to < >. == [[listExpiredDeltaLogs]] Finding Expired Delta Logs -- listExpiredDeltaLogs Internal Method [source, scala] \u00b6 listExpiredDeltaLogs( fileCutOffTime: Long): Iterator[FileStatus] listExpiredDeltaLogs ...FIXME requests the < > for the < > that are (lexicographically) greater or equal to the 0 th checkpoint file (per < > format) of the < > and < > files in the < > (of the < >). In the end, listExpiredDeltaLogs creates a BufferingLogDeletionIterator that...FIXME NOTE: listExpiredDeltaLogs is used exclusively when MetadataCleanup is requested to < >.","title":"MetadataCleanup"},{"location":"MetadataCleanup/#metadatacleanup","text":"MetadataCleanup is an abstraction of < > that can < > the < >. [[implementations]][[self]] NOTE: < > is the default and only known MetadataCleanup in Delta Lake. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.delta.MetadataCleanup logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.MetadataCleanup=ALL","title":"MetadataCleanup"},{"location":"MetadataCleanup/#refer-to-logging","text":"== [[doLogCleanup]] doLogCleanup Method","title":"Refer to Logging.."},{"location":"MetadataCleanup/#source-scala","text":"","title":"[source, scala]"},{"location":"MetadataCleanup/#dologcleanup-unit","text":"","title":"doLogCleanup(): Unit"},{"location":"MetadataCleanup/#note","text":"doLogCleanup is part of the < > to...FIXME.","title":"[NOTE]"},{"location":"MetadataCleanup/#interestingly-this-metadatacleanup-and-abstractions-require-to-be-used-with-only","text":"doLogCleanup < > when the < > table property is enabled. == [[enableExpiredLogCleanup]] enableExpiredLogCleanup Table Property -- enableExpiredLogCleanup Method","title":"Interestingly, this MetadataCleanup and &lt;&gt; abstractions require to be used with &lt;&gt; only."},{"location":"MetadataCleanup/#source-scala_1","text":"","title":"[source, scala]"},{"location":"MetadataCleanup/#enableexpiredlogcleanup-boolean","text":"enableExpiredLogCleanup gives the value of < > table property (< > the < >). NOTE: enableExpiredLogCleanup is used exclusively when MetadataCleanup is requested to < >. == [[deltaRetentionMillis]] logRetentionDuration Table Property -- deltaRetentionMillis Method","title":"enableExpiredLogCleanup: Boolean"},{"location":"MetadataCleanup/#source-scala_2","text":"","title":"[source, scala]"},{"location":"MetadataCleanup/#deltaretentionmillis-long","text":"deltaRetentionMillis gives the value of < > table property (< > the < >). NOTE: deltaRetentionMillis is used when...FIXME == [[cleanUpExpiredLogs]] cleanUpExpiredLogs Internal Method","title":"deltaRetentionMillis: Long"},{"location":"MetadataCleanup/#source-scala_3","text":"","title":"[source, scala]"},{"location":"MetadataCleanup/#cleanupexpiredlogs-unit","text":"cleanUpExpiredLogs calculates a so-called fileCutOffTime based on the < > and the < > table property. cleanUpExpiredLogs prints out the following INFO message to the logs: Starting the deletion of log files older than [date] cleanUpExpiredLogs < > (based on the fileCutOffTime ) and deletes the files (using Hadoop's FileSystem.delete non-recursively). In the end, cleanUpExpiredLogs prints out the following INFO message to the logs: Deleted numDeleted log files older than [date] NOTE: cleanUpExpiredLogs is used exclusively when MetadataCleanup is requested to < >. == [[listExpiredDeltaLogs]] Finding Expired Delta Logs -- listExpiredDeltaLogs Internal Method","title":"cleanUpExpiredLogs(): Unit"},{"location":"MetadataCleanup/#source-scala_4","text":"listExpiredDeltaLogs( fileCutOffTime: Long): Iterator[FileStatus] listExpiredDeltaLogs ...FIXME requests the < > for the < > that are (lexicographically) greater or equal to the 0 th checkpoint file (per < > format) of the < > and < > files in the < > (of the < >). In the end, listExpiredDeltaLogs creates a BufferingLogDeletionIterator that...FIXME NOTE: listExpiredDeltaLogs is used exclusively when MetadataCleanup is requested to < >.","title":"[source, scala]"},{"location":"Operation/","text":"Operation \u00b6 Operation is an abstraction of operations that can be executed on Delta tables. Operation is described by a name and parameters (that are simply used to create a CommitInfo for OptimisticTransactionImpl when committed and, as a way to bypass a transaction, ConvertToDeltaCommand ). Operation may have performance metrics . Contract \u00b6 parameters \u00b6 parameters : Map [ String , Any ] Parameters of the operation (to create a CommitInfo with the JSON-encoded values ) Used when Operation is requested for parameters with the values in JSON format Implementations \u00b6 Sealed Abstract Class Operation is a Scala sealed abstract class which means that all of the implementations are in the same compilation unit (a single file). AddColumns ChangeColumn ComputeStats Convert CreateTable Delete FileNotificationRetention Fsck ManualUpdate Optimize ReplaceColumns ReplaceTable ResetZCubeInfo SetTableProperties StreamingUpdate Truncate UnsetTableProperties Update UpdateColumnMetadata UpdateSchema UpgradeProtocol Write Merge \u00b6 Recorded when a merge operation is committed to a Delta table (when MergeIntoCommand is executed) Creating Instance \u00b6 Operation takes the following to be created: Name Abstract Class Operation is an abstract class and cannot be created directly. It is created indirectly for the concrete Operations . Serializing Parameter Values (to JSON Format) \u00b6 jsonEncodedValues : Map [ String , String ] jsonEncodedValues converts the values of the parameters to JSON format. jsonEncodedValues is used when: OptimisticTransactionImpl is requested to commit ConvertToDeltaCommand command is requested to streamWrite operationMetrics Registry \u00b6 operationMetrics : Set [ String ] operationMetrics is empty by default (and is expected to be overriden by concrete operations ) operationMetrics is used when Operation is requested to transformMetrics . transformMetrics Method \u00b6 transformMetrics ( metrics : Map [ String , SQLMetric ]) : Map [ String , String ] transformMetrics returns a collection of performance metrics ( SQLMetric ) and their values (as a text) that are defined as the operationMetrics . transformMetrics is used when SQLMetricsReporting is requested to getMetricsForOperation .","title":"Operation"},{"location":"Operation/#operation","text":"Operation is an abstraction of operations that can be executed on Delta tables. Operation is described by a name and parameters (that are simply used to create a CommitInfo for OptimisticTransactionImpl when committed and, as a way to bypass a transaction, ConvertToDeltaCommand ). Operation may have performance metrics .","title":"Operation"},{"location":"Operation/#contract","text":"","title":"Contract"},{"location":"Operation/#parameters","text":"parameters : Map [ String , Any ] Parameters of the operation (to create a CommitInfo with the JSON-encoded values ) Used when Operation is requested for parameters with the values in JSON format","title":" parameters"},{"location":"Operation/#implementations","text":"Sealed Abstract Class Operation is a Scala sealed abstract class which means that all of the implementations are in the same compilation unit (a single file). AddColumns ChangeColumn ComputeStats Convert CreateTable Delete FileNotificationRetention Fsck ManualUpdate Optimize ReplaceColumns ReplaceTable ResetZCubeInfo SetTableProperties StreamingUpdate Truncate UnsetTableProperties Update UpdateColumnMetadata UpdateSchema UpgradeProtocol Write","title":"Implementations"},{"location":"Operation/#merge","text":"Recorded when a merge operation is committed to a Delta table (when MergeIntoCommand is executed)","title":"Merge"},{"location":"Operation/#creating-instance","text":"Operation takes the following to be created: Name Abstract Class Operation is an abstract class and cannot be created directly. It is created indirectly for the concrete Operations .","title":"Creating Instance"},{"location":"Operation/#serializing-parameter-values-to-json-format","text":"jsonEncodedValues : Map [ String , String ] jsonEncodedValues converts the values of the parameters to JSON format. jsonEncodedValues is used when: OptimisticTransactionImpl is requested to commit ConvertToDeltaCommand command is requested to streamWrite","title":" Serializing Parameter Values (to JSON Format)"},{"location":"Operation/#operationmetrics-registry","text":"operationMetrics : Set [ String ] operationMetrics is empty by default (and is expected to be overriden by concrete operations ) operationMetrics is used when Operation is requested to transformMetrics .","title":" operationMetrics Registry"},{"location":"Operation/#transformmetrics-method","text":"transformMetrics ( metrics : Map [ String , SQLMetric ]) : Map [ String , String ] transformMetrics returns a collection of performance metrics ( SQLMetric ) and their values (as a text) that are defined as the operationMetrics . transformMetrics is used when SQLMetricsReporting is requested to getMetricsForOperation .","title":" transformMetrics Method"},{"location":"OptimisticTransaction/","text":"OptimisticTransaction \u00b6 OptimisticTransaction is an OptimisticTransactionImpl (which seems more of a class name change than anything more important). When OptimisticTransaction (as a < >) is attempted to be < > (that does < > internally), the < > (of the < >) is requested to < >, e.g. _delta_log/00000000000000000001.json for the attempt version 1 . Only when a FileAlreadyExistsException is thrown a commit is considered unsuccessful and < >. OptimisticTransaction can be associated with a thread as an < >. Creating Instance \u00b6 OptimisticTransaction takes the following to be created: [[deltaLog]] DeltaLog.md[] [[snapshot]] Snapshot.md[] [[clock]] Clock NOTE: The < > and < > are part of the < > contract (which in turn inherits them as a TransactionalWrite.md[] and changes to val from def ). OptimisticTransaction is created for changes to a < > at a given < >. OptimisticTransaction is created when DeltaLog is used for the following: DeltaLog.md#startTransaction[Starting a new transaction] DeltaLog.md#withNewTransaction[Executing a single-threaded operation (in a new transaction)] (for < >, < >, < >, and < > commands as well as for < > for < >) == [[active]] Active Thread-Local OptimisticTransaction [source, scala] \u00b6 active: ThreadLocal[OptimisticTransaction] \u00b6 active is a Java ThreadLocal with the < > of the current thread. ThreadLocal provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its get or set method) has its own, independently initialized copy of the variable. ThreadLocal instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID). active is assigned to the current thread using < > utility and cleared in < >. active is available using < > utility. There can only be one active OptimisticTransaction (or an IllegalStateException is thrown). == [[utilities]] Utilities === [[setActive]] setActive [source, scala] \u00b6 setActive( txn: OptimisticTransaction): Unit setActive simply associates the given OptimisticTransaction as < > with the current thread. setActive throws an IllegalStateException if there is an active OptimisticTransaction already associated: Cannot set a new txn as active when one is already active setActive is used when DeltaLog is requested to < >. === [[clearActive]] clearActive [source, scala] \u00b6 clearActive(): Unit \u00b6 clearActive simply clears the < > transaction (so no transaction is associated with a thread). clearActive is used when DeltaLog is requested to < >. === [[getActive]] getActive [source, scala] \u00b6 getActive(): Option[OptimisticTransaction] \u00b6 getActive simply returns the < > transaction. getActive seems unused. == [[logging]] Logging Enable ALL logging level for org.apache.spark.sql.delta.OptimisticTransaction logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.sql.delta.OptimisticTransaction=ALL \u00b6 Refer to Logging . == [[demo]] Demo [source,scala] \u00b6 import org.apache.spark.sql.delta.DeltaLog val dir = \"/tmp/delta/users\" val log = DeltaLog.forTable(spark, dir) val txn = log.startTransaction() // ...changes to a delta table... val addFile = AddFile(\"foo\", Map.empty, 1L, System.currentTimeMillis(), dataChange = true) val removeFile = addFile.remove val actions = addFile :: removeFile :: Nil txn.commit(actions, op) // You could do the following instead deltaLog.withNewTransaction { txn => // ...transactional changes to a delta table }","title":"OptimisticTransaction"},{"location":"OptimisticTransaction/#optimistictransaction","text":"OptimisticTransaction is an OptimisticTransactionImpl (which seems more of a class name change than anything more important). When OptimisticTransaction (as a < >) is attempted to be < > (that does < > internally), the < > (of the < >) is requested to < >, e.g. _delta_log/00000000000000000001.json for the attempt version 1 . Only when a FileAlreadyExistsException is thrown a commit is considered unsuccessful and < >. OptimisticTransaction can be associated with a thread as an < >.","title":"OptimisticTransaction"},{"location":"OptimisticTransaction/#creating-instance","text":"OptimisticTransaction takes the following to be created: [[deltaLog]] DeltaLog.md[] [[snapshot]] Snapshot.md[] [[clock]] Clock NOTE: The < > and < > are part of the < > contract (which in turn inherits them as a TransactionalWrite.md[] and changes to val from def ). OptimisticTransaction is created for changes to a < > at a given < >. OptimisticTransaction is created when DeltaLog is used for the following: DeltaLog.md#startTransaction[Starting a new transaction] DeltaLog.md#withNewTransaction[Executing a single-threaded operation (in a new transaction)] (for < >, < >, < >, and < > commands as well as for < > for < >) == [[active]] Active Thread-Local OptimisticTransaction","title":"Creating Instance"},{"location":"OptimisticTransaction/#source-scala","text":"","title":"[source, scala]"},{"location":"OptimisticTransaction/#active-threadlocaloptimistictransaction","text":"active is a Java ThreadLocal with the < > of the current thread. ThreadLocal provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its get or set method) has its own, independently initialized copy of the variable. ThreadLocal instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID). active is assigned to the current thread using < > utility and cleared in < >. active is available using < > utility. There can only be one active OptimisticTransaction (or an IllegalStateException is thrown). == [[utilities]] Utilities === [[setActive]] setActive","title":"active: ThreadLocal[OptimisticTransaction]"},{"location":"OptimisticTransaction/#source-scala_1","text":"setActive( txn: OptimisticTransaction): Unit setActive simply associates the given OptimisticTransaction as < > with the current thread. setActive throws an IllegalStateException if there is an active OptimisticTransaction already associated: Cannot set a new txn as active when one is already active setActive is used when DeltaLog is requested to < >. === [[clearActive]] clearActive","title":"[source, scala]"},{"location":"OptimisticTransaction/#source-scala_2","text":"","title":"[source, scala]"},{"location":"OptimisticTransaction/#clearactive-unit","text":"clearActive simply clears the < > transaction (so no transaction is associated with a thread). clearActive is used when DeltaLog is requested to < >. === [[getActive]] getActive","title":"clearActive(): Unit"},{"location":"OptimisticTransaction/#source-scala_3","text":"","title":"[source, scala]"},{"location":"OptimisticTransaction/#getactive-optionoptimistictransaction","text":"getActive simply returns the < > transaction. getActive seems unused. == [[logging]] Logging Enable ALL logging level for org.apache.spark.sql.delta.OptimisticTransaction logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"getActive(): Option[OptimisticTransaction]"},{"location":"OptimisticTransaction/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"OptimisticTransaction/#log4jloggerorgapachesparksqldeltaoptimistictransactionall","text":"Refer to Logging . == [[demo]] Demo","title":"log4j.logger.org.apache.spark.sql.delta.OptimisticTransaction=ALL"},{"location":"OptimisticTransaction/#sourcescala","text":"import org.apache.spark.sql.delta.DeltaLog val dir = \"/tmp/delta/users\" val log = DeltaLog.forTable(spark, dir) val txn = log.startTransaction() // ...changes to a delta table... val addFile = AddFile(\"foo\", Map.empty, 1L, System.currentTimeMillis(), dataChange = true) val removeFile = addFile.remove val actions = addFile :: removeFile :: Nil txn.commit(actions, op) // You could do the following instead deltaLog.withNewTransaction { txn => // ...transactional changes to a delta table }","title":"[source,scala]"},{"location":"OptimisticTransactionImpl/","text":"OptimisticTransactionImpl \u00b6 OptimisticTransactionImpl is an extension of the TransactionalWrite abstraction for optimistic transactions that can modify a delta table (at a given version ) and can be committed eventually. In other words, OptimisticTransactionImpl is a set of actions as part of an Operation that changes the state of a delta table transactionally. Contract \u00b6 Clock \u00b6 clock : Clock DeltaLog \u00b6 deltaLog : DeltaLog DeltaLog (of a delta table) that this transaction is changing deltaLog is part of the TransactionalWrite abstraction and seems to change it to val (from def ). Snapshot \u00b6 snapshot : Snapshot Snapshot (of the delta table ) that this transaction is changing snapshot is part of the TransactionalWrite contract and seems to change it to val (from def ). Implementations \u00b6 OptimisticTransaction Table Version at Reading Time \u00b6 readVersion : Long readVersion requests the Snapshot for the version . readVersion is used when: OptimisticTransactionImpl is requested to updateMetadata and commit AlterDeltaTableCommand , ConvertToDeltaCommand , CreateDeltaTableCommand commands are executed DeltaCommand is requested to commitLarge WriteIntoDelta is requested to write ImplicitMetadataOperation is requested to updateMetadata Transactional Commit \u00b6 commit ( actions : Seq [ Action ], op : DeltaOperations.Operation ) : Long commit commits the transaction (with the Action s and a given Operation ) Usage \u00b6 commit is used when: DeltaLog is requested to upgrade the protocol ALTER delta table commands ( AlterTableSetPropertiesDeltaCommand , AlterTableUnsetPropertiesDeltaCommand , AlterTableAddColumnsDeltaCommand , AlterTableChangeColumnDeltaCommand , AlterTableReplaceColumnsDeltaCommand , AlterTableAddConstraintDeltaCommand , AlterTableDropConstraintDeltaCommand ) are executed ConvertToDeltaCommand command is executed CreateDeltaTableCommand command is executed DeleteCommand command is executed MergeIntoCommand command is executed UpdateCommand command is executed WriteIntoDelta command is executed DeltaSink is requested to addBatch Preparing Commit \u00b6 commit firstly prepares a commit (that gives the final actions to commit that may be different from the given action s). Isolation Level \u00b6 commit determines the isolation level for this commit by checking whether any FileAction (in the given action s) has the dataChange flag enabled. With no data changes, commit uses SnapshotIsolation else Serializable . isBlindAppend \u00b6 commit ...FIXME CommitInfo \u00b6 commit ...FIXME Registering Post-Commit Hook \u00b6 commit registers the GenerateSymlinkManifest post-commit hook when there is a FileAction among the actions and the compatibility.symlinkFormatManifest.enabled table property is enabled. Commit Version \u00b6 commit doCommit with the next version, the actions, attempt number 0 , and the select isolation level. commit prints out the following INFO message to the logs: Committed delta #[commitVersion] to [logPath] Performing Post-Commit Operations \u00b6 commit postCommit (with the version committed and the actions). Executing Post-Commit Hooks \u00b6 In the end, commit runs post-commit hooks and returns the version of the successful commit. doCommitRetryIteratively \u00b6 doCommitRetryIteratively ( attemptVersion : Long , actions : Seq [ Action ], isolationLevel : IsolationLevel ) : Long doCommitRetryIteratively ...FIXME checkForConflicts \u00b6 checkForConflicts ( checkVersion : Long , actions : Seq [ Action ], attemptNumber : Int , commitIsolationLevel : IsolationLevel ) : Long checkForConflicts ...FIXME getNextAttemptVersion \u00b6 getNextAttemptVersion ( previousAttemptVersion : Long ) : Long getNextAttemptVersion ...FIXME getPrettyPartitionMessage \u00b6 getPrettyPartitionMessage ( partitionValues : Map [ String , String ]) : String getPrettyPartitionMessage ...FIXME getOperationMetrics \u00b6 getOperationMetrics ( op : Operation ) : Option [ Map [ String , String ]] getOperationMetrics ...FIXME postCommit \u00b6 postCommit ( commitVersion : Long , commitActions : Seq [ Action ]) : Unit postCommit ...FIXME prepareCommit \u00b6 prepareCommit ( actions : Seq [ Action ], op : DeltaOperations.Operation ) : Seq [ Action ] prepareCommit adds the newMetadata action (if available) to the given action s. prepareCommit verifyNewMetadata if there was one. prepareCommit ...FIXME prepareCommit requests the DeltaLog to protocolWrite . prepareCommit ...FIXME Multiple Metadata Changes Not Allowed \u00b6 prepareCommit throws an AssertionError when there are multiple metadata changes in the transaction (by means of Metadata actions): Cannot change the metadata more than once in a transaction. Committing Transaction Allowed Once Only \u00b6 prepareCommit throws an AssertionError when the committed internal flag is enabled: Transaction already committed. Registering Post-Commit Hook \u00b6 registerPostCommitHook ( hook : PostCommitHook ) : Unit registerPostCommitHook registers ( adds ) the given PostCommitHook to the postCommitHooks internal registry. runPostCommitHooks \u00b6 runPostCommitHooks ( version : Long , committedActions : Seq [ Action ]) : Unit runPostCommitHooks simply runs every post-commit hook registered (in the postCommitHooks internal registry). runPostCommitHooks clears the active transaction (making all follow-up operations non-transactional). Note Hooks may create new transactions. Handling Non-Fatal Exceptions \u00b6 For non-fatal exceptions, runPostCommitHooks prints out the following ERROR message to the logs, records the delta event, and requests the post-commit hook to handle the error . Error when executing post-commit hook [name] for commit [version] AssertionError \u00b6 runPostCommitHooks throws an AssertionError when committed flag is disabled: Can't call post commit hooks before committing CommitInfo \u00b6 OptimisticTransactionImpl creates a CommitInfo when requested to commit with spark.databricks.delta.commitInfo.enabled configuration enabled. OptimisticTransactionImpl uses the CommitInfo to recordDeltaEvent (as a CommitStats ). Attempting Commit \u00b6 doCommit ( attemptVersion : Long , actions : Seq [ Action ], attemptNumber : Int , isolationLevel : IsolationLevel ) : Long doCommit returns the given attemptVersion as the commit version if successful or checkAndRetry . doCommit is used when: OptimisticTransactionImpl is requested to commit (and checkAndRetry ). Internally, doCommit prints out the following DEBUG message to the logs: Attempting to commit version [attemptVersion] with [n] actions with [isolationLevel] isolation level Writing Out \u00b6 doCommit requests the DeltaLog for the LogStore to write out the given action s to a delta file in the log directory with the attemptVersion version, e.g. 00000000000000000001.json doCommit writes the action s out in JSON format . Note LogStores must throw a java.nio.file.FileAlreadyExistsException exception if the delta file already exists. Any FileAlreadyExistsExceptions are caught by doCommit itself to checkAndRetry . Post-Commit Snapshot \u00b6 doCommit requests the DeltaLog to update . IllegalStateException \u00b6 doCommit throws an IllegalStateException when the version of the snapshot after update is smaller than the given attemptVersion version. The committed version is [attemptVersion] but the current version is [version]. CommitStats \u00b6 doCommit records a new CommitStats and returns the given attemptVersion as the commit version. FileAlreadyExistsExceptions \u00b6 doCommit catches FileAlreadyExistsExceptions and checkAndRetry . Retrying Commit \u00b6 checkAndRetry ( checkVersion : Long , actions : Seq [ Action ], attemptNumber : Int ) : Long checkAndRetry ...FIXME checkAndRetry is used when OptimisticTransactionImpl is requested to commit (and attempts a commit that failed with an FileAlreadyExistsException ). verifyNewMetadata \u00b6 verifyNewMetadata ( metadata : Metadata ) : Unit verifyNewMetadata ...FIXME verifyNewMetadata is used when: OptimisticTransactionImpl is requested to prepareCommit and updateMetadata withGlobalConfigDefaults \u00b6 withGlobalConfigDefaults ( metadata : Metadata ) : Metadata withGlobalConfigDefaults ...FIXME withGlobalConfigDefaults is used when: OptimisticTransactionImpl is requested to updateMetadata and updateMetadataForNewTable Looking Up Transaction Version For Given (Streaming Query) ID \u00b6 txnVersion ( id : String ) : Long txnVersion simply registers ( adds ) the given ID in the readTxn internal registry. In the end, txnVersion requests the Snapshot for the transaction version for the given ID or -1 . txnVersion is used when: DeltaSink is requested to add a streaming micro-batch User-Defined Metadata \u00b6 getUserMetadata ( op : Operation ) : Option [ String ] getUserMetadata returns the Operation.md#userMetadata[userMetadata] of the given Operation.md[] (if defined) or the value of DeltaSQLConf.md#DELTA_USER_METADATA[spark.databricks.delta.commitInfo.userMetadata] configuration property. getUserMetadata is used when: OptimisticTransactionImpl is requested to commit (and spark.databricks.delta.commitInfo.enabled configuration property is enabled) ConvertToDeltaCommand is executed (and in turn requests DeltaCommand to commitLarge ) Internal Registries \u00b6 Post-Commit Hooks \u00b6 postCommitHooks : ArrayBuffer [ PostCommitHook ] OptimisticTransactionImpl manages PostCommitHook s that will be executed right after a commit is successful. Post-commit hooks can be registered , but only the GenerateSymlinkManifest post-commit hook is supported. newMetadata \u00b6 newMetadata : Option [ Metadata ] OptimisticTransactionImpl uses the newMetadata internal registry for a new Metadata that should be committed with this transaction. newMetadata is initially undefined ( None ). It can be updated only once and before the transaction writes out any files . newMetadata is used when prepareCommit and doCommit (for statistics). newMetadata is available using metadata method. readPredicates \u00b6 readPredicates : ArrayBuffer [ Expression ] readPredicates holds predicate expressions for partitions the transaction is modifying. readPredicates is added a new predicate expression when filterFiles and readWholeTable . readPredicates is used when checkAndRetry . Internal Properties \u00b6 committed \u00b6 Controls whether the transaction has been committed or not (and prevents prepareCommit from being executed again) Default: false Enabled in postCommit dependsOnFiles \u00b6 Flag that...FIXME Default: false Enabled (set to true ) in filterFiles and readWholeTable Used in commit and checkAndRetry readFiles \u00b6 readTxn \u00b6 Streaming query IDs that have been seen by this transaction A new queryId is added when OptimisticTransactionImpl is requested for txnVersion Used when OptimisticTransactionImpl is requested to checkAndRetry (to fail with a ConcurrentTransactionException for idempotent transactions that have conflicted) snapshotMetadata \u00b6 Metadata of the Snapshot readWholeTable \u00b6 readWholeTable () : Unit readWholeTable simply adds True literal to the readPredicates internal registry. readWholeTable is used when: DeltaSink is requested to add a streaming micro-batch (and the batch reads the same Delta table as this sink is going to write to) updateMetadataForNewTable \u00b6 updateMetadataForNewTable ( metadata : Metadata ) : Unit updateMetadataForNewTable ...FIXME updateMetadataForNewTable is used when: ConvertToDeltaCommand and CreateDeltaTableCommand are executed Metadata \u00b6 metadata : Metadata metadata is part of the TransactionalWrite abstraction. metadata is either the newMetadata (if defined) or the snapshotMetadata . Updating Metadata \u00b6 updateMetadata ( metadata : Metadata ) : Unit updateMetadata updates the newMetadata internal property based on the readVersion : For -1 , updateMetadata updates the configuration of the given metadata with a new metadata based on the SQLConf (of the active SparkSession ), the configuration of the given metadata and a new Protocol For other versions, updateMetadata leaves the given Metadata unchanged updateMetadata is used when: OptimisticTransactionImpl is requested to updateMetadataForNewTable AlterTableSetPropertiesDeltaCommand , AlterTableUnsetPropertiesDeltaCommand , AlterTableAddColumnsDeltaCommand , AlterTableChangeColumnDeltaCommand , AlterTableReplaceColumnsDeltaCommand are executed ConvertToDeltaCommand is executed ImplicitMetadataOperation is requested to updateMetadata AssertionError \u00b6 updateMetadata throws an AssertionError when the hasWritten flag is enabled: Cannot update the metadata in a transaction that has already written data. AssertionError \u00b6 updateMetadata throws an AssertionError when the newMetadata is not empty: Cannot change the metadata more than once in a transaction. Files To Scan Matching Given Predicates \u00b6 filterFiles () : Seq [ AddFile ] // Uses `true` literal to mean that all files match filterFiles ( filters : Seq [ Expression ]) : Seq [ AddFile ] filterFiles gives the files to scan based on the given predicates (filter expressions). Internally, filterFiles requests the Snapshot for the filesForScan (for no projection attributes and the given filters). filterFiles finds the partition predicates among the given filters (and the partition columns of the Metadata ). filterFiles registers ( adds ) the partition predicates (in the readPredicates internal registry) and the files to scan (in the readFiles internal registry). filterFiles is used when: WriteIntoDelta is requested to write DeltaSink is requested to add a streaming micro-batch (with Complete output mode) DeleteCommand , MergeIntoCommand and UpdateCommand are executed CreateDeltaTableCommand is executed","title":"OptimisticTransactionImpl"},{"location":"OptimisticTransactionImpl/#optimistictransactionimpl","text":"OptimisticTransactionImpl is an extension of the TransactionalWrite abstraction for optimistic transactions that can modify a delta table (at a given version ) and can be committed eventually. In other words, OptimisticTransactionImpl is a set of actions as part of an Operation that changes the state of a delta table transactionally.","title":"OptimisticTransactionImpl"},{"location":"OptimisticTransactionImpl/#contract","text":"","title":"Contract"},{"location":"OptimisticTransactionImpl/#clock","text":"clock : Clock","title":" Clock"},{"location":"OptimisticTransactionImpl/#deltalog","text":"deltaLog : DeltaLog DeltaLog (of a delta table) that this transaction is changing deltaLog is part of the TransactionalWrite abstraction and seems to change it to val (from def ).","title":" DeltaLog"},{"location":"OptimisticTransactionImpl/#snapshot","text":"snapshot : Snapshot Snapshot (of the delta table ) that this transaction is changing snapshot is part of the TransactionalWrite contract and seems to change it to val (from def ).","title":" Snapshot"},{"location":"OptimisticTransactionImpl/#implementations","text":"OptimisticTransaction","title":"Implementations"},{"location":"OptimisticTransactionImpl/#table-version-at-reading-time","text":"readVersion : Long readVersion requests the Snapshot for the version . readVersion is used when: OptimisticTransactionImpl is requested to updateMetadata and commit AlterDeltaTableCommand , ConvertToDeltaCommand , CreateDeltaTableCommand commands are executed DeltaCommand is requested to commitLarge WriteIntoDelta is requested to write ImplicitMetadataOperation is requested to updateMetadata","title":" Table Version at Reading Time"},{"location":"OptimisticTransactionImpl/#transactional-commit","text":"commit ( actions : Seq [ Action ], op : DeltaOperations.Operation ) : Long commit commits the transaction (with the Action s and a given Operation )","title":" Transactional Commit"},{"location":"OptimisticTransactionImpl/#usage","text":"commit is used when: DeltaLog is requested to upgrade the protocol ALTER delta table commands ( AlterTableSetPropertiesDeltaCommand , AlterTableUnsetPropertiesDeltaCommand , AlterTableAddColumnsDeltaCommand , AlterTableChangeColumnDeltaCommand , AlterTableReplaceColumnsDeltaCommand , AlterTableAddConstraintDeltaCommand , AlterTableDropConstraintDeltaCommand ) are executed ConvertToDeltaCommand command is executed CreateDeltaTableCommand command is executed DeleteCommand command is executed MergeIntoCommand command is executed UpdateCommand command is executed WriteIntoDelta command is executed DeltaSink is requested to addBatch","title":" Usage"},{"location":"OptimisticTransactionImpl/#preparing-commit","text":"commit firstly prepares a commit (that gives the final actions to commit that may be different from the given action s).","title":" Preparing Commit"},{"location":"OptimisticTransactionImpl/#isolation-level","text":"commit determines the isolation level for this commit by checking whether any FileAction (in the given action s) has the dataChange flag enabled. With no data changes, commit uses SnapshotIsolation else Serializable .","title":" Isolation Level"},{"location":"OptimisticTransactionImpl/#isblindappend","text":"commit ...FIXME","title":" isBlindAppend"},{"location":"OptimisticTransactionImpl/#commitinfo","text":"commit ...FIXME","title":" CommitInfo"},{"location":"OptimisticTransactionImpl/#registering-post-commit-hook","text":"commit registers the GenerateSymlinkManifest post-commit hook when there is a FileAction among the actions and the compatibility.symlinkFormatManifest.enabled table property is enabled.","title":" Registering Post-Commit Hook"},{"location":"OptimisticTransactionImpl/#commit-version","text":"commit doCommit with the next version, the actions, attempt number 0 , and the select isolation level. commit prints out the following INFO message to the logs: Committed delta #[commitVersion] to [logPath]","title":" Commit Version"},{"location":"OptimisticTransactionImpl/#performing-post-commit-operations","text":"commit postCommit (with the version committed and the actions).","title":" Performing Post-Commit Operations"},{"location":"OptimisticTransactionImpl/#executing-post-commit-hooks","text":"In the end, commit runs post-commit hooks and returns the version of the successful commit.","title":" Executing Post-Commit Hooks"},{"location":"OptimisticTransactionImpl/#docommitretryiteratively","text":"doCommitRetryIteratively ( attemptVersion : Long , actions : Seq [ Action ], isolationLevel : IsolationLevel ) : Long doCommitRetryIteratively ...FIXME","title":" doCommitRetryIteratively"},{"location":"OptimisticTransactionImpl/#checkforconflicts","text":"checkForConflicts ( checkVersion : Long , actions : Seq [ Action ], attemptNumber : Int , commitIsolationLevel : IsolationLevel ) : Long checkForConflicts ...FIXME","title":" checkForConflicts"},{"location":"OptimisticTransactionImpl/#getnextattemptversion","text":"getNextAttemptVersion ( previousAttemptVersion : Long ) : Long getNextAttemptVersion ...FIXME","title":" getNextAttemptVersion"},{"location":"OptimisticTransactionImpl/#getprettypartitionmessage","text":"getPrettyPartitionMessage ( partitionValues : Map [ String , String ]) : String getPrettyPartitionMessage ...FIXME","title":" getPrettyPartitionMessage"},{"location":"OptimisticTransactionImpl/#getoperationmetrics","text":"getOperationMetrics ( op : Operation ) : Option [ Map [ String , String ]] getOperationMetrics ...FIXME","title":" getOperationMetrics"},{"location":"OptimisticTransactionImpl/#postcommit","text":"postCommit ( commitVersion : Long , commitActions : Seq [ Action ]) : Unit postCommit ...FIXME","title":" postCommit"},{"location":"OptimisticTransactionImpl/#preparecommit","text":"prepareCommit ( actions : Seq [ Action ], op : DeltaOperations.Operation ) : Seq [ Action ] prepareCommit adds the newMetadata action (if available) to the given action s. prepareCommit verifyNewMetadata if there was one. prepareCommit ...FIXME prepareCommit requests the DeltaLog to protocolWrite . prepareCommit ...FIXME","title":" prepareCommit"},{"location":"OptimisticTransactionImpl/#multiple-metadata-changes-not-allowed","text":"prepareCommit throws an AssertionError when there are multiple metadata changes in the transaction (by means of Metadata actions): Cannot change the metadata more than once in a transaction.","title":" Multiple Metadata Changes Not Allowed"},{"location":"OptimisticTransactionImpl/#committing-transaction-allowed-once-only","text":"prepareCommit throws an AssertionError when the committed internal flag is enabled: Transaction already committed.","title":" Committing Transaction Allowed Once Only"},{"location":"OptimisticTransactionImpl/#registering-post-commit-hook_1","text":"registerPostCommitHook ( hook : PostCommitHook ) : Unit registerPostCommitHook registers ( adds ) the given PostCommitHook to the postCommitHooks internal registry.","title":" Registering Post-Commit Hook"},{"location":"OptimisticTransactionImpl/#runpostcommithooks","text":"runPostCommitHooks ( version : Long , committedActions : Seq [ Action ]) : Unit runPostCommitHooks simply runs every post-commit hook registered (in the postCommitHooks internal registry). runPostCommitHooks clears the active transaction (making all follow-up operations non-transactional). Note Hooks may create new transactions.","title":" runPostCommitHooks"},{"location":"OptimisticTransactionImpl/#handling-non-fatal-exceptions","text":"For non-fatal exceptions, runPostCommitHooks prints out the following ERROR message to the logs, records the delta event, and requests the post-commit hook to handle the error . Error when executing post-commit hook [name] for commit [version]","title":" Handling Non-Fatal Exceptions"},{"location":"OptimisticTransactionImpl/#assertionerror","text":"runPostCommitHooks throws an AssertionError when committed flag is disabled: Can't call post commit hooks before committing","title":" AssertionError"},{"location":"OptimisticTransactionImpl/#commitinfo_1","text":"OptimisticTransactionImpl creates a CommitInfo when requested to commit with spark.databricks.delta.commitInfo.enabled configuration enabled. OptimisticTransactionImpl uses the CommitInfo to recordDeltaEvent (as a CommitStats ).","title":" CommitInfo"},{"location":"OptimisticTransactionImpl/#attempting-commit","text":"doCommit ( attemptVersion : Long , actions : Seq [ Action ], attemptNumber : Int , isolationLevel : IsolationLevel ) : Long doCommit returns the given attemptVersion as the commit version if successful or checkAndRetry . doCommit is used when: OptimisticTransactionImpl is requested to commit (and checkAndRetry ). Internally, doCommit prints out the following DEBUG message to the logs: Attempting to commit version [attemptVersion] with [n] actions with [isolationLevel] isolation level","title":" Attempting Commit"},{"location":"OptimisticTransactionImpl/#writing-out","text":"doCommit requests the DeltaLog for the LogStore to write out the given action s to a delta file in the log directory with the attemptVersion version, e.g. 00000000000000000001.json doCommit writes the action s out in JSON format . Note LogStores must throw a java.nio.file.FileAlreadyExistsException exception if the delta file already exists. Any FileAlreadyExistsExceptions are caught by doCommit itself to checkAndRetry .","title":" Writing Out"},{"location":"OptimisticTransactionImpl/#post-commit-snapshot","text":"doCommit requests the DeltaLog to update .","title":" Post-Commit Snapshot"},{"location":"OptimisticTransactionImpl/#illegalstateexception","text":"doCommit throws an IllegalStateException when the version of the snapshot after update is smaller than the given attemptVersion version. The committed version is [attemptVersion] but the current version is [version].","title":" IllegalStateException"},{"location":"OptimisticTransactionImpl/#commitstats","text":"doCommit records a new CommitStats and returns the given attemptVersion as the commit version.","title":" CommitStats"},{"location":"OptimisticTransactionImpl/#filealreadyexistsexceptions","text":"doCommit catches FileAlreadyExistsExceptions and checkAndRetry .","title":" FileAlreadyExistsExceptions"},{"location":"OptimisticTransactionImpl/#retrying-commit","text":"checkAndRetry ( checkVersion : Long , actions : Seq [ Action ], attemptNumber : Int ) : Long checkAndRetry ...FIXME checkAndRetry is used when OptimisticTransactionImpl is requested to commit (and attempts a commit that failed with an FileAlreadyExistsException ).","title":" Retrying Commit"},{"location":"OptimisticTransactionImpl/#verifynewmetadata","text":"verifyNewMetadata ( metadata : Metadata ) : Unit verifyNewMetadata ...FIXME verifyNewMetadata is used when: OptimisticTransactionImpl is requested to prepareCommit and updateMetadata","title":" verifyNewMetadata"},{"location":"OptimisticTransactionImpl/#withglobalconfigdefaults","text":"withGlobalConfigDefaults ( metadata : Metadata ) : Metadata withGlobalConfigDefaults ...FIXME withGlobalConfigDefaults is used when: OptimisticTransactionImpl is requested to updateMetadata and updateMetadataForNewTable","title":" withGlobalConfigDefaults"},{"location":"OptimisticTransactionImpl/#looking-up-transaction-version-for-given-streaming-query-id","text":"txnVersion ( id : String ) : Long txnVersion simply registers ( adds ) the given ID in the readTxn internal registry. In the end, txnVersion requests the Snapshot for the transaction version for the given ID or -1 . txnVersion is used when: DeltaSink is requested to add a streaming micro-batch","title":" Looking Up Transaction Version For Given (Streaming Query) ID"},{"location":"OptimisticTransactionImpl/#user-defined-metadata","text":"getUserMetadata ( op : Operation ) : Option [ String ] getUserMetadata returns the Operation.md#userMetadata[userMetadata] of the given Operation.md[] (if defined) or the value of DeltaSQLConf.md#DELTA_USER_METADATA[spark.databricks.delta.commitInfo.userMetadata] configuration property. getUserMetadata is used when: OptimisticTransactionImpl is requested to commit (and spark.databricks.delta.commitInfo.enabled configuration property is enabled) ConvertToDeltaCommand is executed (and in turn requests DeltaCommand to commitLarge )","title":" User-Defined Metadata"},{"location":"OptimisticTransactionImpl/#internal-registries","text":"","title":"Internal Registries"},{"location":"OptimisticTransactionImpl/#post-commit-hooks","text":"postCommitHooks : ArrayBuffer [ PostCommitHook ] OptimisticTransactionImpl manages PostCommitHook s that will be executed right after a commit is successful. Post-commit hooks can be registered , but only the GenerateSymlinkManifest post-commit hook is supported.","title":" Post-Commit Hooks"},{"location":"OptimisticTransactionImpl/#newmetadata","text":"newMetadata : Option [ Metadata ] OptimisticTransactionImpl uses the newMetadata internal registry for a new Metadata that should be committed with this transaction. newMetadata is initially undefined ( None ). It can be updated only once and before the transaction writes out any files . newMetadata is used when prepareCommit and doCommit (for statistics). newMetadata is available using metadata method.","title":" newMetadata"},{"location":"OptimisticTransactionImpl/#readpredicates","text":"readPredicates : ArrayBuffer [ Expression ] readPredicates holds predicate expressions for partitions the transaction is modifying. readPredicates is added a new predicate expression when filterFiles and readWholeTable . readPredicates is used when checkAndRetry .","title":" readPredicates"},{"location":"OptimisticTransactionImpl/#internal-properties","text":"","title":"Internal Properties"},{"location":"OptimisticTransactionImpl/#committed","text":"Controls whether the transaction has been committed or not (and prevents prepareCommit from being executed again) Default: false Enabled in postCommit","title":" committed"},{"location":"OptimisticTransactionImpl/#dependsonfiles","text":"Flag that...FIXME Default: false Enabled (set to true ) in filterFiles and readWholeTable Used in commit and checkAndRetry","title":" dependsOnFiles"},{"location":"OptimisticTransactionImpl/#readfiles","text":"","title":" readFiles"},{"location":"OptimisticTransactionImpl/#readtxn","text":"Streaming query IDs that have been seen by this transaction A new queryId is added when OptimisticTransactionImpl is requested for txnVersion Used when OptimisticTransactionImpl is requested to checkAndRetry (to fail with a ConcurrentTransactionException for idempotent transactions that have conflicted)","title":" readTxn"},{"location":"OptimisticTransactionImpl/#snapshotmetadata","text":"Metadata of the Snapshot","title":" snapshotMetadata"},{"location":"OptimisticTransactionImpl/#readwholetable","text":"readWholeTable () : Unit readWholeTable simply adds True literal to the readPredicates internal registry. readWholeTable is used when: DeltaSink is requested to add a streaming micro-batch (and the batch reads the same Delta table as this sink is going to write to)","title":" readWholeTable"},{"location":"OptimisticTransactionImpl/#updatemetadatafornewtable","text":"updateMetadataForNewTable ( metadata : Metadata ) : Unit updateMetadataForNewTable ...FIXME updateMetadataForNewTable is used when: ConvertToDeltaCommand and CreateDeltaTableCommand are executed","title":" updateMetadataForNewTable"},{"location":"OptimisticTransactionImpl/#metadata","text":"metadata : Metadata metadata is part of the TransactionalWrite abstraction. metadata is either the newMetadata (if defined) or the snapshotMetadata .","title":" Metadata"},{"location":"OptimisticTransactionImpl/#updating-metadata","text":"updateMetadata ( metadata : Metadata ) : Unit updateMetadata updates the newMetadata internal property based on the readVersion : For -1 , updateMetadata updates the configuration of the given metadata with a new metadata based on the SQLConf (of the active SparkSession ), the configuration of the given metadata and a new Protocol For other versions, updateMetadata leaves the given Metadata unchanged updateMetadata is used when: OptimisticTransactionImpl is requested to updateMetadataForNewTable AlterTableSetPropertiesDeltaCommand , AlterTableUnsetPropertiesDeltaCommand , AlterTableAddColumnsDeltaCommand , AlterTableChangeColumnDeltaCommand , AlterTableReplaceColumnsDeltaCommand are executed ConvertToDeltaCommand is executed ImplicitMetadataOperation is requested to updateMetadata","title":" Updating Metadata"},{"location":"OptimisticTransactionImpl/#assertionerror_1","text":"updateMetadata throws an AssertionError when the hasWritten flag is enabled: Cannot update the metadata in a transaction that has already written data.","title":" AssertionError"},{"location":"OptimisticTransactionImpl/#assertionerror_2","text":"updateMetadata throws an AssertionError when the newMetadata is not empty: Cannot change the metadata more than once in a transaction.","title":" AssertionError"},{"location":"OptimisticTransactionImpl/#files-to-scan-matching-given-predicates","text":"filterFiles () : Seq [ AddFile ] // Uses `true` literal to mean that all files match filterFiles ( filters : Seq [ Expression ]) : Seq [ AddFile ] filterFiles gives the files to scan based on the given predicates (filter expressions). Internally, filterFiles requests the Snapshot for the filesForScan (for no projection attributes and the given filters). filterFiles finds the partition predicates among the given filters (and the partition columns of the Metadata ). filterFiles registers ( adds ) the partition predicates (in the readPredicates internal registry) and the files to scan (in the readFiles internal registry). filterFiles is used when: WriteIntoDelta is requested to write DeltaSink is requested to add a streaming micro-batch (with Complete output mode) DeleteCommand , MergeIntoCommand and UpdateCommand are executed CreateDeltaTableCommand is executed","title":" Files To Scan Matching Given Predicates"},{"location":"PartitionFiltering/","text":"PartitionFiltering \u00b6 PartitionFiltering is an abstraction of snapshots with partition filtering for scan . Implementations \u00b6 Snapshot is the default and only known PartitionFiltering in Delta Lake. Files to Scan (Matching Projection Attributes and Predicates) \u00b6 filesForScan ( projection : Seq [ Attribute ], filters : Seq [ Expression ], keepStats : Boolean = false ) : DeltaScan filesForScan ...FIXME filesForScan is used when: OptimisticTransactionImpl is requested for the files to scan matching given predicates TahoeLogFileIndex is requested for the files matching predicates and the input files","title":"PartitionFiltering"},{"location":"PartitionFiltering/#partitionfiltering","text":"PartitionFiltering is an abstraction of snapshots with partition filtering for scan .","title":"PartitionFiltering"},{"location":"PartitionFiltering/#implementations","text":"Snapshot is the default and only known PartitionFiltering in Delta Lake.","title":"Implementations"},{"location":"PartitionFiltering/#files-to-scan-matching-projection-attributes-and-predicates","text":"filesForScan ( projection : Seq [ Attribute ], filters : Seq [ Expression ], keepStats : Boolean = false ) : DeltaScan filesForScan ...FIXME filesForScan is used when: OptimisticTransactionImpl is requested for the files to scan matching given predicates TahoeLogFileIndex is requested for the files matching predicates and the input files","title":" Files to Scan (Matching Projection Attributes and Predicates)"},{"location":"PinnedTahoeFileIndex/","text":"PinnedTahoeFileIndex \u00b6 PinnedTahoeFileIndex is a TahoeFileIndex . Creating Instance \u00b6 PinnedTahoeFileIndex takes the following to be created: SparkSession ( Spark SQL ) DeltaLog Hadoop Path Snapshot PinnedTahoeFileIndex is created when: ActiveOptimisticTransactionRule logical optimization rule is executed","title":"PinnedTahoeFileIndex"},{"location":"PinnedTahoeFileIndex/#pinnedtahoefileindex","text":"PinnedTahoeFileIndex is a TahoeFileIndex .","title":"PinnedTahoeFileIndex"},{"location":"PinnedTahoeFileIndex/#creating-instance","text":"PinnedTahoeFileIndex takes the following to be created: SparkSession ( Spark SQL ) DeltaLog Hadoop Path Snapshot PinnedTahoeFileIndex is created when: ActiveOptimisticTransactionRule logical optimization rule is executed","title":"Creating Instance"},{"location":"PostCommitHook/","text":"= PostCommitHook PostCommitHook is an < > of < > that have a < > and can be < > (when OptimisticTransactionImpl is < >). [[contract]] .PostCommitHook Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | handleError a| [[handleError]] [source, scala] \u00b6 handleError( error: Throwable, version: Long): Unit = {} Handles an error while < > Used when OptimisticTransactionImpl is requested to < > (when < >) | name a| [[name]] [source, scala] \u00b6 name: String \u00b6 User-friendly name of the hook for error reporting Used when: DeltaErrors utility is used to < > OptimisticTransactionImpl is requested to < > (when < >) GenerateSymlinkManifestImpl is requested to < > | run a| [[run]] [source, scala] \u00b6 run( spark: SparkSession, txn: OptimisticTransactionImpl, committedActions: Seq[Action]): Unit Executes the post-commit hook Used when OptimisticTransactionImpl is requested to < > (when < >) |=== [[implementations]] NOTE: < > is the default and only known PostCommitHook in Delta Lake.","title":"Post-Commit Hooks"},{"location":"PostCommitHook/#source-scala","text":"handleError( error: Throwable, version: Long): Unit = {} Handles an error while < > Used when OptimisticTransactionImpl is requested to < > (when < >) | name a| [[name]]","title":"[source, scala]"},{"location":"PostCommitHook/#source-scala_1","text":"","title":"[source, scala]"},{"location":"PostCommitHook/#name-string","text":"User-friendly name of the hook for error reporting Used when: DeltaErrors utility is used to < > OptimisticTransactionImpl is requested to < > (when < >) GenerateSymlinkManifestImpl is requested to < > | run a| [[run]]","title":"name: String"},{"location":"PostCommitHook/#source-scala_2","text":"run( spark: SparkSession, txn: OptimisticTransactionImpl, committedActions: Seq[Action]): Unit Executes the post-commit hook Used when OptimisticTransactionImpl is requested to < > (when < >) |=== [[implementations]] NOTE: < > is the default and only known PostCommitHook in Delta Lake.","title":"[source, scala]"},{"location":"PreprocessTableDelete/","text":"PreprocessTableDelete Logical Resolution Rule \u00b6 PreprocessTableDelete is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to < > in a query plan into DeleteCommand.md[]s. PreprocessTableDelete is installed (injected) into a SparkSession using DeltaSparkSessionExtension.md[]. == [[creating-instance]][[conf]] Creating Instance PreprocessTableDelete takes a single SQLConf to be created. PreprocessTableDelete is created when DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support]. == [[apply]] Executing Rule [source, scala] \u00b6 apply( plan: LogicalPlan): LogicalPlan apply resolves ( replaces ) DeltaDelete logical commands (in a logical query plan) into corresponding DeleteCommand.md[]s. apply is part of the Spark SQL's Rule abstraction.","title":"PreprocessTableDelete"},{"location":"PreprocessTableDelete/#preprocesstabledelete-logical-resolution-rule","text":"PreprocessTableDelete is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to < > in a query plan into DeleteCommand.md[]s. PreprocessTableDelete is installed (injected) into a SparkSession using DeltaSparkSessionExtension.md[]. == [[creating-instance]][[conf]] Creating Instance PreprocessTableDelete takes a single SQLConf to be created. PreprocessTableDelete is created when DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support]. == [[apply]] Executing Rule","title":"PreprocessTableDelete Logical Resolution Rule"},{"location":"PreprocessTableDelete/#source-scala","text":"apply( plan: LogicalPlan): LogicalPlan apply resolves ( replaces ) DeltaDelete logical commands (in a logical query plan) into corresponding DeleteCommand.md[]s. apply is part of the Spark SQL's Rule abstraction.","title":"[source, scala]"},{"location":"PreprocessTableMerge/","text":"PreprocessTableMerge Logical Resolution Rule \u00b6 PreprocessTableMerge is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to < > in a query plan into MergeIntoCommand.md[]s. PreprocessTableMerge is installed (injected) into a SparkSession using DeltaSparkSessionExtension.md[]. == [[creating-instance]][[conf]] Creating Instance PreprocessTableMerge takes a single SQLConf to be created. PreprocessTableMerge is created when: DeltaMergeBuilder is executed DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support] == [[apply]] Executing Rule [source, scala] \u00b6 apply( plan: LogicalPlan): LogicalPlan apply resolves ( replaces ) DeltaMergeInto.md[] logical commands (in a logical query plan) into corresponding MergeIntoCommand.md[]s. apply is part of the Spark SQL's Rule abstraction.","title":"PreprocessTableMerge"},{"location":"PreprocessTableMerge/#preprocesstablemerge-logical-resolution-rule","text":"PreprocessTableMerge is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to < > in a query plan into MergeIntoCommand.md[]s. PreprocessTableMerge is installed (injected) into a SparkSession using DeltaSparkSessionExtension.md[]. == [[creating-instance]][[conf]] Creating Instance PreprocessTableMerge takes a single SQLConf to be created. PreprocessTableMerge is created when: DeltaMergeBuilder is executed DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support] == [[apply]] Executing Rule","title":"PreprocessTableMerge Logical Resolution Rule"},{"location":"PreprocessTableMerge/#source-scala","text":"apply( plan: LogicalPlan): LogicalPlan apply resolves ( replaces ) DeltaMergeInto.md[] logical commands (in a logical query plan) into corresponding MergeIntoCommand.md[]s. apply is part of the Spark SQL's Rule abstraction.","title":"[source, scala]"},{"location":"PreprocessTableUpdate/","text":"PreprocessTableUpdate Logical Resolution Rule \u00b6 PreprocessTableUpdate is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to < > in a query plan into UpdateCommand.md[]s. PreprocessTableUpdate is installed (injected) into a SparkSession using DeltaSparkSessionExtension.md[]. == [[creating-instance]][[conf]] Creating Instance PreprocessTableUpdate takes a single SQLConf to be created. PreprocessTableUpdate is created when DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support]. == [[apply]] Executing Rule [source, scala] \u00b6 apply( plan: LogicalPlan): LogicalPlan apply resolves ( replaces ) DeltaUpdateTable logical commands (in a logical query plan) into corresponding UpdateCommand.md[]s. apply is part of the Spark SQL's Rule abstraction.","title":"PreprocessTableUpdate"},{"location":"PreprocessTableUpdate/#preprocesstableupdate-logical-resolution-rule","text":"PreprocessTableUpdate is a post-hoc logical resolution rule ( Rule[LogicalPlan] ) to < > in a query plan into UpdateCommand.md[]s. PreprocessTableUpdate is installed (injected) into a SparkSession using DeltaSparkSessionExtension.md[]. == [[creating-instance]][[conf]] Creating Instance PreprocessTableUpdate takes a single SQLConf to be created. PreprocessTableUpdate is created when DeltaSparkSessionExtension is requested to DeltaSparkSessionExtension.md#apply[register Delta SQL support]. == [[apply]] Executing Rule","title":"PreprocessTableUpdate Logical Resolution Rule"},{"location":"PreprocessTableUpdate/#source-scala","text":"apply( plan: LogicalPlan): LogicalPlan apply resolves ( replaces ) DeltaUpdateTable logical commands (in a logical query plan) into corresponding UpdateCommand.md[]s. apply is part of the Spark SQL's Rule abstraction.","title":"[source, scala]"},{"location":"Protocol/","text":"Protocol \u00b6 Protocol is an Action . Creating Instance \u00b6 Protocol takes the following to be created: Minimum Reader Version Allowed (default: 1 ) Minimum Writer Version Allowed (default: 3 ) Protocol is created when: DeltaTable is requested to upgradeTableProtocol FIXME forNewTable Utility \u00b6 forNewTable ( spark : SparkSession , metadata : Metadata ) : Protocol forNewTable creates a new Protocol for the given SparkSession and Metadata . forNewTable is used when: OptimisticTransactionImpl is requested to updateMetadata and updateMetadataForNewTable InitialSnapshot is requested to computedState apply \u00b6 apply ( spark : SparkSession , metadataOpt : Option [ Metadata ]) : Protocol apply ...FIXME checkProtocolRequirements Utility \u00b6 checkProtocolRequirements ( spark : SparkSession , metadata : Metadata , current : Protocol ) : Option [ Protocol ] checkProtocolRequirements ...FIXME checkProtocolRequirements is used when: OptimisticTransactionImpl is requested to verifyNewMetadata requiredMinimumProtocol Utility \u00b6 requiredMinimumProtocol ( spark : SparkSession , metadata : Metadata ) : ( Protocol , Seq [ String ]) requiredMinimumProtocol ...FIXME requiredMinimumProtocol is used when: Protocol utility is used to create a Protocol and checkProtocolRequirements","title":"Protocol"},{"location":"Protocol/#protocol","text":"Protocol is an Action .","title":"Protocol"},{"location":"Protocol/#creating-instance","text":"Protocol takes the following to be created: Minimum Reader Version Allowed (default: 1 ) Minimum Writer Version Allowed (default: 3 ) Protocol is created when: DeltaTable is requested to upgradeTableProtocol FIXME","title":"Creating Instance"},{"location":"Protocol/#fornewtable-utility","text":"forNewTable ( spark : SparkSession , metadata : Metadata ) : Protocol forNewTable creates a new Protocol for the given SparkSession and Metadata . forNewTable is used when: OptimisticTransactionImpl is requested to updateMetadata and updateMetadataForNewTable InitialSnapshot is requested to computedState","title":" forNewTable Utility"},{"location":"Protocol/#apply","text":"apply ( spark : SparkSession , metadataOpt : Option [ Metadata ]) : Protocol apply ...FIXME","title":" apply"},{"location":"Protocol/#checkprotocolrequirements-utility","text":"checkProtocolRequirements ( spark : SparkSession , metadata : Metadata , current : Protocol ) : Option [ Protocol ] checkProtocolRequirements ...FIXME checkProtocolRequirements is used when: OptimisticTransactionImpl is requested to verifyNewMetadata","title":" checkProtocolRequirements Utility"},{"location":"Protocol/#requiredminimumprotocol-utility","text":"requiredMinimumProtocol ( spark : SparkSession , metadata : Metadata ) : ( Protocol , Seq [ String ]) requiredMinimumProtocol ...FIXME requiredMinimumProtocol is used when: Protocol utility is used to create a Protocol and checkProtocolRequirements","title":" requiredMinimumProtocol Utility"},{"location":"ReadChecksum/","text":"ReadChecksum \u00b6 ReadChecksum is...FIXME","title":"ReadChecksum"},{"location":"ReadChecksum/#readchecksum","text":"ReadChecksum is...FIXME","title":"ReadChecksum"},{"location":"RemoveFile/","text":"RemoveFile \u00b6 RemoveFile is...FIXME","title":"RemoveFile"},{"location":"RemoveFile/#removefile","text":"RemoveFile is...FIXME","title":"RemoveFile"},{"location":"SQLMetricsReporting/","text":"SQLMetricsReporting \u00b6 SQLMetricsReporting is an extension for OptimisticTransactionImpl to track SQL metrics of Operations . Implementations \u00b6 OptimisticTransactionImpl operationSQLMetrics Registry \u00b6 operationSQLMetrics ( spark : SparkSession , metrics : Map [ String , SQLMetric ]) : Unit operationSQLMetrics ...FIXME operationSQLMetrics is used when...FIXME registerSQLMetrics \u00b6 registerSQLMetrics ( spark : SparkSession , metrics : Map [ String , SQLMetric ]) : Unit registerSQLMetrics ...FIXME registerSQLMetrics is used when...FIXME getMetricsForOperation \u00b6 getMetricsForOperation ( operation : Operation ) : Map [ String , String ] getMetricsForOperation ...FIXME getMetricsForOperation is used when OptimisticTransactionImpl is requested to getOperationMetrics .","title":"SQLMetricsReporting"},{"location":"SQLMetricsReporting/#sqlmetricsreporting","text":"SQLMetricsReporting is an extension for OptimisticTransactionImpl to track SQL metrics of Operations .","title":"SQLMetricsReporting"},{"location":"SQLMetricsReporting/#implementations","text":"OptimisticTransactionImpl","title":"Implementations"},{"location":"SQLMetricsReporting/#operationsqlmetrics-registry","text":"operationSQLMetrics ( spark : SparkSession , metrics : Map [ String , SQLMetric ]) : Unit operationSQLMetrics ...FIXME operationSQLMetrics is used when...FIXME","title":" operationSQLMetrics Registry"},{"location":"SQLMetricsReporting/#registersqlmetrics","text":"registerSQLMetrics ( spark : SparkSession , metrics : Map [ String , SQLMetric ]) : Unit registerSQLMetrics ...FIXME registerSQLMetrics is used when...FIXME","title":" registerSQLMetrics"},{"location":"SQLMetricsReporting/#getmetricsforoperation","text":"getMetricsForOperation ( operation : Operation ) : Map [ String , String ] getMetricsForOperation ...FIXME getMetricsForOperation is used when OptimisticTransactionImpl is requested to getOperationMetrics .","title":" getMetricsForOperation"},{"location":"SchemaUtils/","text":"= [[SchemaUtils]] SchemaUtils Utility SchemaUtils is...FIXME == [[mergeSchemas]] mergeSchemas Utility [source, scala] \u00b6 mergeSchemas( tableSchema: StructType, dataSchema: StructType): StructType mergeSchemas ...FIXME [NOTE] \u00b6 mergeSchemas is used when: ConvertToDeltaCommand is requested to ConvertToDeltaCommand.md#performConvert[performConvert] and ConvertToDeltaCommand.md#mergeSchemasInParallel[mergeSchemasInParallel] * ImplicitMetadataOperation is requested to ImplicitMetadataOperation.md#updateMetadata[update metadata] \u00b6","title":"SchemaUtils"},{"location":"SchemaUtils/#source-scala","text":"mergeSchemas( tableSchema: StructType, dataSchema: StructType): StructType mergeSchemas ...FIXME","title":"[source, scala]"},{"location":"SchemaUtils/#note","text":"mergeSchemas is used when: ConvertToDeltaCommand is requested to ConvertToDeltaCommand.md#performConvert[performConvert] and ConvertToDeltaCommand.md#mergeSchemasInParallel[mergeSchemasInParallel]","title":"[NOTE]"},{"location":"SchemaUtils/#implicitmetadataoperation-is-requested-to-implicitmetadataoperationmdupdatemetadataupdate-metadata","text":"","title":"* ImplicitMetadataOperation is requested to ImplicitMetadataOperation.md#updateMetadata[update metadata]"},{"location":"SetTransaction/","text":"= SetTransaction SetTransaction is an < > that denotes the committed < > for an < >. SetTransaction is < > when DeltaSink is requested to < > (for STREAMING UPDATE operation idempotence at query restart). == [[creating-instance]] Creating SetTransaction Instance SetTransaction takes the following to be created: [[appId]] Application ID (e.g. streaming query ID) [[version]] Version (e.g micro-batch ID) [[lastUpdated]] Last Updated (optional) (e.g. milliseconds since the epoch) == [[wrap]] wrap Method [source, scala] \u00b6 wrap: SingleAction \u00b6 NOTE: wrap is part of the < > contract to wrap the action into a < > for serialization. wrap simply creates a new < > with the txn field set to this SetTransaction .","title":"SetTransaction"},{"location":"SetTransaction/#source-scala","text":"","title":"[source, scala]"},{"location":"SetTransaction/#wrap-singleaction","text":"NOTE: wrap is part of the < > contract to wrap the action into a < > for serialization. wrap simply creates a new < > with the txn field set to this SetTransaction .","title":"wrap: SingleAction"},{"location":"SingleAction/","text":"SingleAction \u00b6 SingleAction is...FIXME","title":"SingleAction"},{"location":"SingleAction/#singleaction","text":"SingleAction is...FIXME","title":"SingleAction"},{"location":"Snapshot/","text":"Snapshot \u00b6 Snapshot is an immutable snapshot of the state of the Delta table at the version . Tip Use Demo: DeltaTable, DeltaLog And Snapshots to learn more. Creating Instance \u00b6 Snapshot takes the following to be created: Hadoop Path to the log directory Version LogSegment minFileRetentionTimestamp (that is exactly DeltaLog.minFileRetentionTimestamp ) DeltaLog Timestamp Optional VersionChecksum While being created, Snapshot prints out the following INFO message to the logs and initialize : Created snapshot [this] Snapshot is created when SnapshotManagement is requested for one . Initializing \u00b6 init () : Unit init requests the DeltaLog for the protocolRead for the Protocol . Computed State \u00b6 computedState : State Scala lazy value computedState is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val computedState: State computedState takes the current cached set of actions and reads the latest state (executes a state.select(...).first() query). Note The state.select(...).first() query uses aggregate standard functions (e.g. last , collect_set , sum , count ) and so uses groupBy over the whole dataset indirectly. computedState assumes that the protocol and metadata (actions) are defined. computedState throws an IllegalStateException when the actions are not defined and spark.databricks.delta.stateReconstructionValidation.enabled configuration property is enabled. The [action] of your Delta table couldn't be recovered while Reconstructing version: [version]. Did you manually delete files in the _delta_log directory? Note The state.select(...).first() query uses last with ignoreNulls flag true and so may give no rows for first() . computedState makes sure that the State to be returned has at least the default protocol and metadata (actions) defined. Configuration Properties \u00b6 spark.databricks.delta.snapshotPartitions \u00b6 Snapshot uses the spark.databricks.delta.snapshotPartitions configuration property for the number of partitions to use for state reconstruction . spark.databricks.delta.stateReconstructionValidation.enabled \u00b6 Snapshot uses the spark.databricks.delta.stateReconstructionValidation.enabled configuration property for reconstructing state . State Dataset of Actions \u00b6 state : Dataset [ SingleAction ] state simply requests the cached delta state to get the delta state from the cache . state is used when: Checkpoints utility is used to writeCheckpoint Snapshot is requested for computedState , all files and files removed (tombstones) VacuumCommand utility is requested for garbage collection All AddFile Actions \u00b6 allFiles : Dataset [ AddFile ] allFiles simply takes the state dataset and selects AddFiles (adds where clause for add IS NOT NULL and select over the fields of AddFiles ). Note allFiles simply adds where and select clauses. No computation happens yet as it is (a description of) a distributed computation as a Dataset[AddFile] . import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, \"/tmp/delta/users\") val files = deltaLog.snapshot.allFiles scala> :type files org.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.AddFile] scala> files.show(truncate = false) +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ |path |partitionValues|size|modificationTime|dataChange|stats|tags| +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ |part-00000-4050db39-e0f5-485d-ab3b-3ca72307f621-c000.snappy.parquet|[] |262 |1578083748000 |false |null |null| |part-00000-ba39f292-2970-4528-a40c-8f0aa5f796de-c000.snappy.parquet|[] |262 |1578083570000 |false |null |null| |part-00003-99f9d902-24a7-4f76-a15a-6971940bc245-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00007-03d987f1-5bb3-4b5b-8db9-97b6667107e2-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00011-a759a8c2-507d-46dd-9da7-dc722316214b-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00015-2e685d29-25ed-4262-90a7-5491847fd8d0-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00015-ee0ac1af-e1e0-4422-8245-12da91ced0a2-c000.snappy.parquet|[] |429 |1578083570000 |false |null |null| +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ allFiles is used when: PartitionFiltering is requested for the files to scan (matching projection attributes and predicates) DeltaSourceSnapshot is requested for the initial files (indexed AddFiles ) GenerateSymlinkManifestImpl is requested to generateIncrementalManifest and generateFullManifest DeltaDataSource is requested for an Insertable HadoopFsRelation stateReconstruction Dataset of Actions \u00b6 stateReconstruction : Dataset [ SingleAction ] Note stateReconstruction returns a Dataset[SingleAction] and so does not do any computation per se. stateReconstruction is a Dataset of SingleActions (that is the dataset part) of the cachedState . stateReconstruction loads the log file indices (that gives a Dataset[SingleAction] ). stateReconstruction maps over partitions (using Dataset.mapPartitions ) and canonicalize the paths for AddFile and RemoveFile actions. stateReconstruction adds file column that uses a UDF to assert that input_file_name() belongs to the Delta table. Note This UDF-based check is very clever. stateReconstruction repartitions the Dataset using the path of add or remove actions (with the configurable number of partitions ) and Dataset.sortWithinPartitions by the file column. In the end, stateReconstruction maps over partitions (using Dataset.mapPartitions ) that creates a InMemoryLogReplay , requests it to append the actions (as version 0 ) and checkpoint . stateReconstruction is used when Snapshot is requested for a cached state . Loading Log File Indices \u00b6 loadActions : Dataset [ SingleAction ] loadActions takes fileIndices and...FIXME fileIndices \u00b6 fileIndices : Seq [ DeltaLogFileIndex ] Scala lazy value fileIndices is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val fileIndices: Seq[DeltaLogFileIndex] fileIndices is a collection of the checkpointFileIndexOpt and the deltaFileIndexOpt (if they are available). Commit File Index \u00b6 deltaFileIndexOpt : Option [ DeltaLogFileIndex ] Scala lazy value deltaFileIndexOpt is a Scala lazy value and is initialized once when first accessed. Once computed, it stays unchanged for the Snapshot instance. lazy val deltaFileIndexOpt: Option[DeltaLogFileIndex] deltaFileIndexOpt is a DeltaLogFileIndex (in JsonFileFormat ) for the checkpoint file of the LogSegment . Checkpoint File Index \u00b6 checkpointFileIndexOpt : Option [ DeltaLogFileIndex ] Scala lazy value checkpointFileIndexOpt is a Scala lazy value and is initialized once when first accessed. Once computed, it stays unchanged for the Snapshot instance. lazy val checkpointFileIndexOpt: Option[DeltaLogFileIndex] checkpointFileIndexOpt is a DeltaLogFileIndex (in ParquetFileFormat ) for the delta files of the LogSegment . emptyActions Dataset (of Actions) \u00b6 emptyActions : Dataset [ SingleAction ] emptyActions is an empty dataset of SingleActions for stateReconstruction and load . Transaction Version By App ID \u00b6 transactions : Map [ String , Long ] transactions takes the SetTransaction actions (from the state dataset) and makes them a lookup table of transaction version by appId . Scala lazy value transactions is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val transactions: Map[String, Long] transactions is used when OptimisticTransactionImpl is requested for the transaction version for a given (streaming query) id . All RemoveFile Actions (Tombstones) \u00b6 tombstones : Dataset [ RemoveFile ] tombstones ...FIXME scala> deltaLog.snapshot.tombstones.show(false) +----+-----------------+----------+ |path|deletionTimestamp|dataChange| +----+-----------------+----------+ +----+-----------------+----------+ cachedState \u00b6 cachedState : CachedDS [ SingleAction ] Scala lazy value cachedState is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val cachedState: CachedDS[SingleAction] cachedState creates a Cached Delta State with the following: The dataset part is the stateReconstruction dataset of SingleAction s The name in the format Delta Table State #version - [redactedPath] (with the version and the path redacted) Used when Snapshot is requested for the state ( Dataset[SingleAction] )","title":"Snapshot"},{"location":"Snapshot/#snapshot","text":"Snapshot is an immutable snapshot of the state of the Delta table at the version . Tip Use Demo: DeltaTable, DeltaLog And Snapshots to learn more.","title":"Snapshot"},{"location":"Snapshot/#creating-instance","text":"Snapshot takes the following to be created: Hadoop Path to the log directory Version LogSegment minFileRetentionTimestamp (that is exactly DeltaLog.minFileRetentionTimestamp ) DeltaLog Timestamp Optional VersionChecksum While being created, Snapshot prints out the following INFO message to the logs and initialize : Created snapshot [this] Snapshot is created when SnapshotManagement is requested for one .","title":"Creating Instance"},{"location":"Snapshot/#initializing","text":"init () : Unit init requests the DeltaLog for the protocolRead for the Protocol .","title":" Initializing"},{"location":"Snapshot/#computed-state","text":"computedState : State Scala lazy value computedState is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val computedState: State computedState takes the current cached set of actions and reads the latest state (executes a state.select(...).first() query). Note The state.select(...).first() query uses aggregate standard functions (e.g. last , collect_set , sum , count ) and so uses groupBy over the whole dataset indirectly. computedState assumes that the protocol and metadata (actions) are defined. computedState throws an IllegalStateException when the actions are not defined and spark.databricks.delta.stateReconstructionValidation.enabled configuration property is enabled. The [action] of your Delta table couldn't be recovered while Reconstructing version: [version]. Did you manually delete files in the _delta_log directory? Note The state.select(...).first() query uses last with ignoreNulls flag true and so may give no rows for first() . computedState makes sure that the State to be returned has at least the default protocol and metadata (actions) defined.","title":" Computed State"},{"location":"Snapshot/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"Snapshot/#sparkdatabricksdeltasnapshotpartitions","text":"Snapshot uses the spark.databricks.delta.snapshotPartitions configuration property for the number of partitions to use for state reconstruction .","title":" spark.databricks.delta.snapshotPartitions"},{"location":"Snapshot/#sparkdatabricksdeltastatereconstructionvalidationenabled","text":"Snapshot uses the spark.databricks.delta.stateReconstructionValidation.enabled configuration property for reconstructing state .","title":"spark.databricks.delta.stateReconstructionValidation.enabled"},{"location":"Snapshot/#state-dataset-of-actions","text":"state : Dataset [ SingleAction ] state simply requests the cached delta state to get the delta state from the cache . state is used when: Checkpoints utility is used to writeCheckpoint Snapshot is requested for computedState , all files and files removed (tombstones) VacuumCommand utility is requested for garbage collection","title":" State Dataset of Actions"},{"location":"Snapshot/#all-addfile-actions","text":"allFiles : Dataset [ AddFile ] allFiles simply takes the state dataset and selects AddFiles (adds where clause for add IS NOT NULL and select over the fields of AddFiles ). Note allFiles simply adds where and select clauses. No computation happens yet as it is (a description of) a distributed computation as a Dataset[AddFile] . import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, \"/tmp/delta/users\") val files = deltaLog.snapshot.allFiles scala> :type files org.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.AddFile] scala> files.show(truncate = false) +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ |path |partitionValues|size|modificationTime|dataChange|stats|tags| +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ |part-00000-4050db39-e0f5-485d-ab3b-3ca72307f621-c000.snappy.parquet|[] |262 |1578083748000 |false |null |null| |part-00000-ba39f292-2970-4528-a40c-8f0aa5f796de-c000.snappy.parquet|[] |262 |1578083570000 |false |null |null| |part-00003-99f9d902-24a7-4f76-a15a-6971940bc245-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00007-03d987f1-5bb3-4b5b-8db9-97b6667107e2-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00011-a759a8c2-507d-46dd-9da7-dc722316214b-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00015-2e685d29-25ed-4262-90a7-5491847fd8d0-c000.snappy.parquet|[] |429 |1578083748000 |false |null |null| |part-00015-ee0ac1af-e1e0-4422-8245-12da91ced0a2-c000.snappy.parquet|[] |429 |1578083570000 |false |null |null| +-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+ allFiles is used when: PartitionFiltering is requested for the files to scan (matching projection attributes and predicates) DeltaSourceSnapshot is requested for the initial files (indexed AddFiles ) GenerateSymlinkManifestImpl is requested to generateIncrementalManifest and generateFullManifest DeltaDataSource is requested for an Insertable HadoopFsRelation","title":" All AddFile Actions"},{"location":"Snapshot/#statereconstruction-dataset-of-actions","text":"stateReconstruction : Dataset [ SingleAction ] Note stateReconstruction returns a Dataset[SingleAction] and so does not do any computation per se. stateReconstruction is a Dataset of SingleActions (that is the dataset part) of the cachedState . stateReconstruction loads the log file indices (that gives a Dataset[SingleAction] ). stateReconstruction maps over partitions (using Dataset.mapPartitions ) and canonicalize the paths for AddFile and RemoveFile actions. stateReconstruction adds file column that uses a UDF to assert that input_file_name() belongs to the Delta table. Note This UDF-based check is very clever. stateReconstruction repartitions the Dataset using the path of add or remove actions (with the configurable number of partitions ) and Dataset.sortWithinPartitions by the file column. In the end, stateReconstruction maps over partitions (using Dataset.mapPartitions ) that creates a InMemoryLogReplay , requests it to append the actions (as version 0 ) and checkpoint . stateReconstruction is used when Snapshot is requested for a cached state .","title":" stateReconstruction Dataset of Actions"},{"location":"Snapshot/#loading-log-file-indices","text":"loadActions : Dataset [ SingleAction ] loadActions takes fileIndices and...FIXME","title":" Loading Log File Indices"},{"location":"Snapshot/#fileindices","text":"fileIndices : Seq [ DeltaLogFileIndex ] Scala lazy value fileIndices is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val fileIndices: Seq[DeltaLogFileIndex] fileIndices is a collection of the checkpointFileIndexOpt and the deltaFileIndexOpt (if they are available).","title":" fileIndices"},{"location":"Snapshot/#commit-file-index","text":"deltaFileIndexOpt : Option [ DeltaLogFileIndex ] Scala lazy value deltaFileIndexOpt is a Scala lazy value and is initialized once when first accessed. Once computed, it stays unchanged for the Snapshot instance. lazy val deltaFileIndexOpt: Option[DeltaLogFileIndex] deltaFileIndexOpt is a DeltaLogFileIndex (in JsonFileFormat ) for the checkpoint file of the LogSegment .","title":" Commit File Index"},{"location":"Snapshot/#checkpoint-file-index","text":"checkpointFileIndexOpt : Option [ DeltaLogFileIndex ] Scala lazy value checkpointFileIndexOpt is a Scala lazy value and is initialized once when first accessed. Once computed, it stays unchanged for the Snapshot instance. lazy val checkpointFileIndexOpt: Option[DeltaLogFileIndex] checkpointFileIndexOpt is a DeltaLogFileIndex (in ParquetFileFormat ) for the delta files of the LogSegment .","title":" Checkpoint File Index"},{"location":"Snapshot/#emptyactions-dataset-of-actions","text":"emptyActions : Dataset [ SingleAction ] emptyActions is an empty dataset of SingleActions for stateReconstruction and load .","title":" emptyActions Dataset (of Actions)"},{"location":"Snapshot/#transaction-version-by-app-id","text":"transactions : Map [ String , Long ] transactions takes the SetTransaction actions (from the state dataset) and makes them a lookup table of transaction version by appId . Scala lazy value transactions is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val transactions: Map[String, Long] transactions is used when OptimisticTransactionImpl is requested for the transaction version for a given (streaming query) id .","title":" Transaction Version By App ID"},{"location":"Snapshot/#all-removefile-actions-tombstones","text":"tombstones : Dataset [ RemoveFile ] tombstones ...FIXME scala> deltaLog.snapshot.tombstones.show(false) +----+-----------------+----------+ |path|deletionTimestamp|dataChange| +----+-----------------+----------+ +----+-----------------+----------+","title":" All RemoveFile Actions (Tombstones)"},{"location":"Snapshot/#cachedstate","text":"cachedState : CachedDS [ SingleAction ] Scala lazy value cachedState is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the Snapshot instance. lazy val cachedState: CachedDS[SingleAction] cachedState creates a Cached Delta State with the following: The dataset part is the stateReconstruction dataset of SingleAction s The name in the format Delta Table State #version - [redactedPath] (with the version and the path redacted) Used when Snapshot is requested for the state ( Dataset[SingleAction] )","title":" cachedState"},{"location":"SnapshotIterator/","text":"= SnapshotIterator SnapshotIterator is...FIXME == [[iterator]] iterator Method [source, scala] \u00b6 iterator(): Iterator[IndexedFile] \u00b6 iterator ...FIXME NOTE: iterator is used exclusively when DeltaSource is requested for the < >.","title":"SnapshotIterator"},{"location":"SnapshotIterator/#source-scala","text":"","title":"[source, scala]"},{"location":"SnapshotIterator/#iterator-iteratorindexedfile","text":"iterator ...FIXME NOTE: iterator is used exclusively when DeltaSource is requested for the < >.","title":"iterator(): Iterator[IndexedFile]"},{"location":"SnapshotManagement/","text":"SnapshotManagement \u00b6 SnapshotManagement is an extension for DeltaLog to manage Snapshot s. Demo \u00b6 val name = \"employees\" val dataPath = s\"/tmp/delta/$name\" sql(s\"DROP TABLE $name\") sql(s\"\"\" | CREATE TABLE $name (id bigint, name string, city string) | USING delta | OPTIONS (path='$dataPath') \"\"\".stripMargin) import org.apache.spark.sql.delta.DeltaLog val log = DeltaLog.forTable(spark, dataPath) import org.apache.spark.sql.delta.SnapshotManagement assert(log.isInstanceOf[SnapshotManagement], \"DeltaLog is a SnapshotManagement\") val snapshot = log.update(stalenessAcceptable = false) scala> :type snapshot org.apache.spark.sql.delta.Snapshot assert(snapshot.version == 0) Current Snapshot \u00b6 currentSnapshot : Snapshot currentSnapshot is a registry with the current Snapshot of a Delta table. When DeltaLog is created, currentSnapshot is initialized as getSnapshotAtInit and changed every update . currentSnapshot ...FIXME currentSnapshot is used when: SnapshotManagement is requested to...FIXME Updating Current Snapshot \u00b6 update ( stalenessAcceptable : Boolean = false ) : Snapshot update ...FIXME update is used when: DeltaLog is requested to start a transaction and withNewTransaction OptimisticTransactionImpl is requested to doCommit and getNextAttemptVersion DeltaTableV2 is requested for a Snapshot TahoeLogFileIndex is requested for a Snapshot DeltaHistoryManager is requested to getHistory , getActiveCommitAtTime , checkVersionExists In Delta commands ... tryUpdate \u00b6 tryUpdate ( isAsync : Boolean = false ) : Snapshot tryUpdate ...FIXME tryUpdate is used when SnapshotManagement is requested to update . updateInternal \u00b6 updateInternal ( isAsync : Boolean ) : Snapshot updateInternal ...FIXME updateInternal is used when SnapshotManagement is requested to update (and tryUpdate ). Loading Latest Snapshot \u00b6 getSnapshotAtInit : Snapshot getSnapshotAtInit getLogSegmentFrom for the last checkpoint . getSnapshotAtInit prints out the following INFO message to the logs: Loading version [version][startCheckpoint] getSnapshotAtInit creates a Snapshot for the log segment. getSnapshotAtInit records the current time in lastUpdateTimestamp registry. getSnapshotAtInit prints out the following INFO message to the logs: Returning initial snapshot [snapshot] getSnapshotAtInit is used when SnapshotManagement is created (and initializes the currentSnapshot registry). getLogSegmentFrom \u00b6 getLogSegmentFrom ( startingCheckpoint : Option [ CheckpointMetaData ]) : LogSegment getLogSegmentFrom getLogSegmentForVersion for the version of the given CheckpointMetaData (if specified) as a start checkpoint version or leaves it undefined. getLogSegmentFrom is used when SnapshotManagement is requested for getSnapshotAtInit . getLogSegmentForVersion \u00b6 getLogSegmentForVersion ( startCheckpoint : Option [ Long ], versionToLoad : Option [ Long ] = None ) : LogSegment getLogSegmentForVersion ...FIXME getLogSegmentForVersion is used when SnapshotManagement is requested for getLogSegmentFrom , updateInternal and getSnapshotAt . listFrom \u00b6 listFrom ( startVersion : Long ) : Iterator [ FileStatus ] listFrom ...FIXME Creating Snapshot \u00b6 createSnapshot ( segment : LogSegment , minFileRetentionTimestamp : Long , timestamp : Long ) : Snapshot createSnapshot readChecksum (for the version of the given LogSegment ) and creates a Snapshot . createSnapshot is used when SnapshotManagement is requested for getSnapshotAtInit , getSnapshotAt and update . Last Successful Update Timestamp \u00b6 SnapshotManagement uses lastUpdateTimestamp internal registry for the timestamp of the last successful update.","title":"SnapshotManagement"},{"location":"SnapshotManagement/#snapshotmanagement","text":"SnapshotManagement is an extension for DeltaLog to manage Snapshot s.","title":"SnapshotManagement"},{"location":"SnapshotManagement/#demo","text":"val name = \"employees\" val dataPath = s\"/tmp/delta/$name\" sql(s\"DROP TABLE $name\") sql(s\"\"\" | CREATE TABLE $name (id bigint, name string, city string) | USING delta | OPTIONS (path='$dataPath') \"\"\".stripMargin) import org.apache.spark.sql.delta.DeltaLog val log = DeltaLog.forTable(spark, dataPath) import org.apache.spark.sql.delta.SnapshotManagement assert(log.isInstanceOf[SnapshotManagement], \"DeltaLog is a SnapshotManagement\") val snapshot = log.update(stalenessAcceptable = false) scala> :type snapshot org.apache.spark.sql.delta.Snapshot assert(snapshot.version == 0)","title":"Demo"},{"location":"SnapshotManagement/#current-snapshot","text":"currentSnapshot : Snapshot currentSnapshot is a registry with the current Snapshot of a Delta table. When DeltaLog is created, currentSnapshot is initialized as getSnapshotAtInit and changed every update . currentSnapshot ...FIXME currentSnapshot is used when: SnapshotManagement is requested to...FIXME","title":" Current Snapshot"},{"location":"SnapshotManagement/#updating-current-snapshot","text":"update ( stalenessAcceptable : Boolean = false ) : Snapshot update ...FIXME update is used when: DeltaLog is requested to start a transaction and withNewTransaction OptimisticTransactionImpl is requested to doCommit and getNextAttemptVersion DeltaTableV2 is requested for a Snapshot TahoeLogFileIndex is requested for a Snapshot DeltaHistoryManager is requested to getHistory , getActiveCommitAtTime , checkVersionExists In Delta commands ...","title":" Updating Current Snapshot"},{"location":"SnapshotManagement/#tryupdate","text":"tryUpdate ( isAsync : Boolean = false ) : Snapshot tryUpdate ...FIXME tryUpdate is used when SnapshotManagement is requested to update .","title":" tryUpdate"},{"location":"SnapshotManagement/#updateinternal","text":"updateInternal ( isAsync : Boolean ) : Snapshot updateInternal ...FIXME updateInternal is used when SnapshotManagement is requested to update (and tryUpdate ).","title":" updateInternal"},{"location":"SnapshotManagement/#loading-latest-snapshot","text":"getSnapshotAtInit : Snapshot getSnapshotAtInit getLogSegmentFrom for the last checkpoint . getSnapshotAtInit prints out the following INFO message to the logs: Loading version [version][startCheckpoint] getSnapshotAtInit creates a Snapshot for the log segment. getSnapshotAtInit records the current time in lastUpdateTimestamp registry. getSnapshotAtInit prints out the following INFO message to the logs: Returning initial snapshot [snapshot] getSnapshotAtInit is used when SnapshotManagement is created (and initializes the currentSnapshot registry).","title":" Loading Latest Snapshot"},{"location":"SnapshotManagement/#getlogsegmentfrom","text":"getLogSegmentFrom ( startingCheckpoint : Option [ CheckpointMetaData ]) : LogSegment getLogSegmentFrom getLogSegmentForVersion for the version of the given CheckpointMetaData (if specified) as a start checkpoint version or leaves it undefined. getLogSegmentFrom is used when SnapshotManagement is requested for getSnapshotAtInit .","title":" getLogSegmentFrom"},{"location":"SnapshotManagement/#getlogsegmentforversion","text":"getLogSegmentForVersion ( startCheckpoint : Option [ Long ], versionToLoad : Option [ Long ] = None ) : LogSegment getLogSegmentForVersion ...FIXME getLogSegmentForVersion is used when SnapshotManagement is requested for getLogSegmentFrom , updateInternal and getSnapshotAt .","title":" getLogSegmentForVersion"},{"location":"SnapshotManagement/#listfrom","text":"listFrom ( startVersion : Long ) : Iterator [ FileStatus ] listFrom ...FIXME","title":" listFrom"},{"location":"SnapshotManagement/#creating-snapshot","text":"createSnapshot ( segment : LogSegment , minFileRetentionTimestamp : Long , timestamp : Long ) : Snapshot createSnapshot readChecksum (for the version of the given LogSegment ) and creates a Snapshot . createSnapshot is used when SnapshotManagement is requested for getSnapshotAtInit , getSnapshotAt and update .","title":" Creating Snapshot"},{"location":"SnapshotManagement/#last-successful-update-timestamp","text":"SnapshotManagement uses lastUpdateTimestamp internal registry for the timestamp of the last successful update.","title":" Last Successful Update Timestamp"},{"location":"StagedDeltaTableV2/","text":"StagedDeltaTableV2 \u00b6 StagedDeltaTableV2 is a StagedTable ( Spark SQL ) and a SupportsWrite ( Spark SQL ). Creating Instance \u00b6 StagedDeltaTableV2 takes the following to be created: Identifier Schema Partitions ( Array[Transform] ) Properties Operation (one of Create , CreateOrReplace , Replace ) StagedDeltaTableV2 is created when DeltaCatalog is requested to stageReplace , stageCreateOrReplace or stageCreate . commitStagedChanges \u00b6 commitStagedChanges () : Unit commitStagedChanges ...FIXME commitStagedChanges is part of the StagedTable ( Spark SQL ) abstraction. abortStagedChanges \u00b6 abortStagedChanges () : Unit abortStagedChanges does nothing. abortStagedChanges is part of the StagedTable ( Spark SQL ) abstraction. Creating WriteBuilder \u00b6 newWriteBuilder ( info : LogicalWriteInfo ) : V1WriteBuilder newWriteBuilder ...FIXME newWriteBuilder is part of the SupportsWrite ( Spark SQL ) abstraction.","title":"StagedDeltaTableV2"},{"location":"StagedDeltaTableV2/#stageddeltatablev2","text":"StagedDeltaTableV2 is a StagedTable ( Spark SQL ) and a SupportsWrite ( Spark SQL ).","title":"StagedDeltaTableV2"},{"location":"StagedDeltaTableV2/#creating-instance","text":"StagedDeltaTableV2 takes the following to be created: Identifier Schema Partitions ( Array[Transform] ) Properties Operation (one of Create , CreateOrReplace , Replace ) StagedDeltaTableV2 is created when DeltaCatalog is requested to stageReplace , stageCreateOrReplace or stageCreate .","title":"Creating Instance"},{"location":"StagedDeltaTableV2/#commitstagedchanges","text":"commitStagedChanges () : Unit commitStagedChanges ...FIXME commitStagedChanges is part of the StagedTable ( Spark SQL ) abstraction.","title":" commitStagedChanges"},{"location":"StagedDeltaTableV2/#abortstagedchanges","text":"abortStagedChanges () : Unit abortStagedChanges does nothing. abortStagedChanges is part of the StagedTable ( Spark SQL ) abstraction.","title":" abortStagedChanges"},{"location":"StagedDeltaTableV2/#creating-writebuilder","text":"newWriteBuilder ( info : LogicalWriteInfo ) : V1WriteBuilder newWriteBuilder ...FIXME newWriteBuilder is part of the SupportsWrite ( Spark SQL ) abstraction.","title":" Creating WriteBuilder"},{"location":"StateCache/","text":"StateCache \u00b6 StateCache is an abstraction of state caches that can cache a Dataset and uncache them all . Contract \u00b6 SparkSession \u00b6 spark : SparkSession SparkSession the cached RDDs belong to Implementations \u00b6 DeltaSourceSnapshot Snapshot Cached RDDs \u00b6 cached : ArrayBuffer [ RDD [ _ ]] StateCache tracks cached RDDs in cached internal registry. cached is given a new RDD when StateCache is requested to cache a Dataset . cached is used when StateCache is requested to get a cached Dataset and uncache . Caching Dataset \u00b6 cacheDS [ A ]( ds : Dataset [ A ], name : String ) : CachedDS [ A ] cacheDS creates a new CachedDS . cacheDS is used when: Snapshot is requested for a cached state ) DeltaSourceSnapshot is requested to initialFiles Uncaching All Cached Datasets \u00b6 uncache [ A ]( ds : Dataset [ A ], name : String ) : CachedDS [ A ] uncache uses the isCached internal flag to avoid multiple executions. uncache is used when: DeltaLog utility is used to access deltaLogCache and a cached entry expires SnapshotManagement is requested to update state of a Delta table DeltaSourceSnapshot is requested to close","title":"StateCache"},{"location":"StateCache/#statecache","text":"StateCache is an abstraction of state caches that can cache a Dataset and uncache them all .","title":"StateCache"},{"location":"StateCache/#contract","text":"","title":"Contract"},{"location":"StateCache/#sparksession","text":"spark : SparkSession SparkSession the cached RDDs belong to","title":" SparkSession"},{"location":"StateCache/#implementations","text":"DeltaSourceSnapshot Snapshot","title":"Implementations"},{"location":"StateCache/#cached-rdds","text":"cached : ArrayBuffer [ RDD [ _ ]] StateCache tracks cached RDDs in cached internal registry. cached is given a new RDD when StateCache is requested to cache a Dataset . cached is used when StateCache is requested to get a cached Dataset and uncache .","title":" Cached RDDs"},{"location":"StateCache/#caching-dataset","text":"cacheDS [ A ]( ds : Dataset [ A ], name : String ) : CachedDS [ A ] cacheDS creates a new CachedDS . cacheDS is used when: Snapshot is requested for a cached state ) DeltaSourceSnapshot is requested to initialFiles","title":" Caching Dataset"},{"location":"StateCache/#uncaching-all-cached-datasets","text":"uncache [ A ]( ds : Dataset [ A ], name : String ) : CachedDS [ A ] uncache uses the isCached internal flag to avoid multiple executions. uncache is used when: DeltaLog utility is used to access deltaLogCache and a cached entry expires SnapshotManagement is requested to update state of a Delta table DeltaSourceSnapshot is requested to close","title":" Uncaching All Cached Datasets"},{"location":"TahoeBatchFileIndex/","text":"TahoeBatchFileIndex \u00b6 TahoeBatchFileIndex is a file index of a delta table at a given version . Creating Instance \u00b6 TahoeBatchFileIndex takes the following to be created: SparkSession ( Spark SQL ) Action Type AddFile s DeltaLog Data directory (as Hadoop Path ) Snapshot TahoeBatchFileIndex is created when: DeltaLog is requested for a DataFrame for given AddFiles DeleteCommand and UpdateCommand are executed (and DeltaCommand is requested for a HadoopFsRelation ) Action Type \u00b6 TahoeBatchFileIndex is given an Action Type identifier when created : batch or streaming when DeltaLog is requested for a batch or streaming DataFrame for given AddFiles , respectively delete for DeleteCommand update for UpdateCommand Important Action Type seems not to be used ever. tableVersion \u00b6 tableVersion : Long tableVersion is always the version of the Snapshot . tableVersion is part of the TahoeFileIndex abstraction. matchingFiles \u00b6 matchingFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ], keepStats : Boolean = false ) : Seq [ AddFile ] matchingFiles filterFileList (that gives a DataFrame ) and collects the AddFile s (using Dataset.collect ). matchingFiles is part of the TahoeFileIndex abstraction. Input Files \u00b6 inputFiles : Array [ String ] inputFiles returns the paths of all the given AddFiles . inputFiles is part of the FileIndex abstraction ( Spark SQL ). Partitions \u00b6 partitionSchema : StructType partitionSchema requests the Snapshot for the metadata that is in turn requested for the partitionSchema . partitionSchema is part of the FileIndex abstraction ( Spark SQL ). Estimated Size of Relation \u00b6 sizeInBytes : Long sizeInBytes is a sum of the sizes of all the given AddFiles . sizeInBytes is part of the FileIndex abstraction ( Spark SQL ).","title":"TahoeBatchFileIndex"},{"location":"TahoeBatchFileIndex/#tahoebatchfileindex","text":"TahoeBatchFileIndex is a file index of a delta table at a given version .","title":"TahoeBatchFileIndex"},{"location":"TahoeBatchFileIndex/#creating-instance","text":"TahoeBatchFileIndex takes the following to be created: SparkSession ( Spark SQL ) Action Type AddFile s DeltaLog Data directory (as Hadoop Path ) Snapshot TahoeBatchFileIndex is created when: DeltaLog is requested for a DataFrame for given AddFiles DeleteCommand and UpdateCommand are executed (and DeltaCommand is requested for a HadoopFsRelation )","title":"Creating Instance"},{"location":"TahoeBatchFileIndex/#action-type","text":"TahoeBatchFileIndex is given an Action Type identifier when created : batch or streaming when DeltaLog is requested for a batch or streaming DataFrame for given AddFiles , respectively delete for DeleteCommand update for UpdateCommand Important Action Type seems not to be used ever.","title":" Action Type"},{"location":"TahoeBatchFileIndex/#tableversion","text":"tableVersion : Long tableVersion is always the version of the Snapshot . tableVersion is part of the TahoeFileIndex abstraction.","title":" tableVersion"},{"location":"TahoeBatchFileIndex/#matchingfiles","text":"matchingFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ], keepStats : Boolean = false ) : Seq [ AddFile ] matchingFiles filterFileList (that gives a DataFrame ) and collects the AddFile s (using Dataset.collect ). matchingFiles is part of the TahoeFileIndex abstraction.","title":" matchingFiles"},{"location":"TahoeBatchFileIndex/#input-files","text":"inputFiles : Array [ String ] inputFiles returns the paths of all the given AddFiles . inputFiles is part of the FileIndex abstraction ( Spark SQL ).","title":" Input Files"},{"location":"TahoeBatchFileIndex/#partitions","text":"partitionSchema : StructType partitionSchema requests the Snapshot for the metadata that is in turn requested for the partitionSchema . partitionSchema is part of the FileIndex abstraction ( Spark SQL ).","title":" Partitions"},{"location":"TahoeBatchFileIndex/#estimated-size-of-relation","text":"sizeInBytes : Long sizeInBytes is a sum of the sizes of all the given AddFiles . sizeInBytes is part of the FileIndex abstraction ( Spark SQL ).","title":" Estimated Size of Relation"},{"location":"TahoeFileIndex/","text":"TahoeFileIndex \u00b6 TahoeFileIndex is an extension of the FileIndex abstraction ( Spark SQL ) for file indices of delta tables that can list data files to scan (based on partition and data filters ). The aim of TahoeFileIndex (and FileIndex in general) is to reduce usage of very expensive disk access for file-related information using Hadoop FileSystem API. Contract \u00b6 matchingFiles \u00b6 matchingFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ]) : Seq [ AddFile ] AddFile s matching given partition and data filters (predicates) Used for listing data files Implementations \u00b6 PinnedTahoeFileIndex TahoeBatchFileIndex TahoeLogFileIndex Creating Instance \u00b6 TahoeFileIndex takes the following to be created: SparkSession DeltaLog Hadoop Path Abstract Class TahoeFileIndex is an abstract class and cannot be created directly. It is created indirectly for the concrete TahoeFileIndexes . Root Paths \u00b6 rootPaths : Seq [ Path ] rootPaths is the path only. rootPaths is part of the FileIndex abstraction ( Spark SQL ). Listing Files \u00b6 listFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ]) : Seq [ PartitionDirectory ] listFiles is the path only. listFiles is part of the FileIndex abstraction ( Spark SQL ). Partitions \u00b6 partitionSchema : StructType partitionSchema is the partition schema of (the Metadata of the Snapshot ) of the DeltaLog . partitionSchema is part of the FileIndex abstraction ( Spark SQL ). Version of Delta Table \u00b6 tableVersion : Long tableVersion is the version of (the snapshot of) the DeltaLog . tableVersion is used when TahoeFileIndex is requested for the human-friendly textual representation . Textual Representation \u00b6 toString : String toString returns the following text (using the version and the path of the Delta table): Delta[version=[tableVersion], [truncatedPath]] toString is part of the java.lang.Object contract for a string representation of the object.","title":"TahoeFileIndex"},{"location":"TahoeFileIndex/#tahoefileindex","text":"TahoeFileIndex is an extension of the FileIndex abstraction ( Spark SQL ) for file indices of delta tables that can list data files to scan (based on partition and data filters ). The aim of TahoeFileIndex (and FileIndex in general) is to reduce usage of very expensive disk access for file-related information using Hadoop FileSystem API.","title":"TahoeFileIndex"},{"location":"TahoeFileIndex/#contract","text":"","title":"Contract"},{"location":"TahoeFileIndex/#matchingfiles","text":"matchingFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ]) : Seq [ AddFile ] AddFile s matching given partition and data filters (predicates) Used for listing data files","title":" matchingFiles"},{"location":"TahoeFileIndex/#implementations","text":"PinnedTahoeFileIndex TahoeBatchFileIndex TahoeLogFileIndex","title":"Implementations"},{"location":"TahoeFileIndex/#creating-instance","text":"TahoeFileIndex takes the following to be created: SparkSession DeltaLog Hadoop Path Abstract Class TahoeFileIndex is an abstract class and cannot be created directly. It is created indirectly for the concrete TahoeFileIndexes .","title":"Creating Instance"},{"location":"TahoeFileIndex/#root-paths","text":"rootPaths : Seq [ Path ] rootPaths is the path only. rootPaths is part of the FileIndex abstraction ( Spark SQL ).","title":" Root Paths"},{"location":"TahoeFileIndex/#listing-files","text":"listFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ]) : Seq [ PartitionDirectory ] listFiles is the path only. listFiles is part of the FileIndex abstraction ( Spark SQL ).","title":" Listing Files"},{"location":"TahoeFileIndex/#partitions","text":"partitionSchema : StructType partitionSchema is the partition schema of (the Metadata of the Snapshot ) of the DeltaLog . partitionSchema is part of the FileIndex abstraction ( Spark SQL ).","title":" Partitions"},{"location":"TahoeFileIndex/#version-of-delta-table","text":"tableVersion : Long tableVersion is the version of (the snapshot of) the DeltaLog . tableVersion is used when TahoeFileIndex is requested for the human-friendly textual representation .","title":" Version of Delta Table"},{"location":"TahoeFileIndex/#textual-representation","text":"toString : String toString returns the following text (using the version and the path of the Delta table): Delta[version=[tableVersion], [truncatedPath]] toString is part of the java.lang.Object contract for a string representation of the object.","title":" Textual Representation"},{"location":"TahoeLogFileIndex/","text":"TahoeLogFileIndex \u00b6 TahoeLogFileIndex is a file index . Creating Instance \u00b6 TahoeLogFileIndex takes the following to be created: SparkSession ( Spark SQL ) DeltaLog Data directory of the Delta table (as a Hadoop Path ) Snapshot at analysis Partition Filters (as Catalyst expressions; default: empty) isTimeTravelQuery flag (default: false ) TahoeLogFileIndex is created when: DeltaLog is requested for an Insertable HadoopFsRelation Demo \u00b6 val q = spark.read.format(\"delta\").load(\"/tmp/delta/users\") val plan = q.queryExecution.executedPlan import org.apache.spark.sql.execution.FileSourceScanExec val scan = plan.collect { case e: FileSourceScanExec => e }.head import org.apache.spark.sql.delta.files.TahoeLogFileIndex val index = scan.relation.location.asInstanceOf[TahoeLogFileIndex] scala> println(index) Delta[version=1, file:/tmp/delta/users] matchingFiles Method \u00b6 matchingFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ], keepStats : Boolean = false ) : Seq [ AddFile ] matchingFiles gets the snapshot (with stalenessAcceptable flag off) and requests it for the files to scan (for the index's partition filters , the given partitionFilters and dataFilters ). Note inputFiles and matchingFiles are similar. Both get the snapshot (of the delta table), but they use different filtering expressions and return value types. matchingFiles is part of the TahoeFileIndex abstraction. inputFiles Method \u00b6 inputFiles : Array [ String ] inputFiles gets the snapshot (with stalenessAcceptable flag off) and requests it for the files to scan (for the index's partition filters only). Note inputFiles and matchingFiles are similar. Both get the snapshot , but they use different filtering expressions and return value types. inputFiles is part of the FileIndex contract (Spark SQL). Historical Or Latest Snapshot \u00b6 getSnapshot ( stalenessAcceptable : Boolean ) : Snapshot getSnapshot returns a Snapshot that is either the historical snapshot (for the snapshot version if specified) or requests the DeltaLog to update (and give one). getSnapshot is used when TahoeLogFileIndex is requested for the matching files and the input files . Internal Properties \u00b6 historicalSnapshotOpt \u00b6 Historical snapshot that is the Snapshot for the versionToUse if defined. Used when TahoeLogFileIndex is requested for the (historical or latest) snapshot and the schema of the partition columns","title":"TahoeLogFileIndex"},{"location":"TahoeLogFileIndex/#tahoelogfileindex","text":"TahoeLogFileIndex is a file index .","title":"TahoeLogFileIndex"},{"location":"TahoeLogFileIndex/#creating-instance","text":"TahoeLogFileIndex takes the following to be created: SparkSession ( Spark SQL ) DeltaLog Data directory of the Delta table (as a Hadoop Path ) Snapshot at analysis Partition Filters (as Catalyst expressions; default: empty) isTimeTravelQuery flag (default: false ) TahoeLogFileIndex is created when: DeltaLog is requested for an Insertable HadoopFsRelation","title":"Creating Instance"},{"location":"TahoeLogFileIndex/#demo","text":"val q = spark.read.format(\"delta\").load(\"/tmp/delta/users\") val plan = q.queryExecution.executedPlan import org.apache.spark.sql.execution.FileSourceScanExec val scan = plan.collect { case e: FileSourceScanExec => e }.head import org.apache.spark.sql.delta.files.TahoeLogFileIndex val index = scan.relation.location.asInstanceOf[TahoeLogFileIndex] scala> println(index) Delta[version=1, file:/tmp/delta/users]","title":"Demo"},{"location":"TahoeLogFileIndex/#matchingfiles-method","text":"matchingFiles ( partitionFilters : Seq [ Expression ], dataFilters : Seq [ Expression ], keepStats : Boolean = false ) : Seq [ AddFile ] matchingFiles gets the snapshot (with stalenessAcceptable flag off) and requests it for the files to scan (for the index's partition filters , the given partitionFilters and dataFilters ). Note inputFiles and matchingFiles are similar. Both get the snapshot (of the delta table), but they use different filtering expressions and return value types. matchingFiles is part of the TahoeFileIndex abstraction.","title":" matchingFiles Method"},{"location":"TahoeLogFileIndex/#inputfiles-method","text":"inputFiles : Array [ String ] inputFiles gets the snapshot (with stalenessAcceptable flag off) and requests it for the files to scan (for the index's partition filters only). Note inputFiles and matchingFiles are similar. Both get the snapshot , but they use different filtering expressions and return value types. inputFiles is part of the FileIndex contract (Spark SQL).","title":" inputFiles Method"},{"location":"TahoeLogFileIndex/#historical-or-latest-snapshot","text":"getSnapshot ( stalenessAcceptable : Boolean ) : Snapshot getSnapshot returns a Snapshot that is either the historical snapshot (for the snapshot version if specified) or requests the DeltaLog to update (and give one). getSnapshot is used when TahoeLogFileIndex is requested for the matching files and the input files .","title":" Historical Or Latest Snapshot"},{"location":"TahoeLogFileIndex/#internal-properties","text":"","title":"Internal Properties"},{"location":"TahoeLogFileIndex/#historicalsnapshotopt","text":"Historical snapshot that is the Snapshot for the versionToUse if defined. Used when TahoeLogFileIndex is requested for the (historical or latest) snapshot and the schema of the partition columns","title":" historicalSnapshotOpt"},{"location":"TransactionalWrite/","text":"TransactionalWrite \u00b6 TransactionalWrite is an abstraction of optimistic transactional writers that can write a structured query out to a Delta table . Contract \u00b6 DeltaLog \u00b6 deltaLog : DeltaLog DeltaLog (of a delta table) that this transaction is changing Used when: ActiveOptimisticTransactionRule logical rule is executed OptimisticTransactionImpl is requested to prepare a commit , doCommit , checkAndRetry , and perform post-commit operations (and execute delta log checkpoint ) ConvertToDeltaCommand is executed DeltaCommand is requested to buildBaseRelation and commitLarge MergeIntoCommand is executed TransactionalWrite is requested to write a structured query out to a delta table GenerateSymlinkManifest post-commit hook is executed ImplicitMetadataOperation is requested to updateMetadata DeltaSink is requested to addBatch Metadata \u00b6 metadata : Metadata Metadata (of the delta table ) that this transaction is changing Protocol \u00b6 protocol : Protocol Protocol (of the delta table ) that this transaction is changing Used when: OptimisticTransactionImpl is requested to updateMetadata , verifyNewMetadata and prepareCommit ConvertToDeltaCommand is executed Snapshot \u00b6 snapshot : Snapshot Snapshot (of the delta table ) that this transaction is reading at Implementations \u00b6 OptimisticTransaction hasWritten Flag \u00b6 hasWritten : Boolean = false TransactionalWrite uses hasWritten internal registry to prevent OptimisticTransactionImpl from updating metadata after having written out files . hasWritten is initially false and changes to true after having written out files . Writing Data Out (Result Of Structured Query) \u00b6 writeFiles ( data : Dataset [ _ ]) : Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], writeOptions : Option [ DeltaOptions ]) : Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], isOptimize : Boolean ) : Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], writeOptions : Option [ DeltaOptions ], isOptimize : Boolean ) : Seq [ AddFile ] writeFiles creates a DeltaInvariantCheckerExec and a DelayedCommitProtocol to write out files to the data path (of the DeltaLog ). Learn more writeFiles uses Spark SQL's FileFormatWriter utility to write out a result of a streaming query. Learn about FileFormatWriter in The Internals of Spark SQL online book. writeFiles is executed within SQLExecution.withNewExecutionId . Learn more writeFiles can be tracked using web UI or SQLAppStatusListener (using SparkListenerSQLExecutionStart and SparkListenerSQLExecutionEnd events). Learn about SQLAppStatusListener in The Internals of Spark SQL online book. In the end, writeFiles returns the addedStatuses of the DelayedCommitProtocol committer. Internally, writeFiles turns the hasWritten flag on ( true ). Note After writeFiles , no metadata updates in the transaction are permitted. writeFiles normalize the given data dataset (based on the partitionColumns of the Metadata ). writeFiles getPartitioningColumns based on the partitionSchema of the Metadata . DelayedCommitProtocol Committer \u00b6 writeFiles creates a DelayedCommitProtocol committer for the data path of the DeltaLog . writeFiles gets the invariants from the schema of the Metadata . DeltaInvariantCheckerExec \u00b6 writeFiles requests a new Execution ID (that is used to track all Spark jobs of FileFormatWriter.write in Spark SQL) with a physical query plan of a new DeltaInvariantCheckerExec unary physical operator (with the executed plan of the normalized query execution as the child operator). Creating Committer \u00b6 getCommitter ( outputPath : Path ) : DelayedCommitProtocol getCommitter creates a new DelayedCommitProtocol with the delta job ID and the given outputPath (and no random prefix). getPartitioningColumns \u00b6 getPartitioningColumns ( partitionSchema : StructType , output : Seq [ Attribute ], colsDropped : Boolean ) : Seq [ Attribute ] getPartitioningColumns ...FIXME normalizeData \u00b6 normalizeData ( data : Dataset [ _ ], partitionCols : Seq [ String ]) : ( QueryExecution , Seq [ Attribute ]) normalizeData ...FIXME makeOutputNullable \u00b6 makeOutputNullable ( output : Seq [ Attribute ]) : Seq [ Attribute ] makeOutputNullable ...FIXME Usage \u00b6 writeFiles is used when: DeleteCommand , MergeIntoCommand , UpdateCommand , and WriteIntoDelta commands are executed DeltaSink is requested to add a streaming micro-batch","title":"TransactionalWrite"},{"location":"TransactionalWrite/#transactionalwrite","text":"TransactionalWrite is an abstraction of optimistic transactional writers that can write a structured query out to a Delta table .","title":"TransactionalWrite"},{"location":"TransactionalWrite/#contract","text":"","title":"Contract"},{"location":"TransactionalWrite/#deltalog","text":"deltaLog : DeltaLog DeltaLog (of a delta table) that this transaction is changing Used when: ActiveOptimisticTransactionRule logical rule is executed OptimisticTransactionImpl is requested to prepare a commit , doCommit , checkAndRetry , and perform post-commit operations (and execute delta log checkpoint ) ConvertToDeltaCommand is executed DeltaCommand is requested to buildBaseRelation and commitLarge MergeIntoCommand is executed TransactionalWrite is requested to write a structured query out to a delta table GenerateSymlinkManifest post-commit hook is executed ImplicitMetadataOperation is requested to updateMetadata DeltaSink is requested to addBatch","title":" DeltaLog"},{"location":"TransactionalWrite/#metadata","text":"metadata : Metadata Metadata (of the delta table ) that this transaction is changing","title":" Metadata"},{"location":"TransactionalWrite/#protocol","text":"protocol : Protocol Protocol (of the delta table ) that this transaction is changing Used when: OptimisticTransactionImpl is requested to updateMetadata , verifyNewMetadata and prepareCommit ConvertToDeltaCommand is executed","title":" Protocol"},{"location":"TransactionalWrite/#snapshot","text":"snapshot : Snapshot Snapshot (of the delta table ) that this transaction is reading at","title":" Snapshot"},{"location":"TransactionalWrite/#implementations","text":"OptimisticTransaction","title":"Implementations"},{"location":"TransactionalWrite/#haswritten-flag","text":"hasWritten : Boolean = false TransactionalWrite uses hasWritten internal registry to prevent OptimisticTransactionImpl from updating metadata after having written out files . hasWritten is initially false and changes to true after having written out files .","title":" hasWritten Flag"},{"location":"TransactionalWrite/#writing-data-out-result-of-structured-query","text":"writeFiles ( data : Dataset [ _ ]) : Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], writeOptions : Option [ DeltaOptions ]) : Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], isOptimize : Boolean ) : Seq [ AddFile ] writeFiles ( data : Dataset [ _ ], writeOptions : Option [ DeltaOptions ], isOptimize : Boolean ) : Seq [ AddFile ] writeFiles creates a DeltaInvariantCheckerExec and a DelayedCommitProtocol to write out files to the data path (of the DeltaLog ). Learn more writeFiles uses Spark SQL's FileFormatWriter utility to write out a result of a streaming query. Learn about FileFormatWriter in The Internals of Spark SQL online book. writeFiles is executed within SQLExecution.withNewExecutionId . Learn more writeFiles can be tracked using web UI or SQLAppStatusListener (using SparkListenerSQLExecutionStart and SparkListenerSQLExecutionEnd events). Learn about SQLAppStatusListener in The Internals of Spark SQL online book. In the end, writeFiles returns the addedStatuses of the DelayedCommitProtocol committer. Internally, writeFiles turns the hasWritten flag on ( true ). Note After writeFiles , no metadata updates in the transaction are permitted. writeFiles normalize the given data dataset (based on the partitionColumns of the Metadata ). writeFiles getPartitioningColumns based on the partitionSchema of the Metadata .","title":" Writing Data Out (Result Of Structured Query)"},{"location":"TransactionalWrite/#delayedcommitprotocol-committer","text":"writeFiles creates a DelayedCommitProtocol committer for the data path of the DeltaLog . writeFiles gets the invariants from the schema of the Metadata .","title":" DelayedCommitProtocol Committer"},{"location":"TransactionalWrite/#deltainvariantcheckerexec","text":"writeFiles requests a new Execution ID (that is used to track all Spark jobs of FileFormatWriter.write in Spark SQL) with a physical query plan of a new DeltaInvariantCheckerExec unary physical operator (with the executed plan of the normalized query execution as the child operator).","title":" DeltaInvariantCheckerExec"},{"location":"TransactionalWrite/#creating-committer","text":"getCommitter ( outputPath : Path ) : DelayedCommitProtocol getCommitter creates a new DelayedCommitProtocol with the delta job ID and the given outputPath (and no random prefix).","title":" Creating Committer"},{"location":"TransactionalWrite/#getpartitioningcolumns","text":"getPartitioningColumns ( partitionSchema : StructType , output : Seq [ Attribute ], colsDropped : Boolean ) : Seq [ Attribute ] getPartitioningColumns ...FIXME","title":" getPartitioningColumns"},{"location":"TransactionalWrite/#normalizedata","text":"normalizeData ( data : Dataset [ _ ], partitionCols : Seq [ String ]) : ( QueryExecution , Seq [ Attribute ]) normalizeData ...FIXME","title":" normalizeData"},{"location":"TransactionalWrite/#makeoutputnullable","text":"makeOutputNullable ( output : Seq [ Attribute ]) : Seq [ Attribute ] makeOutputNullable ...FIXME","title":" makeOutputNullable"},{"location":"TransactionalWrite/#usage","text":"writeFiles is used when: DeleteCommand , MergeIntoCommand , UpdateCommand , and WriteIntoDelta commands are executed DeltaSink is requested to add a streaming micro-batch","title":" Usage"},{"location":"VerifyChecksum/","text":"= VerifyChecksum VerifyChecksum is...FIXME == [[validateChecksum]] validateChecksum Method [source, scala] \u00b6 validateChecksum(snapshot: Snapshot): Unit \u00b6 validateChecksum ...FIXME NOTE: validateChecksum is used when...FIXME","title":"VerifyChecksum"},{"location":"VerifyChecksum/#source-scala","text":"","title":"[source, scala]"},{"location":"VerifyChecksum/#validatechecksumsnapshot-snapshot-unit","text":"validateChecksum ...FIXME NOTE: validateChecksum is used when...FIXME","title":"validateChecksum(snapshot: Snapshot): Unit"},{"location":"WriteIntoDeltaBuilder/","text":"WriteIntoDeltaBuilder \u00b6 WriteIntoDeltaBuilder is a WriteBuilder ( Spark SQL ) with support for the following capabilities: SupportsOverwrite ( Spark SQL ) V1WriteBuilder ( Spark SQL ) SupportsTruncate ( Spark SQL ) Creating Instance \u00b6 WriteIntoDeltaBuilder takes the following to be created: DeltaLog Write-Specific Options WriteIntoDeltaBuilder is created when: DeltaTableV2 is requested for a WriteBuilder buildForV1Write \u00b6 buildForV1Write () : InsertableRelation buildForV1Write is part of the V1WriteBuilder ( Spark SQL ) abstraction. buildForV1Write creates an InsertableRelation ( Spark SQL ) that does the following when requested to insert : Creates and executes a WriteIntoDelta command Re-cache all cached plans (by requesting the CacheManager to recacheByPlan for a LogicalRelation over the BaseRelation of the DeltaLog )","title":"WriteIntoDeltaBuilder"},{"location":"WriteIntoDeltaBuilder/#writeintodeltabuilder","text":"WriteIntoDeltaBuilder is a WriteBuilder ( Spark SQL ) with support for the following capabilities: SupportsOverwrite ( Spark SQL ) V1WriteBuilder ( Spark SQL ) SupportsTruncate ( Spark SQL )","title":"WriteIntoDeltaBuilder"},{"location":"WriteIntoDeltaBuilder/#creating-instance","text":"WriteIntoDeltaBuilder takes the following to be created: DeltaLog Write-Specific Options WriteIntoDeltaBuilder is created when: DeltaTableV2 is requested for a WriteBuilder","title":"Creating Instance"},{"location":"WriteIntoDeltaBuilder/#buildforv1write","text":"buildForV1Write () : InsertableRelation buildForV1Write is part of the V1WriteBuilder ( Spark SQL ) abstraction. buildForV1Write creates an InsertableRelation ( Spark SQL ) that does the following when requested to insert : Creates and executes a WriteIntoDelta command Re-cache all cached plans (by requesting the CacheManager to recacheByPlan for a LogicalRelation over the BaseRelation of the DeltaLog )","title":" buildForV1Write"},{"location":"installation/","text":"Installation \u00b6 Delta Lake is a Spark data source and as such installation boils down to using spark-submit's --packages command-line option. Delta Lake also requires DeltaSparkSessionExtension and DeltaCatalog to be registered (using respective configuration properties). Spark SQL Application \u00b6 import org.apache.spark.sql.SparkSession val spark = SparkSession . builder () . appName ( \"...\" ) . config ( \"spark.sql.extensions\" , \"io.delta.sql.DeltaSparkSessionExtension\" ) . config ( \"spark.sql.catalog.spark_catalog\" , \"org.apache.spark.sql.delta.catalog.DeltaCatalog\" ) . getOrCreate Spark Shell \u00b6 ./bin/spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog","title":"Installation"},{"location":"installation/#installation","text":"Delta Lake is a Spark data source and as such installation boils down to using spark-submit's --packages command-line option. Delta Lake also requires DeltaSparkSessionExtension and DeltaCatalog to be registered (using respective configuration properties).","title":"Installation"},{"location":"installation/#spark-sql-application","text":"import org.apache.spark.sql.SparkSession val spark = SparkSession . builder () . appName ( \"...\" ) . config ( \"spark.sql.extensions\" , \"io.delta.sql.DeltaSparkSessionExtension\" ) . config ( \"spark.sql.catalog.spark_catalog\" , \"org.apache.spark.sql.delta.catalog.DeltaCatalog\" ) . getOrCreate","title":" Spark SQL Application"},{"location":"installation/#spark-shell","text":"./bin/spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog","title":" Spark Shell"},{"location":"options/","text":"Options \u00b6 Delta Lake comes with options to fine-tune its uses. They can be defined using option method of the following: DataFrameReader ( Spark SQL ) and DataFrameWriter ( Spark SQL ) for batch queries DataStreamReader ( Spark Structured Streaming ) and DataStreamWriter ( Spark Structured Streaming ) for streaming queries SQL queries checkpointLocation \u00b6 Checkpoint directory for storing checkpoint data of streaming queries ( Spark Structured Streaming ). dataChange \u00b6 Whether to write new data to the table or just rearrange data that is already part of the table. This option declares that the data being written by this job does not change any data in the table and merely rearranges existing data. This makes sure streaming queries reading from this table will not see any new changes Used when: DeltaWriteOptionsImpl is requested for rearrangeOnly Demo Learn more in Demo: dataChange . excludeRegex \u00b6 ignoreChanges \u00b6 ignoreDeletes \u00b6 ignoreFileDeletion \u00b6 maxBytesPerTrigger \u00b6 maxFilesPerTrigger \u00b6 Maximum number of files ( AddFiles ) that DeltaSource is supposed to scan ( read ) in a streaming micro-batch ( trigger ) Default: 1000 Must be at least 1 mergeSchema \u00b6 Enables schema migration (and allows automatic schema merging during a write operation for WriteIntoDelta and DeltaSink ) Equivalent SQL Session configuration: spark.databricks.delta.schema.autoMerge.enabled optimizeWrite \u00b6 Enables...FIXME overwriteSchema \u00b6 path \u00b6 (required) Directory on a Hadoop DFS-compliant file system with an optional time travel identifier Default: (undefined) Note Can also be specified using load method of DataFrameReader and DataStreamReader . queryName \u00b6 replaceWhere \u00b6 Demo Learn more in Demo: replaceWhere . timestampAsOf \u00b6 Timestamp of the version of a Delta table for Time Travel Mutually exclusive with versionAsOf option and the time travel identifier of the path option. userMetadata \u00b6 Defines a user-defined commit metadata Take precedence over spark.databricks.delta.commitInfo.userMetadata Available by inspecting CommitInfo s using DESCRIBE HISTORY or DeltaTable.history . Demo Learn more in Demo: User Metadata for Labelling Commits . versionAsOf \u00b6 Version of a Delta table for Time Travel Mutually exclusive with timestampAsOf option and the time travel identifier of the path option. Used when: DeltaDataSource is requested for a relation","title":"Options"},{"location":"options/#options","text":"Delta Lake comes with options to fine-tune its uses. They can be defined using option method of the following: DataFrameReader ( Spark SQL ) and DataFrameWriter ( Spark SQL ) for batch queries DataStreamReader ( Spark Structured Streaming ) and DataStreamWriter ( Spark Structured Streaming ) for streaming queries SQL queries","title":"Options"},{"location":"options/#checkpointlocation","text":"Checkpoint directory for storing checkpoint data of streaming queries ( Spark Structured Streaming ).","title":" checkpointLocation"},{"location":"options/#datachange","text":"Whether to write new data to the table or just rearrange data that is already part of the table. This option declares that the data being written by this job does not change any data in the table and merely rearranges existing data. This makes sure streaming queries reading from this table will not see any new changes Used when: DeltaWriteOptionsImpl is requested for rearrangeOnly Demo Learn more in Demo: dataChange .","title":" dataChange"},{"location":"options/#excluderegex","text":"","title":" excludeRegex"},{"location":"options/#ignorechanges","text":"","title":" ignoreChanges"},{"location":"options/#ignoredeletes","text":"","title":" ignoreDeletes"},{"location":"options/#ignorefiledeletion","text":"","title":" ignoreFileDeletion"},{"location":"options/#maxbytespertrigger","text":"","title":" maxBytesPerTrigger"},{"location":"options/#maxfilespertrigger","text":"Maximum number of files ( AddFiles ) that DeltaSource is supposed to scan ( read ) in a streaming micro-batch ( trigger ) Default: 1000 Must be at least 1","title":" maxFilesPerTrigger"},{"location":"options/#mergeschema","text":"Enables schema migration (and allows automatic schema merging during a write operation for WriteIntoDelta and DeltaSink ) Equivalent SQL Session configuration: spark.databricks.delta.schema.autoMerge.enabled","title":" mergeSchema"},{"location":"options/#optimizewrite","text":"Enables...FIXME","title":" optimizeWrite"},{"location":"options/#overwriteschema","text":"","title":" overwriteSchema"},{"location":"options/#path","text":"(required) Directory on a Hadoop DFS-compliant file system with an optional time travel identifier Default: (undefined) Note Can also be specified using load method of DataFrameReader and DataStreamReader .","title":" path"},{"location":"options/#queryname","text":"","title":" queryName"},{"location":"options/#replacewhere","text":"Demo Learn more in Demo: replaceWhere .","title":" replaceWhere"},{"location":"options/#timestampasof","text":"Timestamp of the version of a Delta table for Time Travel Mutually exclusive with versionAsOf option and the time travel identifier of the path option.","title":" timestampAsOf"},{"location":"options/#usermetadata","text":"Defines a user-defined commit metadata Take precedence over spark.databricks.delta.commitInfo.userMetadata Available by inspecting CommitInfo s using DESCRIBE HISTORY or DeltaTable.history . Demo Learn more in Demo: User Metadata for Labelling Commits .","title":" userMetadata"},{"location":"options/#versionasof","text":"Version of a Delta table for Time Travel Mutually exclusive with timestampAsOf option and the time travel identifier of the path option. Used when: DeltaDataSource is requested for a relation","title":" versionAsOf"},{"location":"overview/","text":"Delta Lake \u00b6 Delta Lake is an open-source storage management system (storage layer) that brings ACID transactions and time travel to Apache Spark and big data workloads. Important As of 0.7.0 Delta Lake requires Spark 3 (starting from the first 3.0.0 release). Delta Lake is a table format. It introduces DeltaTable abstraction that is simply a parquet table with a transactional log . Changes to (the state of) a delta table are reflected as actions and persisted to the transactional log (in JSON format ). Delta Lake uses OptimisticTransaction for transactional writes . A commit is successful when the transaction can write the actions to a delta file (in the transactional log ). In case the delta file for the commit version already exists, the transaction is retried . Structured queries can write (transactionally) to a delta table using the following interfaces: WriteIntoDelta command for batch queries (Spark SQL) DeltaSink for streaming queries (Spark Structured Streaming) More importantly, multiple queries can write to the same delta table simultaneously (at exactly the same time). Delta Lake provides DeltaTable API to programmatically access Delta tables. A delta table can be created based on a parquet table or from scratch . Delta Lake supports batch and streaming queries (Spark SQL and Structured Streaming, respectively) using delta format. In order to fine tune queries over data in Delta Lake use options . Among the options path option is mandatory. Delta Lake supports reading and writing in batch queries: Batch reads (as a RelationProvider ) Batch writes (as a CreatableRelationProvider ) Delta Lake supports reading and writing in streaming queries: Stream reads (as a Source ) Stream writes (as a Sink ) Delta Lake uses LogStore abstraction to read and write physical log files and checkpoints (using Hadoop FileSystem API ). Delta Tables in Logical Query Plans \u00b6 Delta Table defines DeltaTable Scala extractor to find delta tables in a logical query plan. The extractor finds LogicalRelation s ( Spark SQL ) with HadoopFsRelation ( Spark SQL ) and TahoeFileIndex . Put simply, delta tables are LogicalRelation s with HadoopFsRelation with TahoeFileIndex in logical query plans.","title":"Overview"},{"location":"overview/#delta-lake","text":"Delta Lake is an open-source storage management system (storage layer) that brings ACID transactions and time travel to Apache Spark and big data workloads. Important As of 0.7.0 Delta Lake requires Spark 3 (starting from the first 3.0.0 release). Delta Lake is a table format. It introduces DeltaTable abstraction that is simply a parquet table with a transactional log . Changes to (the state of) a delta table are reflected as actions and persisted to the transactional log (in JSON format ). Delta Lake uses OptimisticTransaction for transactional writes . A commit is successful when the transaction can write the actions to a delta file (in the transactional log ). In case the delta file for the commit version already exists, the transaction is retried . Structured queries can write (transactionally) to a delta table using the following interfaces: WriteIntoDelta command for batch queries (Spark SQL) DeltaSink for streaming queries (Spark Structured Streaming) More importantly, multiple queries can write to the same delta table simultaneously (at exactly the same time). Delta Lake provides DeltaTable API to programmatically access Delta tables. A delta table can be created based on a parquet table or from scratch . Delta Lake supports batch and streaming queries (Spark SQL and Structured Streaming, respectively) using delta format. In order to fine tune queries over data in Delta Lake use options . Among the options path option is mandatory. Delta Lake supports reading and writing in batch queries: Batch reads (as a RelationProvider ) Batch writes (as a CreatableRelationProvider ) Delta Lake supports reading and writing in streaming queries: Stream reads (as a Source ) Stream writes (as a Sink ) Delta Lake uses LogStore abstraction to read and write physical log files and checkpoints (using Hadoop FileSystem API ).","title":"Delta Lake"},{"location":"overview/#delta-tables-in-logical-query-plans","text":"Delta Table defines DeltaTable Scala extractor to find delta tables in a logical query plan. The extractor finds LogicalRelation s ( Spark SQL ) with HadoopFsRelation ( Spark SQL ) and TahoeFileIndex . Put simply, delta tables are LogicalRelation s with HadoopFsRelation with TahoeFileIndex in logical query plans.","title":"Delta Tables in Logical Query Plans"},{"location":"spark-logging/","text":"Logging \u00b6 Spark uses log4j for logging. Note Learn more on Spark Logging in The Internals of Apache Spark online book.","title":"Logging"},{"location":"spark-logging/#logging","text":"Spark uses log4j for logging. Note Learn more on Spark Logging in The Internals of Apache Spark online book.","title":"Logging"},{"location":"time-travel/","text":"Time Travel \u00b6 Delta Lake supports time travelling which is loading a Delta table at a given version or timestamp (defined by path , versionAsOf or timestampAsOf options). Time travel is described using DeltaTimeTravelSpec .","title":"Time Travel"},{"location":"time-travel/#time-travel","text":"Delta Lake supports time travelling which is loading a Delta table at a given version or timestamp (defined by path , versionAsOf or timestampAsOf options). Time travel is described using DeltaTimeTravelSpec .","title":"Time Travel"},{"location":"commands/","text":"Commands \u00b6","title":"Commands"},{"location":"commands/#commands","text":"","title":"Commands"},{"location":"commands/AlterDeltaTableCommand/","text":"AlterDeltaTableCommand \u00b6 AlterDeltaTableCommand is an extension of the DeltaCommand abstraction for Delta commands that alter a DeltaTableV2 . Contract \u00b6 table \u00b6 table : DeltaTableV2 DeltaTableV2 Used when AlterDeltaTableCommand is requested to startTransaction Implementations \u00b6 AlterTableAddColumnsDeltaCommand AlterTableChangeColumnDeltaCommand AlterTableReplaceColumnsDeltaCommand AlterTableSetLocationDeltaCommand AlterTableSetPropertiesDeltaCommand AlterTableUnsetPropertiesDeltaCommand startTransaction \u00b6 startTransaction () : OptimisticTransaction startTransaction simply requests the DeltaTableV2 for the DeltaLog that in turn is requested to startTransaction .","title":"AlterDeltaTableCommand"},{"location":"commands/AlterDeltaTableCommand/#alterdeltatablecommand","text":"AlterDeltaTableCommand is an extension of the DeltaCommand abstraction for Delta commands that alter a DeltaTableV2 .","title":"AlterDeltaTableCommand"},{"location":"commands/AlterDeltaTableCommand/#contract","text":"","title":"Contract"},{"location":"commands/AlterDeltaTableCommand/#table","text":"table : DeltaTableV2 DeltaTableV2 Used when AlterDeltaTableCommand is requested to startTransaction","title":" table"},{"location":"commands/AlterDeltaTableCommand/#implementations","text":"AlterTableAddColumnsDeltaCommand AlterTableChangeColumnDeltaCommand AlterTableReplaceColumnsDeltaCommand AlterTableSetLocationDeltaCommand AlterTableSetPropertiesDeltaCommand AlterTableUnsetPropertiesDeltaCommand","title":"Implementations"},{"location":"commands/AlterDeltaTableCommand/#starttransaction","text":"startTransaction () : OptimisticTransaction startTransaction simply requests the DeltaTableV2 for the DeltaLog that in turn is requested to startTransaction .","title":" startTransaction"},{"location":"commands/AlterTableAddColumnsDeltaCommand/","text":"AlterTableAddColumnsDeltaCommand \u00b6 AlterTableAddColumnsDeltaCommand is...FIXME","title":"AlterTableAddColumnsDeltaCommand"},{"location":"commands/AlterTableAddColumnsDeltaCommand/#altertableaddcolumnsdeltacommand","text":"AlterTableAddColumnsDeltaCommand is...FIXME","title":"AlterTableAddColumnsDeltaCommand"},{"location":"commands/AlterTableAddConstraintDeltaCommand/","text":"AlterTableAddConstraintDeltaCommand \u00b6 AlterTableAddConstraintDeltaCommand is...FIXME","title":"AlterTableAddConstraintDeltaCommand"},{"location":"commands/AlterTableAddConstraintDeltaCommand/#altertableaddconstraintdeltacommand","text":"AlterTableAddConstraintDeltaCommand is...FIXME","title":"AlterTableAddConstraintDeltaCommand"},{"location":"commands/AlterTableChangeColumnDeltaCommand/","text":"AlterTableChangeColumnDeltaCommand \u00b6 AlterTableChangeColumnDeltaCommand is...FIXME","title":"AlterTableChangeColumnDeltaCommand"},{"location":"commands/AlterTableChangeColumnDeltaCommand/#altertablechangecolumndeltacommand","text":"AlterTableChangeColumnDeltaCommand is...FIXME","title":"AlterTableChangeColumnDeltaCommand"},{"location":"commands/AlterTableDropConstraintDeltaCommand/","text":"AlterTableDropConstraintDeltaCommand \u00b6 AlterTableDropConstraintDeltaCommand is...FIXME","title":"AlterTableDropConstraintDeltaCommand"},{"location":"commands/AlterTableDropConstraintDeltaCommand/#altertabledropconstraintdeltacommand","text":"AlterTableDropConstraintDeltaCommand is...FIXME","title":"AlterTableDropConstraintDeltaCommand"},{"location":"commands/AlterTableReplaceColumnsDeltaCommand/","text":"AlterTableReplaceColumnsDeltaCommand \u00b6 AlterTableReplaceColumnsDeltaCommand is...FIXME","title":"AlterTableReplaceColumnsDeltaCommand"},{"location":"commands/AlterTableReplaceColumnsDeltaCommand/#altertablereplacecolumnsdeltacommand","text":"AlterTableReplaceColumnsDeltaCommand is...FIXME","title":"AlterTableReplaceColumnsDeltaCommand"},{"location":"commands/AlterTableSetLocationDeltaCommand/","text":"AlterTableSetLocationDeltaCommand \u00b6 AlterTableSetLocationDeltaCommand is...FIXME","title":"AlterTableSetLocationDeltaCommand"},{"location":"commands/AlterTableSetLocationDeltaCommand/#altertablesetlocationdeltacommand","text":"AlterTableSetLocationDeltaCommand is...FIXME","title":"AlterTableSetLocationDeltaCommand"},{"location":"commands/AlterTableSetPropertiesDeltaCommand/","text":"AlterTableSetPropertiesDeltaCommand \u00b6 AlterTableSetPropertiesDeltaCommand is...FIXME","title":"AlterTableSetPropertiesDeltaCommand"},{"location":"commands/AlterTableSetPropertiesDeltaCommand/#altertablesetpropertiesdeltacommand","text":"AlterTableSetPropertiesDeltaCommand is...FIXME","title":"AlterTableSetPropertiesDeltaCommand"},{"location":"commands/AlterTableUnsetPropertiesDeltaCommand/","text":"AlterTableUnsetPropertiesDeltaCommand \u00b6 AlterTableUnsetPropertiesDeltaCommand is...FIXME","title":"AlterTableUnsetPropertiesDeltaCommand"},{"location":"commands/AlterTableUnsetPropertiesDeltaCommand/#altertableunsetpropertiesdeltacommand","text":"AlterTableUnsetPropertiesDeltaCommand is...FIXME","title":"AlterTableUnsetPropertiesDeltaCommand"},{"location":"commands/ConvertToDeltaCommand/","text":"ConvertToDeltaCommand \u00b6 ConvertToDeltaCommand is a DeltaCommand that converts a parquet table into delta format ( imports it into Delta). ConvertToDeltaCommand is a Spark SQL RunnableCommand (and executed eagerly on the driver for side-effects). ConvertToDeltaCommand requires that the partition schema matches the partitions of the parquet table ( or an AnalysisException is thrown ) ConvertToDeltaCommandBase is the base of ConvertToDeltaCommand -like commands with the only known implementation being ConvertToDeltaCommand itself. Creating Instance \u00b6 ConvertToDeltaCommand takes the following to be created: Parquet table ( TableIdentifier ) Partition schema ( Option[StructType] ) Delta Path ( Option[String] ) ConvertToDeltaCommand is created when: CONVERT TO DELTA statement is used (and DeltaSqlAstBuilder is requested to visitConvert ) DeltaTable.convertToDelta utility is used (and DeltaConvert utility is used to executeConvert ) Executing Command \u00b6 run ( spark : SparkSession ) : Seq [ Row ] run is part of the RunnableCommand contract. run < > from the < > (with the given SparkSession ). run makes sure that the (data source) provider (the database part of the < >) is either delta or parquet . For all other data source providers, run throws an AnalysisException : CONVERT TO DELTA only supports parquet tables, but you are trying to convert a [sourceName] source: [ident] For delta data source provider, run simply prints out the following message to standard output and returns. The table you are trying to convert is already a delta table For parquet data source provider, run uses DeltaLog utility to < >. run then requests DeltaLog to < > and < >. In the end, run < >. In case the < > of the new transaction is greater than -1 , run simply prints out the following message to standard output and returns. The table you are trying to convert is already a delta table Internal Helper Methods \u00b6 performConvert Method \u00b6 performConvert ( spark : SparkSession , txn : OptimisticTransaction , convertProperties : ConvertTarget ) : Seq [ Row ] performConvert makes sure that the directory exists (from the given ConvertProperties which is the table part of the < > of the command). performConvert requests the OptimisticTransaction for the < > that is then requested to < >. performConvert < > in the directory and leaves only files (by filtering out directories using WHERE clause). NOTE: performConvert uses Dataset API to build a distributed computation to query files. [[performConvert-cache]] performConvert caches the Dataset of file names. [[performConvert-schemaBatchSize]] performConvert uses < > configuration property for the number of files per batch for schema inference. performConvert < > for every batch of files and then < >. performConvert < > using the inferred table schema and the < > (if specified). performConvert creates a new < > using the table schema and the < > (if specified). performConvert requests the OptimisticTransaction to < >. [[performConvert-statsBatchSize]] performConvert uses < > configuration property for the number of files per batch for stats collection. performConvert < > (in the < > of the < > of the OptimisticTransaction ) for every file in a batch. [[performConvert-streamWrite]][[performConvert-unpersist]] In the end, performConvert < > (with the OptimisticTransaction , the AddFile s, and Operation.md#Convert[Convert] operation) and unpersists the Dataset of file names. streamWrite Method \u00b6 streamWrite ( spark : SparkSession , txn : OptimisticTransaction , addFiles : Iterator [ AddFile ], op : DeltaOperations.Operation , numFiles : Long ) : Long streamWrite ...FIXME createAddFile Method \u00b6 createAddFile ( file : SerializableFileStatus , basePath : Path , fs : FileSystem , conf : SQLConf ) : AddFile createAddFile creates an AddFile action. Internally, createAddFile ...FIXME createAddFile throws an AnalysisException if the number of fields in the given < > does not match the number of partitions found (at partition discovery phase): Expecting [size] partition column(s): [expectedCols], but found [size] partition column(s): [parsedCols] from parsing the file name: [path] mergeSchemasInParallel Method \u00b6 mergeSchemasInParallel ( sparkSession : SparkSession , filesToTouch : Seq [ FileStatus ], serializedConf : SerializableConfiguration ) : Option [ StructType ] mergeSchemasInParallel ...FIXME constructTableSchema Method \u00b6 constructTableSchema ( spark : SparkSession , dataSchema : StructType , partitionFields : Seq [ StructField ]) : StructType constructTableSchema ...FIXME","title":"ConvertToDeltaCommand"},{"location":"commands/ConvertToDeltaCommand/#converttodeltacommand","text":"ConvertToDeltaCommand is a DeltaCommand that converts a parquet table into delta format ( imports it into Delta). ConvertToDeltaCommand is a Spark SQL RunnableCommand (and executed eagerly on the driver for side-effects). ConvertToDeltaCommand requires that the partition schema matches the partitions of the parquet table ( or an AnalysisException is thrown ) ConvertToDeltaCommandBase is the base of ConvertToDeltaCommand -like commands with the only known implementation being ConvertToDeltaCommand itself.","title":"ConvertToDeltaCommand"},{"location":"commands/ConvertToDeltaCommand/#creating-instance","text":"ConvertToDeltaCommand takes the following to be created: Parquet table ( TableIdentifier ) Partition schema ( Option[StructType] ) Delta Path ( Option[String] ) ConvertToDeltaCommand is created when: CONVERT TO DELTA statement is used (and DeltaSqlAstBuilder is requested to visitConvert ) DeltaTable.convertToDelta utility is used (and DeltaConvert utility is used to executeConvert )","title":"Creating Instance"},{"location":"commands/ConvertToDeltaCommand/#executing-command","text":"run ( spark : SparkSession ) : Seq [ Row ] run is part of the RunnableCommand contract. run < > from the < > (with the given SparkSession ). run makes sure that the (data source) provider (the database part of the < >) is either delta or parquet . For all other data source providers, run throws an AnalysisException : CONVERT TO DELTA only supports parquet tables, but you are trying to convert a [sourceName] source: [ident] For delta data source provider, run simply prints out the following message to standard output and returns. The table you are trying to convert is already a delta table For parquet data source provider, run uses DeltaLog utility to < >. run then requests DeltaLog to < > and < >. In the end, run < >. In case the < > of the new transaction is greater than -1 , run simply prints out the following message to standard output and returns. The table you are trying to convert is already a delta table","title":" Executing Command"},{"location":"commands/ConvertToDeltaCommand/#internal-helper-methods","text":"","title":"Internal Helper Methods"},{"location":"commands/ConvertToDeltaCommand/#performconvert-method","text":"performConvert ( spark : SparkSession , txn : OptimisticTransaction , convertProperties : ConvertTarget ) : Seq [ Row ] performConvert makes sure that the directory exists (from the given ConvertProperties which is the table part of the < > of the command). performConvert requests the OptimisticTransaction for the < > that is then requested to < >. performConvert < > in the directory and leaves only files (by filtering out directories using WHERE clause). NOTE: performConvert uses Dataset API to build a distributed computation to query files. [[performConvert-cache]] performConvert caches the Dataset of file names. [[performConvert-schemaBatchSize]] performConvert uses < > configuration property for the number of files per batch for schema inference. performConvert < > for every batch of files and then < >. performConvert < > using the inferred table schema and the < > (if specified). performConvert creates a new < > using the table schema and the < > (if specified). performConvert requests the OptimisticTransaction to < >. [[performConvert-statsBatchSize]] performConvert uses < > configuration property for the number of files per batch for stats collection. performConvert < > (in the < > of the < > of the OptimisticTransaction ) for every file in a batch. [[performConvert-streamWrite]][[performConvert-unpersist]] In the end, performConvert < > (with the OptimisticTransaction , the AddFile s, and Operation.md#Convert[Convert] operation) and unpersists the Dataset of file names.","title":" performConvert Method"},{"location":"commands/ConvertToDeltaCommand/#streamwrite-method","text":"streamWrite ( spark : SparkSession , txn : OptimisticTransaction , addFiles : Iterator [ AddFile ], op : DeltaOperations.Operation , numFiles : Long ) : Long streamWrite ...FIXME","title":" streamWrite Method"},{"location":"commands/ConvertToDeltaCommand/#createaddfile-method","text":"createAddFile ( file : SerializableFileStatus , basePath : Path , fs : FileSystem , conf : SQLConf ) : AddFile createAddFile creates an AddFile action. Internally, createAddFile ...FIXME createAddFile throws an AnalysisException if the number of fields in the given < > does not match the number of partitions found (at partition discovery phase): Expecting [size] partition column(s): [expectedCols], but found [size] partition column(s): [parsedCols] from parsing the file name: [path]","title":" createAddFile Method"},{"location":"commands/ConvertToDeltaCommand/#mergeschemasinparallel-method","text":"mergeSchemasInParallel ( sparkSession : SparkSession , filesToTouch : Seq [ FileStatus ], serializedConf : SerializableConfiguration ) : Option [ StructType ] mergeSchemasInParallel ...FIXME","title":" mergeSchemasInParallel Method"},{"location":"commands/ConvertToDeltaCommand/#constructtableschema-method","text":"constructTableSchema ( spark : SparkSession , dataSchema : StructType , partitionFields : Seq [ StructField ]) : StructType constructTableSchema ...FIXME","title":" constructTableSchema Method"},{"location":"commands/CreateDeltaTableCommand/","text":"CreateDeltaTableCommand \u00b6 CreateDeltaTableCommand is a RunnableCommand ( Spark SQL ). Creating Instance \u00b6 CreateDeltaTableCommand takes the following to be created: CatalogTable ( Spark SQL ) Existing CatalogTable (if available) SaveMode Optional Data Query ( LogicalPlan ) CreationMode (default: TableCreationModes.Create ) tableByPath flag (default: false ) CreateDeltaTableCommand is created when: DeltaCatalog is requested to create a Delta table Executing Command \u00b6 run ( sparkSession : SparkSession ) : Seq [ Row ] run creates a DeltaLog (for the given table based on a table location) and a DeltaOptions . run starts a transaction (on the DeltaLog ). run branches off based on the optional data query . For data query defined, run creates a WriteIntoDelta and requests it to write . Otherwise, run creates an empty table. Note run does a bit more, but I don't think it's of much interest. run commits the transaction . In the end, run updateCatalog . run is part of the RunnableCommand abstraction. updateCatalog \u00b6 updateCatalog ( spark : SparkSession , table : CatalogTable ) : Unit updateCatalog uses the given SparkSession to access SessionCatalog to createTable or alterTable when the tableByPath flag is off. Otherwise, updateCatalog does nothing. getOperation \u00b6 getOperation ( metadata : Metadata , isManagedTable : Boolean , options : Option [ DeltaOptions ]) : DeltaOperations.Operation getOperation ...FIXME","title":"CreateDeltaTableCommand"},{"location":"commands/CreateDeltaTableCommand/#createdeltatablecommand","text":"CreateDeltaTableCommand is a RunnableCommand ( Spark SQL ).","title":"CreateDeltaTableCommand"},{"location":"commands/CreateDeltaTableCommand/#creating-instance","text":"CreateDeltaTableCommand takes the following to be created: CatalogTable ( Spark SQL ) Existing CatalogTable (if available) SaveMode Optional Data Query ( LogicalPlan ) CreationMode (default: TableCreationModes.Create ) tableByPath flag (default: false ) CreateDeltaTableCommand is created when: DeltaCatalog is requested to create a Delta table","title":"Creating Instance"},{"location":"commands/CreateDeltaTableCommand/#executing-command","text":"run ( sparkSession : SparkSession ) : Seq [ Row ] run creates a DeltaLog (for the given table based on a table location) and a DeltaOptions . run starts a transaction (on the DeltaLog ). run branches off based on the optional data query . For data query defined, run creates a WriteIntoDelta and requests it to write . Otherwise, run creates an empty table. Note run does a bit more, but I don't think it's of much interest. run commits the transaction . In the end, run updateCatalog . run is part of the RunnableCommand abstraction.","title":" Executing Command"},{"location":"commands/CreateDeltaTableCommand/#updatecatalog","text":"updateCatalog ( spark : SparkSession , table : CatalogTable ) : Unit updateCatalog uses the given SparkSession to access SessionCatalog to createTable or alterTable when the tableByPath flag is off. Otherwise, updateCatalog does nothing.","title":" updateCatalog"},{"location":"commands/CreateDeltaTableCommand/#getoperation","text":"getOperation ( metadata : Metadata , isManagedTable : Boolean , options : Option [ DeltaOptions ]) : DeltaOperations.Operation getOperation ...FIXME","title":" getOperation"},{"location":"commands/DeleteCommand/","text":"DeleteCommand \u00b6 DeleteCommand is a < > that < >. DeleteCommand is < > (using < > factory utility) and < > when < > operator is used (indirectly through DeltaTableOperations when requested to < >). == [[creating-instance]] Creating DeleteCommand Instance DeleteCommand takes the following to be created: [[tahoeFileIndex]] TahoeFileIndex [[target]] Target LogicalPlan [[condition]] Optional Catalyst expression == [[apply]] Creating DeleteCommand Instance -- apply Factory Utility [source, scala] \u00b6 apply(delete: Delete): DeleteCommand \u00b6 apply ...FIXME NOTE: apply is used when...FIXME == [[run]] Running Command -- run Method [source, scala] \u00b6 run(sparkSession: SparkSession): Seq[Row] \u00b6 NOTE: run is part of the RunnableCommand contract to...FIXME. run requests the < > for the < >. run requests the DeltaLog to < > for < >. In the end, run re-caches all cached plans (incl. this relation itself) by requesting the CacheManager to recache the < >. == [[performDelete]] performDelete Internal Method [source, scala] \u00b6 performDelete( sparkSession: SparkSession, deltaLog: DeltaLog, txn: OptimisticTransaction): Unit performDelete ...FIXME NOTE: performDelete is used exclusively when DeleteCommand is requested to < >.","title":"DeleteCommand"},{"location":"commands/DeleteCommand/#deletecommand","text":"DeleteCommand is a < > that < >. DeleteCommand is < > (using < > factory utility) and < > when < > operator is used (indirectly through DeltaTableOperations when requested to < >). == [[creating-instance]] Creating DeleteCommand Instance DeleteCommand takes the following to be created: [[tahoeFileIndex]] TahoeFileIndex [[target]] Target LogicalPlan [[condition]] Optional Catalyst expression == [[apply]] Creating DeleteCommand Instance -- apply Factory Utility","title":"DeleteCommand"},{"location":"commands/DeleteCommand/#source-scala","text":"","title":"[source, scala]"},{"location":"commands/DeleteCommand/#applydelete-delete-deletecommand","text":"apply ...FIXME NOTE: apply is used when...FIXME == [[run]] Running Command -- run Method","title":"apply(delete: Delete): DeleteCommand"},{"location":"commands/DeleteCommand/#source-scala_1","text":"","title":"[source, scala]"},{"location":"commands/DeleteCommand/#runsparksession-sparksession-seqrow","text":"NOTE: run is part of the RunnableCommand contract to...FIXME. run requests the < > for the < >. run requests the DeltaLog to < > for < >. In the end, run re-caches all cached plans (incl. this relation itself) by requesting the CacheManager to recache the < >. == [[performDelete]] performDelete Internal Method","title":"run(sparkSession: SparkSession): Seq[Row]"},{"location":"commands/DeleteCommand/#source-scala_2","text":"performDelete( sparkSession: SparkSession, deltaLog: DeltaLog, txn: OptimisticTransaction): Unit performDelete ...FIXME NOTE: performDelete is used exclusively when DeleteCommand is requested to < >.","title":"[source, scala]"},{"location":"commands/DeltaCommand/","text":"DeltaCommand \u00b6 DeltaCommand is a marker interface for commands to work with data in delta tables. Implementations \u00b6 AlterDeltaTableCommand ConvertToDeltaCommand DeleteCommand MergeIntoCommand UpdateCommand VacuumCommandImpl WriteIntoDelta parsePartitionPredicates Method \u00b6 parsePartitionPredicates ( spark : SparkSession , predicate : String ) : Seq [ Expression ] parsePartitionPredicates ...FIXME parsePartitionPredicates is used when...FIXME verifyPartitionPredicates Method \u00b6 verifyPartitionPredicates ( spark : SparkSession , partitionColumns : Seq [ String ], predicates : Seq [ Expression ]) : Unit verifyPartitionPredicates ...FIXME verifyPartitionPredicates is used when...FIXME generateCandidateFileMap Method \u00b6 generateCandidateFileMap ( basePath : Path , candidateFiles : Seq [ AddFile ]) : Map [ String , AddFile ] generateCandidateFileMap ...FIXME generateCandidateFileMap is used when...FIXME removeFilesFromPaths Method \u00b6 removeFilesFromPaths ( deltaLog : DeltaLog , nameToAddFileMap : Map [ String , AddFile ], filesToRewrite : Seq [ String ], operationTimestamp : Long ) : Seq [ RemoveFile ] removeFilesFromPaths ...FIXME removeFilesFromPaths is used when DeleteCommand and UpdateCommand commands are executed. Creating HadoopFsRelation (with TahoeBatchFileIndex) \u00b6 buildBaseRelation ( spark : SparkSession , txn : OptimisticTransaction , actionType : String , rootPath : Path , inputLeafFiles : Seq [ String ], nameToAddFileMap : Map [ String , AddFile ]) : HadoopFsRelation buildBaseRelation converts the given inputLeafFiles to AddFiles (with the given rootPath and nameToAddFileMap ). buildBaseRelation creates a TahoeBatchFileIndex for the actionType , the AddFiles and the rootPath . In the end, buildBaseRelation creates a HadoopFsRelation with the TahoeBatchFileIndex (and the other properties based on the metadata of the given OptimisticTransaction ). Note Learn more on HadoopFsRelation in The Internals of Spark SQL online book. buildBaseRelation is used when DeleteCommand and UpdateCommand commands are executed (with delete and update action types, respectively). getTouchedFile Method \u00b6 getTouchedFile ( basePath : Path , filePath : String , nameToAddFileMap : Map [ String , AddFile ]) : AddFile getTouchedFile ...FIXME getTouchedFile is used when: DeltaCommand is requested to removeFilesFromPaths and create a HadoopFsRelation (for DeleteCommand and UpdateCommand commands) MergeIntoCommand is executed isCatalogTable Method \u00b6 isCatalogTable ( analyzer : Analyzer , tableIdent : TableIdentifier ) : Boolean isCatalogTable ...FIXME isCatalogTable is used when...FIXME isPathIdentifier Method \u00b6 isPathIdentifier ( tableIdent : TableIdentifier ) : Boolean isPathIdentifier ...FIXME isPathIdentifier is used when...FIXME commitLarge \u00b6 commitLarge ( spark : SparkSession , txn : OptimisticTransaction , actions : Iterator [ Action ], op : DeltaOperations.Operation , context : Map [ String , String ], metrics : Map [ String , String ]) : Long commitLarge ...FIXME commitLarge is used when: ConvertToDeltaCommand command is executed","title":"DeltaCommand"},{"location":"commands/DeltaCommand/#deltacommand","text":"DeltaCommand is a marker interface for commands to work with data in delta tables.","title":"DeltaCommand"},{"location":"commands/DeltaCommand/#implementations","text":"AlterDeltaTableCommand ConvertToDeltaCommand DeleteCommand MergeIntoCommand UpdateCommand VacuumCommandImpl WriteIntoDelta","title":"Implementations"},{"location":"commands/DeltaCommand/#parsepartitionpredicates-method","text":"parsePartitionPredicates ( spark : SparkSession , predicate : String ) : Seq [ Expression ] parsePartitionPredicates ...FIXME parsePartitionPredicates is used when...FIXME","title":" parsePartitionPredicates Method"},{"location":"commands/DeltaCommand/#verifypartitionpredicates-method","text":"verifyPartitionPredicates ( spark : SparkSession , partitionColumns : Seq [ String ], predicates : Seq [ Expression ]) : Unit verifyPartitionPredicates ...FIXME verifyPartitionPredicates is used when...FIXME","title":" verifyPartitionPredicates Method"},{"location":"commands/DeltaCommand/#generatecandidatefilemap-method","text":"generateCandidateFileMap ( basePath : Path , candidateFiles : Seq [ AddFile ]) : Map [ String , AddFile ] generateCandidateFileMap ...FIXME generateCandidateFileMap is used when...FIXME","title":" generateCandidateFileMap Method"},{"location":"commands/DeltaCommand/#removefilesfrompaths-method","text":"removeFilesFromPaths ( deltaLog : DeltaLog , nameToAddFileMap : Map [ String , AddFile ], filesToRewrite : Seq [ String ], operationTimestamp : Long ) : Seq [ RemoveFile ] removeFilesFromPaths ...FIXME removeFilesFromPaths is used when DeleteCommand and UpdateCommand commands are executed.","title":" removeFilesFromPaths Method"},{"location":"commands/DeltaCommand/#creating-hadoopfsrelation-with-tahoebatchfileindex","text":"buildBaseRelation ( spark : SparkSession , txn : OptimisticTransaction , actionType : String , rootPath : Path , inputLeafFiles : Seq [ String ], nameToAddFileMap : Map [ String , AddFile ]) : HadoopFsRelation buildBaseRelation converts the given inputLeafFiles to AddFiles (with the given rootPath and nameToAddFileMap ). buildBaseRelation creates a TahoeBatchFileIndex for the actionType , the AddFiles and the rootPath . In the end, buildBaseRelation creates a HadoopFsRelation with the TahoeBatchFileIndex (and the other properties based on the metadata of the given OptimisticTransaction ). Note Learn more on HadoopFsRelation in The Internals of Spark SQL online book. buildBaseRelation is used when DeleteCommand and UpdateCommand commands are executed (with delete and update action types, respectively).","title":" Creating HadoopFsRelation (with TahoeBatchFileIndex)"},{"location":"commands/DeltaCommand/#gettouchedfile-method","text":"getTouchedFile ( basePath : Path , filePath : String , nameToAddFileMap : Map [ String , AddFile ]) : AddFile getTouchedFile ...FIXME getTouchedFile is used when: DeltaCommand is requested to removeFilesFromPaths and create a HadoopFsRelation (for DeleteCommand and UpdateCommand commands) MergeIntoCommand is executed","title":" getTouchedFile Method"},{"location":"commands/DeltaCommand/#iscatalogtable-method","text":"isCatalogTable ( analyzer : Analyzer , tableIdent : TableIdentifier ) : Boolean isCatalogTable ...FIXME isCatalogTable is used when...FIXME","title":" isCatalogTable Method"},{"location":"commands/DeltaCommand/#ispathidentifier-method","text":"isPathIdentifier ( tableIdent : TableIdentifier ) : Boolean isPathIdentifier ...FIXME isPathIdentifier is used when...FIXME","title":" isPathIdentifier Method"},{"location":"commands/DeltaCommand/#commitlarge","text":"commitLarge ( spark : SparkSession , txn : OptimisticTransaction , actions : Iterator [ Action ], op : DeltaOperations.Operation , context : Map [ String , String ], metrics : Map [ String , String ]) : Long commitLarge ...FIXME commitLarge is used when: ConvertToDeltaCommand command is executed","title":" commitLarge"},{"location":"commands/DeltaGenerateCommand/","text":"DeltaGenerateCommand \u00b6 DeltaGenerateCommand is a RunnableCommand ( Spark SQL ) to execute a generate function on a delta table . DeltaGenerateCommand is used for the following: GENERATE SQL command DeltaTable.generate operation DeltaGenerateCommand supports symlink_format_manifest mode name only. Demo \u00b6 val path = \"/tmp/delta/d01\" val tid = s\"delta.`$path`\" val q = s\"GENERATE symlink_format_manifest FOR TABLE $tid\" sql(q).collect Creating Instance \u00b6 DeltaGenerateCommand takes the following to be created: Mode Name TableIdentifier (Spark SQL) DeltaGenerateCommand is created for: GENERATE SQL command (that uses DeltaSqlAstBuilder to parse GENERATE SQL command ) DeltaTable.generate operator (that uses DeltaTableOperations to executeGenerate ) Generate Mode Name \u00b6 DeltaGenerateCommand is given a mode name when created . DeltaGenerateCommand uses a lookup table of the supported generation functions by mode name (yet supports just symlink_format_manifest ). Mode Name Generation Function symlink_format_manifest generateFullManifest Executing Command \u00b6 run ( sparkSession : SparkSession ) : Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run creates a Hadoop Path to (the location of) the delta table (based on DeltaTableIdentifier ). run creates a DeltaLog for the delta table. run executes the generation function for the mode name . run returns no rows (an empty collection). IllegalArgumentException \u00b6 run throws an IllegalArgumentException when executed with an unsupported mode name : Specified mode '[modeName]' is not supported. Supported modes are: [supportedModes] AnalysisException \u00b6 run throws an AnalysisException when executed for a non-delta table: GENERATE is only supported for Delta tables.","title":"DeltaGenerateCommand"},{"location":"commands/DeltaGenerateCommand/#deltageneratecommand","text":"DeltaGenerateCommand is a RunnableCommand ( Spark SQL ) to execute a generate function on a delta table . DeltaGenerateCommand is used for the following: GENERATE SQL command DeltaTable.generate operation DeltaGenerateCommand supports symlink_format_manifest mode name only.","title":"DeltaGenerateCommand"},{"location":"commands/DeltaGenerateCommand/#demo","text":"val path = \"/tmp/delta/d01\" val tid = s\"delta.`$path`\" val q = s\"GENERATE symlink_format_manifest FOR TABLE $tid\" sql(q).collect","title":"Demo"},{"location":"commands/DeltaGenerateCommand/#creating-instance","text":"DeltaGenerateCommand takes the following to be created: Mode Name TableIdentifier (Spark SQL) DeltaGenerateCommand is created for: GENERATE SQL command (that uses DeltaSqlAstBuilder to parse GENERATE SQL command ) DeltaTable.generate operator (that uses DeltaTableOperations to executeGenerate )","title":"Creating Instance"},{"location":"commands/DeltaGenerateCommand/#generate-mode-name","text":"DeltaGenerateCommand is given a mode name when created . DeltaGenerateCommand uses a lookup table of the supported generation functions by mode name (yet supports just symlink_format_manifest ). Mode Name Generation Function symlink_format_manifest generateFullManifest","title":" Generate Mode Name"},{"location":"commands/DeltaGenerateCommand/#executing-command","text":"run ( sparkSession : SparkSession ) : Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run creates a Hadoop Path to (the location of) the delta table (based on DeltaTableIdentifier ). run creates a DeltaLog for the delta table. run executes the generation function for the mode name . run returns no rows (an empty collection).","title":" Executing Command"},{"location":"commands/DeltaGenerateCommand/#illegalargumentexception","text":"run throws an IllegalArgumentException when executed with an unsupported mode name : Specified mode '[modeName]' is not supported. Supported modes are: [supportedModes]","title":" IllegalArgumentException"},{"location":"commands/DeltaGenerateCommand/#analysisexception","text":"run throws an AnalysisException when executed for a non-delta table: GENERATE is only supported for Delta tables.","title":" AnalysisException"},{"location":"commands/DeltaMergeBuilder/","text":"DeltaMergeBuilder \u00b6 DeltaMergeBuilder is a builder interface to describe how to merge data from a source DataFrame into the target delta table (using whenMatched and whenNotMatched conditions). DeltaMergeBuilder is created using DeltaTable.merge operator. In the end, DeltaMergeBuilder is supposed to be executed to take action. DeltaMergeBuilder creates a DeltaMergeInto logical command that is resolved to a MergeIntoCommand runnable logical command (using PreprocessTableMerge logical resolution rule). Creating Instance \u00b6 DeltaMergeBuilder takes the following to be created: Target DeltaTable Source DataFrame Condition Column When Clauses DeltaMergeBuilder is created using DeltaTable.merge operator. Operators \u00b6 whenMatched \u00b6 whenMatched () : DeltaMergeMatchedActionBuilder whenMatched ( condition : Column ) : DeltaMergeMatchedActionBuilder whenMatched ( condition : String ) : DeltaMergeMatchedActionBuilder Creates a DeltaMergeMatchedActionBuilder (for the DeltaMergeBuilder and a condition) whenNotMatched \u00b6 whenNotMatched () : DeltaMergeNotMatchedActionBuilder whenNotMatched ( condition : Column ) : DeltaMergeNotMatchedActionBuilder whenNotMatched ( condition : String ) : DeltaMergeNotMatchedActionBuilder Creates a DeltaMergeNotMatchedActionBuilder (for the DeltaMergeBuilder and a condition) Executing Merge \u00b6 execute () : Unit execute creates a merge plan (that is DeltaMergeInto logical command) and resolves column references . execute runs PreprocessTableMerge logical resolution rule on the DeltaMergeInto logical command (that gives MergeIntoCommand runnable logical command). In the end, execute executes the MergeIntoCommand logical command. Creating Logical Plan for Merge \u00b6 mergePlan : DeltaMergeInto mergePlan creates a DeltaMergeInto logical command. mergePlan is used when DeltaMergeBuilder is requested to execute . Creating DeltaMergeBuilder \u00b6 apply ( targetTable : DeltaTable , source : DataFrame , onCondition : Column ) : DeltaMergeBuilder apply utility creates a new DeltaMergeBuilder for the given parameters and no DeltaMergeIntoClauses . apply is used for DeltaTable.merge operator. Adding DeltaMergeIntoClause \u00b6 withClause ( clause : DeltaMergeIntoClause ) : DeltaMergeBuilder withClause creates a new DeltaMergeBuilder (based on the existing properties, e.g. the DeltaTable ) with the given DeltaMergeIntoClause added to the existing DeltaMergeIntoClauses (to create a more refined DeltaMergeBuilder ). withClause is used when: DeltaMergeMatchedActionBuilder is requested to updateAll , delete and addUpdateClause DeltaMergeNotMatchedActionBuilder is requested to insertAll and addInsertClause","title":"DeltaMergeBuilder"},{"location":"commands/DeltaMergeBuilder/#deltamergebuilder","text":"DeltaMergeBuilder is a builder interface to describe how to merge data from a source DataFrame into the target delta table (using whenMatched and whenNotMatched conditions). DeltaMergeBuilder is created using DeltaTable.merge operator. In the end, DeltaMergeBuilder is supposed to be executed to take action. DeltaMergeBuilder creates a DeltaMergeInto logical command that is resolved to a MergeIntoCommand runnable logical command (using PreprocessTableMerge logical resolution rule).","title":"DeltaMergeBuilder"},{"location":"commands/DeltaMergeBuilder/#creating-instance","text":"DeltaMergeBuilder takes the following to be created: Target DeltaTable Source DataFrame Condition Column When Clauses DeltaMergeBuilder is created using DeltaTable.merge operator.","title":"Creating Instance"},{"location":"commands/DeltaMergeBuilder/#operators","text":"","title":"Operators"},{"location":"commands/DeltaMergeBuilder/#whenmatched","text":"whenMatched () : DeltaMergeMatchedActionBuilder whenMatched ( condition : Column ) : DeltaMergeMatchedActionBuilder whenMatched ( condition : String ) : DeltaMergeMatchedActionBuilder Creates a DeltaMergeMatchedActionBuilder (for the DeltaMergeBuilder and a condition)","title":" whenMatched"},{"location":"commands/DeltaMergeBuilder/#whennotmatched","text":"whenNotMatched () : DeltaMergeNotMatchedActionBuilder whenNotMatched ( condition : Column ) : DeltaMergeNotMatchedActionBuilder whenNotMatched ( condition : String ) : DeltaMergeNotMatchedActionBuilder Creates a DeltaMergeNotMatchedActionBuilder (for the DeltaMergeBuilder and a condition)","title":" whenNotMatched"},{"location":"commands/DeltaMergeBuilder/#executing-merge","text":"execute () : Unit execute creates a merge plan (that is DeltaMergeInto logical command) and resolves column references . execute runs PreprocessTableMerge logical resolution rule on the DeltaMergeInto logical command (that gives MergeIntoCommand runnable logical command). In the end, execute executes the MergeIntoCommand logical command.","title":" Executing Merge"},{"location":"commands/DeltaMergeBuilder/#creating-logical-plan-for-merge","text":"mergePlan : DeltaMergeInto mergePlan creates a DeltaMergeInto logical command. mergePlan is used when DeltaMergeBuilder is requested to execute .","title":" Creating Logical Plan for Merge"},{"location":"commands/DeltaMergeBuilder/#creating-deltamergebuilder","text":"apply ( targetTable : DeltaTable , source : DataFrame , onCondition : Column ) : DeltaMergeBuilder apply utility creates a new DeltaMergeBuilder for the given parameters and no DeltaMergeIntoClauses . apply is used for DeltaTable.merge operator.","title":" Creating DeltaMergeBuilder"},{"location":"commands/DeltaMergeBuilder/#adding-deltamergeintoclause","text":"withClause ( clause : DeltaMergeIntoClause ) : DeltaMergeBuilder withClause creates a new DeltaMergeBuilder (based on the existing properties, e.g. the DeltaTable ) with the given DeltaMergeIntoClause added to the existing DeltaMergeIntoClauses (to create a more refined DeltaMergeBuilder ). withClause is used when: DeltaMergeMatchedActionBuilder is requested to updateAll , delete and addUpdateClause DeltaMergeNotMatchedActionBuilder is requested to insertAll and addInsertClause","title":" Adding DeltaMergeIntoClause"},{"location":"commands/DeltaMergeInto/","text":"DeltaMergeInto Logical Command \u00b6 DeltaMergeInto is a logical command (Spark SQL's Command ). Creating Instance \u00b6 DeltaMergeInto takes the following to be created: Target LogicalPlan Source LogicalPlan Condition Expression Matched Clauses ( Seq[DeltaMergeIntoMatchedClause] ) Optional Non-Matched Clause ( Option[DeltaMergeIntoInsertClause] ) Optional Migrated Schema (default: undefined ) DeltaMergeInto is created (using apply and resolveReferences utilities) when: DeltaMergeBuilder is requested to execute DeltaAnalysis logical resolution rule is executed Utilities \u00b6 apply \u00b6 apply ( target : LogicalPlan , source : LogicalPlan , condition : Expression , whenClauses : Seq [ DeltaMergeIntoClause ]) : DeltaMergeInto apply ...FIXME apply is used when: DeltaMergeBuilder is requested to execute (when mergePlan ) DeltaAnalysis logical resolution rule is executed (and resolves MergeIntoTable logical command) resolveReferences \u00b6 resolveReferences ( merge : DeltaMergeInto , conf : SQLConf )( resolveExpr : ( Expression , LogicalPlan ) => Expression ) : DeltaMergeInto resolveReferences ...FIXME resolveReferences is used when: DeltaMergeBuilder is requested to execute DeltaAnalysis logical resolution rule is executed (and resolves MergeIntoTable logical command)","title":"DeltaMergeInto"},{"location":"commands/DeltaMergeInto/#deltamergeinto-logical-command","text":"DeltaMergeInto is a logical command (Spark SQL's Command ).","title":"DeltaMergeInto Logical Command"},{"location":"commands/DeltaMergeInto/#creating-instance","text":"DeltaMergeInto takes the following to be created: Target LogicalPlan Source LogicalPlan Condition Expression Matched Clauses ( Seq[DeltaMergeIntoMatchedClause] ) Optional Non-Matched Clause ( Option[DeltaMergeIntoInsertClause] ) Optional Migrated Schema (default: undefined ) DeltaMergeInto is created (using apply and resolveReferences utilities) when: DeltaMergeBuilder is requested to execute DeltaAnalysis logical resolution rule is executed","title":"Creating Instance"},{"location":"commands/DeltaMergeInto/#utilities","text":"","title":"Utilities"},{"location":"commands/DeltaMergeInto/#apply","text":"apply ( target : LogicalPlan , source : LogicalPlan , condition : Expression , whenClauses : Seq [ DeltaMergeIntoClause ]) : DeltaMergeInto apply ...FIXME apply is used when: DeltaMergeBuilder is requested to execute (when mergePlan ) DeltaAnalysis logical resolution rule is executed (and resolves MergeIntoTable logical command)","title":" apply"},{"location":"commands/DeltaMergeInto/#resolvereferences","text":"resolveReferences ( merge : DeltaMergeInto , conf : SQLConf )( resolveExpr : ( Expression , LogicalPlan ) => Expression ) : DeltaMergeInto resolveReferences ...FIXME resolveReferences is used when: DeltaMergeBuilder is requested to execute DeltaAnalysis logical resolution rule is executed (and resolves MergeIntoTable logical command)","title":" resolveReferences"},{"location":"commands/DeltaMergeIntoClause/","text":"DeltaMergeIntoClause \u00b6 DeltaMergeIntoClause is...FIXME","title":"DeltaMergeIntoClause"},{"location":"commands/DeltaMergeIntoClause/#deltamergeintoclause","text":"DeltaMergeIntoClause is...FIXME","title":"DeltaMergeIntoClause"},{"location":"commands/DeltaMergeMatchedActionBuilder/","text":"DeltaMergeMatchedActionBuilder \u00b6 DeltaMergeMatchedActionBuilder is a builder interface for DeltaMergeBuilder.whenMatched operator. Creating Instance \u00b6 DeltaMergeMatchedActionBuilder takes the following to be created: DeltaMergeBuilder Optional match condition DeltaMergeMatchedActionBuilder is created when DeltaMergeBuilder is requested to whenMatched (using apply factory method). Operators \u00b6 delete \u00b6 delete () : DeltaMergeBuilder Adds a DeltaMergeIntoDeleteClause (with the matchCondition ) to the DeltaMergeBuilder . update \u00b6 update ( set : Map [ String , Column ]) : DeltaMergeBuilder updateAll \u00b6 updateAll () : DeltaMergeBuilder updateExpr \u00b6 updateExpr ( set : Map [ String , String ]) : DeltaMergeBuilder Creating DeltaMergeMatchedActionBuilder \u00b6 apply ( mergeBuilder : DeltaMergeBuilder , matchCondition : Option [ Column ]) : DeltaMergeMatchedActionBuilder apply creates a DeltaMergeMatchedActionBuilder (for the given parameters). apply is used when DeltaMergeBuilder is requested to whenMatched .","title":"DeltaMergeMatchedActionBuilder"},{"location":"commands/DeltaMergeMatchedActionBuilder/#deltamergematchedactionbuilder","text":"DeltaMergeMatchedActionBuilder is a builder interface for DeltaMergeBuilder.whenMatched operator.","title":"DeltaMergeMatchedActionBuilder"},{"location":"commands/DeltaMergeMatchedActionBuilder/#creating-instance","text":"DeltaMergeMatchedActionBuilder takes the following to be created: DeltaMergeBuilder Optional match condition DeltaMergeMatchedActionBuilder is created when DeltaMergeBuilder is requested to whenMatched (using apply factory method).","title":"Creating Instance"},{"location":"commands/DeltaMergeMatchedActionBuilder/#operators","text":"","title":"Operators"},{"location":"commands/DeltaMergeMatchedActionBuilder/#delete","text":"delete () : DeltaMergeBuilder Adds a DeltaMergeIntoDeleteClause (with the matchCondition ) to the DeltaMergeBuilder .","title":" delete"},{"location":"commands/DeltaMergeMatchedActionBuilder/#update","text":"update ( set : Map [ String , Column ]) : DeltaMergeBuilder","title":" update"},{"location":"commands/DeltaMergeMatchedActionBuilder/#updateall","text":"updateAll () : DeltaMergeBuilder","title":" updateAll"},{"location":"commands/DeltaMergeMatchedActionBuilder/#updateexpr","text":"updateExpr ( set : Map [ String , String ]) : DeltaMergeBuilder","title":" updateExpr"},{"location":"commands/DeltaMergeMatchedActionBuilder/#creating-deltamergematchedactionbuilder","text":"apply ( mergeBuilder : DeltaMergeBuilder , matchCondition : Option [ Column ]) : DeltaMergeMatchedActionBuilder apply creates a DeltaMergeMatchedActionBuilder (for the given parameters). apply is used when DeltaMergeBuilder is requested to whenMatched .","title":" Creating DeltaMergeMatchedActionBuilder"},{"location":"commands/DeltaMergeNotMatchedActionBuilder/","text":"DeltaMergeNotMatchedActionBuilder \u00b6 DeltaMergeNotMatchedActionBuilder is...FIXME","title":"DeltaMergeNotMatchedActionBuilder"},{"location":"commands/DeltaMergeNotMatchedActionBuilder/#deltamergenotmatchedactionbuilder","text":"DeltaMergeNotMatchedActionBuilder is...FIXME","title":"DeltaMergeNotMatchedActionBuilder"},{"location":"commands/DescribeDeltaDetailCommand/","text":"DescribeDeltaDetailCommand (And DescribeDeltaDetailCommandBase) \u00b6 DescribeDeltaDetailCommand represents DESCRIBE DETAIL SQL command at execution (and is < > when DeltaSqlAstBuilder is requested to < >). Like DESCRIBE DETAIL SQL command, DescribeDeltaDetailCommand accepts either a < > or a < > (e.g. '/tmp/delta/t1' or ++delta. /tmp/delta/t1 ++ ) (DESC | DESCRIBE) DETAIL (path | table) [[demo]] .DESCRIBE DETAIL SQL Command's Demo val q = sql(\"DESCRIBE DETAIL '/tmp/delta/users'\") scala> q.show +------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+ |format| id|name|description| location| createdAt| lastModified|partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion| +------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+ | delta|3799b291-dbfa-4f8...|null| null|file:/tmp/delta/u...|2020-01-06 17:08:...|2020-01-06 17:12:28| [city, country]| 4| 2581| []| 1| 2| +------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+ [[implementations]] DescribeDeltaDetailCommand is the default and only known < > in Delta Lake. [[DescribeDeltaDetailCommandBase]] DescribeDeltaDetailCommandBase is a RunnableCommand ( Spark SQL ) for < >. == [[creating-instance]] Creating DescribeDeltaDetailCommand Instance DescribeDeltaDetailCommand takes the following to be created: [[path]] Table path [[tableIdentifier]] Table identifier (e.g. t1 or ++delta. /tmp/delta/t1 ++ ) == [[run]] Running Command -- run Method [source, scala] \u00b6 run(sparkSession: SparkSession): Seq[Row] \u00b6 NOTE: run is part of the RunnableCommand contract to...FIXME. run ...FIXME","title":"DescribeDeltaDetailCommand"},{"location":"commands/DescribeDeltaDetailCommand/#describedeltadetailcommand-and-describedeltadetailcommandbase","text":"DescribeDeltaDetailCommand represents DESCRIBE DETAIL SQL command at execution (and is < > when DeltaSqlAstBuilder is requested to < >). Like DESCRIBE DETAIL SQL command, DescribeDeltaDetailCommand accepts either a < > or a < > (e.g. '/tmp/delta/t1' or ++delta. /tmp/delta/t1 ++ ) (DESC | DESCRIBE) DETAIL (path | table) [[demo]] .DESCRIBE DETAIL SQL Command's Demo val q = sql(\"DESCRIBE DETAIL '/tmp/delta/users'\") scala> q.show +------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+ |format| id|name|description| location| createdAt| lastModified|partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion| +------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+ | delta|3799b291-dbfa-4f8...|null| null|file:/tmp/delta/u...|2020-01-06 17:08:...|2020-01-06 17:12:28| [city, country]| 4| 2581| []| 1| 2| +------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+ [[implementations]] DescribeDeltaDetailCommand is the default and only known < > in Delta Lake. [[DescribeDeltaDetailCommandBase]] DescribeDeltaDetailCommandBase is a RunnableCommand ( Spark SQL ) for < >. == [[creating-instance]] Creating DescribeDeltaDetailCommand Instance DescribeDeltaDetailCommand takes the following to be created: [[path]] Table path [[tableIdentifier]] Table identifier (e.g. t1 or ++delta. /tmp/delta/t1 ++ ) == [[run]] Running Command -- run Method","title":"DescribeDeltaDetailCommand (And DescribeDeltaDetailCommandBase)"},{"location":"commands/DescribeDeltaDetailCommand/#source-scala","text":"","title":"[source, scala]"},{"location":"commands/DescribeDeltaDetailCommand/#runsparksession-sparksession-seqrow","text":"NOTE: run is part of the RunnableCommand contract to...FIXME. run ...FIXME","title":"run(sparkSession: SparkSession): Seq[Row]"},{"location":"commands/DescribeDeltaHistoryCommand/","text":"DescribeDeltaHistoryCommand \u00b6 DescribeDeltaHistoryCommand is a RunnableCommand ( Spark SQL ) that uses DeltaHistoryManager for the commit history of a delta table. DescribeDeltaHistoryCommand is used for DESCRIBE HISTORY SQL command. Creating Instance \u00b6 DescribeDeltaHistoryCommand takes the following to be created: (optional) Directory (optional) TableIdentifier (optional) Number of commits to display Output Attributes (default: CommitInfo ) DescribeDeltaHistoryCommand is created for: DESCRIBE HISTORY SQL command (that uses DeltaSqlAstBuilder to parse DESCRIBE HISTORY SQL command ) Executing Command \u00b6 run ( sparkSession : SparkSession ) : Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run creates a Hadoop Path to (the location of) the delta table (based on DeltaTableIdentifier ). run creates a DeltaLog for the delta table. run requests the DeltaLog for the DeltaHistoryManager that is requested for the commit history .","title":"DescribeDeltaHistoryCommand"},{"location":"commands/DescribeDeltaHistoryCommand/#describedeltahistorycommand","text":"DescribeDeltaHistoryCommand is a RunnableCommand ( Spark SQL ) that uses DeltaHistoryManager for the commit history of a delta table. DescribeDeltaHistoryCommand is used for DESCRIBE HISTORY SQL command.","title":"DescribeDeltaHistoryCommand"},{"location":"commands/DescribeDeltaHistoryCommand/#creating-instance","text":"DescribeDeltaHistoryCommand takes the following to be created: (optional) Directory (optional) TableIdentifier (optional) Number of commits to display Output Attributes (default: CommitInfo ) DescribeDeltaHistoryCommand is created for: DESCRIBE HISTORY SQL command (that uses DeltaSqlAstBuilder to parse DESCRIBE HISTORY SQL command )","title":"Creating Instance"},{"location":"commands/DescribeDeltaHistoryCommand/#executing-command","text":"run ( sparkSession : SparkSession ) : Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run creates a Hadoop Path to (the location of) the delta table (based on DeltaTableIdentifier ). run creates a DeltaLog for the delta table. run requests the DeltaLog for the DeltaHistoryManager that is requested for the commit history .","title":" Executing Command"},{"location":"commands/JoinedRowProcessor/","text":"JoinedRowProcessor \u00b6 JoinedRowProcessor is...FIXME Creating Instance \u00b6 JoinedRowProcessor takes the following to be created: targetRowHasNoMatch Expression sourceRowHasNoMatch Expression Optional matchedCondition1 Expression Optional matchedOutput1 Expressions Optional matchedCondition2 Expression Optional matchedOutput2 Expressions Optional notMatchedCondition Expression Optional notMatchedOutput Expressions Optional noopCopyOutput Expression deleteRowOutput Expressions joinedAttributes Attributes joinedRowEncoder ExpressionEncoder outputRowEncoder ExpressionEncoder JoinedRowProcessor is created when MergeIntoCommand is requested to writeAllChanges . Processing Partition \u00b6 processPartition ( rowIterator : Iterator [ Row ]) : Iterator [ Row ] processPartition ...FIXME processPartition is used when MergeIntoCommand is requested to writeAllChanges .","title":"JoinedRowProcessor"},{"location":"commands/JoinedRowProcessor/#joinedrowprocessor","text":"JoinedRowProcessor is...FIXME","title":"JoinedRowProcessor"},{"location":"commands/JoinedRowProcessor/#creating-instance","text":"JoinedRowProcessor takes the following to be created: targetRowHasNoMatch Expression sourceRowHasNoMatch Expression Optional matchedCondition1 Expression Optional matchedOutput1 Expressions Optional matchedCondition2 Expression Optional matchedOutput2 Expressions Optional notMatchedCondition Expression Optional notMatchedOutput Expressions Optional noopCopyOutput Expression deleteRowOutput Expressions joinedAttributes Attributes joinedRowEncoder ExpressionEncoder outputRowEncoder ExpressionEncoder JoinedRowProcessor is created when MergeIntoCommand is requested to writeAllChanges .","title":"Creating Instance"},{"location":"commands/JoinedRowProcessor/#processing-partition","text":"processPartition ( rowIterator : Iterator [ Row ]) : Iterator [ Row ] processPartition ...FIXME processPartition is used when MergeIntoCommand is requested to writeAllChanges .","title":" Processing Partition"},{"location":"commands/MergeIntoCommand/","text":"MergeIntoCommand \u00b6 MergeIntoCommand is a DeltaCommand that represents a DeltaMergeInto logical command at execution time. MergeIntoCommand is a logical command (Spark SQL's RunnableCommand ). Tip Learn more on the internals of MergeIntoCommand in Demo: Merge Operation . Performance Metrics \u00b6 Name web UI numSourceRows number of source rows numTargetRowsCopied number of target rows rewritten unmodified numTargetRowsInserted number of inserted rows numTargetRowsUpdated number of updated rows numTargetRowsDeleted number of deleted rows numTargetFilesBeforeSkipping number of target files before skipping numTargetFilesAfterSkipping number of target files after skipping numTargetFilesRemoved number of files removed to target numTargetFilesAdded number of files added to target number of target rows rewritten unmodified \u00b6 numTargetRowsCopied performance metric (like the other metrics ) is turned into a non-deterministic user-defined function (UDF). numTargetRowsCopied becomes incrNoopCountExpr UDF. incrNoopCountExpr UDF is resolved on a joined plan and used to create a JoinedRowProcessor for processing partitions of the joined plan Dataset . Creating Instance \u00b6 MergeIntoCommand takes the following to be created: Source Data Target Data ( LogicalPlan ) TahoeFileIndex Condition Expression Matched Clauses ( Seq[DeltaMergeIntoMatchedClause] ) Optional Non-Matched Clause ( Option[DeltaMergeIntoInsertClause] ) Migrated Schema MergeIntoCommand is created when PreprocessTableMerge logical resolution rule is executed (on a DeltaMergeInto logical command). Source Data (to Merge From) \u00b6 When created , MergeIntoCommand is given a LogicalPlan for the source data to merge from (referred to internally as source ). The source LogicalPlan is used twice: Firstly, in one of the following: An inner join (in findTouchedFiles ) that is count in web UI A leftanti join (in writeInsertsOnlyWhenNoMatchedClauses ) Secondly, in rightOuter or fullOuter join (in writeAllChanges ) Tip Enable DEBUG logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger to see the inner-workings of writeAllChanges . Target DeltaLog \u00b6 targetDeltaLog : DeltaLog targetDeltaLog is the DeltaLog of the TahoeFileIndex . targetDeltaLog is used for the following: Start a new transaction when executed To access the Data Path when finding files to rewrite Lazy Value targetDeltaLog is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards. Executing Command \u00b6 run ( spark : SparkSession ) : Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run requests the target DeltaLog to start a new transaction . With spark.databricks.delta.schema.autoMerge.enabled configuration property enabled, run updates the metadata (of the transaction). run determines Delta actions ( RemoveFile s and AddFile s). Describe deltaActions part With spark.databricks.delta.history.metricsEnabled configuration property enabled, run requests the current transaction to register SQL metrics for the Delta operation . run requests the current transaction to commit (with the Delta actions and Merge operation). run records the Delta event. run posts a SparkListenerDriverAccumUpdates Spark event (with the metrics). In the end, run requests the CacheManager to recacheByPlan . Finding Files to Rewrite \u00b6 findTouchedFiles ( spark : SparkSession , deltaTxn : OptimisticTransaction ) : Seq [ AddFile ] Important findTouchedFiles is such a fine piece of art ( a gem ). It uses a custom accumulator, a UDF (to use this accumulator to record touched file names) and input_file_name() standard function for the names of the files read. It is always worth keeping in mind that Delta Lake uses files for data storage and that is why input_file_name() standard function works. It would not work for non-file-based data sources. Example 1: Understanding the Internals of findTouchedFiles The following query writes out a 10-element dataset using the default parquet data source to /tmp/parquet directory: val target = \"/tmp/parquet\" spark . range ( 10 ). write . save ( target ) The number of parquet part files varies based on the number of partitions (CPU cores). The following query loads the parquet dataset back alongside input_file_name() standard function to mimic findTouchedFiles 's behaviour. val FILE_NAME_COL = \"_file_name_\" val dataFiles = spark . read . parquet ( target ). withColumn ( FILE_NAME_COL , input_file_name ()) scala> dataFiles.show(truncate = false) +---+---------------------------------------------------------------------------------------+ |id |_file_name_ | +---+---------------------------------------------------------------------------------------+ |4 |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |0 |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |3 |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |6 |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |1 |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |8 |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |2 |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |7 |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |5 |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |9 |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| +---+---------------------------------------------------------------------------------------+ As you may have thought, not all part files have got data and so they are not included in the dataset. That is when findTouchedFiles uses groupBy operator and count action to calculate match frequency. val counts = dataFiles.groupBy(FILE_NAME_COL).count() scala> counts.show(truncate = false) +---------------------------------------------------------------------------------------+-----+ |_file_name_ |count| +---------------------------------------------------------------------------------------+-----+ |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | +---------------------------------------------------------------------------------------+-----+ Let's load all the part files in the /tmp/parquet directory and find which file(s) have no data. import scala.sys.process._ val cmd = ( s\"ls $target \" #| \"grep .parquet\" ). lineStream val allFiles = cmd . toArray . toSeq . toDF ( FILE_NAME_COL ) . select ( concat ( lit ( s\"file:// $target /\" ), col ( FILE_NAME_COL )) as FILE_NAME_COL ) val joinType = \"left_anti\" // MergeIntoCommand uses inner as it wants data file val noDataFiles = allFiles . join ( dataFiles , Seq ( FILE_NAME_COL ), joinType ) Mind that the data vs non-data datasets could be different, but that should not \"interfere\" with the main reasoning flow. scala> noDataFiles.show(truncate = false) +---------------------------------------------------------------------------------------+ |_file_name_ | +---------------------------------------------------------------------------------------+ |file:///tmp/parquet/part-00000-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| +---------------------------------------------------------------------------------------+ findTouchedFiles registers an accumulator to collect all the distinct files that need to be rewritten ( touched files ). Note The name of the accumulator is internal.metrics.MergeIntoDelta.touchedFiles and internal.metrics part is supposed to hide it from web UI as potentially large (set of file names to be rewritten). findTouchedFiles defines a nondeterministic UDF that adds the file names to the accumulator ( recordTouchedFileName ). findTouchedFiles splits conjunctive predicates ( And binary expressions) in the condition expression and collects the predicates that use the target 's columns ( targetOnlyPredicates ). findTouchedFiles requests the given OptimisticTransaction for the files that match the target-only predicates (and creates a dataSkippedFiles collection of AddFile s). Note This step looks similar to filter predicate pushdown . findTouchedFiles creates one DataFrame for the source data (using Dataset.ofRows utility). Tip Learn more about Dataset.ofRows utility in The Internals of Spark SQL online book. findTouchedFiles builds a logical query plan for the files (matching the predicates) and creates another DataFrame for the target data. findTouchedFiles adds two columns to the target dataframe: _row_id_ for monotonically_increasing_id() standard function _file_name_ for input_file_name() standard function findTouchedFiles creates (a DataFrame that is) an INNER JOIN of the source and target DataFrame s using the condition expression. findTouchedFiles takes the joined dataframe and selects _row_id_ column and the recordTouchedFileName UDF on the _file_name_ column as one . The DataFrame is internally known as collectTouchedFiles . findTouchedFiles uses groupBy operator on _row_id_ to calculate a sum of all the values in the one column (as count column) in the two-column collectTouchedFiles dataset. The DataFrame is internally known as matchedRowCounts . Note No Spark job has been submitted yet. findTouchedFiles is still in \"query preparation\" mode. findTouchedFiles uses filter on the count column (in the matchedRowCounts dataset) with values greater than 1 . If there are any, findTouchedFiles throws an UnsupportedOperationException exception: Cannot perform MERGE as multiple source rows matched and attempted to update the same target row in the Delta table. By SQL semantics of merge, when multiple source rows match on the same target row, the update operation is ambiguous as it is unclear which source should be used to update the matching target row. You can preprocess the source table to eliminate the possibility of multiple matches. Note Since findTouchedFiles uses count action there should be a Spark SQL query reported (and possibly Spark jobs) in web UI. findTouchedFiles requests the touchedFilesAccum accumulator for the touched file names. Example 2: Understanding the Internals of findTouchedFiles val TOUCHED_FILES_ACCUM_NAME = \"MergeIntoDelta.touchedFiles\" val touchedFilesAccum = spark . sparkContext . collectionAccumulator [ String ]( TOUCHED_FILES_ACCUM_NAME ) val recordTouchedFileName = udf { ( fileName : String ) => { touchedFilesAccum . add ( fileName ) 1 }}. asNondeterministic () val target = \"/tmp/parquet\" spark . range ( 10 ). write . save ( target ) val FILE_NAME_COL = \"_file_name_\" val dataFiles = spark . read . parquet ( target ). withColumn ( FILE_NAME_COL , input_file_name ()) val collectTouchedFiles = dataFiles . select ( col ( FILE_NAME_COL ), recordTouchedFileName ( col ( FILE_NAME_COL )). as ( \"one\" )) scala> collectTouchedFiles.show(truncate = false) +---------------------------------------------------------------------------------------+---+ |_file_name_ |one| +---------------------------------------------------------------------------------------+---+ |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | +---------------------------------------------------------------------------------------+---+ import scala.collection.JavaConverters._ val touchedFileNames = touchedFilesAccum . value . asScala . toSeq Use the Stages tab in web UI to review the accumulator values. findTouchedFiles prints out the following TRACE message to the logs: findTouchedFiles: matched files: [touchedFileNames] findTouchedFiles generateCandidateFileMap for the files that match the target-only predicates . findTouchedFiles getTouchedFile for every touched file name. findTouchedFiles updates the following performance metrics: numTargetFilesBeforeSkipping and adds the numOfFiles of the Snapshot of the given OptimisticTransaction numTargetFilesAfterSkipping and adds the number of the files that match the target-only predicates numTargetFilesRemoved and adds the number of the touched files In the end, findTouchedFiles gives the touched files (as AddFile s). Writing All Changes \u00b6 writeAllChanges ( spark : SparkSession , deltaTxn : OptimisticTransaction , filesToRewrite : Seq [ AddFile ]) : Seq [ AddFile ] writeAllChanges builds the target output columns (possibly with some null s for the target columns that are not in the current schema). writeAllChanges builds a target logical query plan for the AddFiles . writeAllChanges determines a join type to use ( rightOuter or fullOuter ). writeAllChanges prints out the following DEBUG message to the logs: writeAllChanges using [joinType] join: source.output: [outputSet] target.output: [outputSet] condition: [condition] newTarget.output: [outputSet] writeAllChanges creates a joinedDF DataFrame that is a join of the DataFrames for the source and the new target logical plans with the given join condition and the join type . writeAllChanges creates a JoinedRowProcessor that is then used to map over partitions of the joined DataFrame . writeAllChanges prints out the following DEBUG message to the logs: writeAllChanges: join output plan: [outputDF.queryExecution] writeAllChanges requests the input OptimisticTransaction to writeFiles (possibly repartitioning by the partition columns if table is partitioned and spark.databricks.delta.merge.repartitionBeforeWrite.enabled configuration property is enabled). writeAllChanges is used when MergeIntoCommand is requested to run . Building Target Logical Query Plan for AddFiles \u00b6 buildTargetPlanWithFiles ( deltaTxn : OptimisticTransaction , files : Seq [ AddFile ]) : LogicalPlan buildTargetPlanWithFiles creates a DataFrame to represent the given AddFile s to access the analyzed logical query plan. buildTargetPlanWithFiles requests the given OptimisticTransaction for the DeltaLog to create a DataFrame (for the Snapshot and the given AddFile s). In the end, buildTargetPlanWithFiles creates a Project logical operator with Alias expressions so the output columns of the analyzed logical query plan (of the DataFrame of the AddFiles ) reference the target's output columns (by name). Note The output columns of the target delta table are associated with a OptimisticTransaction as the Metadata . deltaTxn . metadata . schema writeInsertsOnlyWhenNoMatchedClauses \u00b6 writeInsertsOnlyWhenNoMatchedClauses ( spark : SparkSession , deltaTxn : OptimisticTransaction ) : Seq [ AddFile ] writeInsertsOnlyWhenNoMatchedClauses ...FIXME Exceptions \u00b6 run throws an AnalysisException when the target schema is different than the delta table's (has changed after analysis phase): The schema of your Delta table has changed in an incompatible way since your DataFrame or DeltaTable object was created. Please redefine your DataFrame or DeltaTable object. Changes: [schemaDiff] This check can be turned off by setting the session configuration key spark.databricks.delta.checkLatestSchemaOnRead to false. Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.commands.MergeIntoCommand=ALL Refer to Logging .","title":"MergeIntoCommand"},{"location":"commands/MergeIntoCommand/#mergeintocommand","text":"MergeIntoCommand is a DeltaCommand that represents a DeltaMergeInto logical command at execution time. MergeIntoCommand is a logical command (Spark SQL's RunnableCommand ). Tip Learn more on the internals of MergeIntoCommand in Demo: Merge Operation .","title":"MergeIntoCommand"},{"location":"commands/MergeIntoCommand/#performance-metrics","text":"Name web UI numSourceRows number of source rows numTargetRowsCopied number of target rows rewritten unmodified numTargetRowsInserted number of inserted rows numTargetRowsUpdated number of updated rows numTargetRowsDeleted number of deleted rows numTargetFilesBeforeSkipping number of target files before skipping numTargetFilesAfterSkipping number of target files after skipping numTargetFilesRemoved number of files removed to target numTargetFilesAdded number of files added to target","title":"Performance Metrics"},{"location":"commands/MergeIntoCommand/#number-of-target-rows-rewritten-unmodified","text":"numTargetRowsCopied performance metric (like the other metrics ) is turned into a non-deterministic user-defined function (UDF). numTargetRowsCopied becomes incrNoopCountExpr UDF. incrNoopCountExpr UDF is resolved on a joined plan and used to create a JoinedRowProcessor for processing partitions of the joined plan Dataset .","title":" number of target rows rewritten unmodified"},{"location":"commands/MergeIntoCommand/#creating-instance","text":"MergeIntoCommand takes the following to be created: Source Data Target Data ( LogicalPlan ) TahoeFileIndex Condition Expression Matched Clauses ( Seq[DeltaMergeIntoMatchedClause] ) Optional Non-Matched Clause ( Option[DeltaMergeIntoInsertClause] ) Migrated Schema MergeIntoCommand is created when PreprocessTableMerge logical resolution rule is executed (on a DeltaMergeInto logical command).","title":"Creating Instance"},{"location":"commands/MergeIntoCommand/#source-data-to-merge-from","text":"When created , MergeIntoCommand is given a LogicalPlan for the source data to merge from (referred to internally as source ). The source LogicalPlan is used twice: Firstly, in one of the following: An inner join (in findTouchedFiles ) that is count in web UI A leftanti join (in writeInsertsOnlyWhenNoMatchedClauses ) Secondly, in rightOuter or fullOuter join (in writeAllChanges ) Tip Enable DEBUG logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger to see the inner-workings of writeAllChanges .","title":" Source Data (to Merge From)"},{"location":"commands/MergeIntoCommand/#target-deltalog","text":"targetDeltaLog : DeltaLog targetDeltaLog is the DeltaLog of the TahoeFileIndex . targetDeltaLog is used for the following: Start a new transaction when executed To access the Data Path when finding files to rewrite Lazy Value targetDeltaLog is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards.","title":" Target DeltaLog"},{"location":"commands/MergeIntoCommand/#executing-command","text":"run ( spark : SparkSession ) : Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run requests the target DeltaLog to start a new transaction . With spark.databricks.delta.schema.autoMerge.enabled configuration property enabled, run updates the metadata (of the transaction). run determines Delta actions ( RemoveFile s and AddFile s). Describe deltaActions part With spark.databricks.delta.history.metricsEnabled configuration property enabled, run requests the current transaction to register SQL metrics for the Delta operation . run requests the current transaction to commit (with the Delta actions and Merge operation). run records the Delta event. run posts a SparkListenerDriverAccumUpdates Spark event (with the metrics). In the end, run requests the CacheManager to recacheByPlan .","title":" Executing Command"},{"location":"commands/MergeIntoCommand/#finding-files-to-rewrite","text":"findTouchedFiles ( spark : SparkSession , deltaTxn : OptimisticTransaction ) : Seq [ AddFile ] Important findTouchedFiles is such a fine piece of art ( a gem ). It uses a custom accumulator, a UDF (to use this accumulator to record touched file names) and input_file_name() standard function for the names of the files read. It is always worth keeping in mind that Delta Lake uses files for data storage and that is why input_file_name() standard function works. It would not work for non-file-based data sources. Example 1: Understanding the Internals of findTouchedFiles The following query writes out a 10-element dataset using the default parquet data source to /tmp/parquet directory: val target = \"/tmp/parquet\" spark . range ( 10 ). write . save ( target ) The number of parquet part files varies based on the number of partitions (CPU cores). The following query loads the parquet dataset back alongside input_file_name() standard function to mimic findTouchedFiles 's behaviour. val FILE_NAME_COL = \"_file_name_\" val dataFiles = spark . read . parquet ( target ). withColumn ( FILE_NAME_COL , input_file_name ()) scala> dataFiles.show(truncate = false) +---+---------------------------------------------------------------------------------------+ |id |_file_name_ | +---+---------------------------------------------------------------------------------------+ |4 |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |0 |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |3 |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |6 |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |1 |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |8 |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |2 |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |7 |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |5 |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| |9 |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| +---+---------------------------------------------------------------------------------------+ As you may have thought, not all part files have got data and so they are not included in the dataset. That is when findTouchedFiles uses groupBy operator and count action to calculate match frequency. val counts = dataFiles.groupBy(FILE_NAME_COL).count() scala> counts.show(truncate = false) +---------------------------------------------------------------------------------------+-----+ |_file_name_ |count| +---------------------------------------------------------------------------------------+-----+ |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | +---------------------------------------------------------------------------------------+-----+ Let's load all the part files in the /tmp/parquet directory and find which file(s) have no data. import scala.sys.process._ val cmd = ( s\"ls $target \" #| \"grep .parquet\" ). lineStream val allFiles = cmd . toArray . toSeq . toDF ( FILE_NAME_COL ) . select ( concat ( lit ( s\"file:// $target /\" ), col ( FILE_NAME_COL )) as FILE_NAME_COL ) val joinType = \"left_anti\" // MergeIntoCommand uses inner as it wants data file val noDataFiles = allFiles . join ( dataFiles , Seq ( FILE_NAME_COL ), joinType ) Mind that the data vs non-data datasets could be different, but that should not \"interfere\" with the main reasoning flow. scala> noDataFiles.show(truncate = false) +---------------------------------------------------------------------------------------+ |_file_name_ | +---------------------------------------------------------------------------------------+ |file:///tmp/parquet/part-00000-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet| +---------------------------------------------------------------------------------------+ findTouchedFiles registers an accumulator to collect all the distinct files that need to be rewritten ( touched files ). Note The name of the accumulator is internal.metrics.MergeIntoDelta.touchedFiles and internal.metrics part is supposed to hide it from web UI as potentially large (set of file names to be rewritten). findTouchedFiles defines a nondeterministic UDF that adds the file names to the accumulator ( recordTouchedFileName ). findTouchedFiles splits conjunctive predicates ( And binary expressions) in the condition expression and collects the predicates that use the target 's columns ( targetOnlyPredicates ). findTouchedFiles requests the given OptimisticTransaction for the files that match the target-only predicates (and creates a dataSkippedFiles collection of AddFile s). Note This step looks similar to filter predicate pushdown . findTouchedFiles creates one DataFrame for the source data (using Dataset.ofRows utility). Tip Learn more about Dataset.ofRows utility in The Internals of Spark SQL online book. findTouchedFiles builds a logical query plan for the files (matching the predicates) and creates another DataFrame for the target data. findTouchedFiles adds two columns to the target dataframe: _row_id_ for monotonically_increasing_id() standard function _file_name_ for input_file_name() standard function findTouchedFiles creates (a DataFrame that is) an INNER JOIN of the source and target DataFrame s using the condition expression. findTouchedFiles takes the joined dataframe and selects _row_id_ column and the recordTouchedFileName UDF on the _file_name_ column as one . The DataFrame is internally known as collectTouchedFiles . findTouchedFiles uses groupBy operator on _row_id_ to calculate a sum of all the values in the one column (as count column) in the two-column collectTouchedFiles dataset. The DataFrame is internally known as matchedRowCounts . Note No Spark job has been submitted yet. findTouchedFiles is still in \"query preparation\" mode. findTouchedFiles uses filter on the count column (in the matchedRowCounts dataset) with values greater than 1 . If there are any, findTouchedFiles throws an UnsupportedOperationException exception: Cannot perform MERGE as multiple source rows matched and attempted to update the same target row in the Delta table. By SQL semantics of merge, when multiple source rows match on the same target row, the update operation is ambiguous as it is unclear which source should be used to update the matching target row. You can preprocess the source table to eliminate the possibility of multiple matches. Note Since findTouchedFiles uses count action there should be a Spark SQL query reported (and possibly Spark jobs) in web UI. findTouchedFiles requests the touchedFilesAccum accumulator for the touched file names. Example 2: Understanding the Internals of findTouchedFiles val TOUCHED_FILES_ACCUM_NAME = \"MergeIntoDelta.touchedFiles\" val touchedFilesAccum = spark . sparkContext . collectionAccumulator [ String ]( TOUCHED_FILES_ACCUM_NAME ) val recordTouchedFileName = udf { ( fileName : String ) => { touchedFilesAccum . add ( fileName ) 1 }}. asNondeterministic () val target = \"/tmp/parquet\" spark . range ( 10 ). write . save ( target ) val FILE_NAME_COL = \"_file_name_\" val dataFiles = spark . read . parquet ( target ). withColumn ( FILE_NAME_COL , input_file_name ()) val collectTouchedFiles = dataFiles . select ( col ( FILE_NAME_COL ), recordTouchedFileName ( col ( FILE_NAME_COL )). as ( \"one\" )) scala> collectTouchedFiles.show(truncate = false) +---------------------------------------------------------------------------------------+---+ |_file_name_ |one| +---------------------------------------------------------------------------------------+---+ |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1 | +---------------------------------------------------------------------------------------+---+ import scala.collection.JavaConverters._ val touchedFileNames = touchedFilesAccum . value . asScala . toSeq Use the Stages tab in web UI to review the accumulator values. findTouchedFiles prints out the following TRACE message to the logs: findTouchedFiles: matched files: [touchedFileNames] findTouchedFiles generateCandidateFileMap for the files that match the target-only predicates . findTouchedFiles getTouchedFile for every touched file name. findTouchedFiles updates the following performance metrics: numTargetFilesBeforeSkipping and adds the numOfFiles of the Snapshot of the given OptimisticTransaction numTargetFilesAfterSkipping and adds the number of the files that match the target-only predicates numTargetFilesRemoved and adds the number of the touched files In the end, findTouchedFiles gives the touched files (as AddFile s).","title":" Finding Files to Rewrite"},{"location":"commands/MergeIntoCommand/#writing-all-changes","text":"writeAllChanges ( spark : SparkSession , deltaTxn : OptimisticTransaction , filesToRewrite : Seq [ AddFile ]) : Seq [ AddFile ] writeAllChanges builds the target output columns (possibly with some null s for the target columns that are not in the current schema). writeAllChanges builds a target logical query plan for the AddFiles . writeAllChanges determines a join type to use ( rightOuter or fullOuter ). writeAllChanges prints out the following DEBUG message to the logs: writeAllChanges using [joinType] join: source.output: [outputSet] target.output: [outputSet] condition: [condition] newTarget.output: [outputSet] writeAllChanges creates a joinedDF DataFrame that is a join of the DataFrames for the source and the new target logical plans with the given join condition and the join type . writeAllChanges creates a JoinedRowProcessor that is then used to map over partitions of the joined DataFrame . writeAllChanges prints out the following DEBUG message to the logs: writeAllChanges: join output plan: [outputDF.queryExecution] writeAllChanges requests the input OptimisticTransaction to writeFiles (possibly repartitioning by the partition columns if table is partitioned and spark.databricks.delta.merge.repartitionBeforeWrite.enabled configuration property is enabled). writeAllChanges is used when MergeIntoCommand is requested to run .","title":" Writing All Changes"},{"location":"commands/MergeIntoCommand/#building-target-logical-query-plan-for-addfiles","text":"buildTargetPlanWithFiles ( deltaTxn : OptimisticTransaction , files : Seq [ AddFile ]) : LogicalPlan buildTargetPlanWithFiles creates a DataFrame to represent the given AddFile s to access the analyzed logical query plan. buildTargetPlanWithFiles requests the given OptimisticTransaction for the DeltaLog to create a DataFrame (for the Snapshot and the given AddFile s). In the end, buildTargetPlanWithFiles creates a Project logical operator with Alias expressions so the output columns of the analyzed logical query plan (of the DataFrame of the AddFiles ) reference the target's output columns (by name). Note The output columns of the target delta table are associated with a OptimisticTransaction as the Metadata . deltaTxn . metadata . schema","title":" Building Target Logical Query Plan for AddFiles"},{"location":"commands/MergeIntoCommand/#writeinsertsonlywhennomatchedclauses","text":"writeInsertsOnlyWhenNoMatchedClauses ( spark : SparkSession , deltaTxn : OptimisticTransaction ) : Seq [ AddFile ] writeInsertsOnlyWhenNoMatchedClauses ...FIXME","title":" writeInsertsOnlyWhenNoMatchedClauses"},{"location":"commands/MergeIntoCommand/#exceptions","text":"run throws an AnalysisException when the target schema is different than the delta table's (has changed after analysis phase): The schema of your Delta table has changed in an incompatible way since your DataFrame or DeltaTable object was created. Please redefine your DataFrame or DeltaTable object. Changes: [schemaDiff] This check can be turned off by setting the session configuration key spark.databricks.delta.checkLatestSchemaOnRead to false.","title":" Exceptions"},{"location":"commands/MergeIntoCommand/#logging","text":"Enable ALL logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.delta.commands.MergeIntoCommand=ALL Refer to Logging .","title":"Logging"},{"location":"commands/UpdateCommand/","text":"UpdateCommand \u00b6 UpdateCommand is...FIXME == [[run]] Running Command -- run Method [source, scala] \u00b6 run(sparkSession: SparkSession): Seq[Row] \u00b6 NOTE: run is part of the RunnableCommand contract to...FIXME. run ...FIXME == [[performUpdate]] performUpdate Internal Method [source, scala] \u00b6 performUpdate( sparkSession: SparkSession, deltaLog: DeltaLog, txn: OptimisticTransaction): Unit performUpdate ...FIXME NOTE: performUpdate is used exclusively when UpdateCommand is requested to < >. == [[rewriteFiles]] rewriteFiles Internal Method [source, scala] \u00b6 rewriteFiles( spark: SparkSession, txn: OptimisticTransaction, rootPath: Path, inputLeafFiles: Seq[String], nameToAddFileMap: Map[String, AddFile], condition: Expression): Seq[AddFile] rewriteFiles ...FIXME NOTE: rewriteFiles is used exclusively when UpdateCommand is requested to < >.","title":"UpdateCommand"},{"location":"commands/UpdateCommand/#updatecommand","text":"UpdateCommand is...FIXME == [[run]] Running Command -- run Method","title":"UpdateCommand"},{"location":"commands/UpdateCommand/#source-scala","text":"","title":"[source, scala]"},{"location":"commands/UpdateCommand/#runsparksession-sparksession-seqrow","text":"NOTE: run is part of the RunnableCommand contract to...FIXME. run ...FIXME == [[performUpdate]] performUpdate Internal Method","title":"run(sparkSession: SparkSession): Seq[Row]"},{"location":"commands/UpdateCommand/#source-scala_1","text":"performUpdate( sparkSession: SparkSession, deltaLog: DeltaLog, txn: OptimisticTransaction): Unit performUpdate ...FIXME NOTE: performUpdate is used exclusively when UpdateCommand is requested to < >. == [[rewriteFiles]] rewriteFiles Internal Method","title":"[source, scala]"},{"location":"commands/UpdateCommand/#source-scala_2","text":"rewriteFiles( spark: SparkSession, txn: OptimisticTransaction, rootPath: Path, inputLeafFiles: Seq[String], nameToAddFileMap: Map[String, AddFile], condition: Expression): Seq[AddFile] rewriteFiles ...FIXME NOTE: rewriteFiles is used exclusively when UpdateCommand is requested to < >.","title":"[source, scala]"},{"location":"commands/VacuumCommand/","text":"VacuumCommand Utility \u2014 Garbage Collecting Delta Table \u00b6 VacuumCommand is a concrete VacuumCommandImpl for gc . Garbage Collecting Of Delta Table \u00b6 gc ( spark : SparkSession , deltaLog : DeltaLog , dryRun : Boolean = true , retentionHours : Option [ Double ] = None , clock : Clock = new SystemClock ) : DataFrame gc requests the given DeltaLog to < > (and give the latest < > of the delta table). [[gc-deleteBeforeTimestamp]] gc...FIXME (deleteBeforeTimestamp) gc prints out the following INFO message to the logs: Starting garbage collection (dryRun = [dryRun]) of untracked files older than [deleteBeforeTimestamp] in [path] [[gc-validFiles]] gc requests the Snapshot for the < > and defines a function for every action (in a partition) that does the following: . FIXME gc converts the mapped state dataset (of actions) into a DataFrame with a single path column. [[gc-allFilesAndDirs]] gc...FIXME gc caches the < > dataset. gc prints out the following INFO message to the logs: Deleting untracked files and empty directories in [path] gc...FIXME gc prints out the following message to standard output: Deleted [filesDeleted] files and directories in a total of [dirCounts] directories. gc...FIXME In the end, gc unpersists the < > dataset. [NOTE] \u00b6 gc is used when: DeltaTableOperations is requested to < > (for < > operator) * < > is executed (for delta-sql.md#VACUUM[VACUUM] SQL command) \u00b6 == [[checkRetentionPeriodSafety]] checkRetentionPeriodSafety Method [source, scala] \u00b6 checkRetentionPeriodSafety( spark: SparkSession, retentionMs: Option[Long], configuredRetention: Long): Unit checkRetentionPeriodSafety ...FIXME NOTE: checkRetentionPeriodSafety is used exclusively when VacuumCommand utility is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.sql.delta.commands.VacuumCommand logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.sql.delta.commands.VacuumCommand=ALL \u00b6 Refer to Logging .","title":"VacuumCommand"},{"location":"commands/VacuumCommand/#vacuumcommand-utility-garbage-collecting-delta-table","text":"VacuumCommand is a concrete VacuumCommandImpl for gc .","title":"VacuumCommand Utility &mdash; Garbage Collecting Delta Table"},{"location":"commands/VacuumCommand/#garbage-collecting-of-delta-table","text":"gc ( spark : SparkSession , deltaLog : DeltaLog , dryRun : Boolean = true , retentionHours : Option [ Double ] = None , clock : Clock = new SystemClock ) : DataFrame gc requests the given DeltaLog to < > (and give the latest < > of the delta table). [[gc-deleteBeforeTimestamp]] gc...FIXME (deleteBeforeTimestamp) gc prints out the following INFO message to the logs: Starting garbage collection (dryRun = [dryRun]) of untracked files older than [deleteBeforeTimestamp] in [path] [[gc-validFiles]] gc requests the Snapshot for the < > and defines a function for every action (in a partition) that does the following: . FIXME gc converts the mapped state dataset (of actions) into a DataFrame with a single path column. [[gc-allFilesAndDirs]] gc...FIXME gc caches the < > dataset. gc prints out the following INFO message to the logs: Deleting untracked files and empty directories in [path] gc...FIXME gc prints out the following message to standard output: Deleted [filesDeleted] files and directories in a total of [dirCounts] directories. gc...FIXME In the end, gc unpersists the < > dataset.","title":" Garbage Collecting Of Delta Table"},{"location":"commands/VacuumCommand/#note","text":"gc is used when: DeltaTableOperations is requested to < > (for < > operator)","title":"[NOTE]"},{"location":"commands/VacuumCommand/#is-executed-for-delta-sqlmdvacuumvacuum-sql-command","text":"== [[checkRetentionPeriodSafety]] checkRetentionPeriodSafety Method","title":"* &lt;&gt; is executed (for delta-sql.md#VACUUM[VACUUM] SQL command)"},{"location":"commands/VacuumCommand/#source-scala","text":"checkRetentionPeriodSafety( spark: SparkSession, retentionMs: Option[Long], configuredRetention: Long): Unit checkRetentionPeriodSafety ...FIXME NOTE: checkRetentionPeriodSafety is used exclusively when VacuumCommand utility is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.sql.delta.commands.VacuumCommand logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"commands/VacuumCommand/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"commands/VacuumCommand/#log4jloggerorgapachesparksqldeltacommandsvacuumcommandall","text":"Refer to Logging .","title":"log4j.logger.org.apache.spark.sql.delta.commands.VacuumCommand=ALL"},{"location":"commands/VacuumCommandImpl/","text":"VacuumCommandImpl \u00b6 VacuumCommandImpl is...FIXME == [[delete]] delete Method [source,scala] \u00b6 delete( diff: Dataset[String], fs: FileSystem): Long delete...FIXME delete is used when VacuumCommand utility is requested to VacuumCommand.md#gc[gc]","title":"VacuumCommandImpl"},{"location":"commands/VacuumCommandImpl/#vacuumcommandimpl","text":"VacuumCommandImpl is...FIXME == [[delete]] delete Method","title":"VacuumCommandImpl"},{"location":"commands/VacuumCommandImpl/#sourcescala","text":"delete( diff: Dataset[String], fs: FileSystem): Long delete...FIXME delete is used when VacuumCommand utility is requested to VacuumCommand.md#gc[gc]","title":"[source,scala]"},{"location":"commands/VacuumTableCommand/","text":"VacuumTableCommand \u00b6 VacuumTableCommand is a runnable command ( Spark SQL ) for VACUUM SQL command. Creating Instance \u00b6 VacuumTableCommand takes the following to be created: Path TableIdentifier Optional Horizon Hours dryRun flag VacuumTableCommand requires that either the table or the path is defined and it is the root directory of a delta table. Partition directories are not supported. VacuumTableCommand is created when: DeltaSqlAstBuilder is requested to parse VACUUM SQL command Executing Command \u00b6 run ( sparkSession : SparkSession ) : Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run takes the path to vacuum (either the table or the path ) and finds the root directory of the delta table . run creates a DeltaLog instance for the delta table and gc it (passing in the DeltaLog instance, the dryRun and the horizonHours options). run throws an AnalysisException when executed for a non-root directory of a delta table: Please provide the base path ([baseDeltaPath]) when Vacuuming Delta tables. Vacuuming specific partitions is currently not supported. run throws an AnalysisException when executed for a DeltaLog with the snapshot version being -1 : [deltaTableIdentifier] is not a Delta table. VACUUM is only supported for Delta tables. output \u00b6 The output of VacuumTableCommand is a single path column (of type StringType ).","title":"VacuumTableCommand"},{"location":"commands/VacuumTableCommand/#vacuumtablecommand","text":"VacuumTableCommand is a runnable command ( Spark SQL ) for VACUUM SQL command.","title":"VacuumTableCommand"},{"location":"commands/VacuumTableCommand/#creating-instance","text":"VacuumTableCommand takes the following to be created: Path TableIdentifier Optional Horizon Hours dryRun flag VacuumTableCommand requires that either the table or the path is defined and it is the root directory of a delta table. Partition directories are not supported. VacuumTableCommand is created when: DeltaSqlAstBuilder is requested to parse VACUUM SQL command","title":"Creating Instance"},{"location":"commands/VacuumTableCommand/#executing-command","text":"run ( sparkSession : SparkSession ) : Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run takes the path to vacuum (either the table or the path ) and finds the root directory of the delta table . run creates a DeltaLog instance for the delta table and gc it (passing in the DeltaLog instance, the dryRun and the horizonHours options). run throws an AnalysisException when executed for a non-root directory of a delta table: Please provide the base path ([baseDeltaPath]) when Vacuuming Delta tables. Vacuuming specific partitions is currently not supported. run throws an AnalysisException when executed for a DeltaLog with the snapshot version being -1 : [deltaTableIdentifier] is not a Delta table. VACUUM is only supported for Delta tables.","title":" Executing Command"},{"location":"commands/VacuumTableCommand/#output","text":"The output of VacuumTableCommand is a single path column (of type StringType ).","title":" output"},{"location":"commands/WriteIntoDelta/","text":"WriteIntoDelta Command \u00b6 WriteIntoDelta is a Delta command that can write data(frame) transactionally into a delta table . WriteIntoDelta is a RunnableCommand ( Spark SQL ). Creating Instance \u00b6 WriteIntoDelta takes the following to be created: DeltaLog SaveMode DeltaOptions Names of the partition columns Configuration Data ( DataFrame ) WriteIntoDelta is created when: DeltaLog is requested to create an insertable HadoopFsRelation (when DeltaDataSource is requested to create a relation as a CreatableRelationProvider or a RelationProvider ) DeltaCatalog is requested to createDeltaTable WriteIntoDeltaBuilder is requested to buildForV1Write CreateDeltaTableCommand command is executed DeltaDataSource is requested to create a relation (for writing) (as a CreatableRelationProvider ) ImplicitMetadataOperation \u00b6 WriteIntoDelta is an operation that can update metadata (schema and partitioning) of the delta table . Executing Command \u00b6 run ( sparkSession : SparkSession ) : Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run requests the DeltaLog to start a new transaction . run writes and requests the OptimisticTransaction to commit (with DeltaOperations.Write operation with the SaveMode , partition columns , replaceWhere and userMetadata ). write \u00b6 write ( txn : OptimisticTransaction , sparkSession : SparkSession ) : Seq [ Action ] write checks out whether the write operation is to a delta table that already exists. If so (i.e. the readVersion of the transaction is above -1 ), write branches per the SaveMode : For ErrorIfExists , write throws an AnalysisException . [path] already exists. For Ignore , write does nothing and returns back with no Action s. For Overwrite , write requests the DeltaLog to assert being removable write updateMetadata (with rearrangeOnly option). write ...FIXME write is used when: CreateDeltaTableCommand is executed WriteIntoDelta is executed Demo \u00b6 import org.apache.spark.sql.delta.commands.WriteIntoDelta import org.apache.spark.sql.delta.DeltaLog import org.apache.spark.sql.SaveMode import org.apache.spark.sql.delta.DeltaOptions val tableName = \"/tmp/delta/t1\" val data = spark.range(5).toDF val writeCmd = WriteIntoDelta( deltaLog = DeltaLog.forTable(spark, tableName), mode = SaveMode.Overwrite, options = new DeltaOptions(Map.empty[String, String], spark.sessionState.conf), partitionColumns = Seq.empty[String], configuration = Map.empty[String, String], data) // Review web UI @ http://localhost:4040 writeCmd.run(spark)","title":"WriteIntoDelta"},{"location":"commands/WriteIntoDelta/#writeintodelta-command","text":"WriteIntoDelta is a Delta command that can write data(frame) transactionally into a delta table . WriteIntoDelta is a RunnableCommand ( Spark SQL ).","title":"WriteIntoDelta Command"},{"location":"commands/WriteIntoDelta/#creating-instance","text":"WriteIntoDelta takes the following to be created: DeltaLog SaveMode DeltaOptions Names of the partition columns Configuration Data ( DataFrame ) WriteIntoDelta is created when: DeltaLog is requested to create an insertable HadoopFsRelation (when DeltaDataSource is requested to create a relation as a CreatableRelationProvider or a RelationProvider ) DeltaCatalog is requested to createDeltaTable WriteIntoDeltaBuilder is requested to buildForV1Write CreateDeltaTableCommand command is executed DeltaDataSource is requested to create a relation (for writing) (as a CreatableRelationProvider )","title":"Creating Instance"},{"location":"commands/WriteIntoDelta/#implicitmetadataoperation","text":"WriteIntoDelta is an operation that can update metadata (schema and partitioning) of the delta table .","title":"ImplicitMetadataOperation"},{"location":"commands/WriteIntoDelta/#executing-command","text":"run ( sparkSession : SparkSession ) : Seq [ Row ] run is part of the RunnableCommand ( Spark SQL ) abstraction. run requests the DeltaLog to start a new transaction . run writes and requests the OptimisticTransaction to commit (with DeltaOperations.Write operation with the SaveMode , partition columns , replaceWhere and userMetadata ).","title":" Executing Command"},{"location":"commands/WriteIntoDelta/#write","text":"write ( txn : OptimisticTransaction , sparkSession : SparkSession ) : Seq [ Action ] write checks out whether the write operation is to a delta table that already exists. If so (i.e. the readVersion of the transaction is above -1 ), write branches per the SaveMode : For ErrorIfExists , write throws an AnalysisException . [path] already exists. For Ignore , write does nothing and returns back with no Action s. For Overwrite , write requests the DeltaLog to assert being removable write updateMetadata (with rearrangeOnly option). write ...FIXME write is used when: CreateDeltaTableCommand is executed WriteIntoDelta is executed","title":" write"},{"location":"commands/WriteIntoDelta/#demo","text":"import org.apache.spark.sql.delta.commands.WriteIntoDelta import org.apache.spark.sql.delta.DeltaLog import org.apache.spark.sql.SaveMode import org.apache.spark.sql.delta.DeltaOptions val tableName = \"/tmp/delta/t1\" val data = spark.range(5).toDF val writeCmd = WriteIntoDelta( deltaLog = DeltaLog.forTable(spark, tableName), mode = SaveMode.Overwrite, options = new DeltaOptions(Map.empty[String, String], spark.sessionState.conf), partitionColumns = Seq.empty[String], configuration = Map.empty[String, String], data) // Review web UI @ http://localhost:4040 writeCmd.run(spark)","title":"Demo"},{"location":"commands/vacuum/","text":"Vacuum Command \u00b6 Vacuum command does...FIXME (see < >) Vacuum command can be executed as delta-sql.md#VACUUM[VACUUM] SQL command or < > operator. scala> sql(\"VACUUM delta.`/tmp/delta/t1`\").show Deleted 0 files and directories in a total of 2 directories. +------------------+ | path| +------------------+ |file:/tmp/delta/t1| +------------------+","title":"Vacuum"},{"location":"commands/vacuum/#vacuum-command","text":"Vacuum command does...FIXME (see < >) Vacuum command can be executed as delta-sql.md#VACUUM[VACUUM] SQL command or < > operator. scala> sql(\"VACUUM delta.`/tmp/delta/t1`\").show Deleted 0 files and directories in a total of 2 directories. +------------------+ | path| +------------------+ |file:/tmp/delta/t1| +------------------+","title":"Vacuum Command"},{"location":"contenders/","text":"Contenders \u00b6 As it happens in the open source software world, Delta Lake is not alone in the area of Data Lakes on top of Apache Spark. The following is a list of some other open source projects that seems to compete or cover the same use cases. Apache Iceberg \u00b6 Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to Presto and Spark that use a high-performance format that works just like a SQL table. Videos \u00b6 ACID ORC, Iceberg, and Delta Lake\u2014An Overview of Table Formats for Large Scale Storage and Analytics Introducing Iceberg Tables designed for object stores Introducing Apache Iceberg: Tables Designed for Object Stores Iceberg: a fast table format for S3 Apache Hudi \u00b6 Apache Hudi ingests and manages storage of large analytical datasets over DFS (HDFS or cloud stores) and provides three logical views for query access. Videos \u00b6 Hoodie: An Open Source Incremental Processing Framework From Uber Powering Uber's global network analytics pipelines in real-time with Apache Hudi","title":"Contenders"},{"location":"contenders/#contenders","text":"As it happens in the open source software world, Delta Lake is not alone in the area of Data Lakes on top of Apache Spark. The following is a list of some other open source projects that seems to compete or cover the same use cases.","title":"Contenders"},{"location":"contenders/#apache-iceberg","text":"Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to Presto and Spark that use a high-performance format that works just like a SQL table.","title":"Apache Iceberg"},{"location":"contenders/#videos","text":"ACID ORC, Iceberg, and Delta Lake\u2014An Overview of Table Formats for Large Scale Storage and Analytics Introducing Iceberg Tables designed for object stores Introducing Apache Iceberg: Tables Designed for Object Stores Iceberg: a fast table format for S3","title":"Videos"},{"location":"contenders/#apache-hudi","text":"Apache Hudi ingests and manages storage of large analytical datasets over DFS (HDFS or cloud stores) and provides three logical views for query access.","title":"Apache Hudi"},{"location":"contenders/#videos_1","text":"Hoodie: An Open Source Incremental Processing Framework From Uber Powering Uber's global network analytics pipelines in real-time with Apache Hudi","title":"Videos"},{"location":"demo/","text":"Demos \u00b6 The following demos are available: dataChange replaceWhere Merge Operation Converting Parquet Dataset Into Delta Format Stream Processing of Delta Table Using Delta Lake as Streaming Sink in Structured Streaming Debugging Delta Lake Using IntelliJ IDEA Observing Transaction Retries DeltaTable, DeltaLog And Snapshots Schema Evolution User Metadata for Labelling Commits","title":"Demos"},{"location":"demo/#demos","text":"The following demos are available: dataChange replaceWhere Merge Operation Converting Parquet Dataset Into Delta Format Stream Processing of Delta Table Using Delta Lake as Streaming Sink in Structured Streaming Debugging Delta Lake Using IntelliJ IDEA Observing Transaction Retries DeltaTable, DeltaLog And Snapshots Schema Evolution User Metadata for Labelling Commits","title":"Demos"},{"location":"demo/Converting-Parquet-Dataset-Into-Delta-Format/","text":"Demo: Converting Parquet Dataset Into Delta Format \u00b6 /* spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ assert(spark.version.matches(\"2.4.[2-6]\"), \"Delta Lake supports Spark 2.4.2+\") import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val deltaLake = \"/tmp/delta\" // Create parquet table val users = s\"$deltaLake/users\" import spark.implicits._ val data = Seq( (0L, \"Agata\", \"Warsaw\", \"Poland\"), (1L, \"Jacek\", \"Warsaw\", \"Poland\"), (2L, \"Bartosz\", \"Paris\", \"France\") ).toDF(\"id\", \"name\", \"city\", \"country\") data .write .format(\"parquet\") .partitionBy(\"city\", \"country\") .mode(\"overwrite\") .save(users) // TIP: Use git to version the users directory // to track the changes for import // CONVERT TO DELTA only supports parquet tables // TableIdentifier should be parquet.`users` // Use TableIdentifier to refer to the parquet table // The path itself would work too val tableId = s\"parquet.`$users`\" val partitionSchema = \"city STRING, country STRING\" // Import users table into Delta Lake // Well, convert the parquet table into delta table // Use web UI to monitor execution, e.g. http://localhost:4040 import io.delta.tables.DeltaTable val dt = DeltaTable.convertToDelta(spark, tableId, partitionSchema) assert(dt.isInstanceOf[DeltaTable]) // users table is now in delta format assert(DeltaTable.isDeltaTable(users))","title":"Converting Parquet Dataset Into Delta Format"},{"location":"demo/Converting-Parquet-Dataset-Into-Delta-Format/#demo-converting-parquet-dataset-into-delta-format","text":"/* spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ assert(spark.version.matches(\"2.4.[2-6]\"), \"Delta Lake supports Spark 2.4.2+\") import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val deltaLake = \"/tmp/delta\" // Create parquet table val users = s\"$deltaLake/users\" import spark.implicits._ val data = Seq( (0L, \"Agata\", \"Warsaw\", \"Poland\"), (1L, \"Jacek\", \"Warsaw\", \"Poland\"), (2L, \"Bartosz\", \"Paris\", \"France\") ).toDF(\"id\", \"name\", \"city\", \"country\") data .write .format(\"parquet\") .partitionBy(\"city\", \"country\") .mode(\"overwrite\") .save(users) // TIP: Use git to version the users directory // to track the changes for import // CONVERT TO DELTA only supports parquet tables // TableIdentifier should be parquet.`users` // Use TableIdentifier to refer to the parquet table // The path itself would work too val tableId = s\"parquet.`$users`\" val partitionSchema = \"city STRING, country STRING\" // Import users table into Delta Lake // Well, convert the parquet table into delta table // Use web UI to monitor execution, e.g. http://localhost:4040 import io.delta.tables.DeltaTable val dt = DeltaTable.convertToDelta(spark, tableId, partitionSchema) assert(dt.isInstanceOf[DeltaTable]) // users table is now in delta format assert(DeltaTable.isDeltaTable(users))","title":"Demo: Converting Parquet Dataset Into Delta Format"},{"location":"demo/Debugging-Delta-Lake-Using-IntelliJ-IDEA/","text":"Demo: Debugging Delta Lake Using IntelliJ IDEA \u00b6 Import Delta Lake's sources to IntelliJ IDEA. Configure a new Remote debug configuration in IntelliJ IDEA (e.g. Run > Debug > Edit Configurations...) and simply give it a name and save. Tip Use Option+Ctrl+D to access Debug menu on mac OS. Run spark-shell as follows to enable remote JVM for debugging. export SPARK_SUBMIT_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\ --conf spark.databricks.delta.snapshotPartitions=1","title":"Debugging Delta Lake Using IntelliJ IDEA"},{"location":"demo/Debugging-Delta-Lake-Using-IntelliJ-IDEA/#demo-debugging-delta-lake-using-intellij-idea","text":"Import Delta Lake's sources to IntelliJ IDEA. Configure a new Remote debug configuration in IntelliJ IDEA (e.g. Run > Debug > Edit Configurations...) and simply give it a name and save. Tip Use Option+Ctrl+D to access Debug menu on mac OS. Run spark-shell as follows to enable remote JVM for debugging. export SPARK_SUBMIT_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\ --conf spark.databricks.delta.snapshotPartitions=1","title":"Demo: Debugging Delta Lake Using IntelliJ IDEA"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/","text":"Demo: DeltaTable, DeltaLog And Snapshots \u00b6 Create Delta Table \u00b6 import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val name = \"users\" sql(s\"DROP TABLE IF EXISTS $name\") sql(s\"\"\" | CREATE TABLE $name (id bigint, name string, city string, country string) | USING delta \"\"\".stripMargin) Access Transaction Log (DeltaLog) \u00b6 import org.apache.spark.sql.catalyst.TableIdentifier val tid = TableIdentifier(name) import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, tid) Update the state of the delta table to the most recent version. val snapshot = deltaLog.update() assert(snapshot.version == 0) val state = snapshot.state scala> :type state org.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.SingleAction] Review the cached RDD for the state snapshot in the Storage tab of the web UI (e.g. http://localhost:4040/storage/ ). The \"version\" part of Delta Table State name of the cached RDD should match the version of the snapshot // Show the changes (actions) scala> snapshot.state.show +----+----+------+--------------------+--------+----------+ | txn| add|remove| metaData|protocol|commitInfo| +----+----+------+--------------------+--------+----------+ |null|null| null| null| [1, 2]| null| |null|null| null|[5156c9e3-9668-43...| null| null| +----+----+------+--------------------+--------+----------+ DeltaTable as DataFrame \u00b6 import io.delta.tables.DeltaTable val dt = DeltaTable.forName(name) scala> dt.history.select('version, 'operation, 'operationParameters, 'operationMetrics).show(truncate = false) +-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+ |version| timestamp|userId|userName| operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics|userMetadata| +-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+ | 0|2020-09-29 10:31:30| null| null|CREATE TABLE|[isManaged -> tru...|null| null| null| null| null| true| []| null| +-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+ val users = dt.toDF scala> users.show +---+----+----+-------+ | id|name|city|country| +---+----+----+-------+ +---+----+----+-------+ Add new users \u00b6 val newUsers = Seq( (0L, \"Agata\", \"Warsaw\", \"Poland\"), (1L, \"Bartosz\", \"Paris\", \"France\") ).toDF(\"id\", \"name\", \"city\", \"country\") scala> newUsers.show +---+-------+------+-------+ | id| name| city|country| +---+-------+------+-------+ | 0| Agata|Warsaw| Poland| | 1|Bartosz| Paris| France| +---+-------+------+-------+ newUsers.write.format(\"delta\").mode(\"append\").saveAsTable(name) assert(deltaLog.snapshot.version == 1) Review the cached RDD for the state snapshot in the Storage tab of the web UI (e.g. http://localhost:4040/storage/ ). The \"version\" part of Delta Table State name of the cached RDD should match the version of the snapshot scala> users.show +---+-------+------+-------+ | id| name| city|country| +---+-------+------+-------+ | 1|Bartosz| Paris| France| | 0| Agata|Warsaw| Poland| +---+-------+------+-------+ scala> dt.history.select('version, 'operation, 'operationParameters, 'operationMetrics).show(truncate = false) +-------+------------+------------------------------------------------------------------------+-----------------------------------------------------------+ |version|operation |operationParameters |operationMetrics | +-------+------------+------------------------------------------------------------------------+-----------------------------------------------------------+ |1 |WRITE |[mode -> Append, partitionBy -> []] |[numFiles -> 2, numOutputBytes -> 2299, numOutputRows -> 2]| |0 |CREATE TABLE|[isManaged -> true, description ->, partitionBy -> [], properties -> {}]|[] | +-------+------------+------------------------------------------------------------------------+-----------------------------------------------------------+","title":"DeltaTable, DeltaLog And Snapshots"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#demo-deltatable-deltalog-and-snapshots","text":"","title":"Demo: DeltaTable, DeltaLog And Snapshots"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#create-delta-table","text":"import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val name = \"users\" sql(s\"DROP TABLE IF EXISTS $name\") sql(s\"\"\" | CREATE TABLE $name (id bigint, name string, city string, country string) | USING delta \"\"\".stripMargin)","title":"Create Delta Table"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#access-transaction-log-deltalog","text":"import org.apache.spark.sql.catalyst.TableIdentifier val tid = TableIdentifier(name) import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, tid) Update the state of the delta table to the most recent version. val snapshot = deltaLog.update() assert(snapshot.version == 0) val state = snapshot.state scala> :type state org.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.SingleAction] Review the cached RDD for the state snapshot in the Storage tab of the web UI (e.g. http://localhost:4040/storage/ ). The \"version\" part of Delta Table State name of the cached RDD should match the version of the snapshot // Show the changes (actions) scala> snapshot.state.show +----+----+------+--------------------+--------+----------+ | txn| add|remove| metaData|protocol|commitInfo| +----+----+------+--------------------+--------+----------+ |null|null| null| null| [1, 2]| null| |null|null| null|[5156c9e3-9668-43...| null| null| +----+----+------+--------------------+--------+----------+","title":"Access Transaction Log (DeltaLog)"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#deltatable-as-dataframe","text":"import io.delta.tables.DeltaTable val dt = DeltaTable.forName(name) scala> dt.history.select('version, 'operation, 'operationParameters, 'operationMetrics).show(truncate = false) +-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+ |version| timestamp|userId|userName| operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics|userMetadata| +-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+ | 0|2020-09-29 10:31:30| null| null|CREATE TABLE|[isManaged -> tru...|null| null| null| null| null| true| []| null| +-------+-------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+ val users = dt.toDF scala> users.show +---+----+----+-------+ | id|name|city|country| +---+----+----+-------+ +---+----+----+-------+","title":"DeltaTable as DataFrame"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#add-new-users","text":"val newUsers = Seq( (0L, \"Agata\", \"Warsaw\", \"Poland\"), (1L, \"Bartosz\", \"Paris\", \"France\") ).toDF(\"id\", \"name\", \"city\", \"country\") scala> newUsers.show +---+-------+------+-------+ | id| name| city|country| +---+-------+------+-------+ | 0| Agata|Warsaw| Poland| | 1|Bartosz| Paris| France| +---+-------+------+-------+ newUsers.write.format(\"delta\").mode(\"append\").saveAsTable(name) assert(deltaLog.snapshot.version == 1) Review the cached RDD for the state snapshot in the Storage tab of the web UI (e.g. http://localhost:4040/storage/ ). The \"version\" part of Delta Table State name of the cached RDD should match the version of the snapshot scala> users.show +---+-------+------+-------+ | id| name| city|country| +---+-------+------+-------+ | 1|Bartosz| Paris| France| | 0| Agata|Warsaw| Poland| +---+-------+------+-------+ scala> dt.history.select('version, 'operation, 'operationParameters, 'operationMetrics).show(truncate = false) +-------+------------+------------------------------------------------------------------------+-----------------------------------------------------------+ |version|operation |operationParameters |operationMetrics | +-------+------------+------------------------------------------------------------------------+-----------------------------------------------------------+ |1 |WRITE |[mode -> Append, partitionBy -> []] |[numFiles -> 2, numOutputBytes -> 2299, numOutputRows -> 2]| |0 |CREATE TABLE|[isManaged -> true, description ->, partitionBy -> [], properties -> {}]|[] | +-------+------------+------------------------------------------------------------------------+-----------------------------------------------------------+","title":"Add new users"},{"location":"demo/Observing-Transaction-Retries/","text":"Demo: Observing Transaction Retries \u00b6 Enable ALL logging level for org.apache.spark.sql.delta.OptimisticTransaction logger. You'll be looking for the following DEBUG message in the logs: Attempting to commit version [version] with 13 actions with Serializable isolation level Start with < > and place the following line breakpoints in OptimisticTransactionImpl : . In OptimisticTransactionImpl.doCommit when a transaction is about to deltaLog.store.write (line 388) . In OptimisticTransactionImpl.doCommit when a transaction is about to checkAndRetry after a FileAlreadyExistsException (line 433) . In OptimisticTransactionImpl.checkAndRetry when a transaction calculates nextAttemptVersion (line 453) In order to interfere with a transaction about to be committed, you will use ROOT:WriteIntoDelta.md[WriteIntoDelta] action (it is simple and does the work). Run the command (copy and paste the ROOT:WriteIntoDelta.md#demo[demo code] to spark-shell using paste mode). You should see the following messages in the logs: scala> writeCmd.run(spark) DeltaLog: DELTA: Updating the Delta table's state OptimisticTransaction: Attempting to commit version 6 with 13 actions with Serializable isolation level That's when you \"commit\" another transaction (to simulate two competing transactional writes). Simply create a delta file for the transaction. The commit version in the message above is 6 so the name of the delta file should be 00000000000000000006.json : $ touch /tmp/delta/t1/_delta_log/00000000000000000006.json F9 in IntelliJ IDEA to resume the WriteIntoDelta command. It should stop at checkAndRetry due to FileAlreadyExistsException . Press F9 twice to resume. You should see the following messages in the logs: OptimisticTransaction: No logical conflicts with deltas [6, 7), retrying. OptimisticTransaction: Attempting to commit version 7 with 13 actions with Serializable isolation level Rinse and repeat. You know the drill already. Happy debugging!","title":"Observing Transaction Retries"},{"location":"demo/Observing-Transaction-Retries/#demo-observing-transaction-retries","text":"Enable ALL logging level for org.apache.spark.sql.delta.OptimisticTransaction logger. You'll be looking for the following DEBUG message in the logs: Attempting to commit version [version] with 13 actions with Serializable isolation level Start with < > and place the following line breakpoints in OptimisticTransactionImpl : . In OptimisticTransactionImpl.doCommit when a transaction is about to deltaLog.store.write (line 388) . In OptimisticTransactionImpl.doCommit when a transaction is about to checkAndRetry after a FileAlreadyExistsException (line 433) . In OptimisticTransactionImpl.checkAndRetry when a transaction calculates nextAttemptVersion (line 453) In order to interfere with a transaction about to be committed, you will use ROOT:WriteIntoDelta.md[WriteIntoDelta] action (it is simple and does the work). Run the command (copy and paste the ROOT:WriteIntoDelta.md#demo[demo code] to spark-shell using paste mode). You should see the following messages in the logs: scala> writeCmd.run(spark) DeltaLog: DELTA: Updating the Delta table's state OptimisticTransaction: Attempting to commit version 6 with 13 actions with Serializable isolation level That's when you \"commit\" another transaction (to simulate two competing transactional writes). Simply create a delta file for the transaction. The commit version in the message above is 6 so the name of the delta file should be 00000000000000000006.json : $ touch /tmp/delta/t1/_delta_log/00000000000000000006.json F9 in IntelliJ IDEA to resume the WriteIntoDelta command. It should stop at checkAndRetry due to FileAlreadyExistsException . Press F9 twice to resume. You should see the following messages in the logs: OptimisticTransaction: No logical conflicts with deltas [6, 7), retrying. OptimisticTransaction: Attempting to commit version 7 with 13 actions with Serializable isolation level Rinse and repeat. You know the drill already. Happy debugging!","title":"Demo: Observing Transaction Retries"},{"location":"demo/Using-Delta-Lake-as-Streaming-Sink-in-Structured-Streaming/","text":"Demo: Using Delta Lake as Streaming Sink in Structured Streaming \u00b6 assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession]) assert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\") // Input data \"format\" case class User(id: Long, name: String, city: String) // Any streaming data source would work // Using memory data source // Gives control over the input stream implicit val ctx = spark.sqlContext import org.apache.spark.sql.execution.streaming.MemoryStream val usersIn = MemoryStream[User] val users = usersIn.toDF val deltaLake = \"/tmp/delta-lake\" val checkpointLocation = \"/tmp/delta-checkpointLocation\" val path = s\"$deltaLake/users\" val partitionBy = \"city\" // The streaming query that writes out to Delta Lake val sq = users .writeStream .format(\"delta\") .option(\"checkpointLocation\", checkpointLocation) .option(\"path\", path) .partitionBy(partitionBy) .start // TIP: You could use git to version the path directory // and track the changes of every micro-batch // TIP: Use web UI to monitor execution, e.g. http://localhost:4040 // FIXME: Use DESCRIBE HISTORY every micro-batch val batch1 = Seq( User(0, \"Agata\", \"Warsaw\"), User(1, \"Jacek\", \"Warsaw\")) val offset = usersIn.addData(batch1) sq.processAllAvailable() val history = s\"DESCRIBE HISTORY delta.`$path`\" val clmns = Seq($\"version\", $\"timestamp\", $\"operation\", $\"operationParameters\", $\"isBlindAppend\") val h = sql(history).select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ val batch2 = Seq( User(2, \"Bartek\", \"Paris\"), User(3, \"Jacek\", \"Paris\")) val offset = usersIn.addData(batch2) sq.processAllAvailable() // You have to execute the history SQL command again // It materializes immediately with whatever data is available at the time val h = sql(history).select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | |1 |2019-12-06 10:13:27|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 1]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ val batch3 = Seq( User(4, \"Gorazd\", \"Ljubljana\")) val offset = usersIn.addData(batch3) sq.processAllAvailable() // Let's use DeltaTable API instead import io.delta.tables.DeltaTable val usersDT = DeltaTable.forPath(path) val h = usersDT.history.select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | |1 |2019-12-06 10:13:27|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 1]|true | |2 |2019-12-06 10:20:56|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 2]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+","title":"Using Delta Lake as Streaming Sink in Structured Streaming"},{"location":"demo/Using-Delta-Lake-as-Streaming-Sink-in-Structured-Streaming/#demo-using-delta-lake-as-streaming-sink-in-structured-streaming","text":"assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession]) assert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\") // Input data \"format\" case class User(id: Long, name: String, city: String) // Any streaming data source would work // Using memory data source // Gives control over the input stream implicit val ctx = spark.sqlContext import org.apache.spark.sql.execution.streaming.MemoryStream val usersIn = MemoryStream[User] val users = usersIn.toDF val deltaLake = \"/tmp/delta-lake\" val checkpointLocation = \"/tmp/delta-checkpointLocation\" val path = s\"$deltaLake/users\" val partitionBy = \"city\" // The streaming query that writes out to Delta Lake val sq = users .writeStream .format(\"delta\") .option(\"checkpointLocation\", checkpointLocation) .option(\"path\", path) .partitionBy(partitionBy) .start // TIP: You could use git to version the path directory // and track the changes of every micro-batch // TIP: Use web UI to monitor execution, e.g. http://localhost:4040 // FIXME: Use DESCRIBE HISTORY every micro-batch val batch1 = Seq( User(0, \"Agata\", \"Warsaw\"), User(1, \"Jacek\", \"Warsaw\")) val offset = usersIn.addData(batch1) sq.processAllAvailable() val history = s\"DESCRIBE HISTORY delta.`$path`\" val clmns = Seq($\"version\", $\"timestamp\", $\"operation\", $\"operationParameters\", $\"isBlindAppend\") val h = sql(history).select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ val batch2 = Seq( User(2, \"Bartek\", \"Paris\"), User(3, \"Jacek\", \"Paris\")) val offset = usersIn.addData(batch2) sq.processAllAvailable() // You have to execute the history SQL command again // It materializes immediately with whatever data is available at the time val h = sql(history).select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | |1 |2019-12-06 10:13:27|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 1]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ val batch3 = Seq( User(4, \"Gorazd\", \"Ljubljana\")) val offset = usersIn.addData(batch3) sq.processAllAvailable() // Let's use DeltaTable API instead import io.delta.tables.DeltaTable val usersDT = DeltaTable.forPath(path) val h = usersDT.history.select(clmns: _*).orderBy($\"version\".asc) scala> h.show(truncate = false) +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |version|timestamp |operation |operationParameters |isBlindAppend| +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+ |0 |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 0]|true | |1 |2019-12-06 10:13:27|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 1]|true | |2 |2019-12-06 10:20:56|STREAMING UPDATE|[outputMode -> Append, queryId -> f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -> 2]|true | +-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+","title":"Demo: Using Delta Lake as Streaming Sink in Structured Streaming"},{"location":"demo/dataChange/","text":"Demo: dataChange \u00b6 This demo shows dataChange option in action. In combination with Overwrite mode, dataChange option can be used to transactionally rearrange data in a delta table. Start Spark Shell \u00b6 ./bin/spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog Create Delta Table \u00b6 val path = \"/tmp/delta/d01\" Make sure that there is no delta table at the location. Remove it if exists and start over. import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, path) assert(deltaLog.tableExists == false) Create the demo delta table (using SQL). sql(s\"\"\" CREATE TABLE delta.`$path` USING delta VALUES ((0, 'Jacek'), (1, 'Agata')) AS (id, name) \"\"\") Show History (Before) \u00b6 import io.delta.tables.DeltaTable val dt = DeltaTable . forPath ( path ) assert ( dt . history . count == 1 ) Repartition Table \u00b6 The following dataChange example shows a batch query that repartitions a delta table (perhaps while other queries could be using the delta table). Let's check out the number of partitions. spark.read.format(\"delta\").load(path).rdd.getNumPartitions The key items to pay attention to are: The batch query is independent from any other running streaming or batch queries over the delta table The batch query reads from the same delta table it saves data to The save mode is overwrite dataChange option is disabled spark .read .format(\"delta\") .load(path) .repartition(10) .write .format(\"delta\") .mode(\"overwrite\") .option(\"dataChange\", false) .save(path) Let's check out the number of partitions after the repartition. spark.read.format(\"delta\").load(path).rdd.getNumPartitions Show History (After) \u00b6 assert ( dt . history . count == 2 ) dt.history .select('version, 'operation, 'operationParameters, 'operationMetrics) .orderBy('version.asc) .show(truncate = false) +-------+----------------------+------------------------------------------------------------------------------+-----------------------------------------------------------+ |version|operation |operationParameters |operationMetrics | +-------+----------------------+------------------------------------------------------------------------------+-----------------------------------------------------------+ |0 |CREATE TABLE AS SELECT|{isManaged -> false, description -> null, partitionBy -> [], properties -> {}}|{numFiles -> 1, numOutputBytes -> 1273, numOutputRows -> 1}| |1 |WRITE |{mode -> Overwrite, partitionBy -> []} |{numFiles -> 2, numOutputBytes -> 1992, numOutputRows -> 1}| +-------+----------------------+------------------------------------------------------------------------------+-----------------------------------------------------------+","title":"dataChange"},{"location":"demo/dataChange/#demo-datachange","text":"This demo shows dataChange option in action. In combination with Overwrite mode, dataChange option can be used to transactionally rearrange data in a delta table.","title":"Demo: dataChange"},{"location":"demo/dataChange/#start-spark-shell","text":"./bin/spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog","title":"Start Spark Shell"},{"location":"demo/dataChange/#create-delta-table","text":"val path = \"/tmp/delta/d01\" Make sure that there is no delta table at the location. Remove it if exists and start over. import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, path) assert(deltaLog.tableExists == false) Create the demo delta table (using SQL). sql(s\"\"\" CREATE TABLE delta.`$path` USING delta VALUES ((0, 'Jacek'), (1, 'Agata')) AS (id, name) \"\"\")","title":"Create Delta Table"},{"location":"demo/dataChange/#show-history-before","text":"import io.delta.tables.DeltaTable val dt = DeltaTable . forPath ( path ) assert ( dt . history . count == 1 )","title":"Show History (Before)"},{"location":"demo/dataChange/#repartition-table","text":"The following dataChange example shows a batch query that repartitions a delta table (perhaps while other queries could be using the delta table). Let's check out the number of partitions. spark.read.format(\"delta\").load(path).rdd.getNumPartitions The key items to pay attention to are: The batch query is independent from any other running streaming or batch queries over the delta table The batch query reads from the same delta table it saves data to The save mode is overwrite dataChange option is disabled spark .read .format(\"delta\") .load(path) .repartition(10) .write .format(\"delta\") .mode(\"overwrite\") .option(\"dataChange\", false) .save(path) Let's check out the number of partitions after the repartition. spark.read.format(\"delta\").load(path).rdd.getNumPartitions","title":"Repartition Table"},{"location":"demo/dataChange/#show-history-after","text":"assert ( dt . history . count == 2 ) dt.history .select('version, 'operation, 'operationParameters, 'operationMetrics) .orderBy('version.asc) .show(truncate = false) +-------+----------------------+------------------------------------------------------------------------------+-----------------------------------------------------------+ |version|operation |operationParameters |operationMetrics | +-------+----------------------+------------------------------------------------------------------------------+-----------------------------------------------------------+ |0 |CREATE TABLE AS SELECT|{isManaged -> false, description -> null, partitionBy -> [], properties -> {}}|{numFiles -> 1, numOutputBytes -> 1273, numOutputRows -> 1}| |1 |WRITE |{mode -> Overwrite, partitionBy -> []} |{numFiles -> 2, numOutputBytes -> 1992, numOutputRows -> 1}| +-------+----------------------+------------------------------------------------------------------------------+-----------------------------------------------------------+","title":"Show History (After)"},{"location":"demo/merge-operation/","text":"Demo: Merge Operation \u00b6 This demo shows DeltaTable.merge operation (and the underlying MergeIntoCommand ) in action. Tip Enable ALL logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger as described in Logging . Create Delta Table (Target Data) \u00b6 val path = \"/tmp/delta/demo\" val data = spark . range ( 5 ) data . write . format ( \"delta\" ). mode ( \"overwrite\" ). save ( path ) import io.delta.tables.DeltaTable val target = DeltaTable . forPath ( path ) assert ( target . isInstanceOf [ io.delta.tables.DeltaTable ]) assert ( target . history . count == 1 , \"There must be version 0 only\" ) Source Data \u00b6 case class Person ( id : Long , name : String ) val source = Seq ( Person ( 0 , \"Zero\" ), Person ( 1 , \"One\" )). toDF Note the difference in schemas scala> target.toDF.printSchema root |-- id: long (nullable = true) scala> source.printSchema root |-- id: long (nullable = false) |-- name: string (nullable = true) Merge with Schema Evolution \u00b6 Not only do we update the matching rows, but also update the schema (schema evolution) val mergeBuilder = target . as ( \"to\" ) . merge ( source = source . as ( \"from\" ), condition = $ \"to.id\" === $ \"from.id\" ) assert(mergeBuilder.isInstanceOf[io.delta.tables.DeltaMergeBuilder]) scala> mergeBuilder.execute org.apache.spark.sql.AnalysisException: There must be at least one WHEN clause in a MERGE query; at org.apache.spark.sql.catalyst.plans.logical.DeltaMergeInto$.apply(deltaMerge.scala:217) at io.delta.tables.DeltaMergeBuilder.mergePlan(DeltaMergeBuilder.scala:255) at io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:228) at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:60) at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:48) at io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:121) at io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:225) ... 47 elided val mergeMatchedBuilder = mergeBuilder . whenMatched () assert ( mergeMatchedBuilder . isInstanceOf [ io.delta.tables.DeltaMergeMatchedActionBuilder ]) val mergeBuilderDeleteMatched = mergeMatchedBuilder . delete () assert ( mergeBuilderDeleteMatched . isInstanceOf [ io.delta.tables.DeltaMergeBuilder ]) mergeBuilderDeleteMatched . execute () assert ( target . history . count == 2 , \"There must be two versions only\" )","title":"Merge Operation"},{"location":"demo/merge-operation/#demo-merge-operation","text":"This demo shows DeltaTable.merge operation (and the underlying MergeIntoCommand ) in action. Tip Enable ALL logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger as described in Logging .","title":"Demo: Merge Operation"},{"location":"demo/merge-operation/#create-delta-table-target-data","text":"val path = \"/tmp/delta/demo\" val data = spark . range ( 5 ) data . write . format ( \"delta\" ). mode ( \"overwrite\" ). save ( path ) import io.delta.tables.DeltaTable val target = DeltaTable . forPath ( path ) assert ( target . isInstanceOf [ io.delta.tables.DeltaTable ]) assert ( target . history . count == 1 , \"There must be version 0 only\" )","title":"Create Delta Table (Target Data)"},{"location":"demo/merge-operation/#source-data","text":"case class Person ( id : Long , name : String ) val source = Seq ( Person ( 0 , \"Zero\" ), Person ( 1 , \"One\" )). toDF Note the difference in schemas scala> target.toDF.printSchema root |-- id: long (nullable = true) scala> source.printSchema root |-- id: long (nullable = false) |-- name: string (nullable = true)","title":"Source Data"},{"location":"demo/merge-operation/#merge-with-schema-evolution","text":"Not only do we update the matching rows, but also update the schema (schema evolution) val mergeBuilder = target . as ( \"to\" ) . merge ( source = source . as ( \"from\" ), condition = $ \"to.id\" === $ \"from.id\" ) assert(mergeBuilder.isInstanceOf[io.delta.tables.DeltaMergeBuilder]) scala> mergeBuilder.execute org.apache.spark.sql.AnalysisException: There must be at least one WHEN clause in a MERGE query; at org.apache.spark.sql.catalyst.plans.logical.DeltaMergeInto$.apply(deltaMerge.scala:217) at io.delta.tables.DeltaMergeBuilder.mergePlan(DeltaMergeBuilder.scala:255) at io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:228) at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:60) at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:48) at io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:121) at io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:225) ... 47 elided val mergeMatchedBuilder = mergeBuilder . whenMatched () assert ( mergeMatchedBuilder . isInstanceOf [ io.delta.tables.DeltaMergeMatchedActionBuilder ]) val mergeBuilderDeleteMatched = mergeMatchedBuilder . delete () assert ( mergeBuilderDeleteMatched . isInstanceOf [ io.delta.tables.DeltaMergeBuilder ]) mergeBuilderDeleteMatched . execute () assert ( target . history . count == 2 , \"There must be two versions only\" )","title":"Merge with Schema Evolution"},{"location":"demo/replaceWhere/","text":"Demo: replaceWhere \u00b6 This demo shows replaceWhere predicate option. In combination with Overwrite mode, a replaceWhere option can be used to transactionally replace data that matches a predicate.","title":"replaceWhere"},{"location":"demo/replaceWhere/#demo-replacewhere","text":"This demo shows replaceWhere predicate option. In combination with Overwrite mode, a replaceWhere option can be used to transactionally replace data that matches a predicate.","title":"Demo: replaceWhere"},{"location":"demo/schema-evolution/","text":"Demo: Schema Evolution \u00b6 /* spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ case class PersonV1(id: Long, name: String) import org.apache.spark.sql.Encoders val schemaV1 = Encoders.product[PersonV1].schema scala> schemaV1.printTreeString root |-- id: long (nullable = false) |-- name: string (nullable = true) val dataPath = \"/tmp/delta/people\" // Write data Seq(PersonV1(0, \"Zero\"), PersonV1(1, \"One\")) .toDF .write .format(\"delta\") .mode(\"overwrite\") .option(\"overwriteSchema\", \"true\") .save(dataPath) // Committed delta #0 to file:/tmp/delta/people/_delta_log import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, dataPath) assert(deltaLog.snapshot.version == 0) scala> deltaLog.snapshot.dataSchema.printTreeString root |-- id: long (nullable = true) |-- name: string (nullable = true) import io.delta.tables.DeltaTable val dt = DeltaTable.forPath(dataPath) scala> dt.toDF.show +---+----+ | id|name| +---+----+ | 0|Zero| | 1| One| +---+----+ val main = dt.as(\"main\") case class PersonV2(id: Long, name: String, newField: Boolean) val schemaV2 = Encoders.product[PersonV2].schema scala> schemaV2.printTreeString root |-- id: long (nullable = false) |-- name: string (nullable = true) |-- newField: boolean (nullable = false) val updates = Seq( PersonV2(0, \"ZERO\", newField = true), PersonV2(2, \"TWO\", newField = false)).toDF // Merge two datasets and create a new version // Schema evolution in play main.merge( source = updates.as(\"updates\"), condition = $\"main.id\" === $\"updates.id\") .whenMatched.updateAll .execute val latestPeople = spark .read .format(\"delta\") .load(dataPath) scala> latestPeople.show +---+----+ | id|name| +---+----+ | 0|ZERO| | 1| One| +---+----+","title":"Schema Evolution"},{"location":"demo/schema-evolution/#demo-schema-evolution","text":"/* spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ case class PersonV1(id: Long, name: String) import org.apache.spark.sql.Encoders val schemaV1 = Encoders.product[PersonV1].schema scala> schemaV1.printTreeString root |-- id: long (nullable = false) |-- name: string (nullable = true) val dataPath = \"/tmp/delta/people\" // Write data Seq(PersonV1(0, \"Zero\"), PersonV1(1, \"One\")) .toDF .write .format(\"delta\") .mode(\"overwrite\") .option(\"overwriteSchema\", \"true\") .save(dataPath) // Committed delta #0 to file:/tmp/delta/people/_delta_log import org.apache.spark.sql.delta.DeltaLog val deltaLog = DeltaLog.forTable(spark, dataPath) assert(deltaLog.snapshot.version == 0) scala> deltaLog.snapshot.dataSchema.printTreeString root |-- id: long (nullable = true) |-- name: string (nullable = true) import io.delta.tables.DeltaTable val dt = DeltaTable.forPath(dataPath) scala> dt.toDF.show +---+----+ | id|name| +---+----+ | 0|Zero| | 1| One| +---+----+ val main = dt.as(\"main\") case class PersonV2(id: Long, name: String, newField: Boolean) val schemaV2 = Encoders.product[PersonV2].schema scala> schemaV2.printTreeString root |-- id: long (nullable = false) |-- name: string (nullable = true) |-- newField: boolean (nullable = false) val updates = Seq( PersonV2(0, \"ZERO\", newField = true), PersonV2(2, \"TWO\", newField = false)).toDF // Merge two datasets and create a new version // Schema evolution in play main.merge( source = updates.as(\"updates\"), condition = $\"main.id\" === $\"updates.id\") .whenMatched.updateAll .execute val latestPeople = spark .read .format(\"delta\") .load(dataPath) scala> latestPeople.show +---+----+ | id|name| +---+----+ | 0|ZERO| | 1| One| +---+----+","title":"Demo: Schema Evolution"},{"location":"demo/stream-processing-of-delta-table/","text":"Demo: Stream Processing of Delta Table \u00b6 /* spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ assert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\") import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val deltaTableDir = \"/tmp/delta/users\" val checkpointLocation = \"/tmp/checkpointLocation\" // Initialize the delta table // - No data // - Schema only case class Person(id: Long, name: String, city: String) spark.emptyDataset[Person].write.format(\"delta\").save(deltaTableDir) import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"delta\") .option(\"maxFilesPerTrigger\", 1) // Maximum number of files to scan per micro-batch .load(deltaTableDir) .writeStream .format(\"console\") .option(\"truncate\", false) .option(\"checkpointLocation\", checkpointLocation) .trigger(Trigger.ProcessingTime(10.seconds)) // Useful for debugging .start // The streaming query over delta table // should display the 0th version as Batch 0 ------------------------------------------- Batch: 0 ------------------------------------------- +---+----+----+ |id |name|city| +---+----+----+ +---+----+----+ // Let's write to the delta table val users = Seq( Person(0, \"Jacek\", \"Warsaw\"), Person(1, \"Agata\", \"Warsaw\"), Person(2, \"Jacek\", \"Paris\"), Person(3, \"Domas\", \"Vilnius\")).toDF // More partitions are more file added // And per maxFilesPerTrigger as 1 file addition per micro-batch // You should see more micro-batches scala> println(users.rdd.getNumPartitions) 4 // Change the default SaveMode.ErrorIfExists to more meaningful save mode import org.apache.spark.sql.SaveMode users .write .format(\"delta\") .mode(SaveMode.Append) // Appending rows .save(deltaTableDir) // Immediately after the above write finishes // New batches should be printed out to the console // Per the number of partitions of users dataset // And per maxFilesPerTrigger as 1 file addition // You should see as many micro-batches as files ------------------------------------------- Batch: 1 ------------------------------------------- +---+-----+------+ |id |name |city | +---+-----+------+ |0 |Jacek|Warsaw| +---+-----+------+ ------------------------------------------- Batch: 2 ------------------------------------------- +---+-----+------+ |id |name |city | +---+-----+------+ |1 |Agata|Warsaw| +---+-----+------+ ------------------------------------------- Batch: 3 ------------------------------------------- +---+-----+-----+ |id |name |city | +---+-----+-----+ |2 |Jacek|Paris| +---+-----+-----+ ------------------------------------------- Batch: 4 ------------------------------------------- +---+-----+-------+ |id |name |city | +---+-----+-------+ |3 |Domas|Vilnius| +---+-----+-------+","title":"Stream Processing of Delta Table"},{"location":"demo/stream-processing-of-delta-table/#demo-stream-processing-of-delta-table","text":"/* spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.databricks.delta.snapshotPartitions=1 */ assert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\") import org.apache.spark.sql.SparkSession assert(spark.isInstanceOf[SparkSession]) val deltaTableDir = \"/tmp/delta/users\" val checkpointLocation = \"/tmp/checkpointLocation\" // Initialize the delta table // - No data // - Schema only case class Person(id: Long, name: String, city: String) spark.emptyDataset[Person].write.format(\"delta\").save(deltaTableDir) import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"delta\") .option(\"maxFilesPerTrigger\", 1) // Maximum number of files to scan per micro-batch .load(deltaTableDir) .writeStream .format(\"console\") .option(\"truncate\", false) .option(\"checkpointLocation\", checkpointLocation) .trigger(Trigger.ProcessingTime(10.seconds)) // Useful for debugging .start // The streaming query over delta table // should display the 0th version as Batch 0 ------------------------------------------- Batch: 0 ------------------------------------------- +---+----+----+ |id |name|city| +---+----+----+ +---+----+----+ // Let's write to the delta table val users = Seq( Person(0, \"Jacek\", \"Warsaw\"), Person(1, \"Agata\", \"Warsaw\"), Person(2, \"Jacek\", \"Paris\"), Person(3, \"Domas\", \"Vilnius\")).toDF // More partitions are more file added // And per maxFilesPerTrigger as 1 file addition per micro-batch // You should see more micro-batches scala> println(users.rdd.getNumPartitions) 4 // Change the default SaveMode.ErrorIfExists to more meaningful save mode import org.apache.spark.sql.SaveMode users .write .format(\"delta\") .mode(SaveMode.Append) // Appending rows .save(deltaTableDir) // Immediately after the above write finishes // New batches should be printed out to the console // Per the number of partitions of users dataset // And per maxFilesPerTrigger as 1 file addition // You should see as many micro-batches as files ------------------------------------------- Batch: 1 ------------------------------------------- +---+-----+------+ |id |name |city | +---+-----+------+ |0 |Jacek|Warsaw| +---+-----+------+ ------------------------------------------- Batch: 2 ------------------------------------------- +---+-----+------+ |id |name |city | +---+-----+------+ |1 |Agata|Warsaw| +---+-----+------+ ------------------------------------------- Batch: 3 ------------------------------------------- +---+-----+-----+ |id |name |city | +---+-----+-----+ |2 |Jacek|Paris| +---+-----+-----+ ------------------------------------------- Batch: 4 ------------------------------------------- +---+-----+-------+ |id |name |city | +---+-----+-------+ |3 |Domas|Vilnius| +---+-----+-------+","title":"Demo: Stream Processing of Delta Table"},{"location":"demo/user-metadata-for-labelling-commits/","text":"Demo: User Metadata for Labelling Commits \u00b6 The demo shows how to differentiate commits of a write batch query using userMetadata option. Tip A fine example could be for distinguishing between two or more separate streaming write queries. Creating Delta Table \u00b6 val tableName = \"/tmp/delta-demo-userMetadata\" spark.range(5) .write .format(\"delta\") .save(tableName) Describing History \u00b6 import io.delta.tables.DeltaTable val d = DeltaTable . forPath ( tableName ) We are interested in a subset of the available history metadata. d . history . select ( 'version , 'operation , 'operationParameters , 'userMetadata ) . show ( truncate = false ) +-------+---------+------------------------------------------+------------+ |version|operation|operationParameters |userMetadata| +-------+---------+------------------------------------------+------------+ |0 |WRITE |[mode -> ErrorIfExists, partitionBy -> []]|null | +-------+---------+------------------------------------------+------------+ Appending Data \u00b6 In this step, you're going to append new data to the existing Delta table. You're going to use userMetadata option for a custom user-defined historical marker (e.g. to know when this extra append happended in the life of the Delta table). val userMetadata = \"two more rows for demo\" Since you're appending new rows, it is required to use Append mode. import org.apache.spark.sql.SaveMode.Append The whole append write is as follows: spark . range ( start = 5 , end = 7 ) . write . format ( \"delta\" ) . option ( \"userMetadata\" , userMetadata ) . mode ( Append ) . save ( tableName ) That write query creates another version of the Delta table. Listing Versions with userMetadata \u00b6 For the sake of the demo, you are going to show the versions of the Delta table with userMetadata defined. d . history . select ( 'version , 'operation , 'operationParameters , 'userMetadata ) . where ( 'userMetadata . isNotNull ) . show ( truncate = false ) +-------+---------+-----------------------------------+----------------------+ |version|operation|operationParameters |userMetadata | +-------+---------+-----------------------------------+----------------------+ |1 |WRITE |[mode -> Append, partitionBy -> []]|two more rows for demo| +-------+---------+-----------------------------------+----------------------+","title":"User Metadata for Labelling Commits"},{"location":"demo/user-metadata-for-labelling-commits/#demo-user-metadata-for-labelling-commits","text":"The demo shows how to differentiate commits of a write batch query using userMetadata option. Tip A fine example could be for distinguishing between two or more separate streaming write queries.","title":"Demo: User Metadata for Labelling Commits"},{"location":"demo/user-metadata-for-labelling-commits/#creating-delta-table","text":"val tableName = \"/tmp/delta-demo-userMetadata\" spark.range(5) .write .format(\"delta\") .save(tableName)","title":"Creating Delta Table"},{"location":"demo/user-metadata-for-labelling-commits/#describing-history","text":"import io.delta.tables.DeltaTable val d = DeltaTable . forPath ( tableName ) We are interested in a subset of the available history metadata. d . history . select ( 'version , 'operation , 'operationParameters , 'userMetadata ) . show ( truncate = false ) +-------+---------+------------------------------------------+------------+ |version|operation|operationParameters |userMetadata| +-------+---------+------------------------------------------+------------+ |0 |WRITE |[mode -> ErrorIfExists, partitionBy -> []]|null | +-------+---------+------------------------------------------+------------+","title":"Describing History"},{"location":"demo/user-metadata-for-labelling-commits/#appending-data","text":"In this step, you're going to append new data to the existing Delta table. You're going to use userMetadata option for a custom user-defined historical marker (e.g. to know when this extra append happended in the life of the Delta table). val userMetadata = \"two more rows for demo\" Since you're appending new rows, it is required to use Append mode. import org.apache.spark.sql.SaveMode.Append The whole append write is as follows: spark . range ( start = 5 , end = 7 ) . write . format ( \"delta\" ) . option ( \"userMetadata\" , userMetadata ) . mode ( Append ) . save ( tableName ) That write query creates another version of the Delta table.","title":"Appending Data"},{"location":"demo/user-metadata-for-labelling-commits/#listing-versions-with-usermetadata","text":"For the sake of the demo, you are going to show the versions of the Delta table with userMetadata defined. d . history . select ( 'version , 'operation , 'operationParameters , 'userMetadata ) . where ( 'userMetadata . isNotNull ) . show ( truncate = false ) +-------+---------+-----------------------------------+----------------------+ |version|operation|operationParameters |userMetadata | +-------+---------+-----------------------------------+----------------------+ |1 |WRITE |[mode -> Append, partitionBy -> []]|two more rows for demo| +-------+---------+-----------------------------------+----------------------+","title":"Listing Versions with userMetadata"},{"location":"sql/","text":"Delta SQL \u00b6 Delta Lake registers custom SQL statements (using DeltaSparkSessionExtension to inject DeltaSqlParser with DeltaSqlAstBuilder ). The SQL statements support table of the format delta.`path` (with backticks), e.g. delta.`/tmp/delta/t1` while path is between single quotes, e.g. '/tmp/delta/t1' . The SQL statements can also refer to a table registered in a metastore. Note SQL grammar is described using ANTLR in DeltaSqlBase.g4 . ALTER TABLE ADD CONSTRAINT \u00b6 ALTER TABLE table ADD CONSTRAINT name constraint ALTER TABLE DROP CONSTRAINT \u00b6 ALTER TABLE table DROP CONSTRAINT (IF EXISTS)? name CONVERT TO DELTA \u00b6 CONVERT TO DELTA table (PARTITIONED BY '(' colTypeList ')')? Executes ConvertToDeltaCommand DESCRIBE DETAIL \u00b6 (DESC | DESCRIBE) DETAIL (path | table) Executes DescribeDeltaDetailCommand DESCRIBE HISTORY \u00b6 (DESC | DESCRIBE) HISTORY (path | table) (LIMIT limit)? Executes DescribeDeltaHistoryCommand GENERATE \u00b6 GENERATE modeName FOR TABLE table Executes DeltaGenerateCommand VACUUM \u00b6 VACUUM (path | table) (RETAIN number HOURS)? (DRY RUN)? Executes VacuumTableCommand","title":"Delta SQL"},{"location":"sql/#delta-sql","text":"Delta Lake registers custom SQL statements (using DeltaSparkSessionExtension to inject DeltaSqlParser with DeltaSqlAstBuilder ). The SQL statements support table of the format delta.`path` (with backticks), e.g. delta.`/tmp/delta/t1` while path is between single quotes, e.g. '/tmp/delta/t1' . The SQL statements can also refer to a table registered in a metastore. Note SQL grammar is described using ANTLR in DeltaSqlBase.g4 .","title":"Delta SQL"},{"location":"sql/#alter-table-add-constraint","text":"ALTER TABLE table ADD CONSTRAINT name constraint","title":" ALTER TABLE ADD CONSTRAINT"},{"location":"sql/#alter-table-drop-constraint","text":"ALTER TABLE table DROP CONSTRAINT (IF EXISTS)? name","title":" ALTER TABLE DROP CONSTRAINT"},{"location":"sql/#convert-to-delta","text":"CONVERT TO DELTA table (PARTITIONED BY '(' colTypeList ')')? Executes ConvertToDeltaCommand","title":" CONVERT TO DELTA"},{"location":"sql/#describe-detail","text":"(DESC | DESCRIBE) DETAIL (path | table) Executes DescribeDeltaDetailCommand","title":" DESCRIBE DETAIL"},{"location":"sql/#describe-history","text":"(DESC | DESCRIBE) HISTORY (path | table) (LIMIT limit)? Executes DescribeDeltaHistoryCommand","title":" DESCRIBE HISTORY"},{"location":"sql/#generate","text":"GENERATE modeName FOR TABLE table Executes DeltaGenerateCommand","title":" GENERATE"},{"location":"sql/#vacuum","text":"VACUUM (path | table) (RETAIN number HOURS)? (DRY RUN)? Executes VacuumTableCommand","title":" VACUUM"},{"location":"sql/DeltaSqlAstBuilder/","text":"DeltaSqlAstBuilder \u00b6 DeltaSqlAstBuilder is a command builder for the Delta SQL statements (described in DeltaSqlBase.g4 ANTLR grammar). DeltaSqlParser is used by DeltaSqlParser . SQL Statement Logical Command ALTER TABLE ADD CONSTRAINT AlterTableAddConstraintStatement ALTER TABLE DROP CONSTRAINT AlterTableDropConstraintStatement CONVERT TO DELTA ConvertToDeltaCommand DESCRIBE DETAIL DescribeDeltaDetailCommand DESCRIBE HISTORY DescribeDeltaHistoryCommand GENERATE DeltaGenerateCommand VACUUM VacuumTableCommand","title":"DeltaSqlAstBuilder"},{"location":"sql/DeltaSqlAstBuilder/#deltasqlastbuilder","text":"DeltaSqlAstBuilder is a command builder for the Delta SQL statements (described in DeltaSqlBase.g4 ANTLR grammar). DeltaSqlParser is used by DeltaSqlParser . SQL Statement Logical Command ALTER TABLE ADD CONSTRAINT AlterTableAddConstraintStatement ALTER TABLE DROP CONSTRAINT AlterTableDropConstraintStatement CONVERT TO DELTA ConvertToDeltaCommand DESCRIBE DETAIL DescribeDeltaDetailCommand DESCRIBE HISTORY DescribeDeltaHistoryCommand GENERATE DeltaGenerateCommand VACUUM VacuumTableCommand","title":"DeltaSqlAstBuilder"},{"location":"sql/DeltaSqlParser/","text":"DeltaSqlParser \u00b6 DeltaSqlParser is a SQL parser (Spark SQL's ParserInterface ) for Delta SQL . DeltaSqlParser is registered in a Spark SQL application using DeltaSparkSessionExtension . Creating Instance \u00b6 DeltaSqlParser takes the following to be created: ParserInterface (to fall back to for unsupported SQL) DeltaSqlParser is created when DeltaSparkSessionExtension is requested to register Delta SQL support . DeltaSqlAstBuilder \u00b6 DeltaSqlParser uses DeltaSqlAstBuilder to convert SQL statements to their runtime representation (as a LogicalPlan ). In case an AST could not be converted to a LogicalPlan , DeltaSqlAstBuilder requests the delegate ParserInterface to parse it.","title":"DeltaSqlParser"},{"location":"sql/DeltaSqlParser/#deltasqlparser","text":"DeltaSqlParser is a SQL parser (Spark SQL's ParserInterface ) for Delta SQL . DeltaSqlParser is registered in a Spark SQL application using DeltaSparkSessionExtension .","title":"DeltaSqlParser"},{"location":"sql/DeltaSqlParser/#creating-instance","text":"DeltaSqlParser takes the following to be created: ParserInterface (to fall back to for unsupported SQL) DeltaSqlParser is created when DeltaSparkSessionExtension is requested to register Delta SQL support .","title":"Creating Instance"},{"location":"sql/DeltaSqlParser/#deltasqlastbuilder","text":"DeltaSqlParser uses DeltaSqlAstBuilder to convert SQL statements to their runtime representation (as a LogicalPlan ). In case an AST could not be converted to a LogicalPlan , DeltaSqlAstBuilder requests the delegate ParserInterface to parse it.","title":" DeltaSqlAstBuilder"}]}