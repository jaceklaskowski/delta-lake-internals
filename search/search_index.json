{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"The Internals of Delta Lake 2.2.0","text":"<p>Welcome to The Internals of Delta Lake online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, an IT freelancer specializing in Apache Spark (incl. Spark SQL and Spark Structured Streaming), Delta Lake, Databricks, and Apache Kafka (incl. Kafka Streams and ksqlDB) (with brief forays into a wider data engineering space, e.g. Trino, Dask and dbt, mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring the internals of Delta Lake as much as I have.</p> <p>Flannery O'Connor</p> <p>I write to discover what I know.</p> <p>\"The Internals Of\" series</p> <p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p> <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a deep dive into Delta Lake \ud83d\udd25</p> <p>Last update: 2023-03-13</p>"},{"location":"Action/","title":"Action","text":"<p><code>Action</code> is an abstraction of operations that change the state of a delta table.</p>"},{"location":"Action/#contract","title":"Contract","text":""},{"location":"Action/#json-representation","title":"JSON Representation <pre><code>json: String\n</code></pre> <p>Serializes (converts) the (wrapped) action to JSON format</p> <p><code>json</code> uses Jackson library (with jackson-module-scala) as the JSON processor.</p> <p>Used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to doCommit</li> <li><code>DeltaCommand</code> is requested to commitLarge</li> </ul>","text":""},{"location":"Action/#singleaction-representation","title":"SingleAction Representation <pre><code>wrap: SingleAction\n</code></pre> <p>Wraps the action into a SingleAction for serialization</p> <p>Used when:</p> <ul> <li><code>Snapshot</code> is requested to stateReconstruction</li> <li><code>Action</code> is requested to serialize to JSON format</li> </ul>","text":""},{"location":"Action/#implementations","title":"Implementations","text":"Sealed Trait <p><code>Action</code> is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).</p> <p>Learn more in the Scala Language Specification.</p> <ul> <li>CommitInfo</li> <li>FileAction</li> <li>Metadata</li> <li>Protocol</li> <li>SetTransaction</li> </ul>"},{"location":"Action/#log-schema","title":"Log Schema <pre><code>logSchema: StructType\n</code></pre> <p><code>logSchema</code> is the schema (Spark SQL) of SingleActions for <code>Snapshot</code> to convert a DeltaLogFileIndex to a LogicalRelation and emptyActions.</p> <pre><code>import org.apache.spark.sql.delta.actions.Action.logSchema\nlogSchema.printTreeString\n</code></pre> <pre><code>root\n |-- txn: struct (nullable = true)\n |    |-- appId: string (nullable = true)\n |    |-- version: long (nullable = false)\n |    |-- lastUpdated: long (nullable = true)\n |-- add: struct (nullable = true)\n |    |-- path: string (nullable = true)\n |    |-- partitionValues: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |    |-- size: long (nullable = false)\n |    |-- modificationTime: long (nullable = false)\n |    |-- dataChange: boolean (nullable = false)\n |    |-- stats: string (nullable = true)\n |    |-- tags: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |-- remove: struct (nullable = true)\n |    |-- path: string (nullable = true)\n |    |-- deletionTimestamp: long (nullable = true)\n |    |-- dataChange: boolean (nullable = false)\n |    |-- extendedFileMetadata: boolean (nullable = false)\n |    |-- partitionValues: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |    |-- size: long (nullable = false)\n |    |-- tags: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |-- metaData: struct (nullable = true)\n |    |-- id: string (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- description: string (nullable = true)\n |    |-- format: struct (nullable = true)\n |    |    |-- provider: string (nullable = true)\n |    |    |-- options: map (nullable = true)\n |    |    |    |-- key: string\n |    |    |    |-- value: string (valueContainsNull = true)\n |    |-- schemaString: string (nullable = true)\n |    |-- partitionColumns: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- configuration: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |    |-- createdTime: long (nullable = true)\n |-- protocol: struct (nullable = true)\n |    |-- minReaderVersion: integer (nullable = false)\n |    |-- minWriterVersion: integer (nullable = false)\n |-- cdc: struct (nullable = true)\n |    |-- path: string (nullable = true)\n |    |-- partitionValues: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |    |-- size: long (nullable = false)\n |    |-- tags: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |-- commitInfo: struct (nullable = true)\n |    |-- version: long (nullable = true)\n |    |-- timestamp: timestamp (nullable = true)\n |    |-- userId: string (nullable = true)\n |    |-- userName: string (nullable = true)\n |    |-- operation: string (nullable = true)\n |    |-- operationParameters: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |    |-- job: struct (nullable = true)\n |    |    |-- jobId: string (nullable = true)\n |    |    |-- jobName: string (nullable = true)\n |    |    |-- runId: string (nullable = true)\n |    |    |-- jobOwnerId: string (nullable = true)\n |    |    |-- triggerType: string (nullable = true)\n |    |-- notebook: struct (nullable = true)\n |    |    |-- notebookId: string (nullable = true)\n |    |-- clusterId: string (nullable = true)\n |    |-- readVersion: long (nullable = true)\n |    |-- isolationLevel: string (nullable = true)\n |    |-- isBlindAppend: boolean (nullable = true)\n |    |-- operationMetrics: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |    |-- userMetadata: string (nullable = true)\n</code></pre>","text":""},{"location":"Action/#deserializing-action-from-json","title":"Deserializing Action (from JSON) <pre><code>fromJson(\n  json: String): Action\n</code></pre> <p><code>fromJson</code> utility...FIXME</p> <p><code>fromJson</code>\u00a0is used when:</p> <ul> <li><code>DeltaHistoryManager</code> is requested for CommitInfo of the given delta file</li> <li><code>DeltaLog</code> is requested for the changes of the given delta version and later</li> <li><code>OptimisticTransactionImpl</code> is requested to checkForConflicts</li> <li><code>DeltaCommand</code> is requested to commitLarge</li> </ul>","text":""},{"location":"AddCDCFile/","title":"AddCDCFile","text":"<p><code>AddCDCFile</code> is a FileAction.</p>"},{"location":"AddCDCFile/#creating-instance","title":"Creating Instance","text":"<p><code>AddCDCFile</code> takes the following to be created:</p> <ul> <li> Path <li> Partition values (<code>Map[String, String]</code>) <li> Size (in bytes) <li> Tags (default: <code>null</code>) <p><code>AddCDCFile</code> does not seem to be created\u00a0ever.</p>"},{"location":"AddCDCFile/#datachange","title":"dataChange <pre><code>dataChange: Boolean\n</code></pre> <p><code>dataChange</code>\u00a0is part of the FileAction abstraction.</p> <p><code>dataChange</code>\u00a0is always turned off (<code>false</code>).</p>","text":""},{"location":"AddCDCFile/#converting-to-singleaction","title":"Converting to SingleAction <pre><code>wrap: SingleAction\n</code></pre> <p><code>wrap</code>\u00a0is part of the Action abstraction.</p> <p><code>wrap</code> creates a new SingleAction with the <code>cdc</code> field set to this <code>AddCDCFile</code>.</p>","text":""},{"location":"AddFile/","title":"AddFile","text":"<p><code>AddFile</code> is a FileAction that represents an action of adding a file to a delta table.</p>"},{"location":"AddFile/#creating-instance","title":"Creating Instance","text":"<p><code>AddFile</code> takes the following to be created:</p> <ul> <li> Path <li> Partition values (<code>Map[String, String]</code>) <li> Size (in bytes) <li> Modification time <li> <code>dataChange</code> flag <li>File Statistics</li> <li> Tags (<code>Map[String, String]</code>) (default: <code>null</code>) <p><code>AddFile</code> is created\u00a0when:</p> <ul> <li> <p>ConvertToDeltaCommand is executed (for every data file to import)</p> </li> <li> <p><code>DelayedCommitProtocol</code> is requested to commit a task (after successful write) (for optimistic transactional writers)</p> </li> </ul>"},{"location":"AddFile/#file-statistics","title":"File Statistics <p><code>AddFile</code> can be given a JSON-encoded file statistics when created.</p> <p>The statistics are undefined (<code>null</code>) by default.</p> <p>The statistics can be defined when:</p> <ul> <li><code>TransactionalWrite</code> is requested to write data out (and spark.databricks.delta.stats.collect configuration property is enabled)</li> <li><code>StatisticsCollection</code> utility is used to recompute statistics for a delta table (that seems unused though)</li> </ul>","text":""},{"location":"AddFile/#converting-to-singleaction","title":"Converting to SingleAction <pre><code>wrap: SingleAction\n</code></pre> <p><code>wrap</code>\u00a0is part of the Action abstraction.</p> <p><code>wrap</code> creates a new SingleAction with the <code>add</code> field set to this <code>AddFile</code>.</p>","text":""},{"location":"AddFile/#converting-to-removefile-with-defaults","title":"Converting to RemoveFile with Defaults <pre><code>remove: RemoveFile\n</code></pre> <p><code>remove</code> creates a RemoveFile for the path (with the current time and <code>dataChange</code> flag enabled).</p> <p><code>remove</code> is used when:</p> <ul> <li>MergeIntoCommand is executed</li> <li><code>WriteIntoDelta</code> is requested to write (with <code>Overwrite</code> mode)</li> <li><code>DeltaSink</code> is requested to add a streaming micro-batch (with <code>Complete</code> output mode)</li> </ul>","text":""},{"location":"AddFile/#converting-to-removefile","title":"Converting to RemoveFile <pre><code>removeWithTimestamp(\n  timestamp: Long = System.currentTimeMillis(),\n  dataChange: Boolean = true): RemoveFile\n</code></pre> <p><code>remove</code> creates a new RemoveFile action for the path with the given <code>timestamp</code> and <code>dataChange</code> flag.</p> <p><code>removeWithTimestamp</code> is used when:</p> <ul> <li><code>AddFile</code> is requested to create a RemoveFile action with the defaults</li> <li>CreateDeltaTableCommand, DeleteCommand and UpdateCommand commands are executed</li> <li><code>DeltaCommand</code> is requested to removeFilesFromPaths</li> </ul>","text":""},{"location":"AddFile/#tag","title":"tag <pre><code>tag(\n  tag: AddFile.Tags.KeyType): Option[String]\n</code></pre> <p><code>tag</code> gets the value of the given tag.</p>  <p><code>tag</code> is used when:</p> <ul> <li><code>AddFile</code> is requested for an insertionTime (that does not seem to be used anywhere)</li> </ul>","text":""},{"location":"AddFile/#numlogicalrecords","title":"numLogicalRecords  Signature <pre><code>numLogicalRecords: Option[Long]\n</code></pre> <p><code>numLogicalRecords</code> is part of the FileAction abstraction.</p>   Lazy Value <p><code>numLogicalRecords</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>numLogicalRecords</code> is parsedStatsFields.</p>  <p><code>numLogicalRecords</code> is used when:</p> <ul> <li><code>DeleteCommandMetrics</code> is requested to getDeletedRowsFromAddFilesAndUpdateMetrics</li> <li><code>MergeIntoCommand</code> is requested to writeInsertsOnlyWhenNoMatchedClauses</li> <li><code>TransactionalWrite</code> is requested to writeFiles</li> <li><code>WriteIntoDelta</code> is requested to registerReplaceWhereMetrics</li> </ul>","text":""},{"location":"AdmissionLimits/","title":"AdmissionLimits","text":"<p><code>AdmissionLimits</code> is used by DeltaSource to control how much data should be processed by a single micro-batch.</p>"},{"location":"AdmissionLimits/#creating-instance","title":"Creating Instance","text":"<p><code>AdmissionLimits</code> takes the following to be created:</p> <ul> <li> Maximum Number of Files (based on maxFilesPerTrigger option) <li> Maximum Bytes (based on maxBytesPerTrigger option) <p><code>AdmissionLimits</code> is created\u00a0when:</p> <ul> <li><code>DeltaSource</code> is requested to getChangesWithRateLimit, getStartingOffset, getDefaultReadLimit</li> </ul>"},{"location":"AdmissionLimits/#converting-readlimit-to-admissionlimits","title":"Converting ReadLimit to AdmissionLimits <pre><code>apply(\n  limit: ReadLimit): Option[AdmissionLimits]\n</code></pre> <p><code>apply</code> creates an <code>AdmissionLimits</code> for the given <code>ReadLimit</code> (Spark Structured Streaming).</p>    ReadLimit AdmissionLimits     <code>ReadAllAvailable</code> <code>None</code>   <code>ReadMaxFiles</code> Maximum Number of Files   <code>ReadMaxBytes</code> Maximum Bytes   <code>CompositeLimit</code> Maximum Number of Files and Maximum Bytes    <p><code>apply</code> throws an <code>UnsupportedOperationException</code> for unknown <code>ReadLimit</code>s:</p> <pre><code>Unknown ReadLimit: [limit]\n</code></pre> <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>DeltaSource</code> is requested for the latest available offset</li> </ul>","text":""},{"location":"AdmissionLimits/#admitting-addfile","title":"Admitting AddFile <pre><code>admit(\n  add: Option[AddFile]): Boolean\n</code></pre> <p><code>admit</code>...FIXME</p> <p><code>admit</code>\u00a0is used when:</p> <ul> <li><code>DeltaSource</code> is requested to getChangesWithRateLimit</li> </ul>","text":""},{"location":"AppendDelta/","title":"AppendDelta Scala Extractor","text":"<p><code>AppendDelta</code> is a Scala extractor for DeltaAnalysis logical resolution rule to analyze <code>AppendData</code> (Spark SQL) logical operators and destructure them into a pair of the following:</p> <ul> <li><code>DataSourceV2Relation</code> (Spark SQL)</li> <li>DeltaTableV2</li> </ul>"},{"location":"AppendDelta/#destructuring-appenddata","title":"Destructuring AppendData <pre><code>unapply(\n  a: AppendData): Option[(DataSourceV2Relation, DeltaTableV2)]\n</code></pre> <p><code>unapply</code> requests the given <code>AppendData</code> (Spark SQL) logical operator for the query. If resolved, <code>unapply</code> requests the <code>AppendData</code> for the table (as a NamedRelation). If the table is a <code>DataSourceV2Relation</code> (Spark SQL) that is also a DeltaTableV2, <code>unapply</code> returns the <code>DataSourceV2Relation</code> and the <code>DeltaTableV2</code>.</p> <p>For all other cases, <code>unapply</code> returns <code>None</code> (and destructures nothing).</p> <p><code>unapply</code> is used when:</p> <ul> <li>DeltaAnalysis logical resolution rule is executed (to resolve AppendDelta) logical operator)</li> </ul>","text":""},{"location":"CachedDS/","title":"CachedDS \u2014 Cached Delta State","text":"<p><code>CachedDS</code> is used when StateCache is requested to cacheDS.</p>"},{"location":"CachedDS/#creating-instance","title":"Creating Instance","text":"<p><code>CachedDS</code> takes the following to be created:</p> <ul> <li> <code>Dataset[A]</code> <li> Name <p><code>CachedDS</code> is created when:</p> <ul> <li><code>StateCache</code> is requested to cacheDS</li> </ul>"},{"location":"CachedDS/#cachedds","title":"cachedDs <pre><code>cachedDs: Option[DatasetRefCache[Row]]\n</code></pre>  <p>Note</p> <p><code>cachedDs</code> is a value so initialized immediately when <code>CachedDS</code> is created.</p>  <p><code>cachedDs</code> requests the Dataset for an <code>RDD[InternalRow]</code>.</p> <p><code>cachedDs</code> associates the RDD with the name and marks it to be persist (on the first action).</p> <p><code>cachedDs</code> adds the RDD to the cached registry.</p>  <p>Note</p> <p><code>CachedDS</code> is an internal class of <code>StateCache</code> and has access to its internals.</p>  <p><code>cachedDs</code> is used when:</p> <ul> <li><code>CachedDS</code> is requested to getDF</li> </ul>","text":""},{"location":"CachedDS/#getds","title":"getDS <pre><code>getDS: Dataset[A]\n</code></pre> <p><code>getDS</code> gets the cached DataFrame and converts the rows to <code>A</code> type.</p> <p><code>getDS</code> is used when:</p> <ul> <li><code>Snapshot</code> is requested for the stateDS Dataset</li> <li><code>DeltaSourceSnapshot</code> is requested for the initialFiles Dataset</li> <li><code>DataSkippingReaderBase</code> is requested for the withStatsInternal Dataset</li> </ul>","text":""},{"location":"CheckpointV2/","title":"CheckpointV2","text":"<p><code>CheckpointV2</code> is...FIXME</p>"},{"location":"Checkpoints/","title":"Checkpoints","text":"<p><code>Checkpoints</code> is an abstraction of DeltaLogs that can checkpoint the current state of a delta table.</p> <p> <code>Checkpoints</code> requires to be used with DeltaLog (or subtypes) only."},{"location":"Checkpoints/#contract","title":"Contract","text":""},{"location":"Checkpoints/#datapath","title":"dataPath <pre><code>dataPath: Path\n</code></pre> <p>Hadoop Path to the data directory of the delta table</p>","text":""},{"location":"Checkpoints/#dologcleanup","title":"doLogCleanup <pre><code>doLogCleanup(): Unit\n</code></pre> <p>Performs log cleanup (to remove stale log files)</p> <p>Used when:</p> <ul> <li><code>Checkpoints</code> is requested to checkpointAndCleanUpDeltaLog</li> </ul>","text":""},{"location":"Checkpoints/#logpath","title":"logPath <pre><code>logPath: Path\n</code></pre> <p>Hadoop Path to the log directory of the delta table</p>","text":""},{"location":"Checkpoints/#metadata","title":"Metadata <pre><code>metadata: Metadata\n</code></pre> <p>Metadata of the delta table</p>","text":""},{"location":"Checkpoints/#snapshot","title":"snapshot <pre><code>snapshot: Snapshot\n</code></pre> <p>Snapshot of the delta table</p>","text":""},{"location":"Checkpoints/#store","title":"store <pre><code>store: LogStore\n</code></pre> <p>LogStore</p>","text":""},{"location":"Checkpoints/#implementations","title":"Implementations","text":"<ul> <li>DeltaLog</li> </ul>"},{"location":"Checkpoints/#_last_checkpoint-metadata-file","title":"_last_checkpoint Metadata File <p><code>Checkpoints</code> uses _last_checkpoint metadata file (under the log path) for the following:</p> <ul> <li> <p>Writing checkpoint metadata out</p> </li> <li> <p>Loading checkpoint metadata in</p> </li> </ul>","text":""},{"location":"Checkpoints/#checkpointing","title":"Checkpointing <pre><code>checkpoint(): Unit\ncheckpoint(\n  snapshotToCheckpoint: Snapshot): CheckpointMetaData\n</code></pre> <p><code>checkpoint</code> writes a checkpoint of the current state of the delta table (Snapshot). That produces a checkpoint metadata with the version, the number of actions and possibly parts (for multi-part checkpoints).</p> <p><code>checkpoint</code> requests the LogStore to overwrite the _last_checkpoint file with the JSON-encoded checkpoint metadata.</p> <p>In the end, <code>checkpoint</code> cleans up the expired logs (if enabled).</p> <p><code>checkpoint</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to postCommit (based on checkpoint interval table property)</li> <li><code>DeltaCommand</code> is requested to updateAndCheckpoint</li> </ul>","text":""},{"location":"Checkpoints/#checkpointandcleanupdeltalog","title":"checkpointAndCleanUpDeltaLog <pre><code>checkpointAndCleanUpDeltaLog(\n  snapshotToCheckpoint: Snapshot): Unit\n</code></pre> <p><code>checkpointAndCleanUpDeltaLog</code>...FIXME</p>","text":""},{"location":"Checkpoints/#writing-out-state-checkpoint","title":"Writing Out State Checkpoint <pre><code>writeCheckpoint(\n  spark: SparkSession,\n  deltaLog: DeltaLog,\n  snapshot: Snapshot): CheckpointMetaData\n</code></pre> <p><code>writeCheckpoint</code>...FIXME</p>","text":""},{"location":"Checkpoints/#loading-latest-checkpoint-metadata","title":"Loading Latest Checkpoint Metadata <pre><code>lastCheckpoint: Option[CheckpointMetaData]\n</code></pre> <p><code>lastCheckpoint</code> loadMetadataFromFile (allowing for 3 retries).</p> <p><code>lastCheckpoint</code> is used when:</p> <ul> <li><code>SnapshotManagement</code> is requested to load the latest snapshot</li> <li><code>MetadataCleanup</code> is requested to listExpiredDeltaLogs</li> </ul>","text":""},{"location":"Checkpoints/#loadmetadatafromfile","title":"loadMetadataFromFile <pre><code>loadMetadataFromFile(\n  tries: Int): Option[CheckpointMetaData]\n</code></pre> <p><code>loadMetadataFromFile</code> loads the _last_checkpoint file (in JSON format) and converts it to <code>CheckpointMetaData</code> (with a version, size and parts).</p> <p><code>loadMetadataFromFile</code> uses the LogStore to read the _last_checkpoint file.</p> <p>In case the _last_checkpoint file is corrupted, <code>loadMetadataFromFile</code>...FIXME</p>","text":""},{"location":"ColumnWithDefaultExprUtils/","title":"ColumnWithDefaultExprUtils","text":"<p>IDENTITY Columns feature is unsupported yet</p> <p>Protocol.requiredMinimumProtocol throws an <code>AnalysisException</code> when a delta table uses identity columns:</p> <pre><code>IDENTITY column is not supported\n</code></pre>"},{"location":"ColumnWithDefaultExprUtils/#identity_min_writer_version","title":"IDENTITY_MIN_WRITER_VERSION <p><code>ColumnWithDefaultExprUtils</code> uses <code>6</code> as the minimum version of a writer for writing to <code>IDENTITY</code> columns.</p> <p><code>IDENTITY_MIN_WRITER_VERSION</code> is used when:</p> <ul> <li><code>ColumnWithDefaultExprUtils</code> is used to satisfyProtocol</li> <li><code>Protocol</code> utility is used to determine the required minimum protocol</li> </ul>","text":""},{"location":"ColumnWithDefaultExprUtils/#columnhasdefaultexpr","title":"columnHasDefaultExpr <pre><code>columnHasDefaultExpr(\n  protocol: Protocol,\n  col: StructField): Boolean\n</code></pre> <p><code>columnHasDefaultExpr</code> is an alias of GeneratedColumn.isGeneratedColumn.</p> <p><code>columnHasDefaultExpr</code> is used when:</p> <ul> <li><code>DeltaAnalysis</code> logical resolution rule is requested to <code>resolveQueryColumnsByName</code></li> </ul>","text":""},{"location":"ColumnWithDefaultExprUtils/#hasidentitycolumn","title":"hasIdentityColumn <pre><code>hasIdentityColumn(\n  schema: StructType): Boolean\n</code></pre> <p><code>hasIdentityColumn</code> returns <code>true</code> if the given <code>StructType</code> (Spark SQL) contains an IDENTITY column.</p> <p><code>hasIdentityColumn</code> is used when:</p> <ul> <li><code>Protocol</code> utility is used for the required minimum protocol</li> </ul>","text":""},{"location":"ColumnWithDefaultExprUtils/#isidentitycolumn","title":"isIdentityColumn <pre><code>isIdentityColumn(\n  field: StructField): Boolean\n</code></pre> <p><code>isIdentityColumn</code> uses the <code>Metadata</code> (of the given <code>StructField</code>) to check the existence of delta.identity.start, delta.identity.step and delta.identity.allowExplicitInsert metadata keys.</p>  <p>IDENTITY column</p> <p>IDENTITY column is a column with delta.identity.start, delta.identity.step and delta.identity.allowExplicitInsert metadata.</p>  <p><code>isIdentityColumn</code> is used when:</p> <ul> <li><code>ColumnWithDefaultExprUtils</code> is used to hasIdentityColumn and removeDefaultExpressions</li> </ul>","text":""},{"location":"ColumnWithDefaultExprUtils/#removing-default-expressions","title":"Removing Default Expressions <pre><code>removeDefaultExpressions(\n  schema: StructType,\n  keepGeneratedColumns: Boolean = false): StructType\n</code></pre> <p><code>removeDefaultExpressions</code>...FIXME</p> <p><code>removeDefaultExpressions</code> is used when:</p> <ul> <li><code>DeltaLog</code> is requested to create a BaseRelation and createDataFrame</li> <li><code>OptimisticTransactionImpl</code> is requested to updateMetadataInternal</li> <li><code>DeltaTableV2</code> is requested for the tableSchema</li> <li><code>DeltaDataSource</code> is requested for the sourceSchema</li> <li><code>DeltaSourceBase</code> is requested for the schema</li> </ul>","text":""},{"location":"ColumnWithDefaultExprUtils/#tablehasdefaultexpr","title":"tableHasDefaultExpr <pre><code>tableHasDefaultExpr(\n  protocol: Protocol,\n  metadata: Metadata): Boolean\n</code></pre> <p><code>tableHasDefaultExpr</code> enforcesGeneratedColumns.</p>  <p><code>tableHasDefaultExpr</code> is used when:</p> <ul> <li><code>TransactionalWrite</code> is requested to normalizeData</li> </ul>","text":""},{"location":"CommitInfo/","title":"CommitInfo","text":"<p><code>CommitInfo</code> is an Action defined by the following properties:</p> <ul> <li> Version (optional) <li> Timestamp <li> User ID (optional) <li> User Name (optional) <li> Operation <li> Operation Parameters <li> JobInfo (optional) <li> NotebookInfo (optional) <li> Cluster ID (optional) <li> Read Version (optional) <li> Isolation Level (optional) <li>isBlindAppend flag (optional)</li> <li> Operation Metrics (optional) <li> User metadata (optional) <li> Tags <li>engineInfo</li> <li> Transaction ID <p><code>CommitInfo</code> is created (using apply and empty utilities) when:</p> <ul> <li><code>DeltaHistoryManager</code> is requested for version and commit history (for DeltaTable.history operator and DESCRIBE HISTORY SQL command)</li> <li><code>OptimisticTransactionImpl</code> is requested to commit (with spark.databricks.delta.commitInfo.enabled configuration property enabled)</li> <li><code>DeltaCommand</code> is requested to commitLarge (for ConvertToDeltaCommand command and <code>FileAlreadyExistsException</code> was thrown)</li> </ul> <p><code>CommitInfo</code> is used as a part of OptimisticTransactionImpl and <code>CommitStats</code>.</p>"},{"location":"CommitInfo/#engineinfo","title":"engineInfo <p><code>CommitInfo</code> can be given extra <code>engineInfo</code> identifier (when created) for the engine that made the commit.</p> <p>This <code>engineInfo</code> is by default getEngineInfo.</p>","text":""},{"location":"CommitInfo/#getengineinfo","title":"getEngineInfo <pre><code>getEngineInfo: Option[String]\n</code></pre> <p><code>getEngineInfo</code> is the following text:</p> <pre><code>Apache-Spark/[SPARK_VERSION] Delta-Lake/[VERSION]\n</code></pre>","text":""},{"location":"CommitInfo/#blind-append","title":"Blind Append <p><code>CommitInfo</code> is given <code>isBlindAppend</code> flag (when created) to indicate whether a commit has blindly appended data without caring about existing files.</p> <p><code>isBlindAppend</code> flag is used while checking for logical conflicts with concurrent updates (at commit).</p> <p><code>isBlindAppend</code> flag is always <code>false</code> when <code>DeltaCommand</code> is requested to commitLarge.</p>","text":""},{"location":"CommitInfo/#deltahistorymanager","title":"DeltaHistoryManager <p><code>CommitInfo</code> can be looked up using DeltaHistoryManager for the following:</p> <ul> <li>DESCRIBE HISTORY SQL command</li> <li>DeltaTable.history operation</li> </ul>","text":""},{"location":"CommitInfo/#sparkdatabricksdeltacommitinfoenabled","title":"spark.databricks.delta.commitInfo.enabled <p><code>CommitInfo</code> is added (logged) to a delta log only with spark.databricks.delta.commitInfo.enabled configuration property enabled.</p>","text":""},{"location":"CommitInfo/#creating-empty-commitinfo","title":"Creating Empty CommitInfo <pre><code>empty(\n  version: Option[Long] = None): CommitInfo\n</code></pre> <p><code>empty</code>...FIXME</p> <p><code>empty</code>\u00a0is used when:</p> <ul> <li><code>DeltaHistoryManager</code> is requested to getCommitInfo</li> </ul>","text":""},{"location":"CommitInfo/#creating-commitinfo","title":"Creating CommitInfo <pre><code>apply(\n  time: Long,\n  operation: String,\n  operationParameters: Map[String, String],\n  commandContext: Map[String, String],\n  readVersion: Option[Long],\n  isolationLevel: Option[String],\n  isBlindAppend: Option[Boolean],\n  operationMetrics: Option[Map[String, String]],\n  userMetadata: Option[String],\n  tags: Option[Map[String, String]],\n  txnId: Option[String]): CommitInfo\n</code></pre> <p><code>apply</code> creates a <code>CommitInfo</code> (for the given arguments and based on the given <code>commandContext</code> for the user ID, user name, job, notebook, cluster).</p> <p><code>commandContext</code> argument is always empty, but could be customized using ConvertToDeltaCommandBase.</p> <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to commit (with spark.databricks.delta.commitInfo.enabled configuration property enabled)</li> <li><code>DeltaCommand</code> is requested to commitLarge</li> </ul>","text":""},{"location":"ConflictChecker/","title":"ConflictChecker","text":"<p><code>ConflictChecker</code> is...FIXME</p>"},{"location":"DelayedCommitProtocol/","title":"DelayedCommitProtocol","text":"<p><code>DelayedCommitProtocol</code> is a <code>FileCommitProtocol</code> (Apache Spark) to write out data to a directory and return the files added.</p> <p><code>DelayedCommitProtocol</code> is used to model a distributed write that is orchestrated by the Spark driver with the write itself happening on executors.</p> <p>Note</p> <p><code>FileCommitProtocol</code> allows to track a write job (with a write task per partition) and inform the driver when all the write tasks finished successfully (and were committed) to consider the write job completed.</p> <p><code>TaskCommitMessage</code> (Spark Core) allows to \"transfer\" the file names added (written out) on the executors to the driver for the optimistic transactional writer.</p>"},{"location":"DelayedCommitProtocol/#creating-instance","title":"Creating Instance","text":"<p><code>DelayedCommitProtocol</code> takes the following to be created:</p> <ul> <li>Job ID</li> <li>Data path</li> <li>Length of the random prefix</li> </ul> <p><code>DelayedCommitProtocol</code> is created\u00a0when:</p> <ul> <li><code>TransactionalWrite</code> is requested for a committer (to write data out to the directory)</li> </ul>"},{"location":"DelayedCommitProtocol/#job-id","title":"Job ID <p><code>DelayedCommitProtocol</code> is given a job ID that is always <code>delta</code>.</p>","text":""},{"location":"DelayedCommitProtocol/#data-path","title":"Data Path <p><code>DelayedCommitProtocol</code> is given a <code>path</code> when created.</p> <p>The path is the data directory of a delta table (this <code>DelayedCommitProtocol</code> coordinates a write process to)</p>","text":""},{"location":"DelayedCommitProtocol/#length-of-random-prefix","title":"Length of Random Prefix <p><code>DelayedCommitProtocol</code> can be given a <code>randomPrefixLength</code> when created.</p> <p>The <code>randomPrefixLength</code> is always undefined (<code>None</code>).</p>","text":""},{"location":"DelayedCommitProtocol/#change-data-feed-partition-handling","title":"Change Data Feed Partition Handling <p><code>DelayedCommitProtocol</code> defines 3 values to support Change Data Feed:</p> <ul> <li><code>__is_cdc=false</code></li> <li><code>__is_cdc=true</code></li> <li>A <code>Regex</code> to match on <code>__is_cdc=true</code> text</li> </ul> <p><code>DelayedCommitProtocol</code> uses them for newTaskTempFile (to create temporary files in _change_data directory instead based on the regular expression).</p>","text":""},{"location":"DelayedCommitProtocol/#addedfiles","title":"addedFiles <pre><code>addedFiles: ArrayBuffer[(Map[String, String], String)]\n</code></pre> <p><code>DelayedCommitProtocol</code> uses <code>addedFiles</code> internal registry to track the partition values (if writing happened to a partitioned table) and the relative paths of the files that were added by a write task.</p> <p><code>addedFiles</code> is used on executors only.</p> <p><code>addedFiles</code> is initialized (as an empty collection) when setting up a task.</p> <p><code>addedFiles</code> is used when:</p> <ul> <li><code>DelayedCommitProtocol</code> is requested to commit a task (on an executor and create a <code>TaskCommitMessage</code> with the files added while a task was writing data out)</li> </ul>","text":""},{"location":"DelayedCommitProtocol/#addfiles","title":"AddFiles <pre><code>addedStatuses: ArrayBuffer[AddFile]\n</code></pre> <p><code>DelayedCommitProtocol</code> uses <code>addedStatuses</code> internal registry to track the AddFile files added by write tasks (on executors) once all they finish successfully and the write job is committed (on a driver).</p> <p><code>addedStatuses</code> is used on the driver only.</p> <p><code>addedStatuses</code> is used when:</p> <ul> <li><code>DelayedCommitProtocol</code> is requested to commit a job (on a driver)</li> <li><code>TransactionalWrite</code> is requested to write out a structured query</li> </ul>","text":""},{"location":"DelayedCommitProtocol/#addcdcfiles","title":"AddCDCFiles <pre><code>changeFiles: ArrayBuffer[AddCDCFile]\n</code></pre> <p><code>DelayedCommitProtocol</code> uses <code>changeFiles</code> internal registry to track the AddCDCFile files added by write tasks (on executors) once all they finish successfully and the write job is committed (on a driver).</p> <p><code>changeFiles</code> is used on the driver only.</p> <p><code>changeFiles</code> is used when:</p> <ul> <li><code>DelayedCommitProtocol</code> is requested to commit a job (on a driver)</li> <li><code>TransactionalWrite</code> is requested to write out a structured query</li> </ul>","text":""},{"location":"DelayedCommitProtocol/#setting-up-job","title":"Setting Up Job <pre><code>setupJob(\n  jobContext: JobContext): Unit\n</code></pre> <p><code>setupJob</code>\u00a0is part of the <code>FileCommitProtocol</code> (Apache Spark) abstraction.</p> <p><code>setupJob</code> is a noop.</p>","text":""},{"location":"DelayedCommitProtocol/#committing-job","title":"Committing Job <pre><code>commitJob(\n  jobContext: JobContext,\n  taskCommits: Seq[TaskCommitMessage]): Unit\n</code></pre> <p><code>commitJob</code>\u00a0is part of the <code>FileCommitProtocol</code> (Apache Spark) abstraction.</p>  <p><code>commitJob</code> partitions the given <code>TaskCommitMessage</code>s into a collection of AddFiles and AddCDCFiles.</p> <p>In the end, <code>commitJob</code> adds the <code>AddFile</code>s to addedStatuses registry while the <code>AddCDCFile</code>s to the changeFiles.</p>","text":""},{"location":"DelayedCommitProtocol/#aborting-job","title":"Aborting Job <pre><code>abortJob(\n  jobContext: JobContext): Unit\n</code></pre> <p><code>abortJob</code>\u00a0is part of the <code>FileCommitProtocol</code> (Apache Spark) abstraction.</p> <p><code>abortJob</code> is a noop.</p>","text":""},{"location":"DelayedCommitProtocol/#setting-up-task","title":"Setting Up Task <pre><code>setupTask(\n  taskContext: TaskAttemptContext): Unit\n</code></pre> <p><code>setupTask</code>\u00a0is part of the <code>FileCommitProtocol</code> (Apache Spark) abstraction.</p> <p><code>setupTask</code> initializes the addedFiles internal registry to be empty.</p>","text":""},{"location":"DelayedCommitProtocol/#new-temp-file","title":"New Temp File <pre><code>newTaskTempFile(\n  taskContext: TaskAttemptContext,\n  dir: Option[String],\n  ext: String): String\n</code></pre> <p><code>newTaskTempFile</code>\u00a0is part of the <code>FileCommitProtocol</code> (Apache Spark) abstraction.</p>   <p>Note</p> <p>The given <code>dir</code> defines a partition directory if a query is written out to a partitioned table.</p>  <p><code>newTaskTempFile</code> parses the partition values out of the given <code>dir</code> or falls back to an empty <code>partitionValues</code>.</p> <p><code>newTaskTempFile</code> creates a file name (for the given <code>TaskAttemptContext</code>, <code>ext</code> and the partition values).</p> <p><code>newTaskTempFile</code> builds a relative directory path (using the randomPrefixLength or the optional <code>dir</code> if either is defined).</p>  <p>randomPrefixLength always undefined</p> <p>randomPrefixLength is always undefined (<code>None</code>) so we can safely skip this branch.</p>  <ul> <li> <p>For the directory to be exactly __is_cdc=false, <code>newTaskTempFile</code> returns the file name (with no further changes).</p> </li> <li> <p>For the directory with the __is_cdc=true path prefix, <code>newTaskTempFile</code> replaces the prefix with _change_data and uses the changed directory as the parent of the file name.</p> <pre><code>val subDir = \"__is_cdc=true/a/b/c\"\n\nval cdcPartitionTrue = \"__is_cdc=true\"\nval cdcPartitionTrueRegex = cdcPartitionTrue.r\nval path = cdcPartitionTrueRegex.replaceFirstIn(subDir, \"_change_data\")\n\nassert(path == \"_change_data/a/b/c\")\n</code></pre> </li> <li> <p>For the directory with the __is_cdc=false path prefix, <code>newTaskTempFile</code> removes the prefix and uses the changed directory as the parent of the file name.</p> </li> <li> <p>For other cases, <code>newTaskTempFile</code> uses the directory as the parent of the file name.</p> </li> </ul> <p>When neither the randomPrefixLength nor the partition directory (<code>dir</code>) is defined, <code>newTaskTempFile</code> uses the file name (with no further changes).</p> <p><code>newTaskTempFile</code> adds the partition values and the relative path to the addedFiles internal registry.</p> <p>In the end, <code>newTaskTempFile</code> returns the absolute path of the (relative) path in the directory.</p>","text":""},{"location":"DelayedCommitProtocol/#file-name","title":"File Name <pre><code>getFileName(\n  taskContext: TaskAttemptContext,\n  ext: String,\n  partitionValues: Map[String, String]): String\n</code></pre> <p><code>getFileName</code> returns a file name of the format:</p> <pre><code>[prefix]-[split]-[uuid][ext]\n</code></pre> <p>The file name is created as follows:</p> <ol> <li>The <code>prefix</code> part is one of the following:<ul> <li><code>cdc</code> for the given <code>partitionValues</code> with the __is_cdc partition column with <code>true</code> value</li> <li><code>part</code> otherwise</li> </ul> </li> <li>The <code>split</code> part is the task ID from the given <code>TaskAttemptContext</code> (Apache Hadoop)</li> <li>The <code>uuid</code> part is a random UUID</li> </ol>","text":""},{"location":"DelayedCommitProtocol/#new-temp-file-absolute-path","title":"New Temp File (Absolute Path) <pre><code>newTaskTempFileAbsPath(\n  taskContext: TaskAttemptContext,\n  absoluteDir: String,\n  ext: String): String\n</code></pre> <p><code>newTaskTempFileAbsPath</code>\u00a0is part of the <code>FileCommitProtocol</code> (Apache Spark) abstraction.</p> <p><code>newTaskTempFileAbsPath</code> throws an <code>UnsupportedOperationException</code>:</p> <pre><code>[this] does not support adding files with an absolute path\n</code></pre>","text":""},{"location":"DelayedCommitProtocol/#committing-task","title":"Committing Task <pre><code>commitTask(\n  taskContext: TaskAttemptContext): TaskCommitMessage\n</code></pre> <p><code>commitTask</code>\u00a0is part of the <code>FileCommitProtocol</code> (Apache Spark) abstraction.</p>  <p><code>commitTask</code> creates a <code>TaskCommitMessage</code> with a FileAction (a AddCDCFile or a AddFile) for every file added (if there were any added successfully). Otherwise, <code>commitTask</code> creates an empty <code>TaskCommitMessage</code>.</p>  <p>Note</p> <p>A file is added (to the addedFiles internal registry) when <code>DelayedCommitProtocol</code> is requested for a new file (path).</p>","text":""},{"location":"DelayedCommitProtocol/#buildactionfromaddedfile","title":"buildActionFromAddedFile <pre><code>buildActionFromAddedFile(\n  f: (Map[String, String], String),\n  stat: FileStatus,\n  taskContext: TaskAttemptContext): FileAction\n</code></pre> <p><code>buildActionFromAddedFile</code> removes the __is_cdc virtual partition column and creates a FileAction:</p> <ul> <li>AddCDCFiles for __is_cdc=true partition files</li> <li>AddFiles otherwise</li> </ul>","text":""},{"location":"DelayedCommitProtocol/#aborting-task","title":"Aborting Task <pre><code>abortTask(\n  taskContext: TaskAttemptContext): Unit\n</code></pre> <p><code>abortTask</code>\u00a0is part of the <code>FileCommitProtocol</code> (Apache Spark) abstraction.</p> <p><code>abortTask</code> is a noop.</p>","text":""},{"location":"DelayedCommitProtocol/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.files.DelayedCommitProtocol</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.delta.files.DelayedCommitProtocol=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"DeltaAnalysis/","title":"DeltaAnalysis Logical Resolution Rule","text":"<p><code>DeltaAnalysis</code> is a logical resolution rule (Spark SQL).</p>"},{"location":"DeltaAnalysis/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaAnalysis</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> <code>SQLConf</code> (Spark SQL) <p><code>DeltaAnalysis</code> is created\u00a0when:</p> <ul> <li><code>DeltaSparkSessionExtension</code> is requested to register Delta extensions</li> </ul>"},{"location":"DeltaAnalysis/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> is part of the <code>Rule</code> (Spark SQL) abstraction.</p>  <p><code>apply</code> resolves the following logical operators.</p>","text":""},{"location":"DeltaAnalysis/#altertableaddconstraintstatement","title":"AlterTableAddConstraintStatement <p><code>apply</code> creates an <code>AlterTable</code> (Spark SQL) logical command with an AddConstraint table change.</p>","text":""},{"location":"DeltaAnalysis/#altertabledropconstraintstatement","title":"AlterTableDropConstraintStatement <p><code>apply</code> creates an <code>AlterTable</code> (Spark SQL) logical command with an DropConstraint table change.</p>","text":""},{"location":"DeltaAnalysis/#appenddata","title":"AppendData <p>For <code>AppendData</code> (Spark SQL), <code>apply</code> tries to destructure it to a pair of <code>DataSourceV2Relation</code> (Spark SQL) and a DeltaTableV2.</p> <p><code>apply</code> proceeds for <code>AppendData</code> operators that are not <code>isByName</code>.</p> <p><code>apply</code>...FIXME</p>","text":""},{"location":"DeltaAnalysis/#datasourcev2relation","title":"DataSourceV2Relation","text":""},{"location":"DeltaAnalysis/#deletefromtable","title":"DeleteFromTable","text":""},{"location":"DeltaAnalysis/#deltatable","title":"DeltaTable","text":""},{"location":"DeltaAnalysis/#mergeintotable","title":"MergeIntoTable <pre><code>MergeIntoTable(target, source, condition, matched, notMatched)\n</code></pre> <p><code>apply</code> resolves <code>MergeIntoTable</code> (Spark SQL) logical command into a DeltaMergeInto.</p> <p><code>apply</code> creates the following for the <code>matched</code> actions:</p> <ul> <li>DeltaMergeIntoDeleteClauses for <code>DeleteAction</code>s</li> <li>DeltaMergeIntoUpdateClauses for <code>UpdateAction</code>s</li> </ul> <p><code>apply</code> throws an <code>AnalysisException</code> for <code>InsertAction</code>s:</p> <pre><code>Insert clauses cannot be part of the WHEN MATCHED clause in MERGE INTO.\n</code></pre> <p><code>apply</code> creates the following for the <code>notMatched</code> actions:</p> <ul> <li>DeltaMergeIntoInsertClauses for <code>InsertAction</code>s</li> </ul> <p><code>apply</code> throws an <code>AnalysisException</code> for the other actions:</p> <pre><code>[name] clauses cannot be part of the WHEN NOT MATCHED clause in MERGE INTO.\n</code></pre> <p>In the end, <code>apply</code> creates a DeltaMergeInto logical command (with the matched and not-matched actions).</p>","text":""},{"location":"DeltaAnalysis/#overwritedelta","title":"OverwriteDelta","text":""},{"location":"DeltaAnalysis/#restoretablestatement","title":"RestoreTableStatement","text":""},{"location":"DeltaAnalysis/#updatetable","title":"UpdateTable","text":""},{"location":"DeltaCatalog/","title":"DeltaCatalog","text":"<p><code>DeltaCatalog</code> is a <code>DelegatingCatalogExtension</code> (Spark SQL) and a StagingTableCatalog.</p> <p><code>DeltaCatalog</code> is registered using <code>spark.sql.catalog.spark_catalog</code> (Spark SQL) configuration property.</p>"},{"location":"DeltaCatalog/#stagingtablecatalog","title":"StagingTableCatalog <p><code>DeltaCatalog</code> is a <code>StagingTableCatalog</code> (Spark SQL) that creates a StagedDeltaTableV2 (for delta data source) or a <code>BestEffortStagedTable</code>.</p>","text":""},{"location":"DeltaCatalog/#stagecreate","title":"stageCreate <pre><code>stageCreate(\n  ident: Identifier,\n  schema: StructType,\n  partitions: Array[Transform],\n  properties: util.Map[String, String]): StagedTable\n</code></pre> <p><code>stageCreate</code> is part of the <code>StagingTableCatalog</code> (Spark SQL) abstraction.</p>  <p><code>stageCreate</code> creates a StagedDeltaTableV2 (with <code>TableCreationModes.Create</code> operation) for delta data source only (based on the given <code>properties</code> or spark.sql.sources.default configuration property).</p> <p>Otherwise, <code>stageCreate</code> creates a <code>BestEffortStagedTable</code> (requesting the parent <code>TableCatalog</code> to create a table).</p>","text":""},{"location":"DeltaCatalog/#stagecreateorreplace","title":"stageCreateOrReplace <pre><code>stageCreateOrReplace(\n  ident: Identifier,\n  schema: StructType,\n  partitions: Array[Transform],\n  properties: util.Map[String, String]): StagedTable\n</code></pre> <p><code>stageCreateOrReplace</code> is part of the <code>StagingTableCatalog</code> (Spark SQL) abstraction.</p>  <p><code>stageCreateOrReplace</code> creates a StagedDeltaTableV2 (with <code>TableCreationModes.CreateOrReplace</code> operation) for delta data source only (based on the given <code>properties</code> or spark.sql.sources.default configuration property).</p> <p>Otherwise, <code>stageCreateOrReplace</code> requests the parent <code>TableCatalog</code> to drop the table first and then creates a <code>BestEffortStagedTable</code> (requesting the parent <code>TableCatalog</code> to create the table).</p>","text":""},{"location":"DeltaCatalog/#stagereplace","title":"stageReplace <pre><code>stageReplace(\n  ident: Identifier,\n  schema: StructType,\n  partitions: Array[Transform],\n  properties: util.Map[String, String]): StagedTable\n</code></pre> <p><code>stageReplace</code> is part of the <code>StagingTableCatalog</code> (Spark SQL) abstraction.</p>  <p><code>stageReplace</code> creates a StagedDeltaTableV2 (with <code>TableCreationModes.Replace</code> operation) for delta data source only (based on the given <code>properties</code> or spark.sql.sources.default configuration property).</p> <p>Otherwise, <code>stageReplace</code> requests the parent <code>TableCatalog</code> to drop the table first and then creates a <code>BestEffortStagedTable</code> (requesting the parent <code>TableCatalog</code> to create the table).</p>","text":""},{"location":"DeltaCatalog/#altering-table","title":"Altering Table <pre><code>alterTable(\n  ident: Identifier,\n  changes: TableChange*): Table\n</code></pre> <p><code>alterTable</code> is part of the <code>TableCatalog</code> (Spark SQL) abstraction.</p>  <p><code>alterTable</code> loads the table and continues only when it is a DeltaTableV2. Otherwise, <code>alterTable</code> delegates to the parent <code>TableCatalog</code>.</p> <p><code>alterTable</code> groups the given <code>TableChange</code>s by their (class) type.</p> <p>In addition, <code>alterTable</code> collects the following <code>ColumnChange</code>s together (that are then executed as column updates as AlterTableChangeColumnDeltaCommand):</p> <ul> <li><code>RenameColumn</code></li> <li><code>UpdateColumnComment</code></li> <li><code>UpdateColumnNullability</code></li> <li><code>UpdateColumnPosition</code></li> <li><code>UpdateColumnType</code></li> </ul> <p><code>alterTable</code> executes the table changes as one of AlterDeltaTableCommands.</p>    TableChange AlterDeltaTableCommand     <code>AddColumn</code> AlterTableAddColumnsDeltaCommand   <code>AddConstraint</code> AlterTableAddConstraintDeltaCommand   <code>ColumnChange</code> AlterTableChangeColumnDeltaCommand   <code>DropConstraint</code> AlterTableDropConstraintDeltaCommand   <code>RemoveProperty</code> AlterTableUnsetPropertiesDeltaCommand   <code>SetLocation</code>(<code>SetProperty</code> with <code>location</code> property)catalog delta tables only AlterTableSetLocationDeltaCommand   <code>SetProperty</code> AlterTableSetPropertiesDeltaCommand    <p><code>alterTable</code>...FIXME</p>","text":""},{"location":"DeltaCatalog/#creating-table","title":"Creating Table <pre><code>createTable(\n  ident: Identifier,\n  schema: StructType,\n  partitions: Array[Transform],\n  properties: util.Map[String, String]): Table\n</code></pre> <p><code>createTable</code> is part of the <code>TableCatalog</code> (Spark SQL) abstraction.</p>  <p><code>createTable</code>...FIXME</p>","text":""},{"location":"DeltaCatalog/#loading-table","title":"Loading Table <pre><code>loadTable(\n  ident: Identifier): Table\n</code></pre> <p><code>loadTable</code> is part of the <code>TableCatalog</code> (Spark SQL) abstraction.</p>  <p><code>loadTable</code> loads a table by the given identifier from a catalog.</p> <p>If found and the table is a delta table (Spark SQL's V1Table with <code>delta</code> provider), <code>loadTable</code> creates a DeltaTableV2.</p>","text":""},{"location":"DeltaCatalog/#creating-delta-table","title":"Creating Delta Table <pre><code>createDeltaTable(\n  ident: Identifier,\n  schema: StructType,\n  partitions: Array[Transform],\n  allTableProperties: Map[String, String],\n  writeOptions: Map[String, String],\n  sourceQuery: Option[DataFrame],\n  operation: TableCreationModes.CreationMode): Table\n</code></pre> <p><code>createDeltaTable</code>...FIXME</p> <p><code>createDeltaTable</code> is used when:</p> <ul> <li><code>DeltaCatalog</code> is requested to create a table</li> <li><code>StagedDeltaTableV2</code> is requested to commitStagedChanges</li> </ul>","text":""},{"location":"DeltaCatalog/#operation","title":"Operation <p><code>createDeltaTable</code> is given an argument of type <code>TableCreationModes.CreationMode</code>:</p> <ul> <li><code>Create</code> when DeltaCatalog creates a table</li> <li><code>StagedDeltaTableV2</code> is given a CreationMode when created</li> </ul>","text":""},{"location":"DeltaCatalog/#looking-up-table-provider","title":"Looking Up Table Provider <pre><code>getProvider(\n  properties: util.Map[String, String]): String\n</code></pre> <p><code>getProvider</code> takes the value of the <code>provider</code> from the given <code>properties</code> (if available) or defaults to the value of <code>spark.sql.sources.default</code> (Spark SQL) configuration property.</p> <p><code>getProvider</code> is used when:</p> <ul> <li><code>DeltaCatalog</code> is requested to createTable, stageReplace, stageCreateOrReplace and stageCreate</li> </ul>","text":""},{"location":"DeltaColumnBuilder/","title":"DeltaColumnBuilder","text":"<p><code>DeltaColumnBuilder</code> is a builder interface to create columns programmatically.</p> <p><code>DeltaColumnBuilder</code> is created using DeltaTable.columnBuilder utility.</p> <p>In the end, <code>DeltaColumnBuilder</code> is supposed to be built.</p>","tags":["DeveloperApi"]},{"location":"DeltaColumnBuilder/#iodeltatables-package","title":"io.delta.tables Package","text":"<p><code>DeltaColumnBuilder</code> belongs to <code>io.delta.tables</code> package.</p> <pre><code>import io.delta.tables.DeltaColumnBuilder\n</code></pre>","tags":["DeveloperApi"]},{"location":"DeltaColumnBuilder/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaColumnBuilder</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> Column Name","tags":["DeveloperApi"]},{"location":"DeltaColumnBuilder/#operators","title":"Operators","text":"","tags":["DeveloperApi"]},{"location":"DeltaColumnBuilder/#build","title":"build <pre><code>build(): StructField\n</code></pre> <p>Creates a <code>StructField</code> (Spark SQL)</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaColumnBuilder/#comment","title":"comment <pre><code>comment(\n  comment: String): DeltaColumnBuilder\n</code></pre>","text":"","tags":["DeveloperApi"]},{"location":"DeltaColumnBuilder/#datatype","title":"dataType <pre><code>dataType(\n  dataType: DataType): DeltaColumnBuilder\ndataType(\n  dataType: String): DeltaColumnBuilder\n</code></pre>","text":"","tags":["DeveloperApi"]},{"location":"DeltaColumnBuilder/#generatedalwaysas","title":"generatedAlwaysAs <pre><code>generatedAlwaysAs(\n  expr: String): DeltaColumnBuilder\n</code></pre> <p>Registers the Generation Expression of this field</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaColumnBuilder/#nullable","title":"nullable <pre><code>nullable(\n  nullable: Boolean): DeltaColumnBuilder\n</code></pre>","text":"","tags":["DeveloperApi"]},{"location":"DeltaColumnBuilder/#generation-expression","title":"Generation Expression <pre><code>generationExpr: Option[String] = None\n</code></pre> <p><code>DeltaColumnBuilder</code> uses <code>generationExpr</code> internal registry for the generatedAlwaysAs expression.</p> <p>When requested to build a StructField, <code>DeltaColumnBuilder</code> registers <code>generationExpr</code> under delta.generationExpression key in the metadata (of this field).</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaConfig/","title":"DeltaConfig","text":"<p><code>DeltaConfig</code> (of type <code>T</code>) represents a named configuration property of a delta table with values (of type <code>T</code>).</p>"},{"location":"DeltaConfig/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaConfig</code> takes the following to be created:</p> <ul> <li> Configuration Key <li> Default Value <li> Conversion function (from text representation of the <code>DeltaConfig</code> to the <code>T</code> type, i.e. <code>String =&gt; T</code>) <li> Validation function (that guards from incorrect values, i.e. <code>T =&gt; Boolean</code>) <li> Help message <li> (optional) Minimum version of protocol supported (default: undefined) <p><code>DeltaConfig</code> is created\u00a0when:</p> <ul> <li><code>DeltaConfigs</code> utility is used to build a DeltaConfig</li> </ul>"},{"location":"DeltaConfig/#reading-configuration-property-from-metadata","title":"Reading Configuration Property From Metadata <pre><code>fromMetaData(\n  metadata: Metadata): T\n</code></pre> <p><code>fromMetaData</code> looks up the key in the configuration of the given Metadata. If not found, <code>fromMetaData</code> gives the default value.</p> <p>In the end, <code>fromMetaData</code> converts the text representation to the proper type using fromString conversion function.</p> <p><code>fromMetaData</code> is used when:</p> <ul> <li><code>Checkpoints</code> utility is used to buildCheckpoint</li> <li><code>DeltaErrors</code> utility is used to logFileNotFoundException</li> <li><code>DeltaLog</code> is requested for checkpointInterval and deletedFileRetentionDuration table properties, and to assert a table is not read-only</li> <li><code>MetadataCleanup</code> is requested for the enableExpiredLogCleanup and the deltaRetentionMillis</li> <li><code>OptimisticTransactionImpl</code> is requested to commit</li> <li><code>Snapshot</code> is requested for the numIndexedCols</li> </ul>","text":""},{"location":"DeltaConfig/#demo","title":"Demo <pre><code>import org.apache.spark.sql.delta.{DeltaConfig, DeltaConfigs}\n</code></pre> <pre><code>scala&gt; :type DeltaConfigs.TOMBSTONE_RETENTION\norg.apache.spark.sql.delta.DeltaConfig[org.apache.spark.unsafe.types.CalendarInterval]\n</code></pre> <pre><code>import org.apache.spark.sql.delta.DeltaLog\nval path = \"/tmp/delta/t1\"\nval t1 = DeltaLog.forTable(spark, path)\n</code></pre> <pre><code>val metadata = t1.snapshot.metadata\nval retention = DeltaConfigs.TOMBSTONE_RETENTION.fromMetaData(metadata)\n</code></pre> <pre><code>scala&gt; :type retention\norg.apache.spark.unsafe.types.CalendarInterval\n</code></pre>","text":""},{"location":"DeltaConfigs/","title":"DeltaConfigs (DeltaConfigsBase)","text":"<p><code>DeltaConfigs</code> holds the table properties that can be set on a delta table.</p>"},{"location":"DeltaConfigs/#accessing-deltaconfigs","title":"Accessing DeltaConfigs","text":"<pre><code>import org.apache.spark.sql.delta.OptimisticTransaction\nval txn: OptimisticTransaction = ???\n</code></pre> <pre><code>import org.apache.spark.sql.delta.actions.Metadata\nval metadata: Metadata = txn.metadata\n</code></pre> <pre><code>import org.apache.spark.sql.delta.DeltaConfigs\nDeltaConfigs.CHANGE_DATA_FEED.fromMetaData(metadata)\n</code></pre>"},{"location":"DeltaConfigs/#table-properties","title":"Table Properties","text":"<p>All table properties start with <code>delta.</code> prefix.</p>"},{"location":"DeltaConfigs/#appendonly","title":"appendOnly <p>Whether a delta table is append-only (<code>true</code>) or not (<code>false</code>). When enabled, a table allows appends only and no updates or deletes.</p> <p>Default: <code>false</code></p> <p>Used when:</p> <ul> <li><code>DeltaLog</code> is requested to assertRemovable (that in turn uses <code>DeltaErrors</code> utility to modifyAppendOnlyTableException)</li> <li><code>Protocol</code> utility is used to requiredMinimumProtocol</li> </ul>","text":""},{"location":"DeltaConfigs/#autooptimize","title":"autoOptimize <p>Whether this delta table will automagically optimize the layout of files during writes.</p> <p>Default: <code>false</code></p>","text":""},{"location":"DeltaConfigs/#checkpointinterval","title":"checkpointInterval <p>How often to checkpoint the state of a delta table (at the end of transaction commit)</p> <p>Default: <code>10</code></p>","text":""},{"location":"DeltaConfigs/#checkpointretentionduration","title":"checkpointRetentionDuration <p>How long to keep checkpoint files around before deleting them</p> <p>Default: <code>interval 2 days</code></p> <p>The most recent checkpoint is never deleted. It is acceptable to keep checkpoint files beyond this duration until the next calendar day.</p>","text":""},{"location":"DeltaConfigs/#checkpointwritestatsasjson","title":"checkpoint.writeStatsAsJson <p>Controls whether to write file statistics in the checkpoint in JSON format as the <code>stats</code> column.</p> <p>Default: <code>true</code></p>","text":""},{"location":"DeltaConfigs/#checkpointwritestatsasstruct","title":"checkpoint.writeStatsAsStruct <p>Controls whether to write file statistics in the checkpoint in the struct format in the <code>stats_parsed</code> column and partition values as a struct as <code>partitionValues_parsed</code></p> <p>Default: <code>undefined</code> (<code>Option[Boolean]</code>)</p>","text":""},{"location":"DeltaConfigs/#columnmappingmaxcolumnid","title":"columnMapping.maxColumnId <p>Maximum columnId used in the schema so far for column mapping</p> <p>Cannot be set</p> <p>Default: <code>0</code></p>","text":""},{"location":"DeltaConfigs/#columnmappingmode","title":"columnMapping.mode <p>DeltaColumnMappingMode to read and write parquet data files</p>    Name Description     <code>none</code> (default) A display name is the only valid identifier of a column   <code>id</code> A column ID is the identifier of a column. This mode is used for tables converted from Iceberg and parquet files in this mode will also have corresponding field Ids for each column in their file schema.   <code>name</code> The physical column name is the identifier of a column. Stored as part of <code>StructField</code> metadata in the schema. Used for reading statistics and partition values in the DeltaLog.    <p>Used when:</p> <ul> <li><code>DeltaColumnMappingBase</code> is requested to tryFixMetadata (while <code>OptimisticTransactionImpl</code> is requested to update the metadata)</li> <li><code>DeltaErrors</code> utility is used to create a DeltaColumnMappingUnsupportedException (while <code>OptimisticTransactionImpl</code> is requested to update the metadata)</li> <li><code>DeltaErrors</code> utility is used to create a DeltaColumnMappingUnsupportedException (while ConvertToDeltaCommand is executed)</li> <li><code>Metadata</code> is requested for the column mapping mode (while <code>DeltaFileFormat</code> is requested for the FileFormat)</li> </ul>","text":""},{"location":"DeltaConfigs/#compatibilitysymlinkformatmanifestenabled","title":"compatibility.symlinkFormatManifest.enabled <p>Whether to register the GenerateSymlinkManifest post-commit hook while committing a transaction or not</p> <p>Default: <code>false</code></p>","text":""},{"location":"DeltaConfigs/#dataskippingnumindexedcols","title":"dataSkippingNumIndexedCols <p>The number of columns to collect stats on for data skipping. <code>-1</code> means collecting stats for all columns.</p> <p>Default: <code>32</code></p> <p>Must be larger than or equal to <code>-1</code>.</p> <p>Used when:</p> <ul> <li><code>Snapshot</code> is requested for the maximum number of indexed columns</li> <li><code>TransactionalWrite</code> is requested to write data out</li> </ul>","text":""},{"location":"DeltaConfigs/#deletedfileretentionduration","title":"deletedFileRetentionDuration <p>How long to keep logically deleted data files around before deleting them physically (to prevent failures in stale readers after compactions or partition overwrites)</p> <p>Default: <code>interval 1 week</code></p>","text":""},{"location":"DeltaConfigs/#enablechangedatafeed","title":"enableChangeDataFeed <p>Enables Change Data Feed</p> <p>Default: <code>false</code></p> <p>Legacy configuration: <code>enableChangeDataCapture</code></p> <p>Used when:</p> <ul> <li><code>Protocol</code> is requested for the requiredMinimumProtocol</li> <li><code>DeleteCommand</code> is requested to rewriteFiles</li> <li><code>MergeIntoCommand</code> is requested to writeAllChanges</li> <li><code>UpdateCommand</code> is requested to shouldOutputCdc</li> <li><code>CDCReader</code> is requested to isCDCEnabledOnTable</li> </ul>","text":""},{"location":"DeltaConfigs/#enableexpiredlogcleanup","title":"enableExpiredLogCleanup <p>Whether to clean up expired log files and checkpoints</p> <p>Default: <code>true</code></p>","text":""},{"location":"DeltaConfigs/#enablefullretentionrollback","title":"enableFullRetentionRollback <p>Controls whether or not a delta table can be rolled back to any point within logRetentionDuration. When disabled, the table can be rolled back checkpointRetentionDuration only.</p> <p>Default: <code>true</code></p>","text":""},{"location":"DeltaConfigs/#logretentionduration","title":"logRetentionDuration <p>How long to keep obsolete logs around before deleting them. Delta can keep logs beyond the duration until the next calendar day to avoid constantly creating checkpoints.</p> <p>Default: <code>interval 30 days</code> (<code>CalendarInterval</code>)</p> <p>Examples: <code>2 weeks</code>, <code>365 days</code> (<code>months</code> and <code>years</code> are not accepted)</p> <p>Used when:</p> <ul> <li><code>MetadataCleanup</code> is requested for the deltaRetentionMillis</li> </ul>","text":""},{"location":"DeltaConfigs/#minreaderversion","title":"minReaderVersion <p>The protocol reader version</p> <p>Default: <code>1</code></p> <p>This property is not stored as a table property in the <code>Metadata</code> action. It is stored as its own action. Having it modelled as a table property makes it easier to upgrade, and view the version.</p>","text":""},{"location":"DeltaConfigs/#minwriterversion","title":"minWriterVersion <p>The protocol reader version</p> <p>Default: <code>3</code></p> <p>This property is not stored as a table property in the <code>Metadata</code> action. It is stored as its own action. Having it modelled as a table property makes it easier to upgrade, and view the version.</p>","text":""},{"location":"DeltaConfigs/#randomizefileprefixes","title":"randomizeFilePrefixes <p>Whether to use a random prefix in a file path instead of partition information (may be required for very high volume S3 calls to better be partitioned across S3 servers)</p> <p>Default: <code>false</code></p>","text":""},{"location":"DeltaConfigs/#randomprefixlength","title":"randomPrefixLength <p>The length of the random prefix in a file path for randomizeFilePrefixes</p> <p>Default: <code>2</code></p>","text":""},{"location":"DeltaConfigs/#sampleretentionduration","title":"sampleRetentionDuration <p>How long to keep delta sample files around before deleting them</p> <p>Default: <code>interval 7 days</code></p>","text":""},{"location":"DeltaConfigs/#building-configuration","title":"Building Configuration <pre><code>buildConfig[T](\n  key: String,\n  defaultValue: String,\n  fromString: String =&gt; T,\n  validationFunction: T =&gt; Boolean,\n  helpMessage: String,\n  minimumProtocolVersion: Option[Protocol] = None): DeltaConfig[T]\n</code></pre> <p><code>buildConfig</code> creates a DeltaConfig for the given <code>key</code> (with delta prefix added) and adds it to the entries internal registry.</p> <p><code>buildConfig</code> is used to define all of the configuration properties in a type-safe way and (as a side effect) register them with the system-wide entries internal registry.</p>","text":""},{"location":"DeltaConfigs/#system-wide-configuration-entries-registry","title":"System-Wide Configuration Entries Registry <pre><code>entries: HashMap[String, DeltaConfig[_]]\n</code></pre> <p><code>DeltaConfigs</code> utility (a Scala object) uses <code>entries</code> internal registry of DeltaConfigs by their key.</p> <p>New entries are added in buildConfig.</p> <p><code>entries</code> is used when:</p> <ul> <li>validateConfigurations</li> <li>mergeGlobalConfigs</li> <li>normalizeConfigKey and normalizeConfigKeys</li> </ul>","text":""},{"location":"DeltaConfigs/#mergeglobalconfigs-utility","title":"mergeGlobalConfigs Utility <pre><code>mergeGlobalConfigs(\n  sqlConfs: SQLConf,\n  tableConf: Map[String, String],\n  protocol: Protocol): Map[String, String]\n</code></pre> <p><code>mergeGlobalConfigs</code> finds all spark.databricks.delta.properties.defaults-prefixed configuration properties among the entries.</p> <p><code>mergeGlobalConfigs</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to withGlobalConfigDefaults</li> <li><code>InitialSnapshot</code> is created</li> </ul>","text":""},{"location":"DeltaConfigs/#validateconfigurations-utility","title":"validateConfigurations Utility <pre><code>validateConfigurations(\n  configurations: Map[String, String]): Map[String, String]\n</code></pre> <p><code>validateConfigurations</code>...FIXME</p> <p><code>validateConfigurations</code> is used when:</p> <ul> <li><code>DeltaCatalog</code> is requested to verifyTableAndSolidify and alterTable</li> </ul>","text":""},{"location":"DeltaConfigs/#normalizeconfigkeys-utility","title":"normalizeConfigKeys Utility <pre><code>normalizeConfigKeys(\n  propKeys: Seq[String]): Seq[String]\n</code></pre> <p><code>normalizeConfigKeys</code>...FIXME</p> <p><code>normalizeConfigKeys</code> is used when:</p> <ul> <li>AlterTableUnsetPropertiesDeltaCommand is executed</li> </ul>","text":""},{"location":"DeltaConfigs/#sparkdatabricksdeltapropertiesdefaults-prefix","title":"spark.databricks.delta.properties.defaults Prefix <p>DeltaConfigs uses spark.databricks.delta.properties.defaults prefix for global configuration properties.</p>","text":""},{"location":"DeltaDataSource/","title":"DeltaDataSource","text":"<p><code>DeltaDataSource</code> ties Delta Lake with Spark SQL (and Spark Structured Streaming) together as delta data source that supports batch and streaming queries.</p>"},{"location":"DeltaDataSource/#datasourceregister-and-delta-alias","title":"DataSourceRegister and delta Alias <p><code>DeltaDataSource</code> is a <code>DataSourceRegister</code> (Spark SQL) and registers delta alias.</p> <p><code>DeltaDataSource</code> is registered using META-INF/services/org.apache.spark.sql.sources.DataSourceRegister:</p> <pre><code>org.apache.spark.sql.delta.sources.DeltaDataSource\n</code></pre>","text":""},{"location":"DeltaDataSource/#relationprovider","title":"RelationProvider <p><code>DeltaDataSource</code> is a <code>RelationProvider</code> (Spark SQL).</p>","text":""},{"location":"DeltaDataSource/#creating-baserelation-for-table-scan","title":"Creating BaseRelation for Table Scan <pre><code>createRelation(\n  sqlContext: SQLContext,\n  parameters: Map[String, String]): BaseRelation\n</code></pre> <p><code>createRelation</code> is part of the <code>RelationProvider</code> (Spark SQL) abstraction.</p>  <p><code>createRelation</code> verifies the given parameters (options).</p> <p><code>createRelation</code> extracts time travel specification (from the given <code>parameters</code>).</p> <p><code>createRelation</code> collects CDF-specific options with change data feed enabled:</p> <ul> <li>readChangeFeed (with <code>true</code> value)</li> <li>startingVersion</li> <li>startingTimestamp</li> <li>endingVersion</li> <li>endingTimestamp</li> </ul> <p><code>createRelation</code> creates a DeltaTableV2 (with the given <code>parameters</code> as options when spark.databricks.delta.loadFileSystemConfigsFromDataFrameOptions configuration property is enabled).</p> <p>In the end, <code>createRelation</code> requests the <code>DeltaTableV2</code> for an insertable HadoopFsRelation.</p>  <code>path</code> Parameter is Required <p><code>createRelation</code> makes sure that there is <code>path</code> parameter defined (in the given <code>parameters</code>) or throws an <code>IllegalArgumentException</code>:</p> <pre><code>'path' is not specified\n</code></pre>","text":""},{"location":"DeltaDataSource/#creatablerelationprovider","title":"CreatableRelationProvider <p><code>DeltaDataSource</code> is a <code>CreatableRelationProvider</code> (Spark SQL).</p>","text":""},{"location":"DeltaDataSource/#creating-baserelation-after-data-writing","title":"Creating BaseRelation after Data Writing <pre><code>createRelation(\n  sqlContext: SQLContext,\n  mode: SaveMode,\n  parameters: Map[String, String],\n  data: DataFrame): BaseRelation\n</code></pre> <p><code>createRelation</code> is part of the <code>CreatableRelationProvider</code> (Spark SQL) abstraction.</p>  <p><code>createRelation</code> creates a DeltaLog for the required <code>path</code> parameter (from the given <code>parameters</code>) and the given <code>parameters</code> itself.</p> <p><code>createSource</code> creates a DeltaOptions (with the given <code>parameters</code> and the current <code>SQLConf</code>).</p> <p><code>createSource</code> validateConfigurations (with <code>delta.</code>-prefixed keys in the given<code>parameters</code>).</p> <p><code>createRelation</code> creates and executes a WriteIntoDelta command with the given <code>data</code>.</p> <p>In the end, <code>createRelation</code> requests the <code>DeltaLog</code> for a BaseRelation.</p>  <code>path</code> Parameter is Required <p><code>createRelation</code> makes sure that there is <code>path</code> parameter defined (in the given <code>parameters</code>) or throws an <code>IllegalArgumentException</code>:</p> <pre><code>'path' is not specified\n</code></pre>","text":""},{"location":"DeltaDataSource/#streamsourceprovider","title":"StreamSourceProvider <p><code>DeltaDataSource</code> is a <code>StreamSourceProvider</code> (Spark Structured Streaming).</p>","text":""},{"location":"DeltaDataSource/#creating-deltasource","title":"Creating DeltaSource <pre><code>createSource(\n  sqlContext: SQLContext,\n  metadataPath: String,\n  schema: Option[StructType],\n  providerName: String,\n  parameters: Map[String, String]): Source\n</code></pre> <p><code>createSource</code> creates a DeltaLog for the required <code>path</code> parameter (from the given <code>parameters</code>).</p> <p><code>createSource</code> creates a DeltaOptions (with the given <code>parameters</code> and the current <code>SQLConf</code>).</p> <p>In the end, <code>createSource</code> creates a DeltaSource (with the <code>DeltaLog</code> and the <code>DeltaOptions</code>).</p>  <p><code>createSource</code> makes sure that there is <code>path</code> parameter defined (in the given <code>parameters</code>) or throws an <code>IllegalArgumentException</code>:</p> <pre><code>'path' is not specified\n</code></pre>  <p><code>createSource</code> makes sure that there is no <code>schema</code> specified or throws an <code>AnalysisException</code>:</p> <pre><code>Delta does not support specifying the schema at read time.\n</code></pre>  <p><code>createSource</code> makes sure that there is schema available (in the Snapshot) of the <code>DeltaLog</code> or throws an <code>AnalysisException</code>:</p> <pre><code>Table schema is not set.  Write data into it or use CREATE TABLE to set the schema.\n</code></pre>  <p><code>createSource</code> is part of the <code>StreamSourceProvider</code> (Spark Structured Streaming) abstraction.</p>","text":""},{"location":"DeltaDataSource/#streaming-source-schema","title":"Streaming Source Schema <pre><code>sourceSchema(\n  sqlContext: SQLContext,\n  schema: Option[StructType],\n  providerName: String,\n  parameters: Map[String, String]): (String, StructType)\n</code></pre> <p><code>sourceSchema</code> creates a DeltaLog for the required <code>path</code> parameter (from the given <code>parameters</code>).</p> <p><code>sourceSchema</code> takes the schema (of the Snapshot) of the <code>DeltaLog</code> and removes default expressions.</p> <p>In the end, <code>sourceSchema</code> returns the delta name with the table schema.</p>  <p><code>createSource</code> makes sure that there is no <code>schema</code> specified or throws an <code>AnalysisException</code>:</p> <pre><code>Delta does not support specifying the schema at read time.\n</code></pre>  <p><code>createSource</code> makes sure that there is <code>path</code> parameter defined (in the given <code>parameters</code>) or throws an <code>IllegalArgumentException</code>:</p> <pre><code>'path' is not specified\n</code></pre>  <p><code>createSource</code> makes sure that there is no time travel specified using the following:</p> <ul> <li>path parameter</li> <li>options (in the given <code>parameters</code>)</li> </ul> <p>If either is set, <code>createSource</code> throws an <code>AnalysisException</code>:</p> <pre><code>Cannot time travel views, subqueries or streams.\n</code></pre>  <p><code>sourceSchema</code> is part of the <code>StreamSourceProvider</code> (Spark Structured Streaming) abstraction.</p>","text":""},{"location":"DeltaDataSource/#streamsinkprovider","title":"StreamSinkProvider <p><code>DeltaDataSource</code> is a <code>StreamSinkProvider</code> (Spark Structured Streaming).</p> <p><code>DeltaDataSource</code> supports <code>Append</code> and <code>Complete</code> output modes only.</p>  <p>Tip</p> <p>Consult the demo Using Delta Lake (as Streaming Sink) in Streaming Queries.</p>","text":""},{"location":"DeltaDataSource/#creating-streaming-sink","title":"Creating Streaming Sink <pre><code>createSink(\n  sqlContext: SQLContext,\n  parameters: Map[String, String],\n  partitionColumns: Seq[String],\n  outputMode: OutputMode): Sink\n</code></pre> <p><code>createSink</code> creates a DeltaOptions (with the given <code>parameters</code> and the current <code>SQLConf</code>).</p> <p>In the end, <code>createSink</code> creates a DeltaSink (with the required <code>path</code> parameter, the given <code>partitionColumns</code> and the <code>DeltaOptions</code>).</p>  <p><code>createSink</code> makes sure that there is <code>path</code> parameter defined (in the given <code>parameters</code>) or throws an <code>IllegalArgumentException</code>:</p> <pre><code>'path' is not specified\n</code></pre>  <p><code>createSink</code> makes sure that the given <code>outputMode</code> is either <code>Append</code> or <code>Complete</code>, or throws an <code>IllegalArgumentException</code>:</p> <pre><code>Data source [dataSource] does not support [outputMode] output mode\n</code></pre>  <p><code>createSink</code> is part of the <code>StreamSinkProvider</code> (Spark Structured Streaming) abstraction.</p>","text":""},{"location":"DeltaDataSource/#tableprovider","title":"TableProvider <p><code>DeltaDataSource</code> is a<code>TableProvider</code> (Spark SQL).</p> <p><code>DeltaDataSource</code> allows registering Delta tables in a <code>HiveMetaStore</code>. Delta creates a transaction log at the table root directory, and the Hive MetaStore contains no information but the table format and the location of the table. All table properties, schema and partitioning information live in the transaction log to avoid a split brain situation.</p> <p>The feature was added in SC-34233.</p>","text":""},{"location":"DeltaDataSource/#loading-delta-table","title":"Loading Delta Table <pre><code>getTable(\n  schema: StructType,\n  partitioning: Array[Transform],\n  properties: Map[String, String]): Table\n</code></pre> <p><code>getTable</code> is part of the <code>TableProvider</code> (Spark SQL) abstraction.</p>  <p><code>getTable</code> creates a DeltaTableV2 (with the path from the given <code>properties</code>).</p>  <p><code>getTable</code> throws an <code>IllegalArgumentException</code> when <code>path</code> option is not specified:</p> <pre><code>'path' is not specified\n</code></pre>","text":""},{"location":"DeltaDataSource/#creating-deltatimetravelspec","title":"Creating DeltaTimeTravelSpec <pre><code>getTimeTravelVersion(\n  parameters: Map[String, String]): Option[DeltaTimeTravelSpec]\n</code></pre> <p><code>getTimeTravelVersion</code> reads the following options (from the given <code>parameters</code>):</p> <ul> <li>timestampAsOf</li> <li>versionAsOf</li> <li><code>__time_travel_source__</code></li> </ul> <p><code>getTimeTravelVersion</code> creates a DeltaTimeTravelSpec if either <code>timestampAsOf</code> or <code>versionAsOf</code> is defined. The <code>DeltaTimeTravelSpec</code> is created with the creationSource based on <code>__time_travel_source__</code> (if specified) or defaults to <code>dfReader</code>.</p>  <p>Undocumented Feature</p> <p><code>__time_travel_source__</code> looks like an undocumented feature to use for the creationSource.</p>   <p><code>getTimeTravelVersion</code> is used when:</p> <ul> <li><code>DeltaDataSource</code> is requested to create a relation (as a RelationProvider)</li> </ul>","text":""},{"location":"DeltaDataSource/#parsepathidentifier","title":"parsePathIdentifier <pre><code>parsePathIdentifier(\n  spark: SparkSession,\n  userPath: String): (Path, Seq[(String, String)], Option[DeltaTimeTravelSpec])\n</code></pre> <p><code>parsePathIdentifier</code>...FIXME</p> <p><code>parsePathIdentifier</code> is used when:</p> <ul> <li><code>DeltaTableV2</code> is requested for metadata (for a non-catalog table)</li> </ul>","text":""},{"location":"DeltaDataSource/#readchangefeed","title":"readChangeFeed <p><code>DeltaDataSource</code> utility defines <code>readChangeFeed</code> value to indicate CDC-aware table scan (when it is used as an read option and <code>true</code>).</p> <p><code>readChangeFeed</code> is used alongside the following CDC options:</p> <ul> <li>startingVersion</li> <li>startingTimestamp</li> <li>endingVersion</li> <li>endingTimestamp</li> </ul> <p><code>readChangeFeed</code> is used when:</p> <ul> <li><code>DeltaDataSource</code> is requested to create a BaseRelation</li> </ul>","text":""},{"location":"DeltaErrors/","title":"DeltaErrors Utility","text":""},{"location":"DeltaErrors/#concurrentappendexception","title":"concurrentAppendException <pre><code>concurrentAppendException(\n  conflictingCommit: Option[CommitInfo],\n  partition: String,\n  customRetryMsg: Option[String] = None): ConcurrentAppendException\n</code></pre> <p><code>concurrentAppendException</code> creates a ConcurrentAppendException with the following message:</p> <pre><code>Files were added to [partition] by a concurrent update. [customRetryMsg] | Please try the operation again.\nConflicting commit: [commitinfo]\nRefer to [docs]/concurrency-control.html for more details.\n</code></pre> <p><code>concurrentAppendException</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to check logical conflicts with concurrent updates</li> </ul>","text":""},{"location":"DeltaErrors/#concurrentdeletedeleteexception","title":"concurrentDeleteDeleteException <pre><code>concurrentDeleteDeleteException(\n  conflictingCommit: Option[CommitInfo],\n  file: String): ConcurrentDeleteDeleteException\n</code></pre> <p><code>concurrentDeleteDeleteException</code> creates a ConcurrentDeleteDeleteException with the following message:</p> <pre><code>This transaction attempted to delete one or more files that were deleted (for example [file]) by a concurrent update. Please try the operation again.\nConflicting commit: [commitinfo]\nRefer to [docs]/concurrency-control.html for more details.\n</code></pre> <p><code>concurrentDeleteDeleteException</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to check for logical conflicts with concurrent updates</li> </ul>","text":""},{"location":"DeltaErrors/#concurrentdeletereadexception","title":"concurrentDeleteReadException <pre><code>concurrentDeleteReadException(\n  conflictingCommit: Option[CommitInfo],\n  file: String): ConcurrentDeleteReadException\n</code></pre> <p><code>concurrentDeleteReadException</code> creates a ConcurrentDeleteReadException with the following message:</p> <pre><code>This transaction attempted to read one or more files that were deleted (for example [file]) by a concurrent update. Please try the operation again.\nConflicting commit: [commitinfo]\nRefer to [docs]/concurrency-control.html for more details.\n</code></pre> <p><code>concurrentDeleteReadException</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to check for logical conflicts with concurrent updates</li> </ul>","text":""},{"location":"DeltaErrors/#concurrenttransactionexception","title":"concurrentTransactionException <pre><code>concurrentTransactionException(\n  conflictingCommit: Option[CommitInfo]): ConcurrentTransactionException\n</code></pre> <p><code>concurrentTransactionException</code> creates a ConcurrentTransactionException with the following message:</p> <pre><code>This error occurs when multiple streaming queries are using the same checkpoint to write into this table.\nDid you run multiple instances of the same streaming query at the same time?\nConflicting commit: [commitinfo]\nRefer to [docs]/concurrency-control.html for more details.\n</code></pre> <p><code>concurrentTransactionException</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to check for logical conflicts with concurrent updates</li> </ul>","text":""},{"location":"DeltaErrors/#concurrentwriteexception","title":"concurrentWriteException <pre><code>concurrentWriteException(\n  conflictingCommit: Option[CommitInfo]): ConcurrentWriteException\n</code></pre> <p><code>concurrentWriteException</code> creates a ConcurrentWriteException with the following message:</p> <pre><code>A concurrent transaction has written new data since the current transaction read the table. Please try the operation again.\nConflicting commit: [commitinfo]\nRefer to [docs]/concurrency-control.html for more details.\n</code></pre> <p><code>concurrentWriteException</code>\u00a0is used when:</p> <ul> <li>Convert to Delta command is executed (and <code>DeltaCommand</code> is requested to commitLarge)</li> </ul>","text":""},{"location":"DeltaErrors/#metadatachangedexception","title":"metadataChangedException <pre><code>metadataChangedException(\n  conflictingCommit: Option[CommitInfo]): MetadataChangedException\n</code></pre> <p><code>metadataChangedException</code> creates a MetadataChangedException with the following message:</p> <pre><code>The metadata of the Delta table has been changed by a concurrent update. Please try the operation again.\nConflicting commit: [commitinfo]\nRefer to [docs]/concurrency-control.html for more details.\n</code></pre> <p><code>metadataChangedException</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to check for logical conflicts with concurrent updates</li> </ul>","text":""},{"location":"DeltaErrors/#protocolchangedexception","title":"protocolChangedException <pre><code>protocolChangedException(\n  conflictingCommit: Option[CommitInfo]): ProtocolChangedException\n</code></pre> <p><code>protocolChangedException</code> creates a ProtocolChangedException with the following message:</p> <pre><code>The protocol version of the Delta table has been changed by a concurrent update.\n[additionalInfo]\nPlease try the operation again.\nConflicting commit: [commitinfo]\nRefer to [docs]/concurrency-control.html for more details.\n</code></pre> <p><code>protocolChangedException</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to check for logical conflicts with concurrent updates</li> </ul>","text":""},{"location":"DeltaErrors/#modifyappendonlytableexception","title":"modifyAppendOnlyTableException <pre><code>modifyAppendOnlyTableException: Throwable\n</code></pre> <p><code>modifyAppendOnlyTableException</code> throws an <code>UnsupportedOperationException</code>:</p> <pre><code>This table is configured to only allow appends. If you would like to permit updates or deletes, use 'ALTER TABLE &lt;table_name&gt; SET TBLPROPERTIES (appendOnly=false)'.\n</code></pre> <p><code>modifyAppendOnlyTableException</code> is used when:</p> <ul> <li><code>DeltaLog</code> is requested to assertRemovable</li> </ul>","text":""},{"location":"DeltaErrors/#notnullcolumnmissingexception","title":"notNullColumnMissingException <pre><code>notNullColumnMissingException(\n  constraint: Constraints.NotNull): Throwable\n</code></pre> <p><code>notNullColumnMissingException</code> creates a InvariantViolationException with the following error message:</p> <pre><code>Column [name], which has a NOT NULL constraint, is missing from the data being written into the table.\n</code></pre> <p><code>notNullColumnMissingException</code>\u00a0is used when:</p> <ul> <li><code>DeltaInvariantCheckerExec</code> utility is used to buildInvariantChecks</li> </ul>","text":""},{"location":"DeltaErrors/#reporting-post-commit-hook-failure","title":"Reporting Post-Commit Hook Failure <pre><code>postCommitHookFailedException(\n  failedHook: PostCommitHook,\n  failedOnCommitVersion: Long,\n  extraErrorMessage: String,\n  error: Throwable): Throwable\n</code></pre> <p><code>postCommitHookFailedException</code> throws a <code>RuntimeException</code>:</p> <pre><code>Committing to the Delta table version [failedOnCommitVersion] succeeded but error while executing post-commit hook [failedHook]: [extraErrorMessage]\n</code></pre> <p><code>postCommitHookFailedException</code> is used when:</p> <ul> <li><code>GenerateSymlinkManifestImpl</code> is requested to handleError</li> </ul>","text":""},{"location":"DeltaErrors/#changecolumnmappingmodeonoldprotocol","title":"changeColumnMappingModeOnOldProtocol <pre><code>changeColumnMappingModeOnOldProtocol(\n  oldProtocol: Protocol): Throwable\n</code></pre> <p><code>changeColumnMappingModeOnOldProtocol</code> creates a <code>DeltaColumnMappingUnsupportedException</code> with <code>UNSUPPORTED_COLUMN_MAPPING_PROTOCOL</code> error class.</p> <p><code>changeColumnMappingModeOnOldProtocol</code> is used when:</p> <ul> <li><code>DeltaColumnMappingBase</code> is requested to verifyAndUpdateMetadataChange</li> </ul>","text":""},{"location":"DeltaErrors/#converttodeltawithcolumnmappingnotsupported","title":"convertToDeltaWithColumnMappingNotSupported <pre><code>convertToDeltaWithColumnMappingNotSupported(\n  mode: DeltaColumnMappingMode): Throwable\n</code></pre> <p><code>convertToDeltaWithColumnMappingNotSupported</code> creates a <code>DeltaColumnMappingUnsupportedException</code> with <code>UNSUPPORTED_COLUMN_MAPPING_CONVERT_TO_DELTA</code> error class.</p> <p><code>convertToDeltaWithColumnMappingNotSupported</code> is used when:</p> <ul> <li><code>ConvertToDeltaCommandBase</code> is requested to checkColumnMapping</li> </ul>","text":""},{"location":"DeltaFileFormat/","title":"DeltaFileFormat","text":"<p><code>DeltaFileFormat</code> is an abstraction of format metadata that specify the file format of a delta table.</p>"},{"location":"DeltaFileFormat/#contract","title":"Contract","text":""},{"location":"DeltaFileFormat/#fileformat","title":"FileFormat <pre><code>fileFormat: FileFormat\n</code></pre> <p><code>FileFormat</code> (Spark SQL) of this delta table</p> <p>Default: DeltaParquetFileFormat (with the columnMappingMode and the schema of the given Metadata)</p> <p>Used when:</p> <ul> <li><code>DeltaLog</code> is requested for a relation (in batch queries) and a DataFrame</li> <li><code>DeltaCommand</code> is requested for a relation</li> <li><code>MergeIntoCommand</code> is requested to buildTargetPlanWithFiles</li> <li><code>TransactionalWrite</code> is requested to write data out</li> </ul>","text":""},{"location":"DeltaFileFormat/#metadata","title":"Metadata <pre><code>metadata: Metadata\n</code></pre> <p>Current Metadata</p> <p>Used when:</p> <ul> <li><code>DeltaFileFormat</code> is requested for the FileFormat</li> </ul>","text":""},{"location":"DeltaFileFormat/#sparksession","title":"SparkSession <pre><code>spark: SparkSession\n</code></pre> <p>Current <code>SparkSession</code> (Spark SQL)</p>","text":""},{"location":"DeltaFileFormat/#implementations","title":"Implementations","text":"<ul> <li>DeltaLog</li> </ul>"},{"location":"DeltaFileOperations/","title":"DeltaFileOperations Utilities","text":""},{"location":"DeltaFileOperations/#listusinglogstore","title":"listUsingLogStore <pre><code>listUsingLogStore(\n  logStore: LogStore,\n  subDirs: Iterator[String],\n  recurse: Boolean,\n  hiddenFileNameFilter: String =&gt; Boolean): Iterator[SerializableFileStatus]\n</code></pre> <p><code>listUsingLogStore</code>...FIXME</p> <p><code>listUsingLogStore</code>\u00a0is used when:</p> <ul> <li><code>DeltaFileOperations</code> utility is used to recurseDirectories, recursiveListDirs and localListDirs</li> </ul>","text":""},{"location":"DeltaFileOperations/#locallistdirs","title":"localListDirs <pre><code>localListDirs(\n  spark: SparkSession,\n  dirs: Seq[String],\n  recursive: Boolean = true,\n  fileFilter: String =&gt; Boolean = defaultHiddenFileFilter): Seq[SerializableFileStatus]\n</code></pre> <p><code>localListDirs</code>...FIXME</p> <p><code>localListDirs</code> seems not used.</p>","text":""},{"location":"DeltaFileOperations/#recursedirectories","title":"recurseDirectories <pre><code>recurseDirectories(\n  logStore: LogStore,\n  filesAndDirs: Iterator[SerializableFileStatus],\n  hiddenFileNameFilter: String =&gt; Boolean): Iterator[SerializableFileStatus]\n</code></pre> <p><code>recurseDirectories</code>...FIXME</p> <p><code>recurseDirectories</code>\u00a0is used when:</p> <ul> <li><code>DeltaFileOperations</code> utility is used to listUsingLogStore and recursiveListDirs</li> </ul>","text":""},{"location":"DeltaFileOperations/#recursivelistdirs","title":"recursiveListDirs <pre><code>recursiveListDirs(\n  spark: SparkSession,\n  subDirs: Seq[String],\n  hadoopConf: Broadcast[SerializableConfiguration],\n  hiddenFileNameFilter: String =&gt; Boolean = defaultHiddenFileFilter,\n  fileListingParallelism: Option[Int] = None): Dataset[SerializableFileStatus]\n</code></pre> <p><code>recursiveListDirs</code>...FIXME</p> <p><code>recursiveListDirs</code>\u00a0is used when:</p> <ul> <li><code>ManualListingFileManifest</code> is requested to doList</li> <li><code>VacuumCommand</code> utility is used to gc</li> </ul>","text":""},{"location":"DeltaFileOperations/#trydeletenonrecursive","title":"tryDeleteNonRecursive <pre><code>tryDeleteNonRecursive(\n  fs: FileSystem,\n  path: Path,\n  tries: Int = 3): Boolean\n</code></pre> <p><code>tryDeleteNonRecursive</code>...FIXME</p> <p><code>tryDeleteNonRecursive</code>\u00a0is used when:</p> <ul> <li><code>VacuumCommandImpl</code> is requested to delete</li> </ul>","text":""},{"location":"DeltaHistoryManager/","title":"DeltaHistoryManager","text":"<p><code>DeltaHistoryManager</code> is used for version and commit history of a delta table.</p>"},{"location":"DeltaHistoryManager/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaHistoryManager</code> takes the following to be created:</p> <ul> <li> DeltaLog <li> Maximum number of keys (default: <code>1000</code>) <p><code>DeltaHistoryManager</code> is created\u00a0when:</p> <ul> <li><code>DeltaLog</code> is requested for one</li> <li><code>DeltaTableOperations</code> is requested to execute history command</li> </ul>"},{"location":"DeltaHistoryManager/#version-and-commit-history","title":"Version and Commit History <pre><code>getHistory(\n  start: Long,\n  end: Option[Long] = None): Seq[CommitInfo]\ngetHistory(\n  limitOpt: Option[Int]): Seq[CommitInfo]\n</code></pre> <p><code>getHistory</code>...FIXME</p> <p><code>getHistory</code>\u00a0is used when:</p> <ul> <li><code>DeltaTableOperations</code> is requested to executeHistory (for DeltaTable.history operator)</li> <li>DescribeDeltaHistoryCommand is executed (for DESCRIBE HISTORY SQL command)</li> </ul>","text":""},{"location":"DeltaHistoryManager/#getcommitinfo-utility","title":"getCommitInfo Utility <pre><code>getCommitInfo(\n  logStore: LogStore,\n  basePath: Path,\n  version: Long): CommitInfo\n</code></pre> <p><code>getCommitInfo</code>...FIXME</p>","text":""},{"location":"DeltaHistoryManager/#getactivecommitattime","title":"getActiveCommitAtTime <pre><code>getActiveCommitAtTime(\n  timestamp: Timestamp,\n  canReturnLastCommit: Boolean,\n  mustBeRecreatable: Boolean = true,\n  canReturnEarliestCommit: Boolean = false): Commit\n</code></pre> <p><code>getActiveCommitAtTime</code>...FIXME</p> <p><code>getActiveCommitAtTime</code>\u00a0is used when:</p> <ul> <li><code>DeltaTableUtils</code> utility is used to resolveTimeTravelVersion</li> <li><code>DeltaSource</code> is requested for getStartingVersion</li> </ul>","text":""},{"location":"DeltaJobStatisticsTracker/","title":"DeltaJobStatisticsTracker","text":"<p><code>DeltaJobStatisticsTracker</code> is a <code>WriteJobStatsTracker</code> (Spark SQL) for per-file statistics collection (when spark.databricks.delta.stats.collect is enabled).</p>"},{"location":"DeltaJobStatisticsTracker/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaJobStatisticsTracker</code> takes the following to be created:</p> <ul> <li> Hadoop Configuration <li> Hadoop Path (to a delta table's data directory) <li> Data non-partitioned column <code>Attribute</code>s (Spark SQL) <li> Statistics Column <code>Expression</code> (Spark SQL) <p><code>DeltaJobStatisticsTracker</code> is created when:</p> <ul> <li><code>TransactionalWrite</code> is requested to write data out</li> </ul>"},{"location":"DeltaJobStatisticsTracker/#recorded-per-file-statistics","title":"Recorded Per-File Statistics <pre><code>recordedStats: Map[String, String]\n</code></pre> <p><code>recordedStats</code> is a collection of recorded per-file statistics (that are collected upon processing per-job write task statistics).</p> <p><code>recordedStats</code> is used when:</p> <ul> <li><code>TransactionalWrite</code> is requested to write data out</li> </ul>","text":""},{"location":"DeltaJobStatisticsTracker/#processing-per-job-write-task-statistics","title":"Processing Per-Job Write Task Statistics <pre><code>processStats(\n  stats: Seq[WriteTaskStats],\n  jobCommitTime: Long): Unit\n</code></pre> <p><code>processStats</code> extracts a <code>DeltaFileStatistics</code> (from the given <code>WriteTaskStats</code>) to access collected per-file statistics.</p> <p><code>processStats</code> is part of the <code>WriteJobStatsTracker</code> (Spark SQL) abstraction.</p>","text":""},{"location":"DeltaLog/","title":"DeltaLog","text":"<p><code>DeltaLog</code> is a transaction log (change log) of all the changes to (the state of) a delta table.</p>"},{"location":"DeltaLog/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaLog</code> takes the following to be created:</p> <ul> <li> Log directory (Hadoop Path) <li> Data directory (Hadoop Path) <li> Options (<code>Map[String, String]</code>) <li> <code>Clock</code> <p><code>DeltaLog</code> is created (indirectly via DeltaLog.apply utility) when:</p> <ul> <li>DeltaLog.forTable utility is used</li> </ul>"},{"location":"DeltaLog/#_delta_log-metadata-directory","title":"_delta_log Metadata Directory <p><code>DeltaLog</code> uses _delta_log metadata directory for the transaction log of a Delta table.</p> <p>The <code>_delta_log</code> directory is in the given data path directory (when created using DeltaLog.forTable utility).</p> <p>The <code>_delta_log</code> directory is resolved (in the DeltaLog.apply utility) using the application-wide Hadoop Configuration.</p> <p>Once resolved and turned into a qualified path, the <code>_delta_log</code> directory is cached.</p>","text":""},{"location":"DeltaLog/#deltalog-cache","title":"DeltaLog Cache <pre><code>deltaLogCache: Cache[(Path, Map[String, String]), DeltaLog]\n</code></pre> <p><code>DeltaLog</code> uses Guava's Cache as a cache of <code>DeltaLog</code>s by their HDFS-qualified _delta_log directories (with their<code>fs.</code>-prefixed file system options).</p> <p><code>deltaLogCache</code> is part of <code>DeltaLog</code> Scala object and so becomes an application-wide cache by design (an object in Scala is available as a single instance).</p>","text":""},{"location":"DeltaLog/#caching-deltalog-instance","title":"Caching DeltaLog Instance","text":"<p>A new instance of <code>DeltaLog</code> is added when DeltaLog.apply utility is used and the instance is not available for a path (and file system options).</p>"},{"location":"DeltaLog/#cache-size","title":"Cache Size","text":"<p>The size of the cache is controlled by <code>delta.log.cacheSize</code> system property.</p>"},{"location":"DeltaLog/#deltalog-instance-expiration","title":"DeltaLog Instance Expiration","text":"<p><code>DeltaLog</code>s expire and are automatically removed from the <code>deltaLogCache</code> after 60 minutes (non-configurable) of inactivity. Upon expiration, <code>deltaLogCache</code> requests the Snapshot of the <code>DeltaLog</code> to uncache.</p>"},{"location":"DeltaLog/#cache-clearance","title":"Cache Clearance","text":"<p><code>deltaLogCache</code> is invalidated:</p> <ul> <li> <p>For a delta table using DeltaLog.invalidateCache utility</p> </li> <li> <p>For all delta tables using DeltaLog.clearCache utility</p> </li> </ul>"},{"location":"DeltaLog/#deltalogfortable","title":"DeltaLog.forTable <pre><code>// There are many forTable's\nforTable(...): DeltaLog\n</code></pre> <p><code>forTable</code> is an utility that creates a DeltaLog with _delta_log directory (in the given <code>dataPath</code> directory).</p>","text":""},{"location":"DeltaLog/#demo-creating-deltalog","title":"Demo: Creating DeltaLog <pre><code>import org.apache.spark.sql.SparkSession\nassert(spark.isInstanceOf[SparkSession])\n\nval dataPath = \"/tmp/delta/t1\"\nimport org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, dataPath)\n\nimport org.apache.hadoop.fs.Path\nval expected = new Path(s\"file:$dataPath/_delta_log/_last_checkpoint\")\nassert(deltaLog.LAST_CHECKPOINT == expected)\n</code></pre>","text":""},{"location":"DeltaLog/#tableexists","title":"tableExists <pre><code>tableExists: Boolean\n</code></pre> <p><code>tableExists</code> requests the current Snapshot for the version and checks out whether it is <code>0</code> or higher.</p> <p>is used when:</p> <ul> <li><code>DeltaTable</code> utility is used to isDeltaTable</li> <li>DeltaUnsupportedOperationsCheck logical check rule is executed</li> <li><code>DeltaTableV2</code> is requested to toBaseRelation</li> </ul>","text":""},{"location":"DeltaLog/#accessing-current-version","title":"Accessing Current Version <p>A common idiom (if not the only way) to know the current version of the delta table is to request the <code>DeltaLog</code> for the current state (snapshot) and then for the version.</p> <pre><code>import org.apache.spark.sql.delta.DeltaLog\nassert(deltaLog.isInstanceOf[DeltaLog])\n\nval deltaVersion = deltaLog.snapshot.version\nscala&gt; println(deltaVersion)\n5\n</code></pre>","text":""},{"location":"DeltaLog/#initialization","title":"Initialization <p>When created, <code>DeltaLog</code> does the following:</p> <ol> <li> <p>Creates the LogStore based on spark.delta.logStore.class configuration property</p> </li> <li> <p>Initializes the current snapshot</p> </li> <li> <p>Updates state of the delta table when there is no metadata checkpoint (e.g. the version of the state is <code>-1</code>)</p> </li> </ol> <p>In other words, the version of (the <code>DeltaLog</code> of) a delta table is at version <code>0</code> at the very minimum.</p> <pre><code>assert(deltaLog.snapshot.version &gt;= 0)\n</code></pre>","text":""},{"location":"DeltaLog/#filterfilelist","title":"filterFileList <pre><code>filterFileList(\n  partitionSchema: StructType,\n  files: DataFrame,\n  partitionFilters: Seq[Expression],\n  partitionColumnPrefixes: Seq[String] = Nil): DataFrame\n</code></pre> <p><code>filterFileList</code>...FIXME</p> <p><code>filterFileList</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to checkAndRetry</li> <li><code>PartitionFiltering</code> is requested to filesForScan</li> <li><code>WriteIntoDelta</code> is requested to write</li> <li><code>SnapshotIterator</code> is requested to iterator</li> <li><code>TahoeBatchFileIndex</code> is requested to matchingFiles</li> <li><code>DeltaDataSource</code> utility is requested to verifyAndCreatePartitionFilters</li> </ul>","text":""},{"location":"DeltaLog/#fileformats","title":"FileFormats <p><code>DeltaLog</code> defines two <code>FileFormat</code>s (Spark SQL):</p> <ul> <li> <p> <code>ParquetFileFormat</code> for indices of delta files  <li> <p> <code>JsonFileFormat</code> for indices of checkpoint files   <p>These <code>FileFormat</code>s are used to create DeltaLogFileIndexes for Snapshots that in turn used them for stateReconstruction.</p>","text":""},{"location":"DeltaLog/#logstore","title":"LogStore <p><code>DeltaLog</code> uses a LogStore for...FIXME</p>","text":""},{"location":"DeltaLog/#executing-single-threaded-operation-in-new-transaction","title":"Executing Single-Threaded Operation in New Transaction <pre><code>withNewTransaction[T](\n  thunk: OptimisticTransaction =&gt; T): T\n</code></pre> <p><code>withNewTransaction</code> starts a new transaction (that is active for the whole thread) and executes the given <code>thunk</code> block.</p> <p>In the end, <code>withNewTransaction</code> makes the transaction no longer active.</p> <p><code>withNewTransaction</code> is used when:</p> <ul> <li> <p>DeleteCommand, MergeIntoCommand, UpdateCommand, and WriteIntoDelta commands are executed</p> </li> <li> <p><code>DeltaSink</code> is requested to add a streaming micro-batch</p> </li> </ul>","text":""},{"location":"DeltaLog/#starting-new-transaction","title":"Starting New Transaction <pre><code>startTransaction(): OptimisticTransaction\n</code></pre> <p><code>startTransaction</code> updates and creates a new OptimisticTransaction (for this <code>DeltaLog</code>).</p>  <p>Note</p> <p><code>startTransaction</code> is a \"subset\" of withNewTransaction.</p>  <p><code>startTransaction</code> is used when:</p> <ul> <li> <p><code>DeltaLog</code> is requested to upgradeProtocol</p> </li> <li> <p><code>AlterDeltaTableCommand</code> is requested to startTransaction</p> </li> <li> <p>ConvertToDeltaCommand and CreateDeltaTableCommand are executed</p> </li> </ul>","text":""},{"location":"DeltaLog/#throwing-unsupportedoperationexception-for-append-only-tables","title":"Throwing UnsupportedOperationException for Append-Only Tables <pre><code>assertRemovable(): Unit\n</code></pre> <p><code>assertRemovable</code> throws an <code>UnsupportedOperationException</code> for the appendOnly table property (in the Metadata) enabled (<code>true</code>):</p> <pre><code>This table is configured to only allow appends. If you would like to permit updates or deletes, use 'ALTER TABLE &lt;table_name&gt; SET TBLPROPERTIES (appendOnly=false)'.\n</code></pre> <p><code>assertRemovable</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to prepareCommit</li> <li>DeleteCommand, UpdateCommand, WriteIntoDelta (with <code>Overwrite</code> mode) are executed</li> <li><code>DeltaSink</code> is requested to addBatch (with <code>Complete</code> output mode)</li> </ul>","text":""},{"location":"DeltaLog/#metadata","title":"metadata <pre><code>metadata: Metadata\n</code></pre> <p><code>metadata</code> is part of the Checkpoints abstraction.</p> <p><code>metadata</code> requests the current Snapshot for the metadata or creates a new one (if the current Snapshot is not initialized).</p>","text":""},{"location":"DeltaLog/#update","title":"update <pre><code>update(\n  stalenessAcceptable: Boolean = false): Snapshot\n</code></pre> <p><code>update</code> branches off based on a combination of flags: the given <code>stalenessAcceptable</code> and isSnapshotStale.</p> <p>For the <code>stalenessAcceptable</code> not acceptable (default) and the snapshot not stale, <code>update</code> simply acquires the deltaLogLock lock and updateInternal (with <code>isAsync</code> flag off).</p> <p>For all other cases, <code>update</code>...FIXME</p> <p><code>update</code> is used when:</p> <ul> <li> <p><code>DeltaHistoryManager</code> is requested to getHistory, getActiveCommitAtTime, and checkVersionExists</p> </li> <li> <p><code>DeltaLog</code> is created (with no checkpoint created), and requested to startTransaction and withNewTransaction</p> </li> <li> <p><code>OptimisticTransactionImpl</code> is requested to doCommit and checkAndRetry</p> </li> <li> <p><code>ConvertToDeltaCommand</code> is requested to run and streamWrite</p> </li> <li> <p><code>VacuumCommand</code> utility is used to gc</p> </li> <li> <p><code>TahoeLogFileIndex</code> is requested for the (historical or latest) snapshot</p> </li> <li> <p><code>DeltaDataSource</code> is requested for a relation</p> </li> </ul>","text":""},{"location":"DeltaLog/#tryupdate","title":"tryUpdate <pre><code>tryUpdate(\n  isAsync: Boolean = false): Snapshot\n</code></pre> <p><code>tryUpdate</code>...FIXME</p>","text":""},{"location":"DeltaLog/#snapshot","title":"Snapshot <pre><code>snapshot: Snapshot\n</code></pre> <p><code>snapshot</code> returns the current snapshot.</p> <p><code>snapshot</code> is used when:</p> <ul> <li> <p>OptimisticTransaction is created</p> </li> <li> <p><code>Checkpoints</code> is requested to checkpoint</p> </li> <li> <p><code>DeltaLog</code> is requested for the metadata, to upgradeProtocol, getSnapshotAt, createRelation</p> </li> <li> <p><code>OptimisticTransactionImpl</code> is requested to getNextAttemptVersion</p> </li> <li> <p>DeleteCommand, DeltaGenerateCommand, DescribeDeltaDetailCommand, UpdateCommand commands are executed</p> </li> <li> <p>GenerateSymlinkManifest is executed</p> </li> <li> <p><code>DeltaCommand</code> is requested to buildBaseRelation</p> </li> <li> <p><code>TahoeFileIndex</code> is requested for the table version, partitionSchema</p> </li> <li> <p><code>TahoeLogFileIndex</code> is requested for the table size</p> </li> <li> <p><code>DeltaDataSource</code> is requested for the schema of the streaming delta source</p> </li> <li> <p>DeltaSource is created and requested for the getStartingOffset, getBatch</p> </li> </ul>","text":""},{"location":"DeltaLog/#current-state-snapshot","title":"Current State Snapshot <pre><code>currentSnapshot: Snapshot\n</code></pre> <p><code>currentSnapshot</code> is a Snapshot based on the metadata checkpoint if available or a new <code>Snapshot</code> instance (with version being <code>-1</code>).</p>  <p>Note</p> <p>For a new <code>Snapshot</code> instance (with version being <code>-1</code>) <code>DeltaLog</code> immediately updates the state.</p>  <p>Internally, <code>currentSnapshot</code>...FIXME</p> <p><code>currentSnapshot</code> is available using snapshot method.</p> <p><code>currentSnapshot</code> is used when:</p> <ul> <li><code>DeltaLog</code> is requested to updateInternal, update and tryUpdate</li> </ul>","text":""},{"location":"DeltaLog/#creating-insertable-hadoopfsrelation-for-batch-queries","title":"Creating Insertable HadoopFsRelation For Batch Queries <pre><code>createRelation(\n  partitionFilters: Seq[Expression] = Nil,\n  snapshotToUseOpt: Option[Snapshot] = None,\n  isTimeTravelQuery: Boolean = false,\n  cdcOptions: CaseInsensitiveStringMap = CaseInsensitiveStringMap.empty): BaseRelation\n</code></pre> <p><code>createRelation</code>...FIXME</p> <p>With non-empty <code>cdcOptions</code>, <code>createRelation</code> creates a CDC-aware relation (that represents data change between two snapshots of the table).</p>  <p>When <code>cdcOptions</code> is non-empty</p> <p><code>cdcOptions</code> is empty by default and can only be specified when <code>DeltaTableV2</code> is created.</p>  <p><code>createRelation</code> creates a TahoeLogFileIndex for the data path, the given <code>partitionFilters</code> and a version (if defined).</p> <p><code>createRelation</code>...FIXME</p> <p>In the end, <code>createRelation</code> creates a <code>HadoopFsRelation</code> for the <code>TahoeLogFileIndex</code> and...FIXME. The <code>HadoopFsRelation</code> is also an InsertableRelation.</p> <p><code>createRelation</code> is used when:</p> <ul> <li><code>DeltaTableV2</code> is requested to toBaseRelation</li> <li><code>WriteIntoDeltaBuilder</code> is requested to buildForV1Write</li> <li><code>DeltaDataSource</code> is requested for a writable relation</li> </ul>","text":""},{"location":"DeltaLog/#insert","title":"insert <pre><code>insert(\n  data: DataFrame,\n  overwrite: Boolean): Unit\n</code></pre> <p><code>insert</code>...FIXME</p> <p><code>insert</code> is part of the <code>InsertableRelation</code> (Spark SQL) abstraction.</p>","text":""},{"location":"DeltaLog/#retrieving-state-of-delta-table-at-given-version","title":"Retrieving State Of Delta Table At Given Version <pre><code>getSnapshotAt(\n  version: Long,\n  commitTimestamp: Option[Long] = None,\n  lastCheckpointHint: Option[CheckpointInstance] = None): Snapshot\n</code></pre> <p><code>getSnapshotAt</code>...FIXME</p> <p><code>getSnapshotAt</code> is used when:</p> <ul> <li> <p><code>DeltaLog</code> is requested for a relation, and to updateInternal</p> </li> <li> <p><code>DeltaSource</code> is requested for the snapshot of a delta table at a given version</p> </li> <li> <p><code>TahoeLogFileIndex</code> is requested for historicalSnapshotOpt</p> </li> </ul>","text":""},{"location":"DeltaLog/#checkpoint-interval","title":"Checkpoint Interval <pre><code>checkpointInterval: Int\n</code></pre> <p><code>checkpointInterval</code> is the current value of checkpointInterval table property (from the Metadata).</p> <p><code>checkpointInterval</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to postCommit</li> </ul>","text":""},{"location":"DeltaLog/#changes-actions-of-delta-version-and-later","title":"Changes (Actions) Of Delta Version And Later <pre><code>getChanges(\n  startVersion: Long): Iterator[(Long, Seq[Action])]\n</code></pre> <p><code>getChanges</code> gives all the Actions (changes) per delta log file for the given <code>startVersion</code> (inclusive) of a delta table and later.</p> <pre><code>val dataPath = \"/tmp/delta/users\"\nimport org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, dataPath)\nassert(deltaLog.isInstanceOf[DeltaLog])\nval changesPerVersion = deltaLog.getChanges(startVersion = 0)\n</code></pre>  <p>Internally, <code>getChanges</code> requests the LogStore for files that are lexicographically greater or equal to the delta log file for the given <code>startVersion</code> (in the logPath) and leaves only delta log files (e.g. files with numbers only as file name and <code>.json</code> file extension).</p> <p>For every delta file, <code>getChanges</code> requests the LogStore to read the JSON content (every line is an action), and then deserializes it to an action.</p>","text":""},{"location":"DeltaLog/#creating-dataframe-from-addfiles","title":"Creating DataFrame (From AddFiles) <pre><code>createDataFrame(\n  snapshot: Snapshot,\n  addFiles: Seq[AddFile],\n  isStreaming: Boolean = false,\n  actionTypeOpt: Option[String] = None): DataFrame\n</code></pre> <p><code>createDataFrame</code> uses the action type based on the optional action type (if defined) or uses the following based on the <code>isStreaming</code> flag:</p> <ul> <li>streaming when <code>isStreaming</code> flag is enabled (<code>true</code>)</li> <li>batch when <code>isStreaming</code> flag is disabled (<code>false</code>)</li> </ul> <p><code>createDataFrame</code> creates a new TahoeBatchFileIndex (for the action type, and the given AddFiles, this <code>DeltaLog</code>, and Snapshot).</p> <p><code>createDataFrame</code> creates a <code>HadoopFsRelation</code> (Spark SQL) with the <code>TahoeBatchFileIndex</code> and the other properties based on the given <code>Snapshot</code> (and the associated Metadata).</p> <p>In the end, <code>createDataFrame</code> creates a <code>DataFrame</code> with (a logical query plan with) a <code>LogicalRelation</code> (Spark SQL) over the <code>HadoopFsRelation</code>.</p> <p><code>createDataFrame</code> is used when:</p> <ul> <li>AlterTableAddConstraintDeltaCommand is executed</li> <li>MergeIntoCommand is executed (and requested to buildTargetPlanWithFiles)</li> <li><code>OptimizeExecutor</code> is requested to runOptimizeBinJob</li> <li><code>DeltaSourceBase</code> is requested to createDataFrame</li> <li><code>StatisticsCollection</code> utility is used to recompute</li> </ul>","text":""},{"location":"DeltaLog/#demo-deltalogcreatedataframe","title":"Demo: DeltaLog.createDataFrame <p>Create a delta table with some data to work with. We need data files for this demo.</p> Scala   <pre><code>sql(\"DROP TABLE IF EXISTS delta_demo\")\nspark.range(5).write.format(\"delta\").saveAsTable(\"delta_demo\")\n</code></pre>    <p>Review the data (parquet) files created. These are our AddFiles.</p> <pre><code>$ tree spark-warehouse/delta_demo/\nspark-warehouse/delta_demo/\n\u251c\u2500\u2500 _delta_log\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 00000000000000000000.json\n\u251c\u2500\u2500 part-00000-993a2fad-3643-48f5-b2be-d1b9036fb29d-c000.snappy.parquet\n\u251c\u2500\u2500 part-00003-74b48672-e869-47fc-818b-e422062c1427-c000.snappy.parquet\n\u251c\u2500\u2500 part-00006-91497579-5f25-42e6-82c9-1dc8416fe987-c000.snappy.parquet\n\u251c\u2500\u2500 part-00009-6f3e75fd-828d-4e1b-9d38-7aa65f928a9e-c000.snappy.parquet\n\u251c\u2500\u2500 part-00012-309fbcfe-4d34-45f7-b414-034f676480c6-c000.snappy.parquet\n\u2514\u2500\u2500 part-00015-5d72e873-e4df-493a-8bcf-2a3af9dfd636-c000.snappy.parquet\n\n1 directory, 7 files\n</code></pre> <p>Let's load the delta table.</p> Scala   <pre><code>// FIXME I feel there should be a better way to access a DeltaLog\nval tableName = \"delta_demo\"\nval tableId = spark.sessionState.sqlParser.parseTableIdentifier(tableName)\nval tbl = spark.sessionState.catalog.getTableMetadata(tableId)\nimport org.apache.spark.sql.delta.catalog.DeltaTableV2\nimport org.apache.hadoop.fs.Path\nval table: DeltaTableV2 = DeltaTableV2(\n  spark, new Path(tbl.location), Some(tbl), Some(tableName))\n</code></pre>    <p>We've finally got the DeltaTableV2 so we can proceed.</p> Scala   <pre><code>val txn = table.deltaLog.startTransaction()\n// FIXME Create a fake collection of AddFiles\n// We could avoid transactions and the other extra steps\n// that blur what is demo'ed\nimport org.apache.spark.sql.delta.actions.AddFile\nval fakeAddFile = AddFile(\n  path = \"/a/fake/file/path\",\n  partitionValues = Map.empty,\n  size = 10,\n  modificationTime = 0,\n  dataChange = false)\n// val addFiles: Seq[AddFile] = txn.filterFiles()\nval addFiles = Seq(fakeAddFile)\nval actionType = Some(\"createDataFrame Demo\")\nval df = txn.snapshot.deltaLog.createDataFrame(\n  txn.snapshot,\n  addFiles,\n  actionTypeOpt = actionType)\n</code></pre>    <p>Up to this point, all should work just fine (since no addfiles were checked whether they are available or not).</p> <p>Let's trigger an action to see what happens when Spark SQL (with Delta Lake) decides to access the data.</p> <pre><code>df.show\n</code></pre> <p>The above <code>show</code> action will surely lead to an exception (since the fake file does not really exist).</p> <pre><code>scala&gt; df.show\n22/07/13 14:00:39 ERROR Executor: Exception in task 0.0 in stage 19.0 (TID 179)\njava.io.FileNotFoundException:\nFile /a/fake/file/path does not exist\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n\n    at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:506)\n    at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:130)\n    at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:187)\n    at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n    at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)\n    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n    at org.apache.spark.scheduler.Task.run(Task.scala:131)\n    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\n22/07/13 14:00:39 WARN TaskSetManager: Lost task 0.0 in stage 19.0 (TID 179) (localhost executor driver): java.io.FileNotFoundException:\nFile /a/fake/file/path does not exist\n</code></pre>","text":""},{"location":"DeltaLog/#minfileretentiontimestamp","title":"minFileRetentionTimestamp <pre><code>minFileRetentionTimestamp: Long\n</code></pre> <p><code>minFileRetentionTimestamp</code> is the timestamp that is tombstoneRetentionMillis before the current time (per the given Clock).</p> <p><code>minFileRetentionTimestamp</code> is used when:</p> <ul> <li> <p><code>DeltaLog</code> is requested for the currentSnapshot, to updateInternal, and to getSnapshotAt</p> </li> <li> <p><code>VacuumCommand</code> is requested for garbage collecting of a delta table</p> </li> </ul>","text":""},{"location":"DeltaLog/#tombstoneretentionmillis","title":"tombstoneRetentionMillis <pre><code>tombstoneRetentionMillis: Long\n</code></pre> <p><code>tombstoneRetentionMillis</code> gives the value of deletedFileRetentionDuration table property (from the Metadata).</p> <p><code>tombstoneRetentionMillis</code> is used when:</p> <ul> <li> <p><code>DeltaLog</code> is requested for minFileRetentionTimestamp</p> </li> <li> <p><code>VacuumCommand</code> is requested for garbage collecting of a delta table</p> </li> </ul>","text":""},{"location":"DeltaLog/#updateinternal","title":"updateInternal <pre><code>updateInternal(\n  isAsync: Boolean): Snapshot\n</code></pre> <p><code>updateInternal</code>...FIXME</p> <p><code>updateInternal</code> is used when:</p> <ul> <li><code>DeltaLog</code> is requested to update (directly or via tryUpdate)</li> </ul>","text":""},{"location":"DeltaLog/#invalidating-cached-deltalog-instance-by-path","title":"Invalidating Cached DeltaLog Instance By Path <pre><code>invalidateCache(\n  spark: SparkSession,\n  dataPath: Path): Unit\n</code></pre> <p><code>invalidateCache</code>...FIXME</p> <p><code>invalidateCache</code> is a public API and does not seem to be used at all.</p>","text":""},{"location":"DeltaLog/#protocolread","title":"protocolRead <pre><code>protocolRead(\n  protocol: Protocol): Unit\n</code></pre> <p><code>protocolRead</code>...FIXME</p> <p><code>protocolRead</code> is used when:</p> <ul> <li> <p><code>OptimisticTransactionImpl</code> is requested to validate and retry a commit</p> </li> <li> <p>Snapshot is created</p> </li> <li> <p><code>DeltaSource</code> is requested to verifyStreamHygieneAndFilterAddFiles</p> </li> </ul>","text":""},{"location":"DeltaLog/#upgradeprotocol","title":"upgradeProtocol <pre><code>upgradeProtocol(\n  newVersion: Protocol = Protocol()): Unit\n</code></pre> <p><code>upgradeProtocol</code>...FIXME</p> <p><code>upgradeProtocol</code> is used when:</p> <ul> <li><code>DeltaTable</code> is requested to upgradeTableProtocol</li> </ul>","text":""},{"location":"DeltaLog/#logstoreprovider","title":"LogStoreProvider <p><code>DeltaLog</code> is a LogStoreProvider.</p>","text":""},{"location":"DeltaLog/#looking-up-cached-or-creating-new-deltalog-instance","title":"Looking Up Cached Or Creating New DeltaLog Instance <pre><code>apply(\n  spark: SparkSession,\n  rawPath: Path,\n  clock: Clock = new SystemClock): DeltaLog // (1)!\napply(\n  spark: SparkSession,\n  rawPath: Path,\n  options: Map[String, String],\n  clock: Clock): DeltaLog\n</code></pre> <ol> <li>Uses empty <code>options</code></li> </ol>  <p>Note</p> <p><code>rawPath</code> is a Hadoop Path to the _delta_log directory at the root of the data of a delta table.</p>  <p><code>apply</code> creates a Hadoop <code>Configuration</code> (perhaps with <code>fs.</code>-prefixed options when spark.databricks.delta.loadFileSystemConfigsFromDataFrameOptions configuration property is enabled).</p> <p><code>apply</code> resolves the raw path to be HDFS-qualified (using the given Hadoop <code>Path</code> to get a Hadoop <code>FileSystem</code>).</p> <p>In the end, <code>apply</code> looks up a <code>DeltaLog</code> for the HDFS-qualified path (with the file system options) in the deltaLogCache or creates (and caches) a new DeltaLog.</p>","text":""},{"location":"DeltaLog/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.DeltaLog</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.delta.DeltaLog=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"DeltaLogFileIndex/","title":"DeltaLogFileIndex","text":"<p><code>DeltaLogFileIndex</code> is a <code>FileIndex</code> (Spark SQL) for Snapshot (for the commit and checkpoint files).</p>"},{"location":"DeltaLogFileIndex/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaLogFileIndex</code> takes the following to be created:</p> <ul> <li>FileFormat</li> <li> Files (as Hadoop FileStatuses) <p>While being created, <code>DeltaLogFileIndex</code> prints out the following INFO message to the logs:</p> <pre><code>Created [this]\n</code></pre> <p><code>DeltaLogFileIndex</code> is created (indirectly using apply utility) when <code>Snapshot</code> is requested for <code>DeltaLogFileIndex</code> for commit or checkpoint files.</p>"},{"location":"DeltaLogFileIndex/#fileformat","title":"FileFormat <p><code>DeltaLogFileIndex</code> is given a <code>FileFormat</code> (Spark SQL) when created:</p> <ul> <li><code>JsonFileFormat</code> (Spark SQL) for commit files</li> <li><code>ParquetFileFormat</code> (Spark SQL) for checkpoint files</li> </ul>","text":""},{"location":"DeltaLogFileIndex/#text-representation","title":"Text Representation <pre><code>toString: String\n</code></pre> <p><code>toString</code> returns the following (using the given FileFormat, the number of files and their estimated size):</p> <pre><code>DeltaLogFileIndex([format], numFilesInSegment: [files], totalFileSize: [sizeInBytes])\n</code></pre>","text":""},{"location":"DeltaLogFileIndex/#creating-deltalogfileindex","title":"Creating DeltaLogFileIndex <pre><code>apply(\n  format: FileFormat,\n  files: Seq[FileStatus]): Option[DeltaLogFileIndex]\n</code></pre> <p><code>apply</code> creates a new <code>DeltaLogFileIndex</code> (for a non-empty collection of files).</p> <p><code>apply</code> is used when <code>Snapshot</code> is requested for <code>DeltaLogFileIndex</code> for commit or checkpoint files.</p>","text":""},{"location":"DeltaLogFileIndex/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.DeltaLogFileIndex</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.delta.DeltaLogFileIndex=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"DeltaLogging/","title":"DeltaLogging","text":"<p><code>DeltaLogging</code> is a DeltaProgressReporter.</p>"},{"location":"DeltaOptimizeBuilder/","title":"DeltaOptimizeBuilder","text":"<p><code>DeltaOptimizeBuilder</code> is a builder interface for constructing and executing OPTIMIZE command.</p> <p><code>DeltaOptimizeBuilder</code> is created using DeltaTable.optimize operator.</p> <p>In the end, <code>DeltaOptimizeBuilder</code> is supposed to be executed to take action using the following operators:</p> <ul> <li>executeCompaction</li> <li>executeZOrderBy</li> </ul>"},{"location":"DeltaOptimizeBuilder/#iodeltatables-package","title":"io.delta.tables Package","text":"<p><code>DeltaTableBuilder</code> belongs to <code>io.delta.tables</code> package.</p> <pre><code>import io.delta.tables.DeltaTableBuilder\n</code></pre>"},{"location":"DeltaOptimizeBuilder/#demo","title":"Demo","text":"<pre><code>import io.delta.tables.DeltaTable\nDeltaTable.forName(\"part_delta\")\n.optimize()\n.where(\"p = 0\")\n.executeZOrderBy(\"x\", \"y)\n.show(truncate = false)\n</code></pre>"},{"location":"DeltaOptimizeBuilder/#operators","title":"Operators","text":""},{"location":"DeltaOptimizeBuilder/#executecompaction","title":"executeCompaction <pre><code>executeCompaction(): DataFrame\n</code></pre> <p><code>executeCompaction</code> executes this <code>DeltaOptimizeBuilder</code> (with no <code>zOrderBy</code> attributes).</p>","text":""},{"location":"DeltaOptimizeBuilder/#executezorderby","title":"executeZOrderBy <pre><code>executeZOrderBy(\n  columns: String *): DataFrame\n</code></pre> <p><code>executeZOrderBy</code> executes this <code>DeltaOptimizeBuilder</code> (with the given <code>columns</code> as <code>zOrderBy</code> attributes).</p>","text":""},{"location":"DeltaOptimizeBuilder/#where","title":"where <pre><code>where(\n  partitionFilter: String): DeltaOptimizeBuilder\n</code></pre> <p><code>where</code> registers a <code>partitionFilter</code> and returns this <code>DeltaOptimizeBuilder</code>.</p>","text":""},{"location":"DeltaOptimizeBuilder/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaOptimizeBuilder</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> Table Identifier <p><code>DeltaOptimizeBuilder</code> is created using apply factory method.</p>"},{"location":"DeltaOptimizeBuilder/#creating-deltaoptimizebuilder","title":"Creating DeltaOptimizeBuilder <pre><code>apply(\n  sparkSession: SparkSession,\n  tableIdentifier: String): DeltaOptimizeBuilder\n</code></pre> <p><code>apply</code> creates a DeltaOptimizeBuilder.</p>  <p>A private method</p> <p><code>apply</code> is a private method and can only be executed using DeltaTable.optimize operator.</p>  <p><code>apply</code> is used when:</p> <ul> <li><code>DeltaTable</code> is requested to optimize</li> </ul>","text":""},{"location":"DeltaOptimizeBuilder/#executing","title":"Executing <pre><code>execute(\n  zOrderBy: Seq[UnresolvedAttribute]): DataFrame\n</code></pre> <p><code>execute</code> creates an OptimizeTableCommand (with tableId, the partitionFilter and the given <code>zOrderBy</code> attributes) and executes it (while creating a <code>DataFrame</code>).</p> <p><code>execute</code> is used when:</p> <ul> <li><code>DeltaOptimizeBuilder</code> is requested to executeCompaction and executeZOrderBy</li> </ul>","text":""},{"location":"DeltaOptionParser/","title":"DeltaOptionParser","text":"<p><code>DeltaOptionParser</code> is an abstraction of options for reading from and writing to delta tables.</p>"},{"location":"DeltaOptionParser/#contract","title":"Contract","text":""},{"location":"DeltaOptionParser/#sqlconf","title":"SQLConf <pre><code>sqlConf: SQLConf\n</code></pre> <p>Used when:</p> <ul> <li><code>DeltaWriteOptionsImpl</code> is requested for canMergeSchema</li> </ul>","text":""},{"location":"DeltaOptionParser/#options","title":"Options <pre><code>options: CaseInsensitiveMap[String]\n</code></pre>","text":""},{"location":"DeltaOptionParser/#implementations","title":"Implementations","text":"<ul> <li>DeltaReadOptions</li> <li>DeltaWriteOptions</li> <li>DeltaWriteOptionsImpl</li> </ul>"},{"location":"DeltaOptions/","title":"DeltaOptions","text":"<p><code>DeltaOptions</code> is a type-safe abstraction of the supported write and read options.</p> <p><code>DeltaOptions</code> is used to create WriteIntoDelta command, DeltaSink, and DeltaSource.</p>"},{"location":"DeltaOptions/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaOptions</code> takes the following to be created:</p> <ul> <li> Case-Insensitive Options <li> <code>SQLConf</code> (Spark SQL) <p>When created, <code>DeltaOptions</code> verifies the options.</p> <p><code>DeltaOptions</code> is created\u00a0when:</p> <ul> <li><code>DeltaLog</code> is requested for a relation (for DeltaDataSource as a CreatableRelationProvider and a RelationProvider)</li> <li><code>DeltaCatalog</code> is requested to createDeltaTable</li> <li><code>WriteIntoDeltaBuilder</code> is requested to buildForV1Write</li> <li>CreateDeltaTableCommand is executed</li> <li><code>DeltaDataSource</code> is requested for a streaming source (to create a DeltaSource for Structured Streaming), a streaming sink (to create a DeltaSink for Structured Streaming), and for an insertable HadoopFsRelation</li> </ul>"},{"location":"DeltaOptions/#verifying-options","title":"Verifying Options <pre><code>verifyOptions(\n  options: CaseInsensitiveMap[String]): Unit\n</code></pre> <p><code>verifyOptions</code> finds invalid options among the input <code>options</code>.</p>  <p>Note</p> <p>In the open-source version <code>verifyOptions</code> does really nothing. The underlying objects (<code>recordDeltaEvent</code> and the others) are no-ops.</p>  <p><code>verifyOptions</code> is used when:</p> <ul> <li><code>DeltaOptions</code> is created</li> <li><code>DeltaDataSource</code> is requested for a relation (for loading data in batch queries)</li> </ul>","text":""},{"location":"DeltaOptions/#serializable","title":"Serializable <p><code>DeltaOptions</code> is a <code>Serializable</code> (Java) (so it can be used in Spark tasks).</p>","text":""},{"location":"DeltaParquetFileFormat/","title":"DeltaParquetFileFormat","text":"<p><code>DeltaParquetFileFormat</code> is a <code>ParquetFileFormat</code> (Spark SQL) to support no restrictions on columns names.</p>"},{"location":"DeltaParquetFileFormat/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaParquetFileFormat</code> takes the following to be created:</p> <ul> <li> DeltaColumnMappingMode <li> Reference schema (StructType) <p><code>DeltaParquetFileFormat</code> is created when:</p> <ul> <li><code>DeltaFileFormat</code> is requested for the fileFormat</li> </ul>"},{"location":"DeltaParquetFileFormat/#building-data-reader-with-partition-values","title":"Building Data Reader With Partition Values <pre><code>buildReaderWithPartitionValues(\n  sparkSession: SparkSession,\n  dataSchema: StructType,\n  partitionSchema: StructType,\n  requiredSchema: StructType,\n  filters: Seq[Filter],\n  options: Map[String, String],\n  hadoopConf: Configuration): PartitionedFile =&gt; Iterator[InternalRow]\n</code></pre> <p><code>buildReaderWithPartitionValues</code> prepares the given schemas (e.g., <code>dataSchema</code>, <code>partitionSchema</code> and <code>requiredSchema</code>) before requesting the parent <code>ParquetFileFormat</code> to <code>buildReaderWithPartitionValues</code>.</p> <p><code>buildReaderWithPartitionValues</code> is part of the <code>ParquetFileFormat</code> (Spark SQL) abstraction.</p>","text":""},{"location":"DeltaParquetFileFormat/#preparing-schema","title":"Preparing Schema <pre><code>prepareSchema(\n  inputSchema: StructType): StructType\n</code></pre> <p><code>prepareSchema</code> creates a physical schema (for the <code>inputSchema</code>, the referenceSchema and the DeltaColumnMappingMode).</p>","text":""},{"location":"DeltaParquetFileFormat/#supportfieldname","title":"supportFieldName <pre><code>supportFieldName(\n  name: String): Boolean\n</code></pre> <p><code>supportFieldName</code> is enabled (<code>true</code>) when the columnMappingMode is not <code>NoMapping</code> or requests the parent <code>ParquetFileFormat</code> to <code>supportFieldName</code>.</p> <p><code>supportFieldName</code> is part of the <code>ParquetFileFormat</code> (Spark SQL) abstraction.</p>","text":""},{"location":"DeltaProgressReporter/","title":"DeltaProgressReporter","text":"<p><code>DeltaProgressReporter</code> is an abstraction of progress reporters (loggers).</p>"},{"location":"DeltaProgressReporter/#implementations","title":"Implementations","text":"<ul> <li>DeltaLogging</li> </ul>"},{"location":"DeltaProgressReporter/#withstatuscode","title":"withStatusCode <pre><code>withStatusCode[T](\n  statusCode: String,\n  defaultMessage: String,\n  data: Map[String, Any] = Map.empty)(body: =&gt; T): T\n</code></pre> <p><code>withStatusCode</code> prints out the following INFO message to the logs:</p> <pre><code>[statusCode]: [defaultMessage]\n</code></pre> <p><code>withStatusCode</code> withJobDescription with the given <code>defaultMessage</code> and <code>body</code>.</p> <p><code>withStatusCode</code> prints out the following INFO message to the logs:</p> <pre><code>[statusCode]: Done\n</code></pre> <p><code>withStatusCode</code>\u00a0is used when:</p> <ul> <li><code>PartitionFiltering</code> is requested for the files to scan</li> <li><code>Snapshot</code> is requested for the state</li> <li>DeleteCommand, MergeIntoCommand, UpdateCommand are executed</li> <li><code>GenerateSymlinkManifest</code> is requested to recordManifestGeneration</li> </ul>","text":""},{"location":"DeltaProgressReporter/#withjobdescription","title":"withJobDescription <pre><code>withJobDescription[U](\n  jobDesc: String)(body: =&gt; U): U\n</code></pre> <p><code>withJobDescription</code>...FIXME</p>","text":""},{"location":"DeltaProgressReporter/#logging","title":"Logging <p>Since <code>DeltaProgressReporter</code> is an abstraction, logging is configured using the logger of the implementations.</p>","text":""},{"location":"DeltaReadOptions/","title":"DeltaReadOptions","text":"<p><code>DeltaReadOptions</code> is an extension of the DeltaOptionParser abstraction with the values of the read options of DeltaOptions.</p>"},{"location":"DeltaReadOptions/#excluderegex","title":"excludeRegex <pre><code>excludeRegex: Option[Regex]\n</code></pre> <p><code>excludeRegex</code> uses the options for the value of excludeRegex option and converts it to a scala.util.matching.Regex.</p> <p><code>excludeRegex</code> is used when:</p> <ul> <li><code>DeltaSource</code> is requested for the excludeRegex</li> </ul>","text":""},{"location":"DeltaReadOptions/#failondataloss","title":"failOnDataLoss <pre><code>failOnDataLoss: Boolean\n</code></pre> <p><code>failOnDataLoss</code> uses the options for the value of failOnDataLoss option.</p> <p><code>failOnDataLoss</code> is <code>true</code> by default.</p> <p><code>failOnDataLoss</code> is used when:</p> <ul> <li><code>DeltaSource</code> is requested to getFileChanges</li> <li><code>DeltaSourceCDCSupport</code> is requested to getFileChangesForCDC</li> </ul>","text":""},{"location":"DeltaReadOptions/#ignorechanges","title":"ignoreChanges <pre><code>ignoreChanges: Boolean\n</code></pre> <p><code>ignoreChanges</code>...FIXME</p> <p><code>ignoreChanges</code> is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"DeltaReadOptions/#ignoredeletes","title":"ignoreDeletes <pre><code>ignoreDeletes: Boolean\n</code></pre> <p><code>ignoreDeletes</code>...FIXME</p> <p><code>ignoreDeletes</code> is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"DeltaReadOptions/#ignorefiledeletion","title":"ignoreFileDeletion <pre><code>ignoreFileDeletion: Boolean\n</code></pre> <p><code>ignoreFileDeletion</code>...FIXME</p> <p><code>ignoreFileDeletion</code> is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"DeltaReadOptions/#maxbytespertrigger","title":"maxBytesPerTrigger <pre><code>maxBytesPerTrigger: Option[Long]\n</code></pre> <p><code>maxBytesPerTrigger</code>...FIXME</p> <p><code>maxBytesPerTrigger</code> is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"DeltaReadOptions/#maxfilespertrigger","title":"maxFilesPerTrigger <pre><code>maxFilesPerTrigger: Option[Int]\n</code></pre> <p><code>maxFilesPerTrigger</code>...FIXME</p> <p><code>maxFilesPerTrigger</code> is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"DeltaReadOptions/#readchangefeed","title":"readChangeFeed <pre><code>readChangeFeed: Boolean\n</code></pre> <p><code>readChangeFeed</code> uses the options for the value of readChangeFeed option (if available or falls back to the legacy readChangeData).</p> <p><code>readChangeFeed</code> is used when:</p> <ul> <li><code>DeltaSourceBase</code> is requested for the read schema, to getFileChangesWithRateLimit (indirectly for <code>DeltaSource</code> to determine the latest offset) and getFileChangesAndCreateDataFrame (indirectly for the <code>DeltaSource</code> to get a streaming micro-batch dataframe)</li> </ul>","text":""},{"location":"DeltaReadOptions/#startingtimestamp","title":"startingTimestamp <pre><code>startingTimestamp: Option[String]\n</code></pre> <p><code>startingTimestamp</code>...FIXME</p> <p><code>startingTimestamp</code> is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"DeltaReadOptions/#startingversion","title":"startingVersion <pre><code>startingVersion: Option[DeltaStartingVersion]\n</code></pre> <p><code>startingVersion</code>...FIXME</p> <p><code>startingVersion</code> is used when:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"DeltaRelation/","title":"DeltaRelation","text":"<p><code>DeltaRelation</code> is...FIXME</p>"},{"location":"DeltaSQLConf/","title":"DeltaSQLConf \u2014 spark.databricks.delta Configuration Properties","text":"<p><code>DeltaSQLConf</code> contains spark.databricks.delta-prefixed configuration properties to configure behaviour of Delta Lake.</p>"},{"location":"DeltaSQLConf/#alterlocationbypassschemacheck","title":"alterLocation.bypassSchemaCheck <p>spark.databricks.delta.alterLocation.bypassSchemaCheck enables Alter Table Set Location on Delta to go through even if the Delta table in the new location has a different schema from the original Delta table</p> <p>Default: <code>false</code></p>","text":""},{"location":"DeltaSQLConf/#altertablechangecolumncheckexpressions","title":"alterTable.changeColumn.checkExpressions <p>spark.databricks.delta.alterTable.changeColumn.checkExpressions (internal)</p> <p>Given an ALTER TABLE CHANGE COLUMN command, check whether Constraints or Generated Columns use expressions that reference this column (that will be affected by this change and should be changed along).</p> <p>Turn this off when there is an issue with expression checking logic that prevents a valid column change from going through.</p> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>AlterDeltaTableCommand</code> is requested to checkDependentExpressions</li> </ul>","text":""},{"location":"DeltaSQLConf/#checklatestschemaonread","title":"checkLatestSchemaOnRead <p>spark.databricks.delta.checkLatestSchemaOnRead enables a check that ensures that users won't read corrupt data if the source schema changes in an incompatible way.</p> <p>Default: <code>true</code></p> <p>Delta always tries to give users the latest version of table data without having to call <code>REFRESH TABLE</code> or redefine their DataFrames when used in the context of streaming. There is a possibility that the schema of the latest version of the table may be incompatible with the schema at the time of DataFrame creation.</p>","text":""},{"location":"DeltaSQLConf/#checkpointpartsize","title":"checkpoint.partSize <p>spark.databricks.delta.checkpoint.partSize (internal) is the limit at which we will start parallelizing the checkpoint. We will attempt to write maximum of this many actions per checkpoint.</p> <p>Default: <code>5000000</code></p>","text":""},{"location":"DeltaSQLConf/#commitinfoenabled","title":"commitInfo.enabled <p>spark.databricks.delta.commitInfo.enabled controls whether to log commit information into a Delta log.</p> <p>Default: <code>true</code></p>","text":""},{"location":"DeltaSQLConf/#commitinfousermetadata","title":"commitInfo.userMetadata <p>spark.databricks.delta.commitInfo.userMetadata is an arbitrary user-defined metadata to include in CommitInfo (requires spark.databricks.delta.commitInfo.enabled).</p> <p>Default: (empty)</p>","text":""},{"location":"DeltaSQLConf/#commitlockenabled","title":"commitLock.enabled <p>spark.databricks.delta.commitLock.enabled (internal) controls whether or not to use a lock on a delta table at transaction commit.</p> <p>Default: (undefined)</p> <p>Used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to isCommitLockEnabled</li> </ul>","text":""},{"location":"DeltaSQLConf/#commitvalidationenabled","title":"commitValidation.enabled <p>spark.databricks.delta.commitValidation.enabled (internal) controls whether to perform validation checks before commit or not</p> <p>Default: <code>true</code></p>","text":""},{"location":"DeltaSQLConf/#convertmetadatacheckenabled","title":"convert.metadataCheck.enabled <p>spark.databricks.delta.convert.metadataCheck.enabled enables validation during convert to delta, if there is a difference between the catalog table's properties and the Delta table's configuration, we should error.</p> <p>If disabled, merge the two configurations with the same semantics as update and merge</p> <p>Default: <code>true</code></p>","text":""},{"location":"DeltaSQLConf/#dummyfilemanagernumoffiles","title":"dummyFileManager.numOfFiles <p>spark.databricks.delta.dummyFileManager.numOfFiles (internal) controls how many dummy files to write in DummyFileManager</p> <p>Default: <code>3</code></p>","text":""},{"location":"DeltaSQLConf/#dummyfilemanagerprefix","title":"dummyFileManager.prefix <p>spark.databricks.delta.dummyFileManager.prefix (internal) is the file prefix to use in DummyFileManager</p> <p>Default: <code>.s3-optimization-</code></p>","text":""},{"location":"DeltaSQLConf/#historymaxkeysperlist","title":"history.maxKeysPerList <p>spark.databricks.delta.history.maxKeysPerList (internal) controls how many commits to list when performing a parallel search.</p> <p>The default is the maximum keys returned by S3 per list call. Azure can return 5000, therefore we choose 1000.</p> <p>Default: <code>1000</code></p>","text":""},{"location":"DeltaSQLConf/#historymetricsenabled","title":"history.metricsEnabled <p>spark.databricks.delta.history.metricsEnabled enables metrics reporting in <code>DESCRIBE HISTORY</code> (CommitInfo will record the operation metrics when a <code>OptimisticTransactionImpl</code> is committed and the spark.databricks.delta.commitInfo.enabled configuration property is enabled).</p> <p>Requires spark.databricks.delta.commitInfo.enabled configuration property to be enabled</p> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to getOperationMetrics</li> <li><code>ConvertToDeltaCommand</code> is requested to streamWrite</li> <li><code>SQLMetricsReporting</code> is requested to registerSQLMetrics</li> <li><code>TransactionalWrite</code> is requested to writeFiles</li> </ul>  <p>Github Commit</p> <p>The feature was added as part of [SC-24567][DELTA] Add additional metrics to Describe Delta History commit.</p>","text":""},{"location":"DeltaSQLConf/#importbatchsizeschemainference","title":"import.batchSize.schemaInference <p>spark.databricks.delta.import.batchSize.schemaInference (internal) is the number of files per batch for schema inference during import.</p> <p>Default: <code>1000000</code></p>","text":""},{"location":"DeltaSQLConf/#importbatchsizestatscollection","title":"import.batchSize.statsCollection <p>spark.databricks.delta.import.batchSize.statsCollection (internal) is the number of files per batch for stats collection during import.</p> <p>Default: <code>50000</code></p>","text":""},{"location":"DeltaSQLConf/#ioskippingmdcaddnoise","title":"io.skipping.mdc.addNoise <p>spark.databricks.io.skipping.mdc.addNoise (internal) controls whether or not to add a random byte as a suffix to the interleaved bits when computing the Z-order values for MDC. This can help deal with skew, but may have a negative impact on overall min/max skipping effectiveness.</p> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>SpaceFillingCurveClustering</code> is requested to cluster</li> </ul>","text":""},{"location":"DeltaSQLConf/#ioskippingmdcrangeidmax","title":"io.skipping.mdc.rangeId.max <p>spark.databricks.io.skipping.mdc.rangeId.max (internal) controls the domain of <code>rangeId</code> values to be interleaved. The bigger, the better granularity, but at the expense of performance (more data gets sampled).</p> <p>Default: <code>1000</code></p> <p>Must be greater than <code>1</code></p> <p>Used when:</p> <ul> <li><code>SpaceFillingCurveClustering</code> is requested to cluster</li> </ul>","text":""},{"location":"DeltaSQLConf/#ioskippingstringprefixlength","title":"io.skipping.stringPrefixLength <p>spark.databricks.io.skipping.stringPrefixLength (internal) The length of the prefix of string columns to store in the data skipping index</p> <p>Default: <code>32</code></p> <p>Used when:</p> <ul> <li><code>StatisticsCollection</code> is requested for the statsCollector Column</li> </ul>","text":""},{"location":"DeltaSQLConf/#lastcommitversioninsession","title":"lastCommitVersionInSession <p>spark.databricks.delta.lastCommitVersionInSession is the version of the last commit made in the <code>SparkSession</code> for any delta table (after <code>OptimisticTransactionImpl</code> is done with doCommit or <code>DeltaCommand</code> with commitLarge)</p> <p>Default: (undefined)</p>","text":""},{"location":"DeltaSQLConf/#loadfilesystemconfigsfromdataframeoptions","title":"loadFileSystemConfigsFromDataFrameOptions <p>spark.databricks.delta.loadFileSystemConfigsFromDataFrameOptions (internal) controls whether to load file systems configs provided in <code>DataFrameReader</code> or <code>DataFrameWriter</code> options when calling <code>DataFrameReader.load/DataFrameWriter.save</code> using a Delta table path.</p> <p>Not supported for <code>DataFrameReader.table</code> and <code>DataFrameWriter.saveAsTable</code></p> <p>Default: <code>true</code></p>","text":""},{"location":"DeltaSQLConf/#maxcommitattempts","title":"maxCommitAttempts <p>spark.databricks.delta.maxCommitAttempts (internal) is the maximum number of commit attempts to try for a single commit before failing</p> <p>Default: <code>10000000</code></p> <p>Used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to doCommitRetryIteratively</li> </ul>","text":""},{"location":"DeltaSQLConf/#maxsnapshotlineagelength","title":"maxSnapshotLineageLength <p>spark.databricks.delta.maxSnapshotLineageLength (internal) is the maximum lineage length of a Snapshot before Delta forces to build a Snapshot from scratch</p> <p>Default: <code>50</code></p>","text":""},{"location":"DeltaSQLConf/#mergemaxinsertcount","title":"merge.maxInsertCount <p>spark.databricks.delta.merge.maxInsertCount (internal) is the maximum row count of inserts in each MERGE execution</p> <p>Default: <code>10000L</code></p>","text":""},{"location":"DeltaSQLConf/#mergeoptimizeinsertonlymergeenabled","title":"merge.optimizeInsertOnlyMerge.enabled <p>spark.databricks.delta.merge.optimizeInsertOnlyMerge.enabled (internal) controls merge without any matched clause (i.e., insert-only merge) will be optimized by avoiding rewriting old files and just inserting new files</p> <p>Default: <code>true</code></p>","text":""},{"location":"DeltaSQLConf/#mergeoptimizematchedonlymergeenabled","title":"merge.optimizeMatchedOnlyMerge.enabled <p>spark.databricks.delta.merge.optimizeMatchedOnlyMerge.enabled (internal) controls merge without 'when not matched' clause will be optimized to use a right outer join instead of a full outer join</p> <p>Default: <code>true</code></p>","text":""},{"location":"DeltaSQLConf/#mergerepartitionbeforewriteenabled","title":"merge.repartitionBeforeWrite.enabled <p>spark.databricks.delta.merge.repartitionBeforeWrite.enabled (internal) controls whether MERGE command repartitions output before writing the files (by the table's partition columns)</p> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>MergeIntoCommand</code> is requested to repartitionIfNeeded</li> </ul>","text":""},{"location":"DeltaSQLConf/#optimizemaxfilesize","title":"optimize.maxFileSize <p>spark.databricks.delta.optimize.maxFileSize (internal) Target file size produced by OPTIMIZE command.</p> <p>Default: <code>1024 * 1024 * 1024</code></p> <p>Used when:</p> <ul> <li><code>OptimizeExecutor</code> is requested to optimize</li> </ul>","text":""},{"location":"DeltaSQLConf/#optimizemaxthreads","title":"optimize.maxThreads <p>spark.databricks.delta.optimize.maxThreads (internal) Maximum number of parallel jobs allowed in OPTIMIZE command. Increasing the maximum parallel jobs allows <code>OPTIMIZE</code> command to run faster, but increases the job management on the Spark driver.</p> <p>Default: <code>15</code></p> <p>Used when:</p> <ul> <li><code>OptimizeExecutor</code> is requested to optimize</li> </ul>","text":""},{"location":"DeltaSQLConf/#optimizeminfilesize","title":"optimize.minFileSize <p>spark.databricks.delta.optimize.minFileSize (internal) Files which are smaller than this threshold (in bytes) will be grouped together and rewritten as larger files by the OPTIMIZE command.</p> <p>Default: <code>1024 * 1024 * 1024</code> (1GB)</p> <p>Used when:</p> <ul> <li><code>OptimizeExecutor</code> is requested to optimize</li> </ul>","text":""},{"location":"DeltaSQLConf/#optimizezordercheckstatscollectionenabled","title":"optimize.zorder.checkStatsCollection.enabled <p>spark.databricks.delta.optimize.zorder.checkStatsCollection.enabled (internal) Controls whether there are column statistics available for the <code>zOrderBy</code> columns of OPTIMIZE command</p> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>OptimizeTableCommandBase</code> is requested to validate zOrderBy columns</li> </ul>","text":""},{"location":"DeltaSQLConf/#partitioncolumnvalidityenabled","title":"partitionColumnValidity.enabled <p>spark.databricks.delta.partitionColumnValidity.enabled (internal) enables validation of the partition column names (just like the data columns)</p> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to verify a new metadata (with NoMapping column mapping mode)</li> </ul>","text":""},{"location":"DeltaSQLConf/#propertiesdefaultsminreaderversion","title":"properties.defaults.minReaderVersion <p>spark.databricks.delta.properties.defaults.minReaderVersion is the default reader protocol version to create new tables with, unless a feature that requires a higher version for correctness is enabled.</p> <p>Default: <code>1</code></p> <p>Available values: <code>1</code></p> <p>Used when:</p> <ul> <li><code>Protocol</code> utility is used to create a Protocol</li> </ul>","text":""},{"location":"DeltaSQLConf/#propertiesdefaultsminwriterversion","title":"properties.defaults.minWriterVersion <p>spark.databricks.delta.properties.defaults.minWriterVersion is the default writer protocol version to create new tables with, unless a feature that requires a higher version for correctness is enabled.</p> <p>Default: <code>2</code></p> <p>Available values: <code>1</code>, <code>2</code>, <code>3</code></p> <p>Used when:</p> <ul> <li><code>Protocol</code> utility is used to create a Protocol</li> </ul>","text":""},{"location":"DeltaSQLConf/#replacewhereconstraintcheckenabled","title":"replaceWhere.constraintCheck.enabled <p>spark.databricks.delta.replaceWhere.constraintCheck.enabled controls whether or not replaceWhere on arbitrary expression and arbitrary columns enforces constraints to replace the target table only when all the rows in the source dataframe match that constraint.</p> <p>If disabled, it will skip the constraint check and replace with all the rows from the new dataframe.</p> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>WriteIntoDelta</code> is requested to extract constraints</li> </ul>","text":""},{"location":"DeltaSQLConf/#retentiondurationcheckenabled","title":"retentionDurationCheck.enabled <p>spark.databricks.delta.retentionDurationCheck.enabled adds a check preventing users from running vacuum with a very short retention period, which may end up corrupting a Delta log.</p> <p>Default: <code>true</code></p>","text":""},{"location":"DeltaSQLConf/#samplingenabled","title":"sampling.enabled <p>spark.databricks.delta.sampling.enabled (internal) enables sample-based estimation</p> <p>Default: <code>false</code></p>","text":""},{"location":"DeltaSQLConf/#schemaautomergeenabled","title":"schema.autoMerge.enabled <p>spark.databricks.delta.schema.autoMerge.enabled enables schema merging on appends and overwrites.</p> <p>Default: <code>false</code></p> <p>Equivalent DataFrame option: mergeSchema</p> <p>Used when:</p> <ul> <li><code>DeltaMergeInto</code> utility is used to resolveReferencesAndSchema</li> <li><code>MetadataMismatchErrorBuilder</code> is requested to <code>addSchemaMismatch</code></li> <li><code>DeltaWriteOptionsImpl</code> is requested for canMergeSchema</li> <li><code>MergeIntoCommand</code> is requested for canMergeSchema</li> </ul>","text":""},{"location":"DeltaSQLConf/#schematypecheckenabled","title":"schema.typeCheck.enabled <p>spark.databricks.delta.schema.typeCheck.enabled (internal) controls whether to check unsupported data types while updating a table schema</p> <p>Disabling this flag may allow users to create unsupported Delta tables and should only be used when trying to read/write legacy tables.</p> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested for checkUnsupportedDataType flag</li> <li><code>DeltaErrorsBase</code> is requested to unsupportedDataTypes</li> </ul>","text":""},{"location":"DeltaSQLConf/#snapshotisolationenabled","title":"snapshotIsolation.enabled <p>spark.databricks.delta.snapshotIsolation.enabled (internal) controls whether queries on Delta tables are guaranteed to have snapshot isolation</p> <p>Default: <code>true</code></p>","text":""},{"location":"DeltaSQLConf/#snapshotpartitions","title":"snapshotPartitions <p>spark.databricks.delta.snapshotPartitions (internal) is the number of partitions to use for state reconstruction (when building a snapshot of a Delta table).</p> <p>Default: <code>50</code></p>","text":""},{"location":"DeltaSQLConf/#stalenesslimit","title":"stalenessLimit <p>spark.databricks.delta.stalenessLimit (in millis) allows you to query the last loaded state of the Delta table without blocking on a table update. You can use this configuration to reduce the latency on queries when up-to-date results are not a requirement. Table updates will be scheduled on a separate scheduler pool in a FIFO queue, and will share cluster resources fairly with your query. If a table hasn't updated past this time limit, we will block on a synchronous state update before running the query.</p> <p>Default: <code>0</code> (no tables can be stale)</p>","text":""},{"location":"DeltaSQLConf/#statecorruptionisfatal","title":"state.corruptionIsFatal <p>spark.databricks.delta.state.corruptionIsFatal (internal) throws a fatal error when the recreated Delta State doesn't match committed checksum file</p> <p>Default: <code>true</code></p>","text":""},{"location":"DeltaSQLConf/#statereconstructionvalidationenabled","title":"stateReconstructionValidation.enabled <p>spark.databricks.delta.stateReconstructionValidation.enabled (internal) controls whether to perform validation checks on the reconstructed state</p> <p>Default: <code>true</code></p>","text":""},{"location":"DeltaSQLConf/#statscollect","title":"stats.collect <p>spark.databricks.delta.stats.collect (internal) Enables statistics to be collected while writing files into a Delta table</p> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>TransactionalWrite</code> is requested to writeFiles</li> </ul>","text":""},{"location":"DeltaSQLConf/#statslimitpushdownenabled","title":"stats.limitPushdown.enabled <p>spark.databricks.delta.stats.limitPushdown.enabled (internal) enables using the limit clause and file statistics to prune files before they are collected to the driver</p> <p>Default: <code>true</code></p>","text":""},{"location":"DeltaSQLConf/#statslocalcachemaxnumfiles","title":"stats.localCache.maxNumFiles <p>spark.databricks.delta.stats.localCache.maxNumFiles (internal) is the maximum number of files for a table to be considered a delta small table. Some metadata operations (such as using data skipping) are optimized for small tables using driver local caching and local execution.</p> <p>Default: <code>2000</code></p>","text":""},{"location":"DeltaSQLConf/#statsskipping","title":"stats.skipping <p>spark.databricks.delta.stats.skipping (internal) enables Data Skipping</p> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>DataSkippingReaderBase</code> is requested for the files to scan</li> <li><code>PrepareDeltaScanBase</code> logical optimization is executed</li> </ul>","text":""},{"location":"DeltaSQLConf/#timetravelresolveonidentifierenabled","title":"timeTravel.resolveOnIdentifier.enabled <p>spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled (internal) controls whether to resolve patterns as <code>@v123</code> and <code>@yyyyMMddHHmmssSSS</code> in path identifiers as time travel nodes.</p> <p>Default: <code>true</code></p>","text":""},{"location":"DeltaSQLConf/#vacuumparalleldeleteenabled","title":"vacuum.parallelDelete.enabled <p>spark.databricks.delta.vacuum.parallelDelete.enabled enables parallelizing the deletion of files during vacuum command.</p> <p>Default: <code>false</code></p> <p>Enabling may result hitting rate limits on some storage backends. When enabled, parallelization is controlled by the default number of shuffle partitions.</p>","text":""},{"location":"DeltaSink/","title":"DeltaSink","text":"<p><code>DeltaSink</code> is the <code>Sink</code> (Spark Structured Streaming) of the delta data source for streaming queries.</p>"},{"location":"DeltaSink/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaSink</code> takes the following to be created:</p> <ul> <li> <code>SQLContext</code> <li> Hadoop Path of the delta table (to write data to as configured by the path option) <li> Partition columns <li> <code>OutputMode</code> (Spark Structured Streaming) <li> DeltaOptions <p><code>DeltaSink</code> is created\u00a0when:</p> <ul> <li><code>DeltaDataSource</code> is requested for a streaming sink</li> </ul>"},{"location":"DeltaSink/#deltalog","title":"DeltaLog <pre><code>deltaLog: DeltaLog\n</code></pre> <p><code>deltaLog</code> is a DeltaLog that is created for the delta table when <code>DeltaSink</code> is created.</p> <p><code>deltaLog</code> is used when:</p> <ul> <li>DeltaSink is requested to add a streaming micro-batch</li> </ul>","text":""},{"location":"DeltaSink/#adding-streaming-micro-batch","title":"Adding Streaming Micro-Batch <pre><code>addBatch(\n  batchId: Long,\n  data: DataFrame): Unit\n</code></pre> <p><code>addBatch</code> is part of the <code>Sink</code> (Spark Structured Streaming) abstraction.</p>  <p><code>addBatch</code> requests the DeltaLog to start a new transaction.</p> <p><code>addBatch</code> registers the following performance metrics.</p>    Name web UI     <code>numAddedFiles</code> number of files added.   <code>numRemovedFiles</code> number of files removed.    <p><code>addBatch</code> makes sure that <code>sql.streaming.queryId</code> local property is defined (attached to the query's current thread).</p> <p>If the batch reads the same delta table as this sink is going to write to, <code>addBatch</code> requests the <code>OptimisticTransaction</code> to readWholeTable.</p> <p><code>addBatch</code> updates the metadata.</p> <p><code>addBatch</code> determines the deleted files based on the OutputMode. For <code>Complete</code> output mode, <code>addBatch</code>...FIXME</p> <p><code>addBatch</code> requests the <code>OptimisticTransaction</code> to write data out.</p> <p><code>addBatch</code> updates the <code>numRemovedFiles</code> and <code>numAddedFiles</code> performance metrics, and requests the <code>OptimisticTransaction</code> to register the SQLMetrics.</p> <p>In the end, <code>addBatch</code> requests the <code>OptimisticTransaction</code> to commit (with a new SetTransaction, AddFiles and RemoveFiles, and StreamingUpdate operation).</p>","text":""},{"location":"DeltaSink/#text-representation","title":"Text Representation <pre><code>toString(): String\n</code></pre> <p><code>DeltaSink</code> uses the following text representation (with the path):</p> <pre><code>DeltaSink[path]\n</code></pre>","text":""},{"location":"DeltaSink/#implicitmetadataoperation","title":"ImplicitMetadataOperation <p><code>DeltaSink</code> is an ImplicitMetadataOperation.</p>","text":""},{"location":"DeltaSink/#canmergeschema","title":"canMergeSchema <pre><code>canMergeSchema: Boolean\n</code></pre> <p><code>canMergeSchema</code> is part of the ImplicitMetadataOperation abstraction.</p>  <p><code>canMergeSchema</code> is the value of canMergeSchema option (in the DeltaOptions).</p>","text":""},{"location":"DeltaSink/#canoverwriteschema","title":"canOverwriteSchema <pre><code>canOverwriteSchema: Boolean\n</code></pre> <p><code>canOverwriteSchema</code> is part of the ImplicitMetadataOperation abstraction.</p>  <p><code>canOverwriteSchema</code> is <code>true</code> when all the following hold:</p> <ol> <li>OutputMode is <code>OutputMode.Complete</code></li> <li>canOverwriteSchema is enabled (<code>true</code>) (in the DeltaOptions)</li> </ol>","text":""},{"location":"DeltaSource/","title":"DeltaSource","text":"<p><code>DeltaSource</code> is a DeltaSourceBase of the delta data source for streaming queries.</p>"},{"location":"DeltaSource/#deltasourcecdcsupport","title":"DeltaSourceCDCSupport <p><code>DeltaSource</code> is a DeltaSourceCDCSupport.</p>","text":""},{"location":"DeltaSource/#creating-instance","title":"Creating Instance <p><code>DeltaSource</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> DeltaLog <li> DeltaOptions <li> Filter <code>Expression</code>s (default: empty)  <p><code>DeltaSource</code> is created\u00a0when:</p> <ul> <li><code>DeltaDataSource</code> is requested for a streaming source</li> </ul>","text":""},{"location":"DeltaSource/#demo","title":"Demo <pre><code>val q = spark\n  .readStream               // Creating a streaming query\n  .format(\"delta\")          // Using delta data source\n  .load(\"/tmp/delta/users\") // Over data in a delta table\n  .writeStream\n  .format(\"memory\")\n  .option(\"queryName\", \"demo\")\n  .start\nimport org.apache.spark.sql.execution.streaming.{MicroBatchExecution, StreamingQueryWrapper}\nval plan = q.asInstanceOf[StreamingQueryWrapper]\n  .streamingQuery\n  .asInstanceOf[MicroBatchExecution]\n  .logicalPlan\nimport org.apache.spark.sql.execution.streaming.StreamingExecutionRelation\nval relation = plan.collect { case r: StreamingExecutionRelation =&gt; r }.head\n\nimport org.apache.spark.sql.delta.sources.DeltaSource\nassert(relation.source.asInstanceOf[DeltaSource])\n\nscala&gt; println(relation.source)\nDeltaSource[file:/tmp/delta/users]\n</code></pre>","text":""},{"location":"DeltaSource/#streaming-micro-batch-dataframe","title":"Streaming Micro-Batch DataFrame <pre><code>getBatch(\n  start: Option[Offset],\n  end: Offset): DataFrame\n</code></pre> <p><code>getBatch</code> is part of the <code>Source</code> (Spark Structured Streaming) abstraction.</p>  <p><code>getBatch</code> creates a DeltaSourceOffset for the tableId (aka reservoirId) and the given <code>end</code> offset.</p> <p><code>getBatch</code> determines the <code>startVersion</code>, <code>startIndex</code>, <code>isStartingVersion</code> and <code>startSourceVersion</code> based on the given <code>startOffsetOption</code>:</p> <p>If undefined, <code>getBatch</code> getStartingVersion and does some computation.</p> <p>If specified, <code>getBatch</code> creates a DeltaSourceOffset. Unless the <code>DeltaSourceOffset</code> is isStartingVersion, <code>getBatch</code> cleanUpSnapshotResources. <code>getBatch</code> uses the <code>DeltaSourceOffset</code> for the versions and the index.</p> <p><code>getBatch</code> prints out the following DEBUG message to the logs:</p> <pre><code>start: [startOffsetOption] end: [end]\n</code></pre> <p>In the end, <code>getBatch</code> createDataFrameBetweenOffsets (for the <code>startVersion</code>, <code>startIndex</code>, <code>isStartingVersion</code> and <code>endOffset</code>).</p>","text":""},{"location":"DeltaSource/#latest-available-streaming-offset","title":"Latest Available Streaming Offset <pre><code>latestOffset(\n  startOffset: streaming.Offset,\n  limit: ReadLimit): streaming.Offset\n</code></pre> <p><code>latestOffset</code>\u00a0is part of the <code>SupportsAdmissionControl</code> (Spark Structured Streaming) abstraction.</p>  <p><code>latestOffset</code> determines the latest offset (currentOffset) based on whether the previousOffset internal registry is initialized or not.</p> <p><code>latestOffset</code> prints out the following DEBUG message to the logs (using the previousOffset internal registry).</p> <pre><code>previousOffset -&gt; currentOffset: [previousOffset] -&gt; [currentOffset]\n</code></pre> <p>In the end, <code>latestOffset</code> returns the previousOffset if defined or <code>null</code>.</p>","text":""},{"location":"DeltaSource/#no-previousoffset","title":"No previousOffset <p>For no previousOffset, <code>getOffset</code> retrieves the starting offset (with a new AdmissionLimits for the given <code>ReadLimit</code>).</p>","text":""},{"location":"DeltaSource/#previousoffset-available","title":"previousOffset Available <p>When the previousOffset is defined (which is when the <code>DeltaSource</code> is requested for another micro-batch), <code>latestOffset</code> gets the changes as an indexed AddFiles (with the previousOffset and a new AdmissionLimits for the given <code>ReadLimit</code>).</p> <p><code>latestOffset</code> takes the last AddFile if available.</p> <p>With no <code>AddFile</code>, <code>latestOffset</code> returns the previousOffset.</p> <p>With the previousOffset and the last indexed AddFile both available, <code>latestOffset</code> creates a new DeltaSourceOffset for the version, index, and <code>isLast</code> flag from the last indexed AddFile.</p>  <p>Note</p> <p><code>isStartingVersion</code> local value is enabled (<code>true</code>) when the following holds:</p> <ul> <li> <p>Version of the last indexed AddFile is equal to the reservoirVersion of the previous ending offset</p> </li> <li> <p>isStartingVersion flag of the previous ending offset is enabled (<code>true</code>)</p> </li> </ul>","text":""},{"location":"DeltaSource/#getstartingoffset","title":"getStartingOffset <pre><code>getStartingOffset(\n  limits: Option[AdmissionLimits]): Option[Offset]\n</code></pre> <p><code>getStartingOffset</code>...FIXME (review me)</p> <p><code>getStartingOffset</code> requests the DeltaLog for the version of the delta table (by requesting for the current state (snapshot) and then for the version).</p> <p><code>getStartingOffset</code> takes the last file from the files added (with rate limit) for the version of the delta table, <code>-1L</code> as the <code>fromIndex</code>, and the <code>isStartingVersion</code> flag enabled (<code>true</code>).</p> <p><code>getStartingOffset</code> returns a new DeltaSourceOffset for the tableId, the version and the index of the last file added, and whether the last file added is the last file of its version.</p> <p><code>getStartingOffset</code> returns <code>None</code> (offset not available) when either happens:</p> <ul> <li> <p>the version of the delta table is negative (below <code>0</code>)</p> </li> <li> <p>no files were added in the version</p> </li> </ul> <p><code>getStartingOffset</code> throws an <code>AssertionError</code> when the version of the last file added is smaller than the delta table's version:</p> <pre><code>assertion failed: getChangesWithRateLimit returns an invalid version: [v] (expected: &gt;= [version])\n</code></pre>","text":""},{"location":"DeltaSource/#getchangeswithratelimit","title":"getChangesWithRateLimit <pre><code>getChangesWithRateLimit(\n  fromVersion: Long,\n  fromIndex: Long,\n  isStartingVersion: Boolean): Iterator[IndexedFile]\n</code></pre> <p><code>getChangesWithRateLimit</code> gets the changes (as indexed AddFiles) for the given <code>fromVersion</code>, <code>fromIndex</code>, and <code>isStartingVersion</code> flag.</p>","text":""},{"location":"DeltaSource/#getoffset","title":"getOffset <pre><code>getOffset: Option[Offset]\n</code></pre> <p><code>getOffset</code> is part of the <code>Source</code> (Spark Structured Streaming) abstraction.</p> <p><code>getOffset</code> has been replaced by the newer latestOffset and so throws an <code>UnsupportedOperationException</code> when called:</p> <pre><code>latestOffset(Offset, ReadLimit) should be called instead of this method\n</code></pre>","text":""},{"location":"DeltaSource/#snapshot-management","title":"Snapshot Management <p><code>DeltaSource</code> uses internal registries for the DeltaSourceSnapshot and the version to avoid requesting the DeltaLog for getSnapshotAt.</p>","text":""},{"location":"DeltaSource/#snapshot","title":"Snapshot <p><code>DeltaSource</code> uses <code>initialState</code> internal registry for the DeltaSourceSnapshot of the state of the delta table at the initialStateVersion.</p> <p><code>DeltaSourceSnapshot</code> is used for AddFiles of the delta table at a given version.</p> <p>Initially uninitialized (<code>null</code>).</p> <p><code>DeltaSourceSnapshot</code> is created (initialized) when uninitialized or the version requested is different from the current one.</p> <p><code>DeltaSourceSnapshot</code> is closed and dereferenced (<code>null</code>) when <code>DeltaSource</code> is requested to cleanUpSnapshotResources (due to version change, another micro-batch or stop).</p>","text":""},{"location":"DeltaSource/#version","title":"Version <p><code>DeltaSource</code> uses <code>initialStateVersion</code> internal registry to keep track of the version of DeltaSourceSnapshot (when requested for AddFiles of the delta table at a given version).</p> <p>Changes (alongside the initialState) to the version requested when <code>DeltaSource</code> is requested for the snapshot at a given version (only when the versions are different)</p> <p>Used when:</p> <ul> <li><code>DeltaSource</code> is requested for AddFiles of the delta table at a given version and to cleanUpSnapshotResources (and unpersist the current snapshot)</li> </ul>","text":""},{"location":"DeltaSource/#stopping","title":"Stopping <pre><code>stop(): Unit\n</code></pre> <p><code>stop</code> is part of the <code>Source</code> (Spark Structured Streaming) abstraction.</p> <p><code>stop</code> simply cleanUpSnapshotResources.</p>","text":""},{"location":"DeltaSource/#previous-offset","title":"Previous Offset <p>Ending DeltaSourceOffset of the latest micro-batch</p> <p>Starts uninitialized (<code>null</code>).</p> <p>Used when <code>DeltaSource</code> is requested for the latest available offset.</p>","text":""},{"location":"DeltaSource/#addfiles-of-delta-table-at-given-version","title":"AddFiles of Delta Table at Given Version <pre><code>getSnapshotAt(\n  version: Long): Iterator[IndexedFile]\n</code></pre> <p><code>getSnapshotAt</code> requests the DeltaSourceSnapshot for the data files (as indexed AddFiles).</p> <p>In case the DeltaSourceSnapshot hasn't been initialized yet (<code>null</code>) or the requested version is different from the initialStateVersion, <code>getSnapshotAt</code> does the following:</p> <ol> <li> <p>cleanUpSnapshotResources</p> </li> <li> <p>Requests the DeltaLog for the state (snapshot) of the delta table at the version</p> </li> <li> <p>Creates a new DeltaSourceSnapshot for the state (snapshot) as the current DeltaSourceSnapshot</p> </li> <li> <p>Changes the initialStateVersion internal registry to the requested version</p> </li> </ol> <p><code>getSnapshotAt</code> is used when:</p> <ul> <li><code>DeltaSource</code> is requested to getChanges (with <code>isStartingVersion</code> flag enabled)</li> </ul>","text":""},{"location":"DeltaSource/#getchanges","title":"getChanges <pre><code>getChanges(\n  fromVersion: Long,\n  fromIndex: Long,\n  isStartingVersion: Boolean): Iterator[IndexedFile]\n</code></pre> <p><code>getChanges</code> branches based on <code>isStartingVersion</code> flag (enabled or not):</p> <ul> <li> <p>For <code>isStartingVersion</code> flag enabled (<code>true</code>), <code>getChanges</code> gets the state (snapshot) for the given <code>fromVersion</code> followed by (filtered out) indexed AddFiles for the next version after the given <code>fromVersion</code></p> </li> <li> <p>For <code>isStartingVersion</code> flag disabled (<code>false</code>), <code>getChanges</code> simply gives (filtered out) indexed AddFiles for the given <code>fromVersion</code></p> </li> </ul>  <p>Note</p> <p><code>isStartingVersion</code> flag simply adds the state (snapshot) before (filtered out) indexed AddFiles when enabled (<code>true</code>).</p> <p><code>isStartingVersion</code> flag is enabled when <code>DeltaSource</code> is requested for the following:</p> <ul> <li> <p>Micro-batch with data between start and end offsets and the start offset is not given or is for the starting version</p> </li> <li> <p>Latest available offset with no end offset of the latest micro-batch or the end offset of the latest micro-batch for the starting version</p> </li> </ul>  <p>In the end, <code>getChanges</code> filters out (excludes) indexed AddFiles that are not with the version later than the given <code>fromVersion</code> or the index greater than the given <code>fromIndex</code>.</p> <p><code>getChanges</code> is used when:</p> <ul> <li><code>DeltaSource</code> is requested for the latest available offset (when requested for the files added (with rate limit)) and getBatch</li> </ul>","text":""},{"location":"DeltaSource/#filterandindexdeltalogs","title":"filterAndIndexDeltaLogs <pre><code>filterAndIndexDeltaLogs(\n  startVersion: Long): Iterator[IndexedFile]\n</code></pre> <p><code>filterAndIndexDeltaLogs</code>...FIXME</p>","text":""},{"location":"DeltaSource/#verifystreamhygieneandfilteraddfiles","title":"verifyStreamHygieneAndFilterAddFiles <pre><code>verifyStreamHygieneAndFilterAddFiles(\n  actions: Seq[Action],\n  version: Long): Seq[Action]\n</code></pre> <p><code>verifyStreamHygieneAndFilterAddFiles</code>...FIXME</p>","text":""},{"location":"DeltaSource/#cleanupsnapshotresources","title":"cleanUpSnapshotResources <pre><code>cleanUpSnapshotResources(): Unit\n</code></pre> <p><code>cleanUpSnapshotResources</code> does the following when the initial DeltaSourceSnapshot internal registry is not empty:</p> <ul> <li>Requests the DeltaSourceSnapshot to close (with the <code>unpersistSnapshot</code> flag based on whether the initialStateVersion is earlier than the snapshot version)</li> <li>Dereferences (nullifies) the DeltaSourceSnapshot</li> </ul> <p>Otherwise, <code>cleanUpSnapshotResources</code> does nothing.</p> <p><code>cleanUpSnapshotResources</code> is used when:</p> <ul> <li><code>DeltaSource</code> is requested to getSnapshotAt, getBatch and stop</li> </ul>","text":""},{"location":"DeltaSource/#readlimit","title":"ReadLimit <pre><code>getDefaultReadLimit: ReadLimit\n</code></pre> <p><code>getDefaultReadLimit</code>\u00a0is part of the <code>SupportsAdmissionControl</code> (Spark Structured Streaming) abstraction.</p> <p><code>getDefaultReadLimit</code> creates a AdmissionLimits and requests it for a corresponding ReadLimit.</p>","text":""},{"location":"DeltaSource/#retrieving-last-element-from-iterator","title":"Retrieving Last Element From Iterator <pre><code>iteratorLast[T](\n  iter: Iterator[T]): Option[T]\n</code></pre> <p><code>iteratorLast</code> simply returns the last element of the given <code>Iterator</code> (Scala) or <code>None</code>.</p> <p><code>iteratorLast</code> is used when:</p> <ul> <li><code>DeltaSource</code> is requested to getStartingOffset and getOffset</li> </ul>","text":""},{"location":"DeltaSource/#excluderegex-option","title":"excludeRegex Option <pre><code>excludeRegex: Option[Regex]\n</code></pre> <p><code>excludeRegex</code> requests the DeltaOptions for the value of excludeRegex option.</p>  <p>Refactor It</p> <p><code>excludeRegex</code> should not really be part of <code>DeltaSource</code> (more of DeltaSourceBase) since it's used elsewhere anyway.</p>   <p><code>excludeRegex</code> is used when:</p> <ul> <li><code>DeltaSourceBase</code> is requested to getFileChangesAndCreateDataFrame</li> <li><code>IndexedChangeFileSeq</code> (of DeltaSourceCDCSupport) is requested to isValidIndexedFile</li> </ul>","text":""},{"location":"DeltaSource/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.sources.DeltaSource</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.delta.sources.DeltaSource=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"DeltaSourceBase/","title":"DeltaSourceBase","text":"<p><code>DeltaSourceBase</code> is an extension of the <code>Source</code> (Spark Structured Streaming) abstraction for DeltaSource.</p>"},{"location":"DeltaSourceBase/#read-schema","title":"Read Schema <pre><code>schema: StructType\n</code></pre> <p><code>schema</code> is part of the <code>Source</code> (Spark Structured Streaming) abstraction.</p>  <p><code>schema</code> removes the default expressions from the table schema (from the Metadata of the Snapshot of the DeltaLog).</p> <p>In the end, <code>schema</code> adds the CDF columns to the schema when readChangeFeed option is enabled. Otherwise, <code>schema</code> returns the schema with no CDF columns and default expressions.</p>","text":""},{"location":"DeltaSourceBase/#createdataframebetweenoffsets","title":"createDataFrameBetweenOffsets <pre><code>createDataFrameBetweenOffsets(\n  startVersion: Long,\n  startIndex: Long,\n  isStartingVersion: Boolean,\n  startSourceVersion: Option[Long],\n  startOffsetOption: Option[Offset],\n  endOffset: DeltaSourceOffset): DataFrame\n</code></pre> <p><code>createDataFrameBetweenOffsets</code> getFileChangesAndCreateDataFrame.</p>  <p>Note</p> <p>The <code>startSourceVersion</code> and <code>startOffsetOption</code> input arguments are ignored. It looks like the method should be marked as <code>@obsolete</code> and soon removed.</p>  <p><code>createDataFrameBetweenOffsets</code> is used when:</p> <ul> <li><code>DeltaSource</code> is requested for the streaming micro-batch DataFrame</li> </ul>","text":""},{"location":"DeltaSourceBase/#getfilechangesandcreatedataframe","title":"getFileChangesAndCreateDataFrame <pre><code>getFileChangesAndCreateDataFrame(\n  startVersion: Long,\n  startIndex: Long,\n  isStartingVersion: Boolean,\n  endOffset: DeltaSourceOffset): DataFrame\n</code></pre> <p>With readChangeFeed option enabled, <code>getFileChangesAndCreateDataFrame</code> getCDCFileChangesAndCreateDataFrame.</p> <p>Otherwise, <code>getFileChangesAndCreateDataFrame</code> gets the file changes (as <code>IndexedFile</code>s with AddFiles, RemoveFiles or AddCDCFiles) and take as much file changes so their version and index (these actions belong to) are up to and including DeltaSourceOffset (based on the reservoirVersion and index). <code>getFileChangesAndCreateDataFrame</code> filters out the file changes with the path that matches the excludeRegex option. In the end, <code>getFileChangesAndCreateDataFrame</code> createDataFrame (from the filtered file changes).</p>","text":""},{"location":"DeltaSourceBase/#createdataframe","title":"createDataFrame <pre><code>createDataFrame(\n  indexedFiles: Iterator[IndexedFile]): DataFrame\n</code></pre> <p><code>createDataFrame</code> collects AddFiles from the given <code>indexedFiles</code> collection.</p> <p>In the end, <code>createDataFrame</code> requests the DeltaLog to createDataFrame (for the <code>AddFile</code>s and with <code>isStreaming</code> flag enabled).</p>","text":""},{"location":"DeltaSourceBase/#getstartingoffsetfromspecificdeltaversion","title":"getStartingOffsetFromSpecificDeltaVersion <pre><code>getStartingOffsetFromSpecificDeltaVersion(\n  fromVersion: Long,\n  isStartingVersion: Boolean,\n  limits: Option[AdmissionLimits]): Option[Offset]\n</code></pre> <p><code>getStartingOffsetFromSpecificDeltaVersion</code> getFileChangesWithRateLimit and takes the last <code>IndexedFile</code> (if any).</p> <p><code>getStartingOffsetFromSpecificDeltaVersion</code> returns <code>None</code> for no (last) <code>IndexedFile</code>. Otherwise, <code>getStartingOffsetFromSpecificDeltaVersion</code> buildOffsetFromIndexedFile.</p> <p><code>getStartingOffsetFromSpecificDeltaVersion</code> is used when:</p> <ul> <li><code>DeltaSource</code> is requested for the starting offset</li> </ul>","text":""},{"location":"DeltaSourceBase/#getnextoffsetfrompreviousoffset","title":"getNextOffsetFromPreviousOffset <pre><code>getNextOffsetFromPreviousOffset(\n  previousOffset: DeltaSourceOffset,\n  limits: Option[AdmissionLimits]): Option[Offset]\n</code></pre> <p><code>getNextOffsetFromPreviousOffset</code>...FIXME</p> <p><code>getNextOffsetFromPreviousOffset</code> is used when:</p> <ul> <li><code>DeltaSource</code> is requested for the latest offset</li> </ul>","text":""},{"location":"DeltaSourceBase/#getfilechangeswithratelimit","title":"getFileChangesWithRateLimit <pre><code>getFileChangesWithRateLimit(\n  fromVersion: Long,\n  fromIndex: Long,\n  isStartingVersion: Boolean,\n  limits: Option[AdmissionLimits] = Some(new AdmissionLimits())): ClosableIterator[IndexedFile]\n</code></pre> <p><code>getFileChangesWithRateLimit</code>...FIXME</p> <p><code>getFileChangesWithRateLimit</code> is used when:</p> <ul> <li><code>DeltaSourceBase</code> is requested to getStartingOffsetFromSpecificDeltaVersion and getNextOffsetFromPreviousOffset</li> </ul>","text":""},{"location":"DeltaSourceBase/#buildoffsetfromindexedfile","title":"buildOffsetFromIndexedFile <pre><code>buildOffsetFromIndexedFile(\n  indexedFile: IndexedFile,\n  version: Long,\n  isStartingVersion: Boolean): Option[DeltaSourceOffset]\n</code></pre> <p><code>buildOffsetFromIndexedFile</code>...FIXME</p> <p><code>buildOffsetFromIndexedFile</code> is used when:</p> <ul> <li><code>DeltaSourceBase</code> is requested to getStartingOffsetFromSpecificDeltaVersion and getNextOffsetFromPreviousOffset</li> </ul>","text":""},{"location":"DeltaSourceBase/#supportsadmissioncontrol","title":"SupportsAdmissionControl <p><code>DeltaSourceBase</code> is a <code>SupportsAdmissionControl</code> (Spark Structured Streaming).</p>  <p>Note</p> <p>All the methods of <code>SupportsAdmissionControl</code> are in DeltaSource.</p>","text":""},{"location":"DeltaSourceOffset/","title":"DeltaSourceOffset","text":"<p><code>DeltaSourceOffset</code> is a streaming <code>Offset</code> (Spark Structured Streaming) for DeltaSource.</p>"},{"location":"DeltaSourceOffset/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaSourceOffset</code> takes the following to be created:</p> <ul> <li> Source Version <li> Reservoir ID (aka Table ID) <li> Reservoir Version <li> Index <li>isStartingVersion flag</li> <p><code>DeltaSourceOffset</code> is created (using apply utility)\u00a0when:</p> <ul> <li><code>DeltaSource</code> is requested for the starting and latest offsets</li> </ul>"},{"location":"DeltaSourceOffset/#isstartingversion-flag","title":"isStartingVersion Flag <p><code>DeltaSourceOffset</code> is given <code>isStartingVersion</code> flag when created to denote a query that is starting rather than processing changes.</p> <p><code>isStartingVersion</code> flag is <code>false</code> when:</p> <ul> <li><code>DeltaSourceBase</code> is requested to buildOffsetFromIndexedFile with the last <code>IndexedFile</code> for a given <code>version</code></li> </ul> <p><code>isStartingVersion</code> flag is copied over (continued) as long as the versions are the same when buildOffsetFromIndexedFile</p>","text":""},{"location":"DeltaSourceOffset/#creating-deltasourceoffset","title":"Creating DeltaSourceOffset <pre><code>apply(\n  reservoirId: String,\n  offset: Offset): DeltaSourceOffset\napply(\n  reservoirId: String,\n  reservoirVersion: Long,\n  index: Long,\n  isStartingVersion: Boolean): DeltaSourceOffset\n</code></pre> <p><code>apply</code> creates a DeltaSourceOffset (for the version and the given arguments) or converts a <code>SerializedOffset</code> to a <code>DeltaSourceOffset</code>.</p> <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>DeltaSource</code> is requested for the starting and latest offsets</li> </ul>","text":""},{"location":"DeltaSourceOffset/#validatesourceversion","title":"validateSourceVersion <pre><code>validateSourceVersion(\n  json: String): Unit\n</code></pre> <p><code>validateSourceVersion</code>...FIXME</p>","text":""},{"location":"DeltaSourceOffset/#source-version","title":"Source Version <p><code>DeltaSourceOffset</code> uses <code>1</code> for the version (and does not allow changing it).</p> <p>The version is used when:</p> <ul> <li>DeltaSourceOffset.apply and validateSourceVersion utilities are used</li> </ul>","text":""},{"location":"DeltaSourceSnapshot/","title":"DeltaSourceSnapshot","text":"<p><code>DeltaSourceSnapshot</code> is a SnapshotIterator and a StateCache for DeltaSource.</p>"},{"location":"DeltaSourceSnapshot/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaSourceSnapshot</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> Snapshot <li> Filter Expressions (Spark SQL) <p><code>DeltaSourceSnapshot</code> is created\u00a0when:</p> <ul> <li><code>DeltaSource</code> is requested for the snapshot of a delta table at a given version</li> </ul>"},{"location":"DeltaSourceSnapshot/#initialfiles-dataset-of-indexedfiles","title":"initialFiles Dataset (of IndexedFiles) <pre><code>initialFiles: Dataset[IndexedFile]\n</code></pre>","text":""},{"location":"DeltaSourceSnapshot/#dataset-of-indexed-addfiles","title":"Dataset of Indexed AddFiles <p><code>initialFiles</code> requests the Snapshot for all AddFiles (in the snapshot) (<code>Dataset[AddFile]</code>).</p> <p><code>initialFiles</code> sorts the AddFile dataset (<code>Dataset[AddFile]</code>) by modificationTime and path in ascending order.</p> <p><code>initialFiles</code> indexes the <code>AddFiles</code> (using <code>RDD.zipWithIndex</code> operator) that gives a <code>RDD[(AddFile, Long)]</code>.</p> <p><code>initialFiles</code> converts the <code>RDD</code> to a <code>DataFrame</code> of two columns: <code>add</code> and <code>index</code>.</p> <p><code>initialFiles</code> adds the two new columns:</p> <ul> <li>version</li> <li><code>isLast</code> as <code>false</code> literal</li> </ul> <p><code>initialFiles</code> converts (projects) <code>DataFrame</code> to <code>Dataset[IndexedFile]</code>.</p>","text":""},{"location":"DeltaSourceSnapshot/#creating-cachedds","title":"Creating CachedDS <p><code>initialFiles</code> caches the <code>Dataset[IndexedFile]</code> under the following name (with the version and the redactedPath of this Snapshot):</p> <pre><code>Delta Source Snapshot #[version] - [redactedPath]\n</code></pre>","text":""},{"location":"DeltaSourceSnapshot/#cached-dataset-of-indexed-addfiles","title":"Cached Dataset of Indexed AddFiles <p>In the end, <code>initialFiles</code> requests the CachedDS to getDS.</p>","text":""},{"location":"DeltaSourceSnapshot/#usage","title":"Usage <p><code>initialFiles</code> is used when:</p> <ul> <li><code>SnapshotIterator</code> is requested for the AddFiles</li> </ul>","text":""},{"location":"DeltaSourceSnapshot/#closing","title":"Closing <pre><code>close(\n  unpersistSnapshot: Boolean): Unit\n</code></pre> <p><code>close</code>\u00a0is part of the SnapshotIterator abstraction.</p> <p><code>close</code> requests the Snapshot to uncache when the given <code>unpersistSnapshot</code> flag is enabled.</p>","text":""},{"location":"DeltaSourceUtils/","title":"DeltaSourceUtils Utility","text":""},{"location":"DeltaSourceUtils/#deltagenerationexpression","title":"delta.generationExpression <p><code>DeltaSourceUtils</code> defines <code>delta.generationExpression</code> metadata key for the generation expression of a generated column of a delta table.</p>  <p>Used when:</p> <ul> <li><code>DeltaColumnBuilder</code> is requested to build a StructField</li> <li><code>ColumnWithDefaultExprUtils</code> is requested to removeDefaultExpressions</li> <li>GeneratedColumn utility is used to isGeneratedColumn and getGenerationExpressionStr</li> <li><code>SchemaUtils</code> utility is used to reportDifferences</li> </ul>","text":""},{"location":"DeltaSourceUtils/#deltaidentityallowexplicitinsert","title":"delta.identity.allowExplicitInsert <p><code>DeltaSourceUtils</code> defines <code>delta.identity.allowExplicitInsert</code> metadata key for...FIXME</p> <p>Used when:</p> <ul> <li><code>ColumnWithDefaultExprUtils</code> utility is used to isIdentityColumn and removeDefaultExpressions</li> </ul>","text":""},{"location":"DeltaSourceUtils/#deltaidentitystart","title":"delta.identity.start <p><code>DeltaSourceUtils</code> defines <code>delta.identity.start</code> metadata key for...FIXME</p> <p>Used when:</p> <ul> <li><code>ColumnWithDefaultExprUtils</code> utility is used to isIdentityColumn and removeDefaultExpressions</li> </ul>","text":""},{"location":"DeltaSourceUtils/#deltaidentitystep","title":"delta.identity.step <p><code>DeltaSourceUtils</code> defines <code>delta.identity.step</code> metadata key for...FIXME</p> <p>Used when:</p> <ul> <li><code>ColumnWithDefaultExprUtils</code> utility is used to isIdentityColumn and removeDefaultExpressions</li> </ul>","text":""},{"location":"DeltaSourceUtils/#isdeltadatasourcename","title":"isDeltaDataSourceName <pre><code>isDeltaDataSourceName(\n  name: String): Boolean\n</code></pre> <p><code>isDeltaDataSourceName</code> returns <code>true</code> when the given <code>name</code> is <code>delta</code> (case-insensitively).</p> <p><code>isDeltaDataSourceName</code> is used when:</p> <ul> <li><code>DeltaTableUtils</code> is requested to isValidPath</li> <li><code>DeltaUnsupportedOperationsCheck</code> is requested to fail (to throw an <code>DeltaAnalysisException</code>)</li> <li><code>DeltaCatalog</code> is requested to createTable, stageReplace, stageCreateOrReplace, stageCreate</li> <li><code>SupportsPathIdentifier</code> is requested to <code>hasDeltaNamespace</code></li> <li><code>ConvertToDeltaCommandBase</code> is requested to isPathIdentifier</li> <li><code>DeltaCommand</code> is requested to isPathIdentifier</li> <li><code>DeltaSourceUtils</code> is requested to isDeltaTable</li> </ul>","text":""},{"location":"DeltaSparkSessionExtension/","title":"DeltaSparkSessionExtension","text":"<p>DeltaSparkSessionExtension is used to register (inject) the following extensions to a <code>SparkSession</code>:</p> <ul> <li>Delta SQL support (using DeltaSqlParser)</li> <li>DeltaAnalysis</li> <li>DeltaUnsupportedOperationsCheck</li> <li>PrepareDeltaScan</li> <li>PreprocessTableDelete</li> <li>PreprocessTableMerge</li> <li>PreprocessTableRestore</li> <li>PreprocessTableUpdate</li> <li>RangePartitionIdRewrite</li> </ul> <p><code>DeltaSparkSessionExtension</code> is registered using spark.sql.extensions configuration property (while creating a <code>SparkSession</code> in a Spark application).</p>"},{"location":"DeltaTable/","title":"DeltaTable","text":"<p><code>DeltaTable</code> is the management interface of delta tables.</p>","tags":["DeveloperApi"]},{"location":"DeltaTable/#iodeltatables-package","title":"io.delta.tables Package","text":"<p><code>DeltaTable</code> belongs to <code>io.delta.tables</code> package.</p> <pre><code>import io.delta.tables.DeltaTable\n</code></pre>","tags":["DeveloperApi"]},{"location":"DeltaTable/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaTable</code> takes the following to be created:</p> <ul> <li> Table Data (<code>Dataset[Row]</code>) <li> DeltaTableV2 <p><code>DeltaTable</code> is created using DeltaTable.forPath and DeltaTable.forName utilities (and indirectly using create, createIfNotExists, createOrReplace and replace).</p>","tags":["DeveloperApi"]},{"location":"DeltaTable/#deltalog","title":"DeltaLog <pre><code>deltaLog: DeltaLog\n</code></pre> <p>DeltaLog of the DeltaTableV2.</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#utilities-static-methods","title":"Utilities (Static Methods)","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#columnbuilder","title":"columnBuilder <pre><code>columnBuilder(\n  colName: String): DeltaColumnBuilder\ncolumnBuilder(\n  spark: SparkSession,\n  colName: String): DeltaColumnBuilder\n</code></pre> <p>Creates a DeltaColumnBuilder</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#converttodelta","title":"convertToDelta <pre><code>convertToDelta(\n  spark: SparkSession,\n  identifier: String): DeltaTable\nconvertToDelta(\n  spark: SparkSession,\n  identifier: String,\n  partitionSchema: String): DeltaTable\nconvertToDelta(\n  spark: SparkSession,\n  identifier: String,\n  partitionSchema: StructType): DeltaTable\n</code></pre> <p><code>convertToDelta</code> converts the parquet table to delta format</p>  <p>Note</p> <p>Refer to Demo: Converting Parquet Dataset Into Delta Format for a demo of <code>DeltaTable.convertToDelta</code>.</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#create","title":"create <pre><code>create(): DeltaTableBuilder\ncreate(\n  spark: SparkSession): DeltaTableBuilder\n</code></pre> <p>Creates a DeltaTableBuilder</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#createifnotexists","title":"createIfNotExists <pre><code>createIfNotExists(): DeltaTableBuilder\ncreateIfNotExists(\n  spark: SparkSession): DeltaTableBuilder\n</code></pre> <p>Creates a DeltaTableBuilder (with <code>CreateTableOptions</code> and <code>ifNotExists</code> flag enabled)</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#createorreplace","title":"createOrReplace <pre><code>createOrReplace(): DeltaTableBuilder\ncreateOrReplace(\n  spark: SparkSession): DeltaTableBuilder\n</code></pre> <p>Creates a DeltaTableBuilder (with <code>ReplaceTableOptions</code> and <code>orCreate</code> flag enabled)</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#forname","title":"forName <pre><code>forName(\n  sparkSession: SparkSession,\n  tableName: String): DeltaTable\nforName(\n  tableOrViewName: String): DeltaTable\n</code></pre> <p><code>forName</code> uses <code>ParserInterface</code> (of the given <code>SparkSession</code>) to parse the given table name.</p> <p><code>forName</code> checks whether the given table name is of a Delta table and, if so, creates a DeltaTable with the following:</p> <ul> <li><code>Dataset</code> that represents loading data from the specified table name (using <code>SparkSession.table</code> operator)</li> <li>DeltaTableV2</li> </ul> <p><code>forName</code> throws an <code>AnalysisException</code> when the given table name is for non-Delta table:</p> <pre><code>[deltaTableIdentifier] is not a Delta table.\n</code></pre>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#forpath","title":"forPath <pre><code>forPath(\n  sparkSession: SparkSession,\n  path: String): DeltaTable\nforPath(\n  path: String): DeltaTable\n</code></pre> <p><code>forPath</code> checks whether the given table name is of a Delta table and, if so, creates a DeltaTable with the following:</p> <ul> <li><code>Dataset</code> that represents loading data from the specified <code>path</code> using delta data source</li> <li>DeltaTableV2</li> </ul> <p><code>forPath</code> throws an <code>AnalysisException</code> when the given <code>path</code> does not belong to a delta table:</p> <pre><code>[deltaTableIdentifier] is not a Delta table.\n</code></pre>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#isdeltatable","title":"isDeltaTable <pre><code>isDeltaTable(\n  sparkSession: SparkSession,\n  identifier: String): Boolean\nisDeltaTable(\n  identifier: String): Boolean\n</code></pre> <p><code>isDeltaTable</code>...FIXME</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#replace","title":"replace <pre><code>replace(): DeltaTableBuilder\nreplace(\n  spark: SparkSession): DeltaTableBuilder\n</code></pre> <p>Creates a DeltaTableBuilder (with <code>ReplaceTableOptions</code> and <code>orCreate</code> flag disabled)</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#operators","title":"Operators","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#alias","title":"alias <pre><code>alias(\n  alias: String): DeltaTable\n</code></pre> <p>Applies an alias to the <code>DeltaTable</code> (equivalent to as)</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#as","title":"as <pre><code>as(\n  alias: String): DeltaTable\n</code></pre> <p>Applies an alias to the <code>DeltaTable</code></p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#delete","title":"delete <pre><code>delete(): Unit\ndelete(\n  condition: Column): Unit\ndelete(\n  condition: String): Unit\n</code></pre> <p>Executes DeleteFromTable command</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#generate","title":"generate <pre><code>generate(\n  mode: String): Unit\n</code></pre> <p>Executes the DeltaGenerateCommand</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#history","title":"history <pre><code>history(): DataFrame\nhistory(\n  limit: Int): DataFrame\n</code></pre> <p>Requests the DeltaHistoryManager for history.</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#merge","title":"merge <pre><code>merge(\n  source: DataFrame,\n  condition: Column): DeltaMergeBuilder\nmerge(\n  source: DataFrame,\n  condition: String): DeltaMergeBuilder\n</code></pre> <p>Creates a DeltaMergeBuilder</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#optimize","title":"optimize <pre><code>optimize(): DeltaOptimizeBuilder\n</code></pre> <p>Creates a DeltaOptimizeBuilder</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#restoretotimestamp","title":"restoreToTimestamp <pre><code>restoreToTimestamp(\n  timestamp: String): DataFrame\n</code></pre> <p>Executes Restore</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#restoretoversion","title":"restoreToVersion <pre><code>restoreToVersion(\n  version: Long): DataFrame\n</code></pre> <p>Executes Restore</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#todf","title":"toDF <pre><code>toDF: Dataset[Row]\n</code></pre> <p>Returns the DataFrame representation of the DeltaTable</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#update","title":"update <pre><code>update(\n  condition: Column,\n  set: Map[String, Column]): Unit\nupdate(\n  set: Map[String, Column]): Unit\n</code></pre> <p>Executes UpdateTable command</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#updateexpr","title":"updateExpr <pre><code>updateExpr(\n  set: Map[String, String]): Unit\nupdateExpr(\n  condition: String,\n  set: Map[String, String]): Unit\n</code></pre> <p>Executes UpdateTable command</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#upgradetableprotocol","title":"upgradeTableProtocol <pre><code>upgradeTableProtocol(\n  readerVersion: Int,\n  writerVersion: Int): Unit\n</code></pre> <p><code>upgradeTableProtocol</code> creates a new Protocol (for the given reader and writer versions) and requests the DeltaLog to upgrade the protocol.</p>  <p>Transactional Operation</p> <p><code>upgradeTableProtocol</code> is a transactional operation.</p>   <p>Updates the protocol version of the table to leverage new features.</p> <p>Upgrading the reader version will prevent all clients that have an older version of Delta Lake from accessing this table.</p> <p>Upgrading the writer version will prevent older versions of Delta Lake to write to this table.</p> <p>The reader or writer version cannot be downgraded.</p>  [SC-44271][DELTA] Introduce default protocol version for Delta tables <p><code>upgradeTableProtocol</code> was introduced in [SC-44271][DELTA] Introduce default protocol version for Delta tables commit.</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTable/#vacuum","title":"vacuum <pre><code>vacuum(): DataFrame\nvacuum(\n  retentionHours: Double): DataFrame\n</code></pre> <p>Deletes files and directories (recursively) in the DeltaTable that are not needed by the table (and maintains older versions up to the given retention threshold).</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTableBuilder/","title":"DeltaTableBuilder","text":"<p><code>DeltaTableBuilder</code> is a builder interface to create DeltaTables programmatically.</p> <p><code>DeltaTableBuilder</code> is created using the following DeltaTable utilities:</p> <ul> <li>DeltaTable.create</li> <li>DeltaTable.createIfNotExists</li> <li>DeltaTable.replace</li> <li>DeltaTable.createOrReplace</li> </ul> <p>In the end, <code>DeltaTableBuilder</code> is supposed to be executed to take action.</p>","tags":["DeveloperApi"]},{"location":"DeltaTableBuilder/#iodeltatables-package","title":"io.delta.tables Package","text":"<p><code>DeltaTableBuilder</code> belongs to <code>io.delta.tables</code> package.</p> <pre><code>import io.delta.tables.DeltaTableBuilder\n</code></pre>","tags":["DeveloperApi"]},{"location":"DeltaTableBuilder/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaTableBuilder</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> <code>DeltaTableBuilderOptions</code>","tags":["DeveloperApi"]},{"location":"DeltaTableBuilder/#operators","title":"Operators","text":"","tags":["DeveloperApi"]},{"location":"DeltaTableBuilder/#addcolumn","title":"addColumn <pre><code>addColumn(\n  colName: String,\n  dataType: DataType): DeltaTableBuilder\naddColumn(\n  colName: String,\n  dataType: DataType,\n  nullable: Boolean): DeltaTableBuilder\naddColumn(\n  colName: String,\n  dataType: String): DeltaTableBuilder\naddColumn(\n  colName: String,\n  dataType: String,\n  nullable: Boolean): DeltaTableBuilder\naddColumn(\n  col: StructField): DeltaTableBuilder\n</code></pre> <p>Adds a column (that could be defined using DeltaColumnBuilder)</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTableBuilder/#addcolumns","title":"addColumns <pre><code>addColumns(\n  cols: StructType): DeltaTableBuilder\n</code></pre> <p>Adds columns based on the given <code>StructType</code> (Spark SQL)</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTableBuilder/#comment","title":"comment <pre><code>comment(\n  comment: String): DeltaTableBuilder\n</code></pre>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTableBuilder/#execute","title":"execute <pre><code>execute(): DeltaTable\n</code></pre> <p>Creates a DeltaTable</p>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTableBuilder/#location","title":"location <pre><code>location(\n  location: String): DeltaTableBuilder\n</code></pre>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTableBuilder/#partitionedby","title":"partitionedBy <pre><code>partitionedBy(\n  colNames: String*): DeltaTableBuilder\n</code></pre>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTableBuilder/#property","title":"property <pre><code>property(\n  key: String,\n  value: String): DeltaTableBuilder\n</code></pre>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTableBuilder/#tablename","title":"tableName <pre><code>tableName(\n  identifier: String): DeltaTableBuilder\n</code></pre>","text":"","tags":["DeveloperApi"]},{"location":"DeltaTableIdentifier/","title":"DeltaTableIdentifier","text":"<p><code>DeltaTableIdentifier</code> is an identifier of a delta table by TableIdentifier or directory depending whether it is a catalog table or not (and living non-cataloged).</p>"},{"location":"DeltaTableIdentifier/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaTableIdentifier</code> takes the following to be created:</p> <ul> <li> Path to a delta table (default: undefined) <li> <code>TableIdentifier</code> (default: undefined) <p>Either a path or a table identifier is required.</p>"},{"location":"DeltaTableIdentifier/#creating-deltatableidentifier","title":"Creating DeltaTableIdentifier <pre><code>apply(\n  spark: SparkSession,\n  identifier: TableIdentifier): Option[DeltaTableIdentifier]\n</code></pre> <p><code>apply</code> creates a new DeltaTableIdentifier for the given <code>TableIdentifier</code>:</p> <ol> <li>For a path (to a delta table), <code>apply</code> creates a <code>DeltaTableIdentifier</code> with the path</li> <li>For a delta table, <code>apply</code> creates a <code>DeltaTableIdentifier</code> with a TableIdentifier</li> <li>For all the other cases, <code>apply</code> returns <code>None</code></li> </ol>","text":""},{"location":"DeltaTableIdentifier/#isdeltapath","title":"isDeltaPath <pre><code>isDeltaPath(\n  spark: SparkSession,\n  identifier: TableIdentifier): Boolean\n</code></pre> <p><code>isDeltaPath</code> checks whether the input <code>TableIdentifier</code> represents an (absolute) path to a delta table.</p> <p><code>isDeltaPath</code> is positive (<code>true</code>) when all the following hold:</p> <ol> <li><code>spark.sql.runSQLOnFiles</code> (Spark SQL) configuration property is <code>true</code></li> <li>DeltaSourceUtils.isDeltaTable(identifier.database)</li> <li>The <code>TableIdentifier</code> is not a temporary view</li> <li>The table in the database (as specified in the <code>TableIdentifier</code>) does not exist</li> <li>The table part (of the <code>TableIdentifier</code>) is absolute (starts with <code>/</code>)</li> </ol>","text":""},{"location":"DeltaTableIdentifier/#creating-deltalog","title":"Creating DeltaLog <pre><code>getDeltaLog(\n  spark: SparkSession): DeltaLog\n</code></pre> <p><code>getDeltaLog</code> creates a DeltaLog (for the location).</p>  <p>Note</p> <p><code>getDeltaLog</code> does not seem to be used.</p>","text":""},{"location":"DeltaTableIdentifier/#location-path","title":"Location Path <pre><code>getPath(\n  spark: SparkSession): Path\n</code></pre> <p><code>getPath</code> creates a Hadoop Path for the path if defined or requests <code>SessionCatalog</code> (Spark SQL) for the table metadata and uses the <code>locationUri</code>.</p>","text":""},{"location":"DeltaTableOperations/","title":"DeltaTableOperations \u2014 Delta DML Operations","text":"<p><code>DeltaTableOperations</code> is an abstraction of management services for DeltaTable commands.</p>"},{"location":"DeltaTableOperations/#implementations","title":"Implementations","text":"<ul> <li>DeltaTable</li> </ul>"},{"location":"DeltaTableOperations/#deltatable","title":"DeltaTable <p><code>DeltaTableOperations</code> uses Scala's self-type feature which forces it to be mixed into DeltaTable or its subtypes.</p>","text":""},{"location":"DeltaTableOperations/#deltatable-commands","title":"DeltaTable Commands","text":""},{"location":"DeltaTableOperations/#executedelete","title":"executeDelete <pre><code>executeDelete(\n  condition: Option[Expression]): Unit\n</code></pre> <p><code>executeDelete</code> creates a <code>DeleteFromTable</code> (Spark SQL) logical command (for the analyzed query plan of the DataFrame of the DeltaTable and the given <code>condition</code>).</p> <p><code>executeDelete</code> creates a <code>DataFrame</code> for the <code>DeleteFromTable</code>.</p>  <p>DeleteFromTable and DeltaDelete</p> <p><code>DeleteFromTable</code> is resolved to DeltaDelete logical command.</p>  <p><code>executeDelete</code> is used when:</p> <ul> <li><code>DeltaTable</code> is requested to delete</li> </ul>","text":""},{"location":"DeltaTableOperations/#executegenerate","title":"executeGenerate <pre><code>executeGenerate(\n  tblIdentifier: String, mode: String): Unit\n</code></pre> <p><code>executeGenerate</code> creates a <code>DataFrame</code> for a DeltaGenerateCommand.</p> <p><code>executeGenerate</code> is used when:</p> <ul> <li><code>DeltaTable</code> is requested to generate</li> </ul>","text":""},{"location":"DeltaTableOperations/#executehistory","title":"executeHistory <pre><code>executeHistory(\n  deltaLog: DeltaLog,\n  limit: Option[Int] = None,\n  tableId: Option[TableIdentifier] = None): DataFrame\n</code></pre> <p><code>executeHistory</code> requests the given DeltaLog for the DeltaHistoryManager for the history.</p> <p><code>executeHistory</code> is used when:</p> <ul> <li><code>DeltaTable</code> is requested to history</li> </ul>","text":""},{"location":"DeltaTableOperations/#executerestore","title":"executeRestore <pre><code>executeRestore(\n  table: DeltaTableV2,\n  versionAsOf: Option[Long],\n  timestampAsOf: Option[String]): DataFrame\n</code></pre> <p><code>executeRestore</code> creates a RestoreTableStatement for a new <code>DataSourceV2Relation</code> (Spark SQL) (for the given DeltaTableV2 and the given <code>versionAsOf</code> and <code>timestampAsOf</code> time travel metadata).</p> <p><code>executeRestore</code> creates a <code>DataFrame</code> for the <code>DeleteFromTable</code>.</p> <p><code>executeRestore</code> is used when:</p> <ul> <li><code>DeltaTable</code> is requested to restoreToVersion and restoreToTimestamp</li> </ul>","text":""},{"location":"DeltaTableOperations/#executeupdate","title":"executeUpdate <pre><code>executeUpdate(\n  set: Map[String, Column],\n  condition: Option[Column]): Unit\n</code></pre> <p><code>executeUpdate</code> creates a <code>UpdateTable</code> (Spark SQL) logical command (for the analyzed query plan of the DataFrame of the DeltaTable and the given <code>condition</code>).</p> <p><code>executeDelete</code> creates a <code>DataFrame</code> for the <code>UpdateTable</code>.</p>  <p>UpdateTable and DeltaUpdateTable</p> <p><code>UpdateTable</code> is resolved to DeltaUpdateTable logical command.</p>  <p><code>executeUpdate</code> is used when:</p> <ul> <li><code>DeltaTable</code> is requested to update and updateExpr</li> </ul>","text":""},{"location":"DeltaTableOperations/#executevacuum","title":"executeVacuum <pre><code>executeVacuum(\n  deltaLog: DeltaLog,\n  retentionHours: Option[Double],\n  tableId: Option[TableIdentifier] = None): DataFrame\n</code></pre> <p><code>executeVacuum</code> runs garbage collection of the given DeltaLog.</p> <p><code>executeVacuum</code> is used when:</p> <ul> <li><code>DeltaTable</code> is requested to vacuum</li> </ul>","text":""},{"location":"DeltaTableUtils/","title":"DeltaTableUtils Utility","text":""},{"location":"DeltaTableUtils/#extractifpathcontainstimetravel","title":"extractIfPathContainsTimeTravel <pre><code>extractIfPathContainsTimeTravel(\n  session: SparkSession,\n  path: String): (String, Option[DeltaTimeTravelSpec])\n</code></pre> <p><code>extractIfPathContainsTimeTravel</code> uses the internal spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled configuration property to find time travel patterns in the given <code>path</code>.</p> <p><code>extractIfPathContainsTimeTravel</code>...FIXME</p> <p><code>extractIfPathContainsTimeTravel</code>\u00a0is used when:</p> <ul> <li><code>DeltaDataSource</code> is requested to sourceSchema and parsePathIdentifier</li> </ul>","text":""},{"location":"DeltaTableUtils/#finddeltatableroot","title":"findDeltaTableRoot <pre><code>findDeltaTableRoot(\n  spark: SparkSession,\n  path: Path,\n  options: Map[String, String] = Map.empty): Option[Path]\n</code></pre> <p><code>findDeltaTableRoot</code> traverses the Hadoop DFS-compliant path upwards (to the root directory of the file system) until <code>_delta_log</code> or <code>_samples</code> directories are found, or the root directory is reached.</p> <p>For <code>_delta_log</code> or <code>_samples</code> directories, <code>findDeltaTableRoot</code> returns the parent directory (of <code>_delta_log</code> directory).</p> <p><code>findDeltaTableRoot</code>\u00a0is used when:</p> <ul> <li>DeltaTable.isDeltaTable utility is used</li> <li>VacuumTableCommand is executed</li> <li><code>DeltaTableUtils</code> utility is used to isDeltaTable</li> <li><code>DeltaDataSource</code> utility is used to parsePathIdentifier</li> </ul>","text":""},{"location":"DeltaTableUtils/#ispredicatepartitioncolumnsonly","title":"isPredicatePartitionColumnsOnly <pre><code>isPredicatePartitionColumnsOnly(\n  condition: Expression,\n  partitionColumns: Seq[String],\n  spark: SparkSession): Boolean\n</code></pre> <p><code>isPredicatePartitionColumnsOnly</code> holds <code>true</code> when all of the references of the <code>condition</code> expression are among the <code>partitionColumns</code>.</p> <p><code>isPredicatePartitionColumnsOnly</code>\u00a0is used when:</p> <ul> <li><code>DeltaTableUtils</code> is used to isPredicateMetadataOnly</li> <li><code>OptimisticTransactionImpl</code> is requested for the filterFiles</li> <li><code>DeltaSourceSnapshot</code> is requested for the partition and data filters</li> </ul>","text":""},{"location":"DeltaTableUtils/#isdeltatable","title":"isDeltaTable <pre><code>isDeltaTable(\n  table: CatalogTable): Boolean\nisDeltaTable(\n  spark: SparkSession,\n  path: Path): Boolean\nisDeltaTable(\n  spark: SparkSession,\n  tableName: TableIdentifier): Boolean\n</code></pre> <p><code>isDeltaTable</code>...FIXME</p> <p><code>isDeltaTable</code>\u00a0is used when:</p> <ul> <li><code>DeltaCatalog</code> is requested to loadTable</li> <li>DeltaTable.forName, DeltaTable.forPath and DeltaTable.isDeltaTable utilities are used</li> <li><code>DeltaTableIdentifier</code> utility is used to create a DeltaTableIdentifier from a TableIdentifier</li> <li><code>DeltaUnsupportedOperationsCheck</code> is requested to fail</li> </ul>","text":""},{"location":"DeltaTableUtils/#resolvetimetravelversion","title":"resolveTimeTravelVersion <pre><code>resolveTimeTravelVersion(\n  conf: SQLConf,\n  deltaLog: DeltaLog,\n  tt: DeltaTimeTravelSpec): (Long, String)\n</code></pre> <p><code>resolveTimeTravelVersion</code>...FIXME</p> <p><code>resolveTimeTravelVersion</code> is used when:</p> <ul> <li><code>DeltaLog</code> is requested to create a relation (per partition filters and time travel)</li> <li><code>DeltaTableV2</code> is requested for a Snapshot</li> </ul>","text":""},{"location":"DeltaTableUtils/#splitmetadataanddatapredicates","title":"splitMetadataAndDataPredicates <pre><code>splitMetadataAndDataPredicates(\n  condition: Expression,\n  partitionColumns: Seq[String],\n  spark: SparkSession): (Seq[Expression], Seq[Expression])\n</code></pre> <p><code>splitMetadataAndDataPredicates</code> splits conjunctive (and) predicates in the given <code>condition</code> expression and partitions them into two collections based on the isPredicateMetadataOnly predicate (with the given <code>partitionColumns</code>).</p> <p><code>splitMetadataAndDataPredicates</code>\u00a0is used when:</p> <ul> <li><code>PartitionFiltering</code> is requested for filesForScan</li> <li>DeleteCommand is executed (with a delete condition)</li> <li>UpdateCommand is executed</li> </ul>","text":""},{"location":"DeltaTableUtils/#ispredicatemetadataonly","title":"isPredicateMetadataOnly <pre><code>isPredicateMetadataOnly(\n  condition: Expression,\n  partitionColumns: Seq[String],\n  spark: SparkSession): Boolean\n</code></pre> <p><code>isPredicateMetadataOnly</code> holds <code>true</code> when the following hold about the given <code>condition</code>:</p> <ol> <li>Is partition column only (given the <code>partitionColumns</code>)</li> <li>Does not contain a subquery</li> </ol>","text":""},{"location":"DeltaTableV2/","title":"DeltaTableV2","text":"<p><code>DeltaTableV2</code> is a logical representation of a writable Delta table.</p>"},{"location":"DeltaTableV2/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaTableV2</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> Path (Hadoop HDFS) <li>CatalogTable Metadata</li> <li> Optional Table ID <li>Optional DeltaTimeTravelSpec</li> <li>Options</li> <li>CDC Options</li> <p><code>DeltaTableV2</code> is created when:</p> <ul> <li><code>DeltaTable</code> utility is used to forPath and forName</li> <li><code>DeltaCatalog</code> is requested to load a table</li> <li><code>DeltaDataSource</code> is requested to load a table or create a table relation</li> </ul>"},{"location":"DeltaTableV2/#table-metadata-catalogtable","title":"Table Metadata (CatalogTable) <pre><code>catalogTable: Option[CatalogTable] = None\n</code></pre> <p><code>DeltaTableV2</code> can be given <code>CatalogTable</code> (Spark SQL) when created. It is undefined by default.</p> <p><code>catalogTable</code> is specified when:</p> <ul> <li>DeltaTable.forName is used (for a cataloged delta table)</li> <li>PreprocessTableRestore logical resolution rule is executed (with a RestoreTableStatement over a cataloged delta table)</li> <li><code>DeltaCatalog</code> is requested to load a table (that is a <code>V1Table</code> and a cataloged delta table)</li> </ul> <p><code>catalogTable</code> is used when:</p> <ul> <li><code>DeltaTableV2</code> is requested for the rootPath (to avoid parsing the path), the name, the properties and the CatalogTable itself</li> <li>DeltaAnalysis logical resolution rule is requested to resolve a RestoreTableStatement (for a <code>TableIdentifier</code>)</li> <li><code>DeltaRelation</code> utility is used to fromV2Relation</li> <li>AlterTableSetLocationDeltaCommand is executed</li> </ul>","text":""},{"location":"DeltaTableV2/#cdc-options","title":"CDC Options <pre><code>cdcOptions: CaseInsensitiveStringMap\n</code></pre> <p><code>DeltaTableV2</code> can be given <code>cdcOptions</code> when created. It is empty by default (and most of the time).</p> <p><code>cdcOptions</code> is specified when:</p> <ul> <li><code>DeltaDataSource</code> is requested to create a relation (for CDC read)</li> <li><code>DeltaTableV2</code> is requested to withOptions</li> </ul> <p><code>cdcOptions</code> is used when:</p> <ul> <li><code>DeltaTableV2</code> is requested for a BaseRelation</li> </ul>","text":""},{"location":"DeltaTableV2/#options","title":"Options <p><code>DeltaTableV2</code> can be given options (as a <code>Map[String, String]</code>). Options are empty by default.</p> <p>The options are defined when <code>DeltaDataSource</code> is requested for a relation with spark.databricks.delta.loadFileSystemConfigsFromDataFrameOptions configuration property enabled.</p> <p>The options are used for the following:</p> <ul> <li>Looking up <code>path</code> or <code>paths</code> options</li> <li>Creating the DeltaLog</li> </ul>","text":""},{"location":"DeltaTableV2/#deltalog","title":"DeltaLog <p><code>DeltaTableV2</code> creates a DeltaLog for the rootPath and the given options.</p>","text":""},{"location":"DeltaTableV2/#table","title":"Table <p><code>DeltaTableV2</code> is a <code>Table</code> (Spark SQL).</p>","text":""},{"location":"DeltaTableV2/#supportswrite","title":"SupportsWrite <p><code>DeltaTableV2</code> is a <code>SupportsWrite</code> (Spark SQL).</p>","text":""},{"location":"DeltaTableV2/#v2tablewithv1fallback","title":"V2TableWithV1Fallback <p><code>DeltaTableV2</code> is a <code>V2TableWithV1Fallback</code> (Spark SQL).</p>","text":""},{"location":"DeltaTableV2/#v1table","title":"v1Table <pre><code>v1Table: CatalogTable\n</code></pre> <p><code>v1Table</code> is part of the <code>V2TableWithV1Fallback</code> (Spark SQL) abstraction.</p>  <p><code>v1Table</code> returns the CatalogTable (with <code>CatalogStatistics</code> removed if DeltaTimeTravelSpec has also been specified).</p>  <p><code>v1Table</code> expects that the (optional) CatalogTable metadata is specified or throws a <code>DeltaIllegalStateException</code>:</p> <pre><code>v1Table call is not expected with path based DeltaTableV2\n</code></pre>","text":""},{"location":"DeltaTableV2/#deltatimetravelspec","title":"DeltaTimeTravelSpec <p><code>DeltaTableV2</code> may be given a DeltaTimeTravelSpec when created.</p> <p><code>DeltaTimeTravelSpec</code> is assumed not to be defined by default (<code>None</code>).</p> <p><code>DeltaTableV2</code> is given a <code>DeltaTimeTravelSpec</code> when:</p> <ul> <li><code>DeltaDataSource</code> is requested for a BaseRelation</li> </ul> <p><code>DeltaTimeTravelSpec</code> is used for timeTravelSpec.</p>","text":""},{"location":"DeltaTableV2/#properties","title":"Properties <pre><code>properties(): Map[String, String]\n</code></pre> <p><code>properties</code>\u00a0is part of the <code>Table</code> (Spark SQL) abstraction.</p> <p><code>properties</code> requests the Snapshot for the table properties and adds the following:</p>    Name Value     <code>provider</code> <code>delta</code>   <code>location</code> path   <code>comment</code> description (of the Metadata) if available   <code>Type</code> table type of the CatalogTable if available","text":""},{"location":"DeltaTableV2/#table-capabilities","title":"Table Capabilities <pre><code>capabilities(): Set[TableCapability]\n</code></pre> <p><code>capabilities</code>\u00a0is part of the <code>Table</code> (Spark SQL) abstraction.</p> <p><code>capabilities</code> is the following:</p> <ul> <li><code>ACCEPT_ANY_SCHEMA</code> (Spark SQL)</li> <li><code>BATCH_READ</code> (Spark SQL)</li> <li><code>V1_BATCH_WRITE</code> (Spark SQL)</li> <li><code>OVERWRITE_BY_FILTER</code> (Spark SQL)</li> <li><code>TRUNCATE</code> (Spark SQL)</li> </ul>","text":""},{"location":"DeltaTableV2/#creating-writebuilder","title":"Creating WriteBuilder <pre><code>newWriteBuilder(\n  info: LogicalWriteInfo): WriteBuilder\n</code></pre> <p><code>newWriteBuilder</code>\u00a0is part of the <code>SupportsWrite</code> (Spark SQL) abstraction.</p> <p><code>newWriteBuilder</code> creates a WriteIntoDeltaBuilder (for the DeltaLog and the options from the <code>LogicalWriteInfo</code>).</p>","text":""},{"location":"DeltaTableV2/#snapshot","title":"Snapshot <pre><code>snapshot: Snapshot\n</code></pre> <p><code>DeltaTableV2</code> has a Snapshot. In other words, <code>DeltaTableV2</code> represents a Delta table at a specific version.</p>  <p>Scala lazy value</p> <p><code>snapshot</code> is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged.</p>  <p><code>DeltaTableV2</code> uses the DeltaLog to load it at a given version (based on the optional timeTravelSpec) or update to the latest version.</p> <p><code>snapshot</code> is used when <code>DeltaTableV2</code> is requested for the schema, partitioning and properties.</p>","text":""},{"location":"DeltaTableV2/#deltatimetravelspec_1","title":"DeltaTimeTravelSpec <pre><code>timeTravelSpec: Option[DeltaTimeTravelSpec]\n</code></pre> <p><code>DeltaTableV2</code> may have a DeltaTimeTravelSpec specified that is either given or extracted from the path (for timeTravelByPath).</p> <p><code>timeTravelSpec</code> throws an <code>AnalysisException</code> when timeTravelOpt and timeTravelByPath are both defined:</p> <pre><code>Cannot specify time travel in multiple formats.\n</code></pre> <p><code>timeTravelSpec</code> is used when <code>DeltaTableV2</code> is requested for a Snapshot and BaseRelation.</p>","text":""},{"location":"DeltaTableV2/#deltatimetravelspec-by-path","title":"DeltaTimeTravelSpec by Path <pre><code>timeTravelByPath: Option[DeltaTimeTravelSpec]\n</code></pre>  <p>Scala lazy value</p> <p><code>timeTravelByPath</code> is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged.</p>  <p><code>timeTravelByPath</code> is undefined when CatalogTable is defined.</p> <p>With no CatalogTable defined, <code>DeltaTableV2</code> parses the given Path for the <code>timeTravelByPath</code> (that resolvePath under the covers).</p>","text":""},{"location":"DeltaTableV2/#converting-to-insertable-hadoopfsrelation","title":"Converting to Insertable HadoopFsRelation <pre><code>toBaseRelation: BaseRelation\n</code></pre> <p><code>toBaseRelation</code> verifyAndCreatePartitionFilters for the Path, the current Snapshot and partitionFilters.</p> <p>In the end, <code>toBaseRelation</code> requests the DeltaLog for an insertable HadoopFsRelation.</p> <p><code>toBaseRelation</code> is used when:</p> <ul> <li><code>DeltaDataSource</code> is requested to createRelation</li> <li><code>DeltaRelation</code> utility is used to <code>fromV2Relation</code></li> </ul>","text":""},{"location":"DeltaUnsupportedOperationsCheck/","title":"DeltaUnsupportedOperationsCheck","text":"<p>DeltaUnsupportedOperationsCheck is a logical check rule that adds helpful error messages when Delta is being used with unsupported Hive operations or if an unsupported operation is executed.</p>"},{"location":"DeltaWriteOptions/","title":"DeltaWriteOptions","text":"<p><code>DeltaWriteOptions</code> is a type-safe abstraction of the write-related DeltaOptions.</p> <p><code>DeltaWriteOptions</code> is DeltaWriteOptionsImpl and DeltaOptionParser.</p>"},{"location":"DeltaWriteOptions/#replacewhere","title":"replaceWhere <pre><code>replaceWhere: Option[String]\n</code></pre> <p><code>replaceWhere</code> is the value of replaceWhere option.</p> <p><code>replaceWhere</code> is used when:</p> <ul> <li><code>WriteIntoDelta</code> command is created and executed</li> <li><code>CreateDeltaTableCommand</code> command is requested for a Delta Operation (for history purposes)</li> </ul>","text":""},{"location":"DeltaWriteOptions/#usermetadata","title":"userMetadata <pre><code>userMetadata: Option[String]\n</code></pre> <p><code>userMetadata</code> is the value of userMetadata option.</p>","text":""},{"location":"DeltaWriteOptions/#optimizewrite","title":"optimizeWrite <pre><code>optimizeWrite: Option[Boolean]\n</code></pre> <p><code>optimizeWrite</code> is the value of optimizeWrite option.</p>","text":""},{"location":"DeltaWriteOptionsImpl/","title":"DeltaWriteOptionsImpl","text":"<p><code>DeltaWriteOptionsImpl</code> is an extension of the DeltaOptionParser abstraction.</p> <p>Fun Fact</p> <p>Despite the suffix (<code>Impl</code>), <code>DeltaWriteOptionsImpl</code> is not an implementation (class) but a trait.</p>"},{"location":"DeltaWriteOptionsImpl/#canmergeschema","title":"canMergeSchema <pre><code>canMergeSchema: Boolean\n</code></pre> <p><code>canMergeSchema</code> is the value of mergeSchema option (if defined) or spark.databricks.delta.schema.autoMerge.enabled configuration property.</p> <p><code>canMergeSchema</code> is used when:</p> <ul> <li><code>WriteIntoDelta</code> is created</li> <li><code>DeltaSink</code> is created</li> </ul>","text":""},{"location":"DeltaWriteOptionsImpl/#canoverwriteschema","title":"canOverwriteSchema <pre><code>canOverwriteSchema: Boolean\n</code></pre> <p><code>canOverwriteSchema</code> is the value of overwriteSchema option (in the options).</p> <p><code>canOverwriteSchema</code> is used when:</p> <ul> <li><code>CreateDeltaTableCommand</code> is executed (and replaceMetadataIfNecessary)</li> <li><code>WriteIntoDelta</code> is created</li> <li><code>DeltaSink</code> is created</li> </ul>","text":""},{"location":"DeltaWriteOptionsImpl/#rearrangeonly","title":"rearrangeOnly <pre><code>rearrangeOnly: Boolean\n</code></pre> <p><code>rearrangeOnly</code> is the value of dataChange option.</p> <p><code>rearrangeOnly</code> is used when:</p> <ul> <li><code>WriteIntoDelta</code> is requested to write</li> </ul>","text":""},{"location":"FileAction/","title":"FileAction","text":"<p><code>FileAction</code>\u00a0is an extension of the Action abstraction for data file-related actions.</p>"},{"location":"FileAction/#contract","title":"Contract","text":""},{"location":"FileAction/#datachange","title":"dataChange <pre><code>dataChange: Boolean\n</code></pre> <p>Controls the transaction isolation level for committing a transaction</p>    Isolation Level Description     SnapshotIsolation No data changes (<code>dataChange</code> is <code>false</code> for all <code>FileAction</code>s to be committed)   Serializable      <p>There can be no RemoveFiles with <code>dataChange</code> enabled for appendOnly unmodifiable tables (or an UnsupportedOperationException is thrown).</p>    dataChange Value When     <code>false</code> <code>InMemoryLogReplay</code> is requested to replay a version   <code>true</code> ConvertToDeltaCommand is executed (and requested to create an AddFile with the flag turned on)   Opposite of dataChange option <code>WriteIntoDelta</code> is requested to write (with dataChange option turned off for rearrange-only writes)    <p><code>dataChange</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to commit (and determines the isolation level), prepareCommit, attempt a commit (for <code>bytesNew</code> statistics)</li> <li><code>DeltaSource</code> is requested to getChanges (and verifyStreamHygieneAndFilterAddFiles)</li> </ul>","text":""},{"location":"FileAction/#numlogicalrecords","title":"numLogicalRecords <pre><code>numLogicalRecords: Option[Long]\n</code></pre> <p>Always <code>None</code>:</p> <ul> <li>AddCDCFile</li> <li>RemoveFile</li> </ul> <p>See:</p> <ul> <li>AddFile</li> </ul>","text":""},{"location":"FileAction/#partition-values","title":"Partition Values <pre><code>partitionValues: Map[String, String]\n</code></pre> <p>Partition columns to their values of this logical file</p>  <p>Note</p> <p><code>partitionValues</code> is not used.</p>","text":""},{"location":"FileAction/#path","title":"Path <pre><code>path: String\n</code></pre>","text":""},{"location":"FileAction/#tags","title":"Tags <pre><code>tags: Map[String, String]\n</code></pre> <p>Metadata about this logical file</p> <p>Used to get the value of a tag</p>","text":""},{"location":"FileAction/#implementations","title":"Implementations","text":"Sealed Trait <p><code>FileAction</code> is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).</p> <p>Learn more in the Scala Language Specification.</p> <ul> <li>AddCDCFile</li> <li>AddFile</li> <li>RemoveFile</li> </ul>"},{"location":"FileAction/#tag-value","title":"Tag Value <pre><code>getTag(tagName: String): Option[String]\n</code></pre> <p><code>getTag</code> gets the value of the given tag (by <code>tagName</code>), if available.</p>  <p><code>getTag</code> is used when:</p> <ul> <li><code>AddFile</code> is requested for a tag value</li> </ul>","text":""},{"location":"FileNames/","title":"FileNames","text":"<p>= FileNames Utility</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| checkpointPrefix a| [[checkpointPrefix]] Creates a Hadoop <code>Path</code> for a file name with a given <code>version</code>:</p> <pre><code>[version][%020d].checkpoint\n</code></pre> <p>E.g. <code>00000000000000000005.checkpoint</code></p> <p>| isCheckpointFile a| [[isCheckpointFile]]</p> <p>| isDeltaFile a| [[isDeltaFile]]</p> <p>|===</p> <p>== [[deltaFile]] Creating Hadoop Path To Delta File -- <code>deltaFile</code> Utility</p>"},{"location":"FileNames/#source-scala","title":"[source, scala]","text":"<p>deltaFile(   path: Path,   version: Long): Path</p> <p><code>deltaFile</code> creates a Hadoop <code>Path</code> to a file of the format <code>[version][%020d].json</code> in the <code>path</code> directory, e.g. <code>00000000000000000001.json</code>.</p> <p>NOTE: <code>deltaFile</code> is used when...FIXME</p>"},{"location":"GenerateSymlinkManifest/","title":"GenerateSymlinkManifest (GenerateSymlinkManifestImpl)","text":"<p><code>GenerateSymlinkManifest</code> is a post-commit hook to generate incremental and full Hive-style manifests for delta tables.</p> <p><code>GenerateSymlinkManifest</code> is registered when <code>OptimisticTransactionImpl</code> is requested to commit (with delta.compatibility.symlinkFormatManifest.enabled table property enabled).</p>"},{"location":"GenerateSymlinkManifest/#executing-post-commit-hook","title":"Executing Post-Commit Hook <pre><code>run(\n  spark: SparkSession,\n  txn: OptimisticTransactionImpl,\n  committedActions: Seq[Action]): Unit\n</code></pre> <p><code>run</code> is part of the PostCommitHook abstraction.</p> <p><code>run</code> generates an incremental manifest for the committed actions (the deltaLog and snapshot are from the <code>OptimisticTransactionImpl</code>).</p>","text":""},{"location":"GenerateSymlinkManifest/#generateincrementalmanifest","title":"generateIncrementalManifest <pre><code>generateIncrementalManifest(\n  spark: SparkSession,\n  deltaLog: DeltaLog,\n  txnReadSnapshot: Snapshot,\n  actions: Seq[Action]): Unit\n</code></pre> <p><code>generateIncrementalManifest</code>...FIXME</p>","text":""},{"location":"GenerateSymlinkManifest/#generatefullmanifest","title":"generateFullManifest <pre><code>generateFullManifest(\n  spark: SparkSession,\n  deltaLog: DeltaLog): Unit\n</code></pre> <p><code>generateFullManifest</code>...FIXME</p> <p><code>generateFullManifest</code> is used when:</p> <ul> <li><code>GenerateSymlinkManifestImpl</code> is requested to generateIncrementalManifest</li> <li>DeltaGenerateCommand is executed (with <code>symlink_format_manifest</code> mode)</li> </ul>","text":""},{"location":"ImplicitMetadataOperation/","title":"ImplicitMetadataOperation","text":"<p><code>ImplicitMetadataOperation</code> is an abstraction of operations that can update the metadata of a delta table (while writing out a new data).</p> <p><code>ImplicitMetadataOperation</code> operations can update schema by merging and overwriting schema.</p>"},{"location":"ImplicitMetadataOperation/#contract","title":"Contract","text":""},{"location":"ImplicitMetadataOperation/#canmergeschema","title":"canMergeSchema <pre><code>canMergeSchema: Boolean\n</code></pre> <p>Used when:</p> <ul> <li><code>ImplicitMetadataOperation</code> is requested to update the metadata</li> </ul>","text":""},{"location":"ImplicitMetadataOperation/#canoverwriteschema","title":"canOverwriteSchema <pre><code>canOverwriteSchema: Boolean\n</code></pre> <p>Used when:</p> <ul> <li><code>ImplicitMetadataOperation</code> is requested to update the metadata</li> </ul>","text":""},{"location":"ImplicitMetadataOperation/#implementations","title":"Implementations","text":"<ul> <li>DeltaSink</li> <li>MergeIntoCommand</li> <li>WriteIntoDelta</li> </ul>"},{"location":"ImplicitMetadataOperation/#updating-metadata","title":"Updating Metadata <pre><code>updateMetadata(\n  spark: SparkSession,\n  txn: OptimisticTransaction,\n  schema: StructType,\n  partitionColumns: Seq[String],\n  configuration: Map[String, String],\n  isOverwriteMode: Boolean,\n  rearrangeOnly: Boolean): Unit\n</code></pre> <p><code>updateMetadata</code> dropColumnMappingMetadata from the given <code>schema</code> (that produces <code>dataSchema</code>).</p> <p><code>updateMetadata</code> mergeSchema (with the <code>dataSchema</code> and the <code>isOverwriteMode</code> and <code>canOverwriteSchema</code> flags).</p> <p><code>updateMetadata</code> normalizePartitionColumns.</p> <p><code>updateMetadata</code> branches off based on the following conditions:</p> <ol> <li>Delta table is just being created</li> <li>Overwriting schema is enabled (i.e. <code>isOverwriteMode</code> and <code>canOverwriteSchema</code> flags are enabled, and either the schema is new or partitioning changed)</li> <li>Merging schema is enabled the schema is new and the canMergeSchema is enabled (but the partitioning has not changed)</li> <li>Data or Partitioning Schema has changed</li> </ol>","text":""},{"location":"ImplicitMetadataOperation/#table-being-created","title":"Table Being Created <p><code>updateMetadata</code> creates a new Metadata with the following:</p> <ul> <li>Uses the value of <code>comment</code> key (in the configuration) for the description</li> <li>FIXME</li> </ul> <p><code>updateMetadata</code> requests the given OptimisticTransaction to updateMetadata.</p>","text":""},{"location":"ImplicitMetadataOperation/#overwriting-schema","title":"Overwriting Schema <p><code>updateMetadata</code>...FIXME</p>","text":""},{"location":"ImplicitMetadataOperation/#merging-schema","title":"Merging Schema <p><code>updateMetadata</code>...FIXME</p>","text":""},{"location":"ImplicitMetadataOperation/#new-data-or-partitioning-schema","title":"New Data or Partitioning Schema <p><code>updateMetadata</code>...FIXME</p>","text":""},{"location":"ImplicitMetadataOperation/#isoverwritemode","title":"isOverwriteMode <p><code>updateMetadata</code> is given <code>isOverwriteMode</code> flag as follows:</p> <ul> <li>Only <code>false</code> for MergeIntoCommand with canMergeSchema enabled</li> <li><code>true</code> for WriteIntoDelta in Overwrite save mode; <code>false</code> otherwise</li> <li><code>true</code> for DeltaSink in Complete output mode; <code>false</code> otherwise</li> </ul>","text":""},{"location":"ImplicitMetadataOperation/#rearrangeonly","title":"rearrangeOnly <p><code>updateMetadata</code> is given <code>rearrangeOnly</code> flag as follows:</p> <ul> <li>Only <code>false</code> for MergeIntoCommand with canMergeSchema enabled</li> <li>rearrangeOnly option for WriteIntoDelta</li> <li><code>false</code> for DeltaSink</li> </ul>","text":""},{"location":"ImplicitMetadataOperation/#configuration","title":"configuration <p><code>updateMetadata</code> is given <code>configuration</code> as follows:</p> <ul> <li>The existing configuration (of the metadata of the transaction) for MergeIntoCommand with canMergeSchema enabled</li> <li>configuration of the <code>WriteIntoDelta</code> command (while writing out)</li> <li>Always empty for DeltaSink</li> </ul>","text":""},{"location":"ImplicitMetadataOperation/#usage","title":"Usage <p><code>updateMetadata</code> is used when:</p> <ul> <li>MergeIntoCommand command is executed (with canMergeSchema is enabled)</li> <li><code>WriteIntoDelta</code> command is requested to write</li> <li><code>DeltaSink</code> is requested to add a streaming micro-batch</li> </ul>","text":""},{"location":"ImplicitMetadataOperation/#normalizing-partition-columns","title":"Normalizing Partition Columns <pre><code>normalizePartitionColumns(\n  spark: SparkSession,\n  partitionCols: Seq[String],\n  schema: StructType): Seq[String]\n</code></pre> <p><code>normalizePartitionColumns</code>...FIXME</p>","text":""},{"location":"ImplicitMetadataOperation/#mergeschema","title":"mergeSchema <pre><code>mergeSchema(\n  txn: OptimisticTransaction,\n  dataSchema: StructType,\n  isOverwriteMode: Boolean,\n  canOverwriteSchema: Boolean): StructType\n</code></pre> <p><code>mergeSchema</code>...FIXME</p>","text":""},{"location":"InMemoryLogReplay/","title":"InMemoryLogReplay","text":"<p><code>InMemoryLogReplay</code> is used at the very last phase of state reconstruction (of a cached delta state).</p> <p><code>InMemoryLogReplay</code> handles a single partition of the state reconstruction dataset (based on the spark.databricks.delta.snapshotPartitions configuration property).</p>"},{"location":"InMemoryLogReplay/#creating-instance","title":"Creating Instance","text":"<p><code>InMemoryLogReplay</code> takes the following to be created:</p> <ul> <li> <code>minFileRetentionTimestamp</code> (Snapshot.minFileRetentionTimestamp) <p><code>InMemoryLogReplay</code> is created\u00a0when:</p> <ul> <li><code>Snapshot</code> is requested for state reconstruction</li> </ul>"},{"location":"InMemoryLogReplay/#lifecycle","title":"Lifecycle","text":"<p>The lifecycle of <code>InMemoryLogReplay</code> is as follows:</p> <ol> <li> <p>Created (with Snapshot.minFileRetentionTimestamp)</p> </li> <li> <p>Append all SingleActions of a partition (based on the spark.databricks.delta.snapshotPartitions configuration property)</p> </li> <li> <p>Checkpoint</p> </li> </ol>"},{"location":"InMemoryLogReplay/#replaying-version","title":"Replaying Version <pre><code>append(\n  version: Long,\n  actions: Iterator[Action]): Unit\n</code></pre> <p><code>append</code> sets the currentVersion to the given <code>version</code>.</p> <p><code>append</code> adds the given actions to their respective registries.</p>    Action Registry     SetTransaction transactions by appId   Metadata currentMetaData   Protocol currentProtocolVersion   AddFile 1. activeFiles by path and with dataChange flag disabled     2. Removes the path from tombstones (so there's only one FileAction for a path)   RemoveFile 1. Removes the path from activeFiles (so there's only one FileAction for a path)     2. tombstones by path and with dataChange flag disabled   CommitInfo Ignored   AddCDCFile Ignored    <p><code>append</code> throws an <code>AssertionError</code> when the currentVersion is <code>-1</code> or one before the given <code>version</code>:</p> <pre><code>Attempted to replay version [version], but state is at [currentVersion]\n</code></pre>","text":""},{"location":"InMemoryLogReplay/#current-state-of-delta-table","title":"Current State of Delta Table <pre><code>checkpoint: Iterator[Action]\n</code></pre> <p><code>checkpoint</code> returns an <code>Iterator</code> (Scala) of Actions in the following order:</p> <ul> <li>currentProtocolVersion if defined (non-<code>null</code>)</li> <li>currentMetaData if defined (non-<code>null</code>)</li> <li>SetTransactions</li> <li>AddFiles and RemoveFiles sorted by path (lexicographically)</li> </ul>","text":""},{"location":"InMemoryLogReplay/#gettombstones","title":"getTombstones <pre><code>getTombstones: Iterable[FileAction]\n</code></pre> <p><code>getTombstones</code> uses the tombstones internal registry for RemoveFiles with deletionTimestamp after (greater than) the minFileRetentionTimestamp.</p>","text":""},{"location":"InitialSnapshot/","title":"InitialSnapshot","text":"<p><code>InitialSnapshot</code> is an initial Snapshot of a delta table (at the logPath) that is about to be created (so there is no delta log yet).</p> <p><code>InitialSnapshot</code> is a Snapshot with the following:</p> Snapshot Value LogSegment Empty transaction log directory (for the logPath) minFileRetentionTimestamp -1 minSetTransactionRetentionTimestamp (undefined) Path logPath Timestamp -1 Version -1 VersionChecksum (undefined)"},{"location":"InitialSnapshot/#creating-instance","title":"Creating Instance","text":"<p><code>InitialSnapshot</code> takes the following to be created:</p> <ul> <li> Path (Apache Hadoop) of the Transaction Log <li> DeltaLog <li> Metadata <p><code>InitialSnapshot</code> is created when:</p> <ul> <li><code>SnapshotManagement</code> is requested to createSnapshotAtInitInternal and installLogSegmentInternal</li> <li><code>ConvertToDeltaCommandBase</code> is requested to createDeltaActions</li> </ul>"},{"location":"InitialSnapshot/#metadata","title":"Metadata <p><code>InitialSnapshot</code> can be given a Metadata when created. Unless given, <code>InitialSnapshot</code> creates a Metadata with the following:</p>    Metadata Value     configuration mergeGlobalConfigs   createdTime Current time (in ms)","text":""},{"location":"InitialSnapshot/#computedstate","title":"computedState  Signature <pre><code>computedState: Snapshot.State\n</code></pre> <p><code>computedState</code> is part of the Snapshot abstraction.</p>  <p><code>computedState</code> initialState.</p>  Lazy Value <p><code>computedState</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>   Question <p>Why does <code>InitialSnapshot</code> define the private initialState to initialize <code>computedState</code>? They could be one, couldn't they?</p>","text":""},{"location":"InitialSnapshot/#initial-state","title":"Initial State <pre><code>initialState: Snapshot.State\n</code></pre> <p><code>initialState</code>...FIXME</p>","text":""},{"location":"IsolationLevel/","title":"IsolationLevel","text":"<p><code>IsolationLevel</code> is an abstraction of consistency guarantees to be provided when <code>OptimisticTransaction</code> is committed.</p>"},{"location":"IsolationLevel/#implementations","title":"Implementations","text":"<p>Sealed Trait</p> <p><code>IsolationLevel</code> is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).</p>"},{"location":"IsolationLevel/#serializable","title":"Serializable <p><code>Serializable</code> is the most strict consistency guarantee.</p> <p><code>Serializable</code> is the isolation level for data-changing commits.</p> <p>For <code>Serializable</code> commits, <code>OptimisticTransactionImpl</code> adds extra <code>addedFilesToCheckForConflicts</code> (<code>changedData</code> or <code>blindAppend</code> AddFiles) when checkForConflicts.</p> <p><code>Serializable</code> is a valid table isolation level.</p> <p>For operations that do not modify data in a table, there is no difference between Serializable and SnapshotIsolation.</p> <p><code>Serializable</code> is used for ConvertToDeltaCommand command.</p>","text":""},{"location":"IsolationLevel/#snapshotisolation","title":"SnapshotIsolation <p><code>SnapshotIsolation</code> is the least strict consistency guarantee.</p> <p><code>SnapshotIsolation</code> is the isolation level for commits with no data changed.</p> <p>For <code>SnapshotIsolation</code> commits, <code>OptimisticTransactionImpl</code> adds no extra <code>addedFilesToCheckForConflicts</code> when checkForConflicts.</p> <p>For operations that do not modify data in a table, there is no difference between Serializable and SnapshotIsolation.</p>","text":""},{"location":"IsolationLevel/#writeserializable","title":"WriteSerializable <p>The default <code>IsolationLevel</code></p> <p>For <code>WriteSerializable</code> commits, <code>OptimisticTransactionImpl</code> adds extra <code>addedFilesToCheckForConflicts</code> (<code>changedData</code> AddFiles) when checkForConflicts. Blind appends don't (seem to) conflict with <code>WriteSerializable</code> commits.</p> <p><code>WriteSerializable</code> a valid table isolation level.</p>","text":""},{"location":"IsolationLevel/#consistency-guarantee-strictness-ordering","title":"Consistency Guarantee Strictness Ordering <p>The following are all the isolation levels in descending order of guarantees provided:</p> <ol> <li>Serializable (the most strict level)</li> <li>WriteSerializable</li> <li>SnapshotIsolation (the least strict one)</li> </ol>","text":""},{"location":"IsolationLevel/#valid-table-isolation-levels","title":"Valid Table Isolation Levels  <p>Not Used in OSS Delta Lake</p> <p>This feature is not used.</p>  <p>The following are the valid isolation levels that can be specified as the table isolation level:</p> <ul> <li>Serializable</li> <li>WriteSerializable</li> </ul>","text":""},{"location":"LogSegment/","title":"LogSegment","text":"<p><code>LogSegment</code> are the delta and checkpoint files that all together are a given version of a delta table (in the logPath).</p>"},{"location":"LogSegment/#creating-instance","title":"Creating Instance","text":"<p><code>LogSegment</code> takes the following to be created:</p> <ul> <li> Log Path (Apache Hadoop) <li> Version <li> Delta <code>FileStatus</code>es (Apache Hadoop) <li> Checkpoint <code>FileStatus</code>es (Apache Hadoop) <li> Checkpoint Version <li> Timestamp of the Last Commit <p><code>LogSegment</code> is created\u00a0when:</p> <ul> <li><code>SnapshotManagement</code> is requested for the LogSegment at a given version</li> </ul>"},{"location":"LogStore/","title":"LogStore (io.delta.storage)","text":"<p><code>LogStore</code> is an abstraction of transaction log stores (to read and write Delta log files).</p> <p><code>LogStore</code> is created for LogStoreAdaptor.</p>"},{"location":"LogStore/#iodeltastorage","title":"io.delta.storage","text":"<p><code>LogStore</code> is part of <code>io.delta.storage</code> package meant for Delta Lake developers.</p> <p>Note</p> <p>There is another internal LogStore in <code>org.apache.spark.sql.delta.storage</code> package.</p>"},{"location":"LogStore/#contract","title":"Contract","text":""},{"location":"LogStore/#ispartialwritevisible","title":"isPartialWriteVisible <pre><code>Boolean isPartialWriteVisible(\n  Path path,\n  Configuration hadoopConf)\n</code></pre> <p>Used when:</p> <ul> <li><code>LogStoreAdaptor</code> is requested to isPartialWriteVisible</li> </ul>","text":""},{"location":"LogStore/#listfrom","title":"listFrom <pre><code>Iterator&lt;FileStatus&gt; listFrom(\n  Path path,\n  Configuration hadoopConf) throws FileNotFoundException\n</code></pre> <p>Used when:</p> <ul> <li><code>LogStoreAdaptor</code> is requested to listFrom</li> </ul>","text":""},{"location":"LogStore/#read","title":"read <pre><code>CloseableIterator&lt;String&gt; read(\n  Path path,\n  Configuration hadoopConf)\n</code></pre> <p>Used when:</p> <ul> <li><code>LogStoreAdaptor</code> is requested to read and readAsIterator</li> </ul>","text":""},{"location":"LogStore/#resolvepathonphysicalstorage","title":"resolvePathOnPhysicalStorage <pre><code>Path resolvePathOnPhysicalStorage(\n  Path path,\n  Configuration hadoopConf)\n</code></pre> <p>Used when:</p> <ul> <li><code>LogStoreAdaptor</code> is requested to resolvePathOnPhysicalStorage</li> </ul>","text":""},{"location":"LogStore/#write","title":"write <pre><code>void write(\n  Path path,\n  Iterator&lt;String&gt; actions,\n  Boolean overwrite,\n  Configuration hadoopConf) throws FileAlreadyExistsException\n</code></pre> <p>Used when:</p> <ul> <li><code>LogStoreAdaptor</code> is requested to write</li> </ul>","text":""},{"location":"LogStore/#creating-instance","title":"Creating Instance","text":"<p><code>LogStore</code> takes the following to be created:</p> <ul> <li> <code>Configuration</code> (Apache Hadoop) Abstract Class <p><code>LogStore</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete LogStores.</p>"},{"location":"Metadata/","title":"Metadata","text":"<p><code>Metadata</code> is an Action to update the metadata of a delta table (indirectly via the Snapshot).</p> <p>Use DescribeDeltaDetailCommand to review the metadata of a delta table.</p>"},{"location":"Metadata/#creating-instance","title":"Creating Instance","text":"<p><code>Metadata</code> takes the following to be created:</p> <ul> <li>Id</li> <li> Name (default: <code>null</code>) <li> Description (default: <code>null</code>) <li> Format (default: empty) <li> Schema (default: <code>null</code>) <li> Partition Columns (default: <code>Nil</code>) <li> Table Configuration (default: <code>Map.empty</code>) <li> Created Time (default: current time) <p><code>Metadata</code> is created when:</p> <ul> <li><code>DeltaLog</code> is requested for the metadata (but that should be rare)</li> <li><code>InitialSnapshot</code> is created</li> <li>ConvertToDeltaCommand is executed</li> <li><code>ImplicitMetadataOperation</code> is requested to updateMetadata</li> </ul>"},{"location":"Metadata/#updating-metadata","title":"Updating Metadata","text":"<p><code>Metadata</code> can be updated in a transaction once only (and only when created for an uninitialized table, when readVersion is <code>-1</code>).</p> <pre><code>txn.metadata\n</code></pre>"},{"location":"Metadata/#demo","title":"Demo","text":"<pre><code>val path = \"/tmp/delta/users\"\n\nimport org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, path)\n\nimport org.apache.spark.sql.delta.actions.Metadata\nassert(deltaLog.snapshot.metadata.isInstanceOf[Metadata])\n\ndeltaLog.snapshot.metadata.id\n</code></pre>"},{"location":"Metadata/#table-id","title":"Table ID <p><code>Metadata</code> uses a Table ID (aka reservoirId) to uniquely identify a delta table and is never going to change through the history of the table.</p> <p><code>Metadata</code> can be given a table ID when created or defaults to a random UUID (Java).</p>  <p>Note</p> <p>When I asked the question tableId and reservoirId - Why two different names for metadata ID? on delta-users mailing list, Tathagata Das wrote:</p>  <p>Any reference to \"reservoir\" is just legacy code. In the early days of this project, the project was called \"Tahoe\" and each table is called a \"reservoir\" (Tahoe is one of the 2<sup>nd</sup> deepest lake in US, and is a very large reservoir of water ;) ). So you may still find those two terms all around the codebase.</p> <p>In some cases, like DeltaSourceOffset, the term <code>reservoirId</code> is in the json that is written to the streaming checkpoint directory. So we cannot change that for backward compatibility.</p>","text":""},{"location":"Metadata/#column-mapping-mode","title":"Column Mapping Mode <pre><code>columnMappingMode: DeltaColumnMappingMode\n</code></pre> <p><code>columnMappingMode</code> is the value of columnMapping.mode table property (from this Metadata).</p> <p><code>columnMappingMode</code> is used when:</p> <ul> <li><code>DeltaFileFormat</code> is requested for the FileFormat</li> </ul>","text":""},{"location":"Metadata/#data-schema-of-delta-table","title":"Data Schema (of Delta Table) <pre><code>dataSchema: StructType\n</code></pre>  Lazy Value <p><code>dataSchema</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>dataSchema</code> is the schema without the partition columns (and is the columns written out to data files).</p> <p><code>dataSchema</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to verify a new metadata</li> <li><code>Snapshot</code> is requested for the data schema</li> </ul>","text":""},{"location":"MetadataCleanup/","title":"MetadataCleanup","text":"<p><code>MetadataCleanup</code> is an abstraction of metadata cleaners that can clean up expired checkpoints and delta logs of a delta table.</p> <p> <code>MetadataCleanup</code> requires to be used with DeltaLog (or subtypes) only."},{"location":"MetadataCleanup/#implementations","title":"Implementations","text":"<ul> <li>DeltaLog</li> </ul>"},{"location":"MetadataCleanup/#table-properties","title":"Table Properties","text":""},{"location":"MetadataCleanup/#enableexpiredlogcleanup","title":"enableExpiredLogCleanup <p><code>MetadataCleanup</code> uses enableExpiredLogCleanup table configuration to enable log cleanup.</p>","text":""},{"location":"MetadataCleanup/#logretentionduration","title":"logRetentionDuration <p><code>MetadataCleanup</code> uses logRetentionDuration table configuration for cleanUpExpiredLogs (to determine <code>fileCutOffTime</code>).</p>","text":""},{"location":"MetadataCleanup/#cleaning-up-expired-logs","title":"Cleaning Up Expired Logs <pre><code>doLogCleanup(): Unit\n</code></pre> <p><code>doLogCleanup</code>\u00a0is part of the Checkpoints abstraction.</p>  <p><code>doLogCleanup</code> cleanUpExpiredLogs when enabled.</p>","text":""},{"location":"MetadataCleanup/#cleanupexpiredlogs","title":"cleanUpExpiredLogs <pre><code>cleanUpExpiredLogs(): Unit\n</code></pre> <p><code>cleanUpExpiredLogs</code> calculates a <code>fileCutOffTime</code> based on the current time and the logRetentionDuration table property.</p> <p><code>cleanUpExpiredLogs</code> prints out the following INFO message to the logs:</p> <pre><code>Starting the deletion of log files older than [date]\n</code></pre> <p><code>cleanUpExpiredLogs</code> finds the expired delta logs (based on the <code>fileCutOffTime</code>) and deletes the files (using Hadoop's FileSystem.delete non-recursively). <code>cleanUpExpiredLogs</code> counts the files deleted (and uses it in the summary INFO message).</p> <p>In the end, <code>cleanUpExpiredLogs</code> prints out the following INFO message to the logs:</p> <pre><code>Deleted [numDeleted] log files older than [date]\n</code></pre>","text":""},{"location":"MetadataCleanup/#finding-expired-log-files","title":"Finding Expired Log Files <pre><code>listExpiredDeltaLogs(\n  fileCutOffTime: Long): Iterator[FileStatus]\n</code></pre> <p><code>listExpiredDeltaLogs</code> loads the most recent checkpoint if available.</p> <p>If the last checkpoint is not available, <code>listExpiredDeltaLogs</code> returns an empty iterator.</p> <p><code>listExpiredDeltaLogs</code> requests the LogStore for the paths (in the same directory) that are (lexicographically) greater or equal to the <code>0</code>th checkpoint file (per checkpointPrefix format) of the checkpoint and delta files in the log directory.</p> <p>In the end, <code>listExpiredDeltaLogs</code> creates a <code>BufferingLogDeletionIterator</code> that...FIXME</p>","text":""},{"location":"MetadataCleanup/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for the Implementations logger to see what happens inside.</p>","text":""},{"location":"Operation/","title":"Operation","text":"<p><code>Operation</code> is an abstraction of operations that can be executed on a Delta table.</p> <p>Operation is described by a name and parameters (that are simply used to create a CommitInfo for <code>OptimisticTransactionImpl</code> when committed and, as a way to bypass a transaction, ConvertToDeltaCommand).</p> <p>Operation may have performance metrics.</p>"},{"location":"Operation/#contract","title":"Contract","text":""},{"location":"Operation/#parameters","title":"Parameters <pre><code>parameters: Map[String, Any]\n</code></pre> <p>Parameters of the operation (to create a CommitInfo with the JSON-encoded values)</p> <p>Used when:</p> <ul> <li><code>Operation</code> is requested for the parameters with the values in JSON format</li> </ul>","text":""},{"location":"Operation/#implementations","title":"Implementations","text":"<p>Sealed Abstract Class</p> <p><code>Operation</code> is a Scala sealed abstract class which means that all of the implementations are in the same compilation unit (a single file).</p>"},{"location":"Operation/#addcolumns","title":"AddColumns <p>Name: <code>ADD COLUMNS</code></p> <p>Parameters:</p> <ul> <li><code>columns</code></li> </ul> <p>Used when:</p> <ul> <li>AlterTableAddColumnsDeltaCommand is executed (and committed to a Delta table)</li> </ul>","text":""},{"location":"Operation/#addconstraint","title":"AddConstraint","text":""},{"location":"Operation/#changecolumn","title":"ChangeColumn    Name Parameters changesData     <code>CHANGE COLUMN</code> <ul><li><code>column</code></li><li><code>position</code> (optional)</li></ul> <code>false</code>    <p>Used when:</p> <ul> <li>AlterTableChangeColumnDeltaCommand is executed</li> </ul>","text":""},{"location":"Operation/#convert","title":"Convert","text":""},{"location":"Operation/#createtable","title":"CreateTable","text":""},{"location":"Operation/#delete","title":"Delete","text":""},{"location":"Operation/#dropconstraint","title":"DropConstraint","text":""},{"location":"Operation/#manualupdate","title":"ManualUpdate","text":""},{"location":"Operation/#merge","title":"Merge <p>Name: <code>MERGE</code></p> <p>Parameters:</p> <ul> <li>predicate</li> <li>matchedPredicates</li> <li>notMatchedPredicates</li> </ul> <p>changesData: <code>true</code></p> <p>Used when:</p> <ul> <li>MergeIntoCommand is executed (and committed to a Delta table)</li> </ul>","text":""},{"location":"Operation/#optimize","title":"Optimize    Name Parameters changesData     <code>OPTIMIZE</code> <code>predicate</code> true    <p>Used when:</p> <ul> <li><code>OptimizeExecutor</code> is requested to optimize</li> </ul>","text":""},{"location":"Operation/#renamecolumn","title":"RenameColumn    Name Parameters changesData     <code>RENAME COLUMN</code> <ul><li>oldColumnPath</li><li>newColumnPath</li></ul> false    <p>Used when:</p> <ul> <li>AlterTableChangeColumnDeltaCommand is executed</li> </ul>","text":""},{"location":"Operation/#replacecolumns","title":"ReplaceColumns","text":""},{"location":"Operation/#replacetable","title":"ReplaceTable","text":""},{"location":"Operation/#settableproperties","title":"SetTableProperties <p>Name: <code>SET TBLPROPERTIES</code></p> <p>Parameters:</p> <ul> <li>properties</li> </ul> <p>Used when:</p> <ul> <li>AlterTableSetPropertiesDeltaCommand is executed</li> </ul>","text":""},{"location":"Operation/#streamingupdate","title":"StreamingUpdate <p>Name: <code>STREAMING UPDATE</code></p> <p>Parameters:</p> <ul> <li>outputMode</li> <li>queryId</li> <li>epochId</li> </ul> <p>Used when:</p> <ul> <li><code>DeltaSink</code> is requested to addBatch</li> </ul>","text":""},{"location":"Operation/#truncate","title":"Truncate","text":""},{"location":"Operation/#unsettableproperties","title":"UnsetTableProperties","text":""},{"location":"Operation/#update","title":"Update","text":""},{"location":"Operation/#updatecolumnmetadata","title":"UpdateColumnMetadata","text":""},{"location":"Operation/#updateschema","title":"UpdateSchema","text":""},{"location":"Operation/#upgradeprotocol","title":"UpgradeProtocol","text":""},{"location":"Operation/#write","title":"Write","text":""},{"location":"Operation/#creating-instance","title":"Creating Instance","text":"<p><code>Operation</code> takes the following to be created:</p> <ul> <li> Name of this operation Abstract Class <p><code>Operation</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete Operations.</p>"},{"location":"Operation/#serializing-parameter-values-to-json-format","title":"Serializing Parameter Values (to JSON Format) <pre><code>jsonEncodedValues: Map[String, String]\n</code></pre> <p><code>jsonEncodedValues</code> converts the values of the parameters to JSON format.</p> <p><code>jsonEncodedValues</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to commit</li> <li><code>ConvertToDeltaCommand</code> command is requested to streamWrite</li> </ul>","text":""},{"location":"Operation/#operation-metrics","title":"Operation Metrics <pre><code>operationMetrics: Set[String]\n</code></pre> <p><code>operationMetrics</code> is empty by default (and is expected to be overriden by concrete operations).</p> <p><code>operationMetrics</code> is used when:</p> <ul> <li><code>Operation</code> is requested to transformMetrics</li> </ul>","text":""},{"location":"Operation/#transforming-performance-metrics","title":"Transforming Performance Metrics <pre><code>transformMetrics(\n  metrics: Map[String, SQLMetric]): Map[String, String]\n</code></pre> <p><code>transformMetrics</code> returns a collection of <code>SQLMetric</code>s (Spark SQL) and their values (as text) that are defined as the operation metrics.</p> <p><code>transformMetrics</code> is used when:</p> <ul> <li><code>SQLMetricsReporting</code> is requested for operation metrics</li> </ul>","text":""},{"location":"Operation/#user-metadata","title":"User Metadata <pre><code>userMetadata: Option[String]\n</code></pre> <p><code>userMetadata</code> is undefined (<code>None</code>) by default (and is expected to be overriden by concrete operations).</p> <p><code>userMetadata</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested for the user metadata</li> </ul>","text":""},{"location":"Operation/#changesdata-flag","title":"changesData Flag <pre><code>changesData: Boolean\n</code></pre> <p><code>changesData</code> is disabled (<code>false</code>) by default (and is expected to be overriden by concrete operations).</p>  <p>Note</p> <p><code>changesData</code> seems not used.</p>","text":""},{"location":"OptimisticTransaction/","title":"OptimisticTransaction","text":"<p><code>OptimisticTransaction</code> is an OptimisticTransactionImpl (which seems more of a class name change than anything more important).</p> <p><code>OptimisticTransaction</code> is created for changes to a delta table at a given version.</p> <p>When <code>OptimisticTransaction</code> (as a OptimisticTransactionImpl) is about to be committed (that does doCommit internally), the LogStore (of the delta table) is requested to write actions to a delta file (e.g. <code>_delta_log/00000000000000000001.json</code> for the attempt version <code>1</code>). Unless a <code>FileAlreadyExistsException</code> is thrown a commit is considered successful or retried.</p> <p><code>OptimisticTransaction</code> can be associated with a thread as an active transaction.</p>"},{"location":"OptimisticTransaction/#demo","title":"Demo","text":"<pre><code>import org.apache.spark.sql.delta.DeltaLog\nval dir = \"/tmp/delta/users\"\nval log = DeltaLog.forTable(spark, dir)\n\nval txn = log.startTransaction()\n\n// ...changes to a delta table...\nval addFile = AddFile(\"foo\", Map.empty, 1L, System.currentTimeMillis(), dataChange = true)\nval removeFile = addFile.remove\nval actions = addFile :: removeFile :: Nil\n\ntxn.commit(actions, op)\n</code></pre> <p>Alternatively, you could do the following instead.</p> <pre><code>deltaLog.withNewTransaction { txn =&gt;\n  // ...transactional changes to a delta table\n}\n</code></pre>"},{"location":"OptimisticTransaction/#creating-instance","title":"Creating Instance","text":"<p><code>OptimisticTransaction</code> takes the following to be created:</p> <ul> <li> DeltaLog <li> Snapshot <li> <code>Clock</code> <p>Note</p> <p>The DeltaLog and Snapshot are part of the OptimisticTransactionImpl abstraction (which in turn inherits them as a TransactionalWrite and simply changes to <code>val</code> from <code>def</code>).</p> <p><code>OptimisticTransaction</code> is created\u00a0when <code>DeltaLog</code> is used for the following:</p> <ul> <li>Starting a new transaction</li> <li>Executing a single-threaded operation (in a new transaction)</li> </ul>"},{"location":"OptimisticTransaction/#active-thread-local-optimistictransaction","title":"Active Thread-Local OptimisticTransaction <pre><code>active: ThreadLocal[OptimisticTransaction]\n</code></pre> <p><code>active</code> is a Java ThreadLocal with the <code>OptimisticTransaction</code> of the current thread.</p>  <p>ThreadLocal</p> <p><code>ThreadLocal</code> provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its get or set method) has its own, independently initialized copy of the variable.</p> <p><code>ThreadLocal</code> instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID).</p>  <p><code>active</code> is assigned to the current thread using setActive utility and cleared in clearActive.</p> <p><code>active</code> is available using getActive utility.</p> <p>There can only be one active <code>OptimisticTransaction</code> (or an <code>IllegalStateException</code> is thrown).</p>","text":""},{"location":"OptimisticTransaction/#setactive","title":"setActive <pre><code>setActive(\n  txn: OptimisticTransaction): Unit\n</code></pre> <p><code>setActive</code> associates the given <code>OptimisticTransaction</code> as active with the current thread.</p> <p><code>setActive</code> throws an <code>IllegalStateException</code> if there is an active OptimisticTransaction already associated:</p> <pre><code>Cannot set a new txn as active when one is already active\n</code></pre> <p><code>setActive</code> is used when:</p> <ul> <li><code>DeltaLog</code> is requested to execute an operation in a new transaction</li> </ul>","text":""},{"location":"OptimisticTransaction/#clearactive","title":"clearActive <pre><code>clearActive(): Unit\n</code></pre> <p><code>clearActive</code> clears the active transaction (so no transaction is associated with the current thread).</p> <p><code>clearActive</code> is used when:</p> <ul> <li><code>DeltaLog</code> is requested to execute an operation in a new transaction</li> </ul>","text":""},{"location":"OptimisticTransaction/#getactive","title":"getActive <pre><code>getActive(): Option[OptimisticTransaction]\n</code></pre> <p>getActive returns the active transaction (if available).</p> <p>getActive seems unused.</p>","text":""},{"location":"OptimisticTransaction/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.OptimisticTransaction</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.delta.OptimisticTransaction=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"OptimisticTransactionImpl/","title":"OptimisticTransactionImpl","text":"<p><code>OptimisticTransactionImpl</code> is an extension of the TransactionalWrite abstraction for optimistic transactions that can modify a delta table (at a given version) and can be committed eventually.</p> <p>In other words, <code>OptimisticTransactionImpl</code> is a set of actions as part of an Operation that changes the state of a delta table transactionally.</p>"},{"location":"OptimisticTransactionImpl/#contract","title":"Contract","text":""},{"location":"OptimisticTransactionImpl/#clock","title":"Clock <pre><code>clock: Clock\n</code></pre>","text":""},{"location":"OptimisticTransactionImpl/#deltalog","title":"DeltaLog <pre><code>deltaLog: DeltaLog\n</code></pre> <p>DeltaLog (of a delta table) this transaction commits changes to</p> <p><code>deltaLog</code> is part of the TransactionalWrite abstraction and seems to change it to <code>val</code> (from <code>def</code>).</p>","text":""},{"location":"OptimisticTransactionImpl/#snapshot","title":"Snapshot <pre><code>snapshot: Snapshot\n</code></pre> <p>Snapshot (of the delta table) this transaction commits changes to</p> <p><code>snapshot</code> is part of the TransactionalWrite contract and seems to change it to <code>val</code> (from <code>def</code>).</p>","text":""},{"location":"OptimisticTransactionImpl/#implementations","title":"Implementations","text":"<ul> <li>OptimisticTransaction</li> </ul>"},{"location":"OptimisticTransactionImpl/#table-version-at-reading-time","title":"Table Version at Reading Time <pre><code>readVersion: Long\n</code></pre> <p><code>readVersion</code> requests the Snapshot for the version.</p> <p><code>readVersion</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to updateMetadata and commit</li> <li>AlterDeltaTableCommand, ConvertToDeltaCommand, CreateDeltaTableCommand commands are executed</li> <li><code>DeltaCommand</code> is requested to commitLarge</li> <li><code>WriteIntoDelta</code> is requested to write</li> <li><code>ImplicitMetadataOperation</code> is requested to updateMetadata</li> </ul>","text":""},{"location":"OptimisticTransactionImpl/#transactional-commit","title":"Transactional Commit <pre><code>commit(\n  actions: Seq[Action],\n  op: DeltaOperations.Operation): Long\n</code></pre> <p><code>commit</code> attempts to commit the given Actions (as part of the Operation) and gives the commit version.</p>","text":""},{"location":"OptimisticTransactionImpl/#usage","title":"Usage <p><code>commit</code>\u00a0is used when:</p> <ul> <li>ALTER TABLE commands are executed<ul> <li>AlterTableAddColumnsDeltaCommand</li> <li>AlterTableAddConstraintDeltaCommand</li> <li>AlterTableChangeColumnDeltaCommand</li> <li>AlterTableDropColumnsDeltaCommand</li> <li>AlterTableDropConstraintDeltaCommand</li> <li>AlterTableReplaceColumnsDeltaCommand</li> <li>AlterTableSetPropertiesDeltaCommand</li> <li>AlterTableUnsetPropertiesDeltaCommand</li> </ul> </li> <li>ConvertToDeltaCommand is executed</li> <li>CreateDeltaTableCommand is executed</li> <li>DeleteCommand is executed</li> <li><code>DeltaLog</code> is requested to upgrade the protocol</li> <li><code>DeltaSink</code> is requested to add a streaming micro-batch</li> <li>MergeIntoCommand is executed</li> <li>OptimizeTableCommand is executed (and requests <code>OptimizeExecutor</code> to commitAndRetry)</li> <li><code>StatisticsCollection</code> is requested to recompute</li> <li>UpdateCommand is executed</li> <li>WriteIntoDelta is executed</li> </ul>","text":""},{"location":"OptimisticTransactionImpl/#performcdcmetadatacheck","title":"performCdcMetadataCheck <pre><code>performCdcMetadataCheck(): Unit\n</code></pre> <p><code>performCdcMetadataCheck</code>...FIXME</p>","text":""},{"location":"OptimisticTransactionImpl/#preparing-commit","title":"Preparing Commit <p><code>commit</code> then prepares a commit (that gives the final actions to commit that may be different from the given actions).</p>","text":""},{"location":"OptimisticTransactionImpl/#isolation-level","title":"Isolation Level <p><code>commit</code> determines the isolation level based on FileActions (in the given actions) and their dataChange flag.</p> <p>With all actions with dataChange flag disabled (<code>false</code>), <code>commit</code> assumes no data changed and chooses SnapshotIsolation else Serializable.</p>","text":""},{"location":"OptimisticTransactionImpl/#blind-append","title":"Blind Append <p><code>commit</code> is considered blind append when the following all hold:</p> <ol> <li>There are only AddFiles among FileActions in the actions (onlyAddFiles)</li> <li>It does not depend on files, i.e. the readPredicates and readFiles are empty (dependsOnFiles)</li> </ol>","text":""},{"location":"OptimisticTransactionImpl/#commitinfo","title":"CommitInfo <p><code>commit</code>...FIXME</p>","text":""},{"location":"OptimisticTransactionImpl/#registering-post-commit-hook","title":"Registering Post-Commit Hook <p><code>commit</code> registers the GenerateSymlinkManifest post-commit hook when there is a FileAction among the actions and the compatibility.symlinkFormatManifest.enabled table property is enabled.</p>","text":""},{"location":"OptimisticTransactionImpl/#docommitretryiteratively","title":"doCommitRetryIteratively <p><code>commit</code> doCommitRetryIteratively.</p> <p><code>commit</code> prints out the following INFO message to the logs:</p> <pre><code>Committed delta #[commitVersion] to [logPath]\n</code></pre>","text":""},{"location":"OptimisticTransactionImpl/#performing-post-commit-operations","title":"Performing Post-Commit Operations <p><code>commit</code> postCommit (with the version committed and the <code>needsCheckpoint</code> flag).</p>","text":""},{"location":"OptimisticTransactionImpl/#executing-post-commit-hooks","title":"Executing Post-Commit Hooks <p>In the end, commit runs post-commit hooks and returns the version of the successful commit.</p>","text":""},{"location":"OptimisticTransactionImpl/#docommitretryiteratively_1","title":"doCommitRetryIteratively <pre><code>doCommitRetryIteratively(\n  attemptVersion: Long,\n  currentTransactionInfo: CurrentTransactionInfo,\n  isolationLevel: IsolationLevel): (Long, CurrentTransactionInfo, Boolean)\n</code></pre> <p><code>doCommitRetryIteratively</code> acquires a lock on the delta table if enabled for the commit.</p> <p><code>doCommitRetryIteratively</code> uses <code>attemptNumber</code> internal counter to track the number of attempts. In case of a <code>FileAlreadyExistsException</code>, <code>doCommitRetryIteratively</code> increments the <code>attemptNumber</code> and tries over.</p> <p>In the end, <code>doCommitRetryIteratively</code> returns a tuple with the following:</p> <ol> <li>Commit version (from the given <code>attemptVersion</code> inclusive up to spark.databricks.delta.maxCommitAttempts)</li> <li><code>CurrentTransactionInfo</code></li> <li>Whether the commit needs checkpoint or not (<code>needsCheckpoint</code>)</li> </ol>  <p>Firstly, <code>doCommitRetryIteratively</code> does the first attempt at commit. If successful, the commit is done.</p> <p>If there is a retry, <code>doCommitRetryIteratively</code> checkForConflicts followed by another attempt at commit.</p> <p>If the number of commit attempts (<code>attemptNumber</code>) is above the spark.databricks.delta.maxCommitAttempts configuration property, <code>doCommitRetryIteratively</code> throws a DeltaIllegalStateException:</p> <pre><code>This commit has failed as it has been tried &lt;numAttempts&gt; times but did not succeed.\nThis can be caused by the Delta table being committed continuously by many concurrent commits.\n\nCommit started at version: [attemptNumber]\nCommit failed at version: [attemptVersion]\nNumber of actions attempted to commit: [numActions]\nTotal time spent attempting this commit: [timeSpent] ms\n</code></pre>","text":""},{"location":"OptimisticTransactionImpl/#checking-logical-conflicts-with-concurrent-updates","title":"Checking Logical Conflicts with Concurrent Updates <pre><code>checkForConflicts(\n  checkVersion: Long,\n  actions: Seq[Action],\n  attemptNumber: Int,\n  commitIsolationLevel: IsolationLevel): Long\n</code></pre> <p><code>checkForConflicts</code> checks for logical conflicts (of the given <code>actions</code>) with concurrent updates (actions of the commits since the transaction has started).</p> <p><code>checkForConflicts</code> gives the next possible commit version unless the following happened between the time of read (<code>checkVersion</code>) and the time of this commit attempt:</p> <ol> <li>Client is up to date with the table protocol for reading and writing (and hence allowed to access the table)</li> <li>Protocol version has changed</li> <li>Metadata has changed</li> <li>AddFiles have been added that the txn should have read based on the given IsolationLevel (Concurrent Append)</li> <li>AddFiles that the txn read have been deleted (Concurrent Delete)</li> <li>Files have been deleted by the txn and since the time of read (Concurrent Delete)</li> <li>Idempotent transactions have conflicted (Multiple Streaming Queries with the same checkpoint location)</li> </ol> <p><code>checkForConflicts</code> takes the next possible commit version.</p> <p>For every commit since the time of read (<code>checkVersion</code>) and this commit attempt, <code>checkForConflicts</code> does the following:</p> <ul> <li> <p>FIXME</p> </li> <li> <p>Prints out the following INFO message to the logs:</p> <pre><code>Completed checking for conflicts Version: [version] Attempt: [attemptNumber] Time: [totalCheckAndRetryTime] ms\n</code></pre> </li> </ul> <p>In the end, <code>checkForConflicts</code> prints out the following INFO message to the logs:</p> <pre><code>No logical conflicts with deltas [[checkVersion], [nextAttemptVersion]), retrying.\n</code></pre>","text":""},{"location":"OptimisticTransactionImpl/#getprettypartitionmessage","title":"getPrettyPartitionMessage <pre><code>getPrettyPartitionMessage(\n  partitionValues: Map[String, String]): String\n</code></pre> <p><code>getPrettyPartitionMessage</code>...FIXME</p>","text":""},{"location":"OptimisticTransactionImpl/#postcommit","title":"postCommit <pre><code>postCommit(\n  commitVersion: Long,\n  needsCheckpoint: Boolean): Unit\n</code></pre> <p><code>postCommit</code> turns the committed flag on.</p> <p>With the given <code>needsCheckpoint</code> enabled (that comes indirectly from doCommit), <code>postCommit</code> requests the DeltaLog for the Snapshot at the given <code>commitVersion</code> followed by checkpointing.</p>","text":""},{"location":"OptimisticTransactionImpl/#preparecommit","title":"prepareCommit <pre><code>prepareCommit(\n  actions: Seq[Action],\n  op: DeltaOperations.Operation): Seq[Action]\n</code></pre> <p><code>prepareCommit</code> adds the newMetadata action (if available) to the given actions.</p> <p><code>prepareCommit</code> verifyNewMetadata if there was one.</p> <p><code>prepareCommit</code>...FIXME</p> <p><code>prepareCommit</code> requests the DeltaLog to protocolWrite.</p> <p><code>prepareCommit</code>...FIXME</p>","text":""},{"location":"OptimisticTransactionImpl/#multiple-metadata-changes-not-allowed","title":"Multiple Metadata Changes Not Allowed <p><code>prepareCommit</code> throws an <code>AssertionError</code> when there are multiple metadata changes in the transaction (by means of Metadata actions):</p> <pre><code>Cannot change the metadata more than once in a transaction.\n</code></pre>","text":""},{"location":"OptimisticTransactionImpl/#committing-transaction-allowed-once-only","title":"Committing Transaction Allowed Once Only <p>prepareCommit throws an <code>AssertionError</code> when the committed internal flag is enabled:</p> <pre><code>Transaction already committed.\n</code></pre>","text":""},{"location":"OptimisticTransactionImpl/#performcdccolumnmappingcheck","title":"performCdcColumnMappingCheck <pre><code>performCdcColumnMappingCheck(\n  actions: Seq[Action],\n  op: DeltaOperations.Operation): Unit\n</code></pre> <p><code>performCdcColumnMappingCheck</code>...FIXME</p>","text":""},{"location":"OptimisticTransactionImpl/#registering-post-commit-hook_1","title":"Registering Post-Commit Hook <pre><code>registerPostCommitHook(\n  hook: PostCommitHook): Unit\n</code></pre> <p><code>registerPostCommitHook</code> registers (adds) the given PostCommitHook to the postCommitHooks internal registry.</p>","text":""},{"location":"OptimisticTransactionImpl/#runpostcommithooks","title":"runPostCommitHooks <pre><code>runPostCommitHooks(\n  version: Long,\n  committedActions: Seq[Action]): Unit\n</code></pre> <p><code>runPostCommitHooks</code> simply runs every post-commit hook registered (in the postCommitHooks internal registry).</p> <p><code>runPostCommitHooks</code> clears the active transaction (making all follow-up operations non-transactional).</p>  <p>Note</p> <p>Hooks may create new transactions.</p>","text":""},{"location":"OptimisticTransactionImpl/#handling-non-fatal-exceptions","title":"Handling Non-Fatal Exceptions <p>For non-fatal exceptions, <code>runPostCommitHooks</code> prints out the following ERROR message to the logs, records the delta event, and requests the post-commit hook to handle the error.</p> <pre><code>Error when executing post-commit hook [name] for commit [version]\n</code></pre>","text":""},{"location":"OptimisticTransactionImpl/#assertionerror","title":"AssertionError <p><code>runPostCommitHooks</code> throws an <code>AssertionError</code> when committed flag is disabled:</p> <pre><code>Can't call post commit hooks before committing\n</code></pre>","text":""},{"location":"OptimisticTransactionImpl/#next-possible-commit-version","title":"Next Possible Commit Version <pre><code>getNextAttemptVersion(\n  previousAttemptVersion: Long): Long\n</code></pre> <p><code>getNextAttemptVersion</code> requests the DeltaLog to update (and give the latest state snapshot of the delta table).</p> <p>In the end, <code>getNextAttemptVersion</code> requests the <code>Snapshot</code> for the version and increments it.</p>  <p>Note</p> <p>The input <code>previousAttemptVersion</code> argument is not used.</p>","text":""},{"location":"OptimisticTransactionImpl/#operation-metrics","title":"Operation Metrics <pre><code>getOperationMetrics(\n  op: Operation): Option[Map[String, String]]\n</code></pre> <p><code>getOperationMetrics</code> gives the metrics of the given Operation when the spark.databricks.delta.history.metricsEnabled configuration property is enabled. Otherwise, <code>getOperationMetrics</code> gives <code>None</code>.</p>","text":""},{"location":"OptimisticTransactionImpl/#commitinfo_1","title":"CommitInfo <p><code>OptimisticTransactionImpl</code> creates a CommitInfo when requested to commit with spark.databricks.delta.commitInfo.enabled configuration enabled.</p> <p><code>OptimisticTransactionImpl</code> uses the <code>CommitInfo</code> to <code>recordDeltaEvent</code> (as a <code>CommitStats</code>).</p>","text":""},{"location":"OptimisticTransactionImpl/#attempting-commit","title":"Attempting Commit <pre><code>doCommit(\n  attemptVersion: Long,\n  currentTransactionInfo: CurrentTransactionInfo,\n  attemptNumber: Int,\n  isolationLevel: IsolationLevel): Boolean\n</code></pre> <p><code>doCommit</code> returns whether or not this commit (attempt) should trigger checkpointing.</p> <p><code>doCommit</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to doCommitRetryIteratively</li> </ul>  <p><code>doCommit</code> requests the given <code>CurrentTransactionInfo</code> for the final actions to commit (Actions).</p> <p><code>doCommit</code> prints out the following DEBUG message to the logs:</p> <pre><code>Attempting to commit version [attemptVersion] with [n] actions with [isolationLevel] isolation level\n</code></pre>","text":""},{"location":"OptimisticTransactionImpl/#writing-out","title":"Writing Out <p><code>doCommit</code> requests the DeltaLog for the LogStore to write out the actions to a delta file in the log directory with the <code>attemptVersion</code> version, e.g.</p> <pre><code>00000000000000000001.json\n</code></pre> <p><code>doCommit</code> writes the actions out in JSON format.</p>  <p>Note</p> <p>LogStores must throw a <code>java.nio.file.FileAlreadyExistsException</code> exception if the delta file already exists. Any <code>FileAlreadyExistsExceptions</code> are caught by doCommit itself to checkAndRetry.</p>","text":""},{"location":"OptimisticTransactionImpl/#lastcommitversioninsession","title":"lastCommitVersionInSession <p><code>doCommit</code> sets the spark.databricks.delta.lastCommitVersionInSession configuration property to the given <code>attemptVersion</code>.</p>","text":""},{"location":"OptimisticTransactionImpl/#post-commit-snapshot","title":"Post-Commit Snapshot <p><code>doCommit</code> requests the DeltaLog to update.</p>","text":""},{"location":"OptimisticTransactionImpl/#needs-checkpointing","title":"Needs Checkpointing <p><code>doCommit</code> determines whether or not this commit should trigger checkpointing based on the committed version (<code>attemptVersion</code>).</p> <p>A commit triggers checkpointing when the following all hold:</p> <ol> <li>The committed version is any version greater than <code>0</code></li> <li>The committed version is a multiple of delta.checkpointInterval table property</li> </ol>","text":""},{"location":"OptimisticTransactionImpl/#commitstats","title":"CommitStats <p><code>doCommit</code> records a new <code>CommitStats</code> event.</p>","text":""},{"location":"OptimisticTransactionImpl/#retrying-commit","title":"Retrying Commit <pre><code>checkAndRetry(\n  checkVersion: Long,\n  actions: Seq[Action],\n  attemptNumber: Int): Long\n</code></pre> <p><code>checkAndRetry</code>...FIXME</p> <p><code>checkAndRetry</code> is used when OptimisticTransactionImpl is requested to commit (and attempts a commit that failed with an <code>FileAlreadyExistsException</code>).</p>","text":""},{"location":"OptimisticTransactionImpl/#verifying-new-metadata","title":"Verifying New Metadata <pre><code>verifyNewMetadata(\n  metadata: Metadata): Unit\n</code></pre> <p><code>verifyNewMetadata</code> validates the given Metadata (and throws an exception if incorrect).</p> <p><code>verifyNewMetadata</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to prepareCommit and updateMetadata</li> </ul>  <p><code>verifyNewMetadata</code> asserts that there are no column duplicates in the schema (of the given Metadata). <code>verifyNewMetadata</code> throws a <code>DeltaAnalysisException</code> if there are duplicates.</p> <p><code>verifyNewMetadata</code> branches off based on the DeltaColumnMappingMode (of the given Metadata):</p> <ul> <li> <p>In NoMapping mode, <code>verifyNewMetadata</code> checks the data schema and checks the partition columns (of the given Metadata).</p> <p>In case of <code>AnalysisException</code> and spark.databricks.delta.partitionColumnValidity.enabled configuration property enabled, <code>verifyNewMetadata</code> throws a <code>DeltaAnalysisException</code>.</p> </li> <li> <p>For the other DeltaColumnMappingModes, <code>verifyNewMetadata</code> checkColumnIdAndPhysicalNameAssignments of the schema.</p> </li> </ul> <p><code>verifyNewMetadata</code> validates generated columns if there are any (in the schema).</p> <p>With spark.databricks.delta.schema.typeCheck.enabled configuration property enabled, <code>verifyNewMetadata</code>...FIXME</p> <p>In the end, <code>verifyNewMetadata</code> checks the protocol requirements and, in case the protocol has been updated, records it in the newProtocol registry.</p>","text":""},{"location":"OptimisticTransactionImpl/#newprotocol","title":"newProtocol <pre><code>newProtocol: Option[Protocol]\n</code></pre> <p><code>OptimisticTransactionImpl</code> defines <code>newProtocol</code> registry for a new Protocol.</p> <p><code>newProtocol</code> is undefined (<code>None</code>) by default.</p> <p><code>newProtocol</code> is defined when:</p> <ul> <li>updateMetadataInternal</li> <li>verifyNewMetadata</li> </ul> <p><code>newProtocol</code> is used for the protocol and to prepareCommit.</p>","text":""},{"location":"OptimisticTransactionImpl/#withglobalconfigdefaults","title":"withGlobalConfigDefaults <pre><code>withGlobalConfigDefaults(\n  metadata: Metadata): Metadata\n</code></pre> <p><code>withGlobalConfigDefaults</code>...FIXME</p> <p><code>withGlobalConfigDefaults</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to updateMetadata and updateMetadataForNewTable</li> </ul>","text":""},{"location":"OptimisticTransactionImpl/#looking-up-transaction-version-by-streaming-query-id","title":"Looking Up Transaction Version (by Streaming Query ID) <pre><code>txnVersion(\n  id: String): Long\n</code></pre> <p><code>txnVersion</code> simply registers (adds) the given ID in the readTxn internal registry.</p> <p>In the end, <code>txnVersion</code> requests the Snapshot for the transaction version for the given ID or <code>-1</code>.</p> <p><code>txnVersion</code> is used when:</p> <ul> <li><code>DeltaSink</code> is requested to add a streaming micro-batch</li> </ul>","text":""},{"location":"OptimisticTransactionImpl/#user-defined-metadata","title":"User-Defined Metadata <pre><code>getUserMetadata(\n  op: Operation): Option[String]\n</code></pre> <p><code>getUserMetadata</code> returns the Operation.md#userMetadata[userMetadata] of the given Operation.md[] (if defined) or the value of DeltaSQLConf.md#DELTA_USER_METADATA[spark.databricks.delta.commitInfo.userMetadata] configuration property.</p> <p><code>getUserMetadata</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to commit (and spark.databricks.delta.commitInfo.enabled configuration property is enabled)</li> <li>ConvertToDeltaCommand is executed (and in turn requests <code>DeltaCommand</code> to commitLarge)</li> </ul>","text":""},{"location":"OptimisticTransactionImpl/#internal-registries","title":"Internal Registries","text":""},{"location":"OptimisticTransactionImpl/#post-commit-hooks","title":"Post-Commit Hooks <pre><code>postCommitHooks: ArrayBuffer[PostCommitHook]\n</code></pre> <p><code>OptimisticTransactionImpl</code> manages PostCommitHooks that will be executed right after a commit is successful.</p> <p>Post-commit hooks can be registered, but only the GenerateSymlinkManifest post-commit hook is supported.</p>","text":""},{"location":"OptimisticTransactionImpl/#newmetadata","title":"newMetadata <pre><code>newMetadata: Option[Metadata]\n</code></pre> <p><code>OptimisticTransactionImpl</code> uses the <code>newMetadata</code> internal registry for a new Metadata that should be committed with this transaction.</p> <p><code>newMetadata</code> is initially undefined (<code>None</code>). It can be updated only once and before the transaction writes out any files.</p> <p><code>newMetadata</code> is used when prepareCommit and doCommit (for statistics).</p> <p><code>newMetadata</code> is available using metadata method.</p>","text":""},{"location":"OptimisticTransactionImpl/#readfiles","title":"readFiles <pre><code>readFiles: HashSet[AddFile]\n</code></pre> <p><code>OptimisticTransactionImpl</code> uses <code>readFiles</code> registry to track AddFiles that have been seen (scanned) by this transaction (when requested to filterFiles).</p> <p>Used to determine isBlindAppend and checkForConflicts (and fail if the files have been deleted that the txn read).</p>","text":""},{"location":"OptimisticTransactionImpl/#readpredicates","title":"readPredicates <pre><code>readPredicates: ArrayBuffer[Expression]\n</code></pre> <p><code>readPredicates</code> holds predicate expressions for partitions the transaction is modifying.</p> <p><code>readPredicates</code> is added a new predicate expression when filterFiles and readWholeTable.</p> <p><code>readPredicates</code> is used when checkAndRetry.</p>","text":""},{"location":"OptimisticTransactionImpl/#internal-properties","title":"Internal Properties","text":""},{"location":"OptimisticTransactionImpl/#committed","title":"committed <p>Controls whether the transaction has been committed or not (and prevents prepareCommit from being executed again)</p> <p>Default: <code>false</code></p> <p>Enabled in postCommit</p>","text":""},{"location":"OptimisticTransactionImpl/#readtxn","title":"readTxn <p>Streaming query IDs that have been seen by this transaction</p> <p>A new queryId is added when <code>OptimisticTransactionImpl</code> is requested for txnVersion</p> <p>Used when <code>OptimisticTransactionImpl</code> is requested to checkAndRetry (to fail with a <code>ConcurrentTransactionException</code> for idempotent transactions that have conflicted)</p>","text":""},{"location":"OptimisticTransactionImpl/#snapshotmetadata","title":"snapshotMetadata <p>Metadata of the Snapshot</p>","text":""},{"location":"OptimisticTransactionImpl/#readwholetable","title":"readWholeTable <pre><code>readWholeTable(): Unit\n</code></pre> <p><code>readWholeTable</code> simply adds <code>True</code> literal to the readPredicates internal registry.</p> <p><code>readWholeTable</code> is used when:</p> <ul> <li><code>DeltaSink</code> is requested to add a streaming micro-batch (and the batch reads the same Delta table as this sink is going to write to)</li> </ul>","text":""},{"location":"OptimisticTransactionImpl/#updatemetadatafornewtable","title":"updateMetadataForNewTable <pre><code>updateMetadataForNewTable(\n  metadata: Metadata): Unit\n</code></pre> <p><code>updateMetadataForNewTable</code>...FIXME</p> <p><code>updateMetadataForNewTable</code> is used when:</p> <ul> <li>ConvertToDeltaCommand and CreateDeltaTableCommand are executed</li> </ul>","text":""},{"location":"OptimisticTransactionImpl/#metadata","title":"Metadata <pre><code>metadata: Metadata\n</code></pre> <p><code>metadata</code> is part of the TransactionalWrite abstraction.</p> <p><code>metadata</code> is either the newMetadata (if defined) or the snapshotMetadata.</p>","text":""},{"location":"OptimisticTransactionImpl/#updating-metadata","title":"Updating Metadata <pre><code>updateMetadata(\n  _metadata: Metadata): Unit\n</code></pre> <p><code>updateMetadata</code> asserts the following:</p> <ul> <li>The current transaction has not written data out yet (and the hasWritten flag is still disabled since it is not allowed to update the metadata in a transaction that has already written data)</li> <li>The metadata has not been changed already (and the newMetadata has not been assigned yet since it is not allowed to change the metadata more than once in a transaction)</li> </ul> <p>In the end, <code>updateMetadata</code> updateMetadataInternal.</p>  <p><code>updateMetadata</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to updateMetadataForNewTable</li> <li>AlterTableSetPropertiesDeltaCommand, AlterTableUnsetPropertiesDeltaCommand, AlterTableAddColumnsDeltaCommand, AlterTableChangeColumnDeltaCommand, AlterTableReplaceColumnsDeltaCommand are executed</li> <li>RestoreTableCommand is executed</li> <li><code>ImplicitMetadataOperation</code> is requested to updateMetadata</li> </ul>","text":""},{"location":"OptimisticTransactionImpl/#updatemetadatainternal","title":"updateMetadataInternal <pre><code>updateMetadataInternal(\n  _metadata: Metadata): Unit\n</code></pre> <p><code>updateMetadataInternal</code>...FIXME</p>","text":""},{"location":"OptimisticTransactionImpl/#files-to-scan-matching-given-predicates","title":"Files To Scan Matching Given Predicates <pre><code>filterFiles(): Seq[AddFile] // (1)\nfilterFiles(\n  filters: Seq[Expression]): Seq[AddFile]\n</code></pre> <ol> <li>No filters = all files</li> </ol> <p><code>filterFiles</code> gives the files to scan for the given predicates (filter expressions).</p> <p>Internally, <code>filterFiles</code> requests the Snapshot for the filesForScan (for no projection attributes and the given filters).</p> <p><code>filterFiles</code> finds the partition predicates among the given filters (and the partition columns of the Metadata).</p> <p><code>filterFiles</code> registers (adds) the partition predicates (in the readPredicates internal registry) and the files to scan (in the readFiles internal registry).</p> <p><code>filterFiles</code> is used when:</p> <ul> <li><code>DeltaSink</code> is requested to add a streaming micro-batch (with <code>Complete</code> output mode)</li> <li>DeleteCommand, MergeIntoCommand and UpdateCommand, WriteIntoDelta are executed</li> <li>CreateDeltaTableCommand is executed</li> </ul>","text":""},{"location":"OptimisticTransactionImpl/#lockcommitifenabled","title":"lockCommitIfEnabled <pre><code>lockCommitIfEnabled[T](\n  body: =&gt; T): T\n</code></pre> <p><code>lockCommitIfEnabled</code> executes the <code>body</code> with a lock on a delta table when isCommitLockEnabled. Otherwise, <code>lockCommitIfEnabled</code> does not acquire a lock.</p> <p><code>lockCommitIfEnabled</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to doCommitRetryIteratively</li> </ul>","text":""},{"location":"OptimisticTransactionImpl/#iscommitlockenabled","title":"isCommitLockEnabled <pre><code>isCommitLockEnabled: Boolean\n</code></pre> <p><code>isCommitLockEnabled</code> is the value of spark.databricks.delta.commitLock.enabled configuration property (if defined) or isPartialWriteVisible (requesting the LogStore from the DeltaLog).</p>  <p>Note</p> <p><code>isCommitLockEnabled</code> is <code>true</code> by default given the following:</p> <ol> <li>spark.databricks.delta.commitLock.enabled configuration property is undefined by default</li> <li>isPartialWriteVisible is <code>true</code> by default</li> </ol>","text":""},{"location":"OptimisticTransactionImpl/#logging","title":"Logging <p><code>OptimisticTransactionImpl</code> is a Scala trait and logging is configured using the logger of the implementations.</p>","text":""},{"location":"PartitionFiltering/","title":"PartitionFiltering","text":"<p><code>PartitionFiltering</code> is an abstraction of snapshots with partition filtering for scan.</p>"},{"location":"PartitionFiltering/#implementations","title":"Implementations","text":"<p>Snapshot is the default and only known <code>PartitionFiltering</code> in Delta Lake.</p>"},{"location":"PartitionFiltering/#files-to-scan-matching-projection-attributes-and-predicates","title":"Files to Scan (Matching Projection Attributes and Predicates) <pre><code>filesForScan(\n  projection: Seq[Attribute],\n  filters: Seq[Expression],\n  keepStats: Boolean = false): DeltaScan\n</code></pre> <p><code>filesForScan</code>...FIXME</p> <p><code>filesForScan</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested for the files to scan matching given predicates</li> <li><code>TahoeLogFileIndex</code> is requested for the files matching predicates and the input files</li> </ul>","text":""},{"location":"PinnedTahoeFileIndex/","title":"PinnedTahoeFileIndex","text":"<p><code>PinnedTahoeFileIndex</code> is a TahoeFileIndex.</p>"},{"location":"PinnedTahoeFileIndex/#creating-instance","title":"Creating Instance","text":"<p><code>PinnedTahoeFileIndex</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> DeltaLog <li> Hadoop Path <li> Snapshot <p><code>PinnedTahoeFileIndex</code> is created\u00a0when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"PostCommitHook/","title":"PostCommitHook","text":"<p><code>PostCommitHook</code> is an abstraction of post-commit hooks that can be executed (at the end of transaction commit).</p>"},{"location":"PostCommitHook/#contract","title":"Contract","text":""},{"location":"PostCommitHook/#name","title":"Name <pre><code>name: String\n</code></pre> <p>User-friendly name of the hook for error reporting</p>","text":""},{"location":"PostCommitHook/#executing-post-commit-hook","title":"Executing Post-Commit Hook <pre><code>run(\n  spark: SparkSession,\n  txn: OptimisticTransactionImpl,\n  committedActions: Seq[Action]): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to runPostCommitHooks (at the end of transaction commit).</li> </ul>","text":""},{"location":"PostCommitHook/#implementations","title":"Implementations","text":"<ul> <li>GenerateSymlinkManifestImpl</li> </ul>"},{"location":"PreprocessTableDelete/","title":"PreprocessTableDelete Logical Resolution Rule","text":"<p><code>PreprocessTableDelete</code> is a post-hoc logical resolution rule (<code>Rule[LogicalPlan]</code>) to resolve DeltaDelete commands in a logical query plan into DeleteCommands.</p> <p><code>PreprocessTableDelete</code> is installed (injected) into a <code>SparkSession</code> using DeltaSparkSessionExtension.</p>"},{"location":"PreprocessTableDelete/#creating-instance","title":"Creating Instance","text":"<p><code>PreprocessTableDelete</code> takes the following to be created:</p> <ul> <li> <code>SQLConf</code> (Spark SQL) <p><code>PreprocessTableDelete</code> is created when:</p> <ul> <li>DeltaSparkSessionExtension is executed (and registers Delta SQL support)</li> </ul>"},{"location":"PreprocessTableDelete/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>\u00a0is part of the <code>Rule</code> (Spark SQL) abstraction.</p> <p>apply resolves (replaces) DeltaDelete logical commands (in a logical query plan) into DeleteCommands.</p>","text":""},{"location":"PreprocessTableDelete/#tocommand","title":"toCommand <pre><code>toCommand(\n  d: DeltaDelete): DeleteCommand\n</code></pre> <p><code>toCommand</code>...FIXME</p>","text":""},{"location":"PreprocessTableMerge/","title":"PreprocessTableMerge Logical Resolution Rule","text":"<p><code>PreprocessTableMerge</code> is a post-hoc logical resolution rule (Spark SQL) to resolve DeltaMergeInto logical commands (in a logical query plan) into MergeIntoCommands.</p> <p><code>PreprocessTableMerge</code> is injected (installed) into a <code>SparkSession</code> using DeltaSparkSessionExtension.</p>"},{"location":"PreprocessTableMerge/#creating-instance","title":"Creating Instance","text":"<p><code>PreprocessTableMerge</code> takes the following to be created:</p> <ul> <li> <code>SQLConf</code> (Spark SQL) <p><code>PreprocessTableMerge</code> is created\u00a0when:</p> <ul> <li><code>DeltaSparkSessionExtension</code> is requested to register Delta SQL support</li> <li><code>DeltaMergeBuilder</code> is requested to execute</li> </ul>"},{"location":"PreprocessTableMerge/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>\u00a0is part of the <code>Rule</code> (Spark SQL) abstraction.</p> <p>In summary, <code>apply</code> resolves (replaces) DeltaMergeInto logical commands (in a logical query plan) into corresponding MergeIntoCommands.</p> <p>Internally, <code>apply</code>...FIXME</p>","text":""},{"location":"PreprocessTableRestore/","title":"PreprocessTableRestore Logical Resolution","text":"<p><code>PreprocessTableRestore</code> is a logical resolution rule (<code>Rule[LogicalPlan]</code>) to resolve delta tables in RestoreTableStatements.</p>"},{"location":"PreprocessTableRestore/#creating-instance","title":"Creating Instance","text":"<p><code>PreprocessTableRestore</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <p><code>PreprocessTableRestore</code> is created when:</p> <ul> <li><code>DeltaSparkSessionExtension</code> is requested to register delta extensions</li> </ul>"},{"location":"PreprocessTableRestore/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> is part of the <code>Rule</code> (Spark SQL) abstraction.</p> <p><code>apply</code> resolves RestoreTableStatements with <code>UnresolvedRelation</code>s of delta tables to <code>RestoreTableStatement</code>s with DeltaTableV2s.</p>","text":""},{"location":"PreprocessTableRestore/#restoretablestatement","title":"RestoreTableStatement <p>For a RestoreTableStatement with a <code>TimeTravel</code> on an <code>UnresolvedRelation</code>, <code>apply</code> tries to look up the relation in the <code>SessionCatalog</code> (Spark SQL).</p> <p>For a delta table, <code>apply</code> requests the <code>SessionCatalog</code> for a table metadata and creates a DeltaTableV2 (with the location from the catalog).</p> <p>For a delta table registered in a catalog, <code>apply</code> requests the <code>SessionCatalog</code> for the metadata and creates a DeltaTableV2 (with the location from the catalog).</p> <p>For the table identifier that is a valid path of a delta table, <code>apply</code> creates a DeltaTableV2 (with the location).</p>  <p>Note</p> <p>For all other cases, <code>apply</code> throws an exception.</p>  <p>In the end, <code>apply</code> creates a <code>DataSourceV2Relation</code> with the <code>DeltaTableV2</code> as a child of a new RestoreTableStatement.</p>","text":""},{"location":"PreprocessTableUpdate/","title":"PreprocessTableUpdate Logical Resolution Rule","text":"<p><code>PreprocessTableUpdate</code> is a post-hoc logical resolution rule (<code>Rule[LogicalPlan]</code>) to resolve DeltaUpdateTable commands in a logical query plan into UpdateCommands.</p> <p><code>PreprocessTableUpdate</code> is installed (injected) into a <code>SparkSession</code> using DeltaSparkSessionExtension.</p>"},{"location":"PreprocessTableUpdate/#creating-instance","title":"Creating Instance","text":"<p><code>PreprocessTableUpdate</code> takes the following to be created:</p> <ul> <li> <code>SQLConf</code> (Spark SQL) <p><code>PreprocessTableUpdate</code> is created when:</p> <ul> <li>DeltaSparkSessionExtension is executed (and registers Delta SQL support)</li> </ul>"},{"location":"PreprocessTableUpdate/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>\u00a0is part of the <code>Rule</code> (Spark SQL) abstraction.</p> <p>apply resolves (replaces) DeltaUpdateTable logical commands (in a logical query plan) into UpdateCommands.</p>","text":""},{"location":"PreprocessTableUpdate/#tocommand","title":"toCommand <pre><code>toCommand(\n  update: DeltaUpdateTable): UpdateCommand\n</code></pre> <p><code>toCommand</code>...FIXME</p>","text":""},{"location":"Protocol/","title":"Protocol","text":"<p><code>Protocol</code> is an Action.</p>"},{"location":"Protocol/#creating-instance","title":"Creating Instance","text":"<p><code>Protocol</code> takes the following to be created:</p> <ul> <li> Minimum Reader Version Allowed (default: <code>1</code>) <li> Minimum Writer Version Allowed (default: <code>3</code>) <p><code>Protocol</code> is created\u00a0when:</p> <ul> <li><code>DeltaTable</code> is requested to upgradeTableProtocol</li> <li>FIXME</li> </ul>"},{"location":"Protocol/#fornewtable","title":"forNewTable <pre><code>forNewTable(\n  spark: SparkSession,\n  metadata: Metadata): Protocol\n</code></pre> <p><code>forNewTable</code> creates a new Protocol for the given <code>SparkSession</code> and Metadata.</p> <p><code>forNewTable</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to updateMetadata and updateMetadataForNewTable</li> <li><code>InitialSnapshot</code> is requested to <code>computedState</code></li> </ul>","text":""},{"location":"Protocol/#apply","title":"apply <pre><code>apply(\n  spark: SparkSession,\n  metadataOpt: Option[Metadata]): Protocol\n</code></pre> <p><code>apply</code>...FIXME</p>","text":""},{"location":"Protocol/#checkprotocolrequirements","title":"checkProtocolRequirements <pre><code>checkProtocolRequirements(\n  spark: SparkSession,\n  metadata: Metadata,\n  current: Protocol): Option[Protocol]\n</code></pre> <p><code>checkProtocolRequirements</code> asserts that the table configuration does not contain delta.minReaderVersion or throws an <code>AssertionError</code>:</p> <pre><code>Should not have the protocol version (delta.minReaderVersion) as part of table properties\n</code></pre> <p><code>checkProtocolRequirements</code> asserts that the table configuration does not contain delta.minWriterVersion or throws an <code>AssertionError</code>:</p> <pre><code>Should not have the protocol version (delta.minWriterVersion) as part of table properties\n</code></pre> <p><code>checkProtocolRequirements</code> determines the required minimum protocol.</p> <p><code>checkProtocolRequirements</code>...FIXME</p> <p><code>checkProtocolRequirements</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to verify a new metadata</li> </ul>","text":""},{"location":"Protocol/#required-minimum-protocol","title":"Required Minimum Protocol <pre><code>requiredMinimumProtocol(\n  spark: SparkSession,\n  metadata: Metadata): (Protocol, Seq[String])\n</code></pre> <p><code>requiredMinimumProtocol</code> creates a Protocol with <code>0</code> for the minimum reader and writer versions.</p> <pre><code>Protocol(0, 0)\n</code></pre> <p><code>requiredMinimumProtocol</code> tracks features used (in <code>featuresUsed</code>).</p> <p><code>requiredMinimumProtocol</code> determines the required minimum Protocol checking for the following features (in order):</p> <ol> <li>Column-Level Invariants</li> <li>Append Only Table</li> <li>CHECK Constraints</li> <li>Generated Columns</li> <li>Change Data Feed</li> <li>IDENTITY Columns (Unsupported)</li> <li>Column Mapping</li> </ol> <p>In the end, <code>requiredMinimumProtocol</code> returns the required <code>Protocol</code> and the features used.</p>  <p><code>requiredMinimumProtocol</code>\u00a0is used when:</p> <ul> <li><code>Protocol</code> is requested for a new Protocol and checkProtocolRequirements</li> </ul>","text":""},{"location":"Protocol/#column-invariants","title":"Column Invariants <p><code>requiredMinimumProtocol</code> checks for column-level invariants (in the schema of the given Metadata).</p> <p>If used, <code>requiredMinimumProtocol</code> sets the minWriterVersion to <code>2</code>.</p> <pre><code>Protocol(0, 2)\n</code></pre>","text":""},{"location":"Protocol/#append-only-table","title":"Append-Only Table <p><code>requiredMinimumProtocol</code> reads appendOnly table property (from the table configuration of the given Metadata).</p> <p>If set, <code>requiredMinimumProtocol</code> creates a new Protocol with the minWriterVersion to be <code>3</code>.</p> <pre><code>Protocol(0, 3)\n</code></pre>","text":""},{"location":"Protocol/#check-constraints","title":"CHECK Constraints <p><code>requiredMinimumProtocol</code> checks for CHECK constraints (in the given Metadata).</p> <p>If used, <code>requiredMinimumProtocol</code> creates a new Protocol with the minWriterVersion to be <code>3</code>.</p> <pre><code>Protocol(0, 3)\n</code></pre>","text":""},{"location":"Protocol/#generated-columns","title":"Generated Columns <p><code>requiredMinimumProtocol</code> checks for generated columns (in the schema of the given Metadata).</p> <p>If used, <code>requiredMinimumProtocol</code> creates a new Protocol with the minWriterVersion to be 4.</p> <pre><code>Protocol(0, 4)\n</code></pre>","text":""},{"location":"Protocol/#change-data-feed","title":"Change Data Feed <p><code>requiredMinimumProtocol</code> checks whether delta.enableChangeDataFeed table property is enabled (in the given Metadata).</p> <p>If enabled, <code>requiredMinimumProtocol</code> creates a new Protocol with the minWriterVersion to be <code>4</code>.</p> <pre><code>Protocol(0, 4)\n</code></pre>","text":""},{"location":"Protocol/#identity-columns-unsupported","title":"IDENTITY Columns (Unsupported) <p><code>requiredMinimumProtocol</code> checks for identity columns (in the schema of the given Metadata).</p> <p>If used, <code>requiredMinimumProtocol</code> creates a new Protocol with the minWriterVersion to be 6.</p> <pre><code>Protocol(0, 6)\n</code></pre>  <p>AnalysisException</p> <p>In the end, <code>requiredMinimumProtocol</code> throws an <code>AnalysisException</code>:</p> <pre><code>IDENTITY column is not supported\n</code></pre>","text":""},{"location":"Protocol/#column-mapping","title":"Column Mapping <p><code>requiredMinimumProtocol</code> checks for column mapping (in the given Metadata).</p> <p>If used, <code>requiredMinimumProtocol</code> creates a new Protocol.</p> <pre><code>Protocol(2, 5)\n</code></pre>","text":""},{"location":"Protocol/#demo","title":"Demo <pre><code>import org.apache.spark.sql.delta.actions.{Metadata, Protocol}\nimport org.apache.spark.sql.delta.DeltaConfigs\n\nval configuration = Map(\n  DeltaConfigs.IS_APPEND_ONLY.key -&gt; \"true\") // (1)!\nval metadata = Metadata(configuration = configuration)\nval protocol = Protocol.forNewTable(spark, metadata)\n</code></pre> <ol> <li>Append-only table</li> </ol> <pre><code>assert(\n  protocol.minReaderVersion == 1,\n  \"minReaderVersion should be the default 1\")\nassert(\n  protocol.minWriterVersion == 2,\n  \"minWriterVersion should be 2 because of append-only tables\")\n</code></pre>","text":""},{"location":"ReadChecksum/","title":"ReadChecksum","text":"<p><code>ReadChecksum</code> is...FIXME</p>"},{"location":"RemoveFile/","title":"RemoveFile","text":"<p><code>RemoveFile</code> is a FileAction that represents an action of removing (deleting) a file from a delta table.</p>"},{"location":"RemoveFile/#creating-instance","title":"Creating Instance","text":"<p><code>RemoveFile</code> takes the following to be created:</p> <ul> <li> Path <li> Deletion Timestamp (optional) <li> <code>dataChange</code> flag <li> <code>extendedFileMetadata</code> flag (default: <code>false</code>) <li> Partition values (default: <code>null</code>) <li> Size (in bytes) (default: <code>0</code>) <li> Tags (<code>Map[String, String]</code>) (default: <code>null</code>) <p><code>RemoveFile</code> is created\u00a0when:</p> <ul> <li><code>AddFile</code> action is requested to removeWithTimestamp</li> </ul>"},{"location":"SQLMetricsReporting/","title":"SQLMetricsReporting","text":"<p><code>SQLMetricsReporting</code> is an extension for OptimisticTransactionImpl to track performance metrics of Operations for reporting.</p>"},{"location":"SQLMetricsReporting/#implementations","title":"Implementations","text":"<ul> <li>OptimisticTransactionImpl</li> </ul>"},{"location":"SQLMetricsReporting/#operationsqlmetrics-registry","title":"operationSQLMetrics Registry <pre><code>operationSQLMetrics: Map[String, SQLMetric]\n</code></pre> <p><code>SQLMetricsReporting</code> uses <code>operationSQLMetrics</code> internal registry for <code>SQLMetric</code>s (Spark SQL) by their names.</p> <p><code>SQLMetric</code>s are registered only when spark.databricks.delta.history.metricsEnabled configuration property is enabled.</p> <p><code>operationSQLMetrics</code> is used when <code>SQLMetricsReporting</code> is requested for the following:</p> <ul> <li>Operation Metrics</li> <li>getMetric</li> </ul>","text":""},{"location":"SQLMetricsReporting/#registering-sqlmetrics","title":"Registering SQLMetrics <pre><code>registerSQLMetrics(\n  spark: SparkSession,\n  metrics: Map[String, SQLMetric]): Unit\n</code></pre> <p><code>registerSQLMetrics</code> adds (registers) the given metrics to the operationSQLMetrics internal registry only when spark.databricks.delta.history.metricsEnabled configuration property is enabled.</p> <p><code>registerSQLMetrics</code> is used when:</p> <ul> <li>DeleteCommand, MergeIntoCommand, UpdateCommand commands are executed</li> <li><code>TransactionalWrite</code> is requested to writeFiles</li> <li><code>DeltaSink</code> is requested to addBatch</li> </ul>","text":""},{"location":"SQLMetricsReporting/#operation-metrics","title":"Operation Metrics <pre><code>getMetricsForOperation(\n  operation: Operation): Map[String, String]\n</code></pre> <p><code>getMetricsForOperation</code> requests the given Operation to transform the operation metrics.</p> <p><code>getMetricsForOperation</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested for the operation metrics</li> </ul>","text":""},{"location":"SQLMetricsReporting/#looking-up-operation-metric","title":"Looking Up Operation Metric <pre><code>getMetric(\n  name: String): Option[SQLMetric]\n</code></pre> <p><code>getMetric</code> uses the operationSQLMetrics registry to look up the <code>SQLMetric</code> by name.</p> <p><code>getMetric</code> is used when:</p> <ul> <li>UpdateCommand is executed</li> </ul>","text":""},{"location":"SchemaMergingUtils/","title":"SchemaMergingUtils","text":""},{"location":"SchemaMergingUtils/#asserting-no-column-name-duplication","title":"Asserting No Column Name Duplication <pre><code>checkColumnNameDuplication(\n  schema: StructType,\n  colType: String): Unit\n</code></pre> <p><code>checkColumnNameDuplication</code> explodes the nested field names in the given schema (<code>StructType</code>) and throws a <code>DeltaAnalysisException</code> if there are duplicates.</p>  Possible performance improvement <p>I think it's possible to make <code>checkColumnNameDuplication</code> faster as it currently seems to do more than is really required to check for column duplication.</p> <p>A schema is a tree so a duplication is when there are two nodes of the same name (lowercase) at any given level. If there is no duplicates at the highest level, there's no need to check duplicates down the tree.</p>   colType <p>The name of <code>colType</code> input argument is misleading and does not really say what it is for. It is used only for an error message to describe the operation that led to column duplication.</p>  <p><code>checkColumnNameDuplication</code> is used when:</p> <ul> <li><code>DeltaLog</code> is requested to upgrade the protocol</li> <li><code>OptimisticTransactionImpl</code> is requested to verify a new metadata</li> <li>AlterTableAddColumnsDeltaCommand and AlterTableReplaceColumnsDeltaCommand are executed</li> <li><code>SchemaMergingUtils</code> utility is used to mergeSchemas</li> </ul>","text":""},{"location":"SchemaMergingUtils/#demo","title":"Demo <pre><code>import org.apache.spark.sql.delta.schema.SchemaMergingUtils\nimport org.apache.spark.sql.types._\nval duplicatedCol = StructField(\"duplicatedCol\", StringType)\nval schema = (new StructType)\n  .add(duplicatedCol)\n  .add(duplicatedCol)\n</code></pre> <pre><code>SchemaMergingUtils.checkColumnNameDuplication(schema, colType = \"in the demo\")\n</code></pre> <pre><code>org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the demo: duplicatedcol\n  at org.apache.spark.sql.delta.DeltaAnalysisException$.apply(DeltaSharedExceptions.scala:57)\n  at org.apache.spark.sql.delta.schema.SchemaMergingUtils$.checkColumnNameDuplication(SchemaMergingUtils.scala:117)\n  ... 49 elided\n</code></pre>","text":""},{"location":"SchemaMergingUtils/#explodenestedfieldnames","title":"explodeNestedFieldNames <pre><code>explodeNestedFieldNames(\n  schema: StructType): Seq[String]\n</code></pre> <p><code>explodeNestedFieldNames</code> explodes the given schema into a collection of column names (name parts separated by <code>.</code>).</p> <p><code>explodeNestedFieldNames</code> is used when:</p> <ul> <li><code>SchemaMergingUtils</code> utility is used to checkColumnNameDuplication</li> <li><code>SchemaUtils</code> utility is used to normalizeColumnNames and checkSchemaFieldNames</li> </ul>","text":""},{"location":"SchemaMergingUtils/#demo_1","title":"Demo <pre><code>import org.apache.spark.sql.types._\nval m = MapType(keyType = LongType, valueType = StringType)\nval s = (new StructType)\n  .add(StructField(\"id\", LongType))\n  .add(StructField(\"name\", StringType))\nval a = ArrayType(elementType = (new StructType)\n  .add(StructField(\"id\", LongType))\n  .add(StructField(\"name\", StringType)))\nval schema = (new StructType)\n  .add(StructField(\"l\", LongType))\n  .add(StructField(\"s\", s))\n  .add(StructField(\"a\", a))\n  .add(StructField(\"m\", m))\n</code></pre> <pre><code>import org.apache.spark.sql.delta.schema.SchemaMergingUtils\nval colNames = SchemaMergingUtils.explodeNestedFieldNames(schema)\n</code></pre> <pre><code>colNames.foreach(println)\n</code></pre> <pre><code>l\ns\ns.id\ns.name\na\na.element.id\na.element.name\nm\n</code></pre>","text":""},{"location":"SchemaMergingUtils/#exploding-schema","title":"Exploding Schema <pre><code>explode(\n  schema: StructType): Seq[(Seq[String], StructField)]\n</code></pre> <p><code>explode</code> explodes the given schema (<code>StructType</code>) into a collection of pairs of column name parts and the associated  <code>StructField</code>. The nested fields (<code>StructType</code>s, <code>ArrayType</code>s and <code>MapType</code>s) are flattened out.</p> <p><code>explode</code> is used when:</p> <ul> <li><code>DeltaColumnMappingBase</code> is requested to getPhysicalNameFieldMap</li> <li><code>SchemaMergingUtils</code> utility is used to explodeNestedFieldNames</li> </ul>","text":""},{"location":"SchemaMergingUtils/#demo_2","title":"Demo  <p>FIXME</p> <p>Move the examples to Delta Lake (as unit tests).</p>","text":""},{"location":"SchemaMergingUtils/#maptype","title":"MapType <pre><code>import org.apache.spark.sql.delta.schema.SchemaMergingUtils\nimport org.apache.spark.sql.types._\n\nval m = MapType(keyType = LongType, valueType = StringType)\nval schemaWithMap = (new StructType).add(StructField(\"m\", m))\nval r = SchemaMergingUtils.explode(schemaWithMap)\nr.foreach(println)\n</code></pre> <pre><code>(List(m),StructField(m,MapType(LongType,StringType,true),true))\n</code></pre> <pre><code>r.map { case (ns, f) =&gt; s\"${ns.mkString} -&gt; ${f.dataType.sql}\" }.foreach(println)\n</code></pre> <pre><code>m -&gt; MAP&lt;BIGINT, STRING&gt;\n</code></pre>","text":""},{"location":"SchemaMergingUtils/#arraytype","title":"ArrayType <pre><code>import org.apache.spark.sql.delta.schema.SchemaMergingUtils\nimport org.apache.spark.sql.types._\n\nval idName = (new StructType)\n  .add(StructField(\"id\", LongType))\n  .add(StructField(\"name\", StringType))\nval a = ArrayType(elementType = idName)\nval schemaWithArray = (new StructType).add(StructField(\"a\", a))\nval r = SchemaMergingUtils.explode(schemaWithArray)\nr.foreach(println)\n</code></pre> <pre><code>(List(a),StructField(a,ArrayType(StructType(StructField(id,LongType,true), StructField(name,StringType,true)),true),true))\n(List(a, element, id),StructField(id,LongType,true))\n(List(a, element, name),StructField(name,StringType,true))\n</code></pre> <pre><code>r.map { case (ns, f) =&gt; s\"${ns.mkString} -&gt; ${f.dataType.sql}\" }.foreach(println)\n</code></pre> <pre><code>a -&gt; ARRAY&lt;STRUCT&lt;`id`: BIGINT, `name`: STRING&gt;&gt;\naelementid -&gt; BIGINT\naelementname -&gt; STRING\n</code></pre>","text":""},{"location":"SchemaMergingUtils/#structtype","title":"StructType <pre><code>import org.apache.spark.sql.delta.schema.SchemaMergingUtils\nimport org.apache.spark.sql.types._\n\nval s = (new StructType)\n  .add(StructField(\"id\", LongType))\n  .add(StructField(\"name\", StringType))\nval schemaWithStructType = (new StructType).add(StructField(\"s\", s))\nval r = SchemaMergingUtils.explode(schemaWithStructType)\nr.foreach(println)\n</code></pre> <pre><code>(List(s),StructField(s,StructType(StructField(id,LongType,true), StructField(name,StringType,true)),true))\n(List(s, id),StructField(id,LongType,true))\n(List(s, name),StructField(name,StringType,true))\n</code></pre> <pre><code>r.map { case (ns, f) =&gt; s\"${ns.mkString} -&gt; ${f.dataType.sql}\" }.foreach(println)\n</code></pre> <pre><code>s -&gt; STRUCT&lt;`id`: BIGINT, `name`: STRING&gt;\nsid -&gt; BIGINT\nsname -&gt; STRING\n</code></pre>","text":""},{"location":"SchemaMergingUtils/#complex-schema","title":"Complex Schema <pre><code>import org.apache.spark.sql.types._\nval m = MapType(keyType = LongType, valueType = StringType)\nval s = (new StructType)\n  .add(StructField(\"id\", LongType))\n  .add(StructField(\"name\", StringType))\nval a = ArrayType(elementType = (new StructType)\n  .add(StructField(\"id\", LongType))\n  .add(StructField(\"name\", StringType)))\nval schema = (new StructType)\n  .add(StructField(\"l\", LongType))\n  .add(StructField(\"s\", s))\n  .add(StructField(\"a\", a))\n  .add(StructField(\"m\", m))\n</code></pre> <pre><code>import org.apache.spark.sql.delta.schema.SchemaMergingUtils\nval r = SchemaMergingUtils.explode(schema)\n</code></pre> <pre><code>r.foreach(println)\n</code></pre> <pre><code>(List(l),StructField(l,LongType,true))\n(List(s),StructField(s,StructType(StructField(id,LongType,true), StructField(name,StringType,true)),true))\n(List(s, id),StructField(id,LongType,true))\n(List(s, name),StructField(name,StringType,true))\n(List(a),StructField(a,ArrayType(StructType(StructField(id,LongType,true), StructField(name,StringType,true)),true),true))\n(List(a, element, id),StructField(id,LongType,true))\n(List(a, element, name),StructField(name,StringType,true))\n(List(m),StructField(m,MapType(LongType,StringType,true),true))\n</code></pre> <pre><code>r.map { case (ns, f) =&gt; s\"${ns.mkString} -&gt; ${f.dataType.sql}\" }.foreach(println)\n</code></pre> <pre><code>l -&gt; BIGINT\ns -&gt; STRUCT&lt;`id`: BIGINT, `name`: STRING&gt;\nsid -&gt; BIGINT\nsname -&gt; STRING\na -&gt; ARRAY&lt;STRUCT&lt;`id`: BIGINT, `name`: STRING&gt;&gt;\naelementid -&gt; BIGINT\naelementname -&gt; STRING\nm -&gt; MAP&lt;BIGINT, STRING&gt;\n</code></pre>","text":""},{"location":"SchemaUtils/","title":"SchemaUtils Utility","text":""},{"location":"SchemaUtils/#mergeschemas","title":"mergeSchemas <pre><code>mergeSchemas(\n  tableSchema: StructType,\n  dataSchema: StructType,\n  allowImplicitConversions: Boolean = false,\n  keepExistingType: Boolean = false,\n  fixedTypeColumns: Set[String] = Set.empty): StructType\n</code></pre> <p><code>mergeSchemas</code>...FIXME</p> <p><code>mergeSchemas</code>\u00a0is used when:</p> <ul> <li><code>DeltaMergeInto</code> utility is used to resolveReferencesAndSchema</li> <li><code>ParquetTable</code> is requested to mergeSchemasInParallel and inferSchema</li> <li><code>ImplicitMetadataOperation</code> is requested to update a metadata (and mergeSchema)</li> </ul>","text":""},{"location":"SchemaUtils/#asserting-valid-column-names-in-nomapping-mode","title":"Asserting Valid Column Names in NoMapping Mode <pre><code>checkSchemaFieldNames(\n  schema: StructType,\n  columnMappingMode: DeltaColumnMappingMode): Unit\n</code></pre> <p><code>checkSchemaFieldNames</code> does nothing (and simply returns) for all the DeltaColumnMappingModes but NoMapping.</p> <p>For NoMapping, <code>checkSchemaFieldNames</code> explodes the nested field names and asserts that column names are valid. In case of a validation exception, <code>checkSchemaFieldNames</code> throws a DeltaAnalysisException.</p> <p><code>checkSchemaFieldNames</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to verify a new metadata</li> <li>AlterTableAddColumnsDeltaCommand and AlterTableReplaceColumnsDeltaCommand are executed</li> </ul>","text":""},{"location":"SchemaUtils/#asserting-valid-column-names","title":"Asserting Valid Column Names <pre><code>checkFieldNames(\n  names: Seq[String]): Unit\n</code></pre> <p><code>checkFieldNames</code> throws an <code>AnalysisException</code> when there is a column name (in <code>names</code>) with one of the illegal characters:</p> <pre><code> ,;{}()\\n\\t=\n</code></pre> <p><code>checkFieldNames</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to verify a new metadata</li> <li><code>SchemaUtils</code> is used to assert valid column names in NoMapping mode</li> </ul>","text":""},{"location":"SchemaUtils/#demo","title":"Demo <pre><code>import org.apache.spark.sql.delta.schema.SchemaUtils\nval colName = \"\\n\"\nSchemaUtils.checkFieldNames(Seq(colName))\n</code></pre> <pre><code>org.apache.spark.sql.AnalysisException:  Column name \" \" contains invalid character(s). Please use alias to rename it.\n  at org.apache.spark.sql.errors.QueryCompilationErrors$.columnNameContainsInvalidCharactersError(QueryCompilationErrors.scala:2102)\n  at org.apache.spark.sql.delta.schema.SchemaUtils$.$anonfun$checkFieldNames$1(SchemaUtils.scala:908)\n  at org.apache.spark.sql.delta.schema.SchemaUtils$.$anonfun$checkFieldNames$1$adapted(SchemaUtils.scala:905)\n  at scala.collection.immutable.List.foreach(List.scala:431)\n  at org.apache.spark.sql.delta.schema.SchemaUtils$.checkFieldNames(SchemaUtils.scala:905)\n  ... 49 elided\n</code></pre>","text":""},{"location":"SchemaUtils/#finddependentgeneratedcolumns","title":"findDependentGeneratedColumns <pre><code>findDependentGeneratedColumns(\n  sparkSession: SparkSession,\n  targetColumn: Seq[String],\n  protocol: Protocol,\n  schema: StructType): Seq[StructField]\n</code></pre> <p><code>findDependentGeneratedColumns</code>...FIXME</p> <p><code>findDependentGeneratedColumns</code> is used when:</p> <ul> <li><code>AlterDeltaTableCommand</code> is requested to checkDependentExpressions</li> </ul>","text":""},{"location":"SchemaUtils/#findcolumnposition","title":"findColumnPosition <pre><code>findColumnPosition(\n  column: Seq[String],\n  schema: StructType,\n  resolver: Resolver = DELTA_COL_RESOLVER): (Seq[Int], Int)\n</code></pre> <p><code>findColumnPosition</code>...FIXME</p> <p><code>findColumnPosition</code> is used when:</p> <ul> <li>AlterTableAddColumnsDeltaCommand, AlterTableDropColumnsDeltaCommand, AlterTableChangeColumnDeltaCommand and OptimizeTableCommand are executed</li> </ul>","text":""},{"location":"SetTransaction/","title":"SetTransaction","text":"<p><code>SetTransaction</code> is an Action defined by the following properties:</p> <ul> <li> Application ID (i.e. streaming query ID) <li> Version (i.e. micro-batch ID) <li> Last Updated (optional) (i.e. milliseconds since the epoch) <p><code>SetTransaction</code> is created when:</p> <ul> <li><code>DeltaSink</code> is requested to add a streaming micro-batch (for <code>STREAMING UPDATE</code> operation idempotence at query restart)</li> </ul>"},{"location":"SetTransaction/#demo","title":"Demo","text":"<pre><code>val path = \"/tmp/delta/users\"\n\nimport org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, path)\n\nimport org.apache.spark.sql.delta.actions.SetTransaction\nassert(deltaLog.snapshot.setTransactions.isInstanceOf[Seq[SetTransaction]])\n\ndeltaLog.snapshot.setTransactions\n</code></pre>"},{"location":"SingleAction/","title":"SingleAction","text":"<p><code>SingleAction</code> is...FIXME</p>"},{"location":"Snapshot/","title":"Snapshot","text":"<p><code>Snapshot</code> is an immutable snapshot of the state of a delta table (in the deltaLog) at the given version.</p> <p><code>Snapshot</code> uses aggregation expressions while computing state (as State).</p> <p><code>Snapshot</code> loads the actions (per the DeltaLogFileIndices) and builds a <code>DataFrame</code>.</p>"},{"location":"Snapshot/#creating-instance","title":"Creating Instance","text":"<p><code>Snapshot</code> takes the following to be created:</p> <ul> <li> Hadoop Path to the log directory <li> Version <li> LogSegment <li> <code>minFileRetentionTimestamp</code> (that is exactly DeltaLog.minFileRetentionTimestamp) <li> DeltaLog <li> Timestamp <li> <code>VersionChecksum</code> <p>While being created, <code>Snapshot</code> prints out the following INFO message to the logs and initialize:</p> <pre><code>Created snapshot [this]\n</code></pre> <p><code>Snapshot</code> is created when:</p> <ul> <li><code>SnapshotManagement</code> is requested for a Snapshot</li> </ul>"},{"location":"Snapshot/#initializing","title":"Initializing <pre><code>init(): Unit\n</code></pre> <p><code>init</code> requests the DeltaLog for the protocolRead for the Protocol.</p>","text":""},{"location":"Snapshot/#maximum-number-of-indexed-columns","title":"Maximum Number of Indexed Columns <pre><code>numIndexedCols: Int\n</code></pre> <p><code>numIndexedCols</code> is the value of dataSkippingNumIndexedCols table property.</p>  Lazy Value <p><code>numIndexedCols</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>numIndexedCols</code> is part of the StatisticsCollection abstraction.</p>","text":""},{"location":"Snapshot/#demo","title":"Demo <ul> <li>Demo: DeltaTable, DeltaLog And Snapshots</li> </ul>","text":""},{"location":"Snapshot/#computed-state","title":"Computed State <pre><code>computedState: State\n</code></pre>  Lazy Value <p><code>computedState</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>computedState</code> takes the current cached set of actions and reads the latest state (executes a <code>state.select(...).first()</code> query) with the aggregations (that are then mapped to a State).</p>  <p>Note</p> <p>The <code>state.select(...).first()</code> query uses aggregate standard functions (e.g. <code>last</code>, <code>collect_set</code>, <code>sum</code>, <code>count</code>) and so uses <code>groupBy</code> over the whole dataset indirectly.</p>  <p></p> <p><code>computedState</code> asserts that the <code>State</code> to be returned has at least the default protocol and metadata (actions) defined.</p>  <p>While executing the aggregation query, <code>computedState</code> withStatusCode with the following:</p>    Property Value     statusCode DELTA   defaultMessage Compute snapshot for version: version     <p>Tip</p> <p>Use event logs for the INFO messages and web UI to monitor execution of the aggregation query with the following job description:</p> <pre><code>Delta: Compute snapshot for version: [version]\n</code></pre>   <p><code>computedState</code> assumes that the protocol and metadata (actions) are defined. <code>computedState</code> throws an <code>IllegalStateException</code> when the actions are not defined and spark.databricks.delta.stateReconstructionValidation.enabled configuration property is enabled.</p> <pre><code>The [action] of your Delta table couldn't be recovered while Reconstructing\nversion: [version]. Did you manually delete files in the _delta_log directory?\n</code></pre>  <p>Note</p> <p>The <code>state.select(...).first()</code> query uses <code>last</code> with <code>ignoreNulls</code> flag <code>true</code> and so may give no rows for <code>first()</code>.</p>","text":""},{"location":"Snapshot/#aggregationstocomputestate","title":"aggregationsToComputeState <pre><code>aggregationsToComputeState: Map[String, Column]\n</code></pre>    Alias Aggregation Expression     <code>sizeInBytes</code> <code>coalesce(sum(col(\"add.size\")), lit(0L))</code>   <code>numOfSetTransactions</code> <code>count(col(\"txn\"))</code>   <code>numOfFiles</code> <code>count(col(\"add\"))</code>   <code>numOfRemoves</code> <code>count(col(\"remove\"))</code>   <code>numOfMetadata</code> <code>count(col(\"metaData\"))</code>   <code>numOfProtocol</code> <code>count(col(\"protocol\"))</code>   <code>setTransactions</code> <code>collect_set(col(\"txn\"))</code>   <code>metadata</code> <code>last(col(\"metaData\"), ignoreNulls = true)</code>   <code>protocol</code> <code>last(col(\"protocol\"), ignoreNulls = true)</code>   <code>fileSizeHistogram</code> <code>lit(null).cast(FileSizeHistogram.schema)</code>","text":""},{"location":"Snapshot/#configuration-properties","title":"Configuration Properties","text":""},{"location":"Snapshot/#sparkdatabricksdeltasnapshotpartitions","title":"spark.databricks.delta.snapshotPartitions <p><code>Snapshot</code> uses the spark.databricks.delta.snapshotPartitions configuration property for the number of partitions to use for state reconstruction.</p>","text":""},{"location":"Snapshot/#sparkdatabricksdeltastatereconstructionvalidationenabled","title":"spark.databricks.delta.stateReconstructionValidation.enabled <p><code>Snapshot</code> uses the spark.databricks.delta.stateReconstructionValidation.enabled configuration property for reconstructing state.</p>","text":""},{"location":"Snapshot/#state-dataset-of-actions","title":"State Dataset (of Actions) <pre><code>state: Dataset[SingleAction]\n</code></pre> <p><code>state</code> requests the cached delta table state for the current state (from the cache).</p> <p><code>state</code> is used when:</p> <ul> <li><code>Checkpoints</code> utility is used to writeCheckpoint</li> <li><code>Snapshot</code> is requested for computedState, all files and files removed (tombstones)</li> <li><code>VacuumCommand</code> utility is requested for garbage collection</li> </ul>","text":""},{"location":"Snapshot/#cached-state","title":"Cached State <pre><code>cachedState: CachedDS[SingleAction]\n</code></pre>  Lazy Value <p><code>cachedState</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>withStatsCache</code> caches the stateReconstruction DataFrame under the following name (with the version and the redactedPath):</p> <pre><code>Delta Table State #[version] - [redactedPath]\n</code></pre>","text":""},{"location":"Snapshot/#all-addfiles","title":"All AddFiles <pre><code>allFiles: Dataset[AddFile]\n</code></pre> <p><code>allFiles</code> simply takes the state dataset and selects AddFiles (adds <code>where</code> clause for <code>add IS NOT NULL</code> and <code>select</code> over the fields of AddFiles).</p>  <p>Note</p> <p><code>allFiles</code> simply adds <code>where</code> and <code>select</code> clauses. No computation happens yet as it is (a description of) a distributed computation as a <code>Dataset[AddFile]</code>.</p>  <pre><code>import org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, \"/tmp/delta/users\")\nval files = deltaLog.snapshot.allFiles\n\nscala&gt; :type files\norg.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.AddFile]\n\nscala&gt; files.show(truncate = false)\n+-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+\n|path                                                               |partitionValues|size|modificationTime|dataChange|stats|tags|\n+-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+\n|part-00000-4050db39-e0f5-485d-ab3b-3ca72307f621-c000.snappy.parquet|[]             |262 |1578083748000   |false     |null |null|\n|part-00000-ba39f292-2970-4528-a40c-8f0aa5f796de-c000.snappy.parquet|[]             |262 |1578083570000   |false     |null |null|\n|part-00003-99f9d902-24a7-4f76-a15a-6971940bc245-c000.snappy.parquet|[]             |429 |1578083748000   |false     |null |null|\n|part-00007-03d987f1-5bb3-4b5b-8db9-97b6667107e2-c000.snappy.parquet|[]             |429 |1578083748000   |false     |null |null|\n|part-00011-a759a8c2-507d-46dd-9da7-dc722316214b-c000.snappy.parquet|[]             |429 |1578083748000   |false     |null |null|\n|part-00015-2e685d29-25ed-4262-90a7-5491847fd8d0-c000.snappy.parquet|[]             |429 |1578083748000   |false     |null |null|\n|part-00015-ee0ac1af-e1e0-4422-8245-12da91ced0a2-c000.snappy.parquet|[]             |429 |1578083570000   |false     |null |null|\n+-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+\n</code></pre> <p><code>allFiles</code> is used when:</p> <ul> <li> <p><code>PartitionFiltering</code> is requested for the files to scan (matching projection attributes and predicates)</p> </li> <li> <p><code>DeltaSourceSnapshot</code> is requested for the initial files (indexed AddFiles)</p> </li> <li> <p><code>GenerateSymlinkManifestImpl</code> is requested to generateIncrementalManifest and generateFullManifest</p> </li> <li> <p><code>DeltaDataSource</code> is requested for an Insertable HadoopFsRelation</p> </li> </ul>","text":""},{"location":"Snapshot/#statereconstruction-dataset-of-actions","title":"stateReconstruction Dataset of Actions <pre><code>stateReconstruction: Dataset[SingleAction]\n</code></pre>  <p>Note</p> <p><code>stateReconstruction</code> returns a <code>Dataset[SingleAction]</code> and so does not do any computation per se.</p>  <p><code>stateReconstruction</code> is a <code>Dataset</code> of SingleActions (that is the dataset part) of the cachedState.</p> <p><code>stateReconstruction</code> loads the log file indices (that gives a <code>Dataset[SingleAction]</code>).</p> <p><code>stateReconstruction</code> maps over partitions (using <code>Dataset.mapPartitions</code>) and canonicalize the paths for AddFile and RemoveFile actions.</p> <p><code>stateReconstruction</code> adds <code>file</code> column that uses a UDF to assert that <code>input_file_name()</code> belongs to the Delta table.</p>  <p>Note</p> <p>This UDF-based check is very clever.</p>  <p><code>stateReconstruction</code> repartitions the <code>Dataset</code> using the path of add or remove actions (with the configurable number of partitions) and <code>Dataset.sortWithinPartitions</code> by the <code>file</code> column.</p> <p>In the end, <code>stateReconstruction</code> maps over partitions (using <code>Dataset.mapPartitions</code>) that creates a InMemoryLogReplay, requests it to append the actions (as version <code>0</code>) and checkpoint.</p> <p><code>stateReconstruction</code> is used when:</p> <ul> <li><code>Snapshot</code> is requested for a cached Delta table state</li> </ul>","text":""},{"location":"Snapshot/#loading-actions","title":"Loading Actions <pre><code>loadActions: Dataset[SingleAction]\n</code></pre> <p><code>loadActions</code> creates a union of <code>Dataset[SingleAction]</code>s for the indices (as LogicalRelations over a <code>HadoopFsRelation</code>) or defaults to an empty dataset.</p>","text":""},{"location":"Snapshot/#indextorelation","title":"indexToRelation <pre><code>indexToRelation(\n  index: DeltaLogFileIndex,\n  schema: StructType = logSchema): LogicalRelation\n</code></pre> <p><code>indexToRelation</code> converts the DeltaLogFileIndex to a <code>LogicalRelation</code> (Spark SQL) leaf logical operator (using the logSchema).</p> <p><code>indexToRelation</code> creates a <code>LogicalRelation</code> over a <code>HadoopFsRelation</code> (Spark SQL) with the given index and the schema.</p>","text":""},{"location":"Snapshot/#emptyactions-dataset-of-actions","title":"emptyActions Dataset (of Actions) <pre><code>emptyActions: Dataset[SingleAction]\n</code></pre> <p><code>emptyActions</code> is an empty dataset of SingleActions for loadActions (and <code>InitialSnapshot</code>'s <code>state</code>).</p>","text":""},{"location":"Snapshot/#table-properties","title":"Table Properties <pre><code>getProperties: mutable.HashMap[String, String]\n</code></pre> <p><code>getProperties</code> returns the following:</p> <ul> <li>Configuration (of the Metadata) without <code>path</code></li> <li>delta.minReaderVersion to be the minReaderVersion (of the Protocol)</li> <li>delta.minWriterVersion to be the minWriterVersion (of the Protocol)</li> </ul> <p><code>getProperties</code> is used when:</p> <ul> <li><code>DeltaTableV2</code> is requested for the table properties</li> </ul>","text":""},{"location":"Snapshot/#file-indices","title":"File Indices <pre><code>fileIndices: Seq[DeltaLogFileIndex]\n</code></pre>  Lazy Value <p><code>fileIndices</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>fileIndices</code> is the checkpointFileIndexOpt and the deltaFileIndexOpt (if available).</p>","text":""},{"location":"Snapshot/#commit-file-index","title":"Commit File Index <pre><code>deltaFileIndexOpt: Option[DeltaLogFileIndex]\n</code></pre>  Lazy Value <p><code>deltaFileIndexOpt</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>deltaFileIndexOpt</code> is a DeltaLogFileIndex (in <code>JsonFileFormat</code>) for the checkpoint file of the LogSegment.</p>","text":""},{"location":"Snapshot/#checkpoint-file-index","title":"Checkpoint File Index <pre><code>checkpointFileIndexOpt: Option[DeltaLogFileIndex]\n</code></pre>  Lazy Value <p><code>checkpointFileIndexOpt</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>checkpointFileIndexOpt</code> is a DeltaLogFileIndex (in <code>ParquetFileFormat</code>) for the delta files of the LogSegment.</p>","text":""},{"location":"Snapshot/#transaction-version-by-app-id","title":"Transaction Version By App ID <pre><code>transactions: Map[String, Long]\n</code></pre> <p><code>transactions</code> takes the SetTransaction actions (from the state dataset) and makes them a lookup table of transaction version by appId.</p>  <p>Scala lazy value</p> <p><code>transactions</code> is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the <code>Snapshot</code> instance.</p> <pre><code>lazy val transactions: Map[String, Long]\n</code></pre>  <p><code>transactions</code> is used when <code>OptimisticTransactionImpl</code> is requested for the transaction version for a given (streaming query) id.</p>","text":""},{"location":"Snapshot/#all-removefile-actions-tombstones","title":"All RemoveFile Actions (Tombstones) <pre><code>tombstones: Dataset[RemoveFile]\n</code></pre> <p><code>tombstones</code>...FIXME</p> <pre><code>scala&gt; deltaLog.snapshot.tombstones.show(false)\n+----+-----------------+----------+\n|path|deletionTimestamp|dataChange|\n+----+-----------------+----------+\n+----+-----------------+----------+\n</code></pre>","text":""},{"location":"Snapshot/#data-schema-of-delta-table","title":"Data Schema (of Delta Table) <pre><code>dataSchema: StructType\n</code></pre> <p><code>dataSchema</code> requests the Metadata for the data schema.</p> <p><code>dataSchema</code> is part of the StatisticsCollection abstraction.</p>","text":""},{"location":"Snapshot/#metadata","title":"Metadata <pre><code>metadata: Metadata\n</code></pre> <p><code>metadata</code> is part of the SnapshotDescriptor and DataSkippingReaderBase abstractions.</p>  <p><code>metadata</code> requests the computedState for the Metadata.</p>","text":""},{"location":"SnapshotDescriptor/","title":"SnapshotDescriptor","text":"<p><code>SnapshotDescriptor</code> is an abstraction of descriptions of the versioned snapshots of a delta table.</p>"},{"location":"SnapshotDescriptor/#contract","title":"Contract","text":""},{"location":"SnapshotDescriptor/#deltalog","title":"DeltaLog <pre><code>deltaLog: DeltaLog\n</code></pre> <p>DeltaLog</p> <p>Used when:</p> <ul> <li><code>GeneratedColumn</code> is requested to generatePartitionFilters (for reporting purposes only)</li> </ul>","text":""},{"location":"SnapshotDescriptor/#metadata","title":"Metadata <pre><code>metadata: Metadata\n</code></pre> <p>Metadata</p> <p>Used when:</p> <ul> <li><code>GeneratedColumn</code> is requested to generatePartitionFilters</li> <li><code>SnapshotDescriptor</code> is requested for the table schema</li> <li><code>TahoeFileIndex</code> is requested for the partition schema</li> </ul>","text":""},{"location":"SnapshotDescriptor/#protocol","title":"Protocol <pre><code>protocol: Protocol\n</code></pre> <p>Protocol</p> <p>Used when:</p> <ul> <li><code>GeneratedColumn</code> is requested to generatePartitionFilters</li> </ul>","text":""},{"location":"SnapshotDescriptor/#version","title":"Version <pre><code>version: Long\n</code></pre> <p>Used when:</p> <ul> <li><code>TahoeFileIndex</code> is requested for the string representation</li> </ul>","text":""},{"location":"SnapshotDescriptor/#implementations","title":"Implementations","text":"<ul> <li>Snapshot</li> <li>TahoeFileIndex</li> </ul>"},{"location":"SnapshotDescriptor/#table-schema","title":"Table Schema <pre><code>schema: StructType\n</code></pre> <p><code>schema</code> requests the Metadata for the table schema.</p>  <p><code>schema</code> is used when:</p> <ul> <li><code>DeltaCatalog</code> is requested to alterTable</li> <li><code>DeltaTableV2</code> is requested for the table schema</li> <li><code>ShowTableColumnsCommand</code> is executed</li> <li><code>TahoeLogFileIndex</code> is requested to getSnapshot</li> <li><code>DeltaDataSource</code> is requested for the source schema and createSource</li> <li><code>DeltaSourceBase</code> is requested for the table schema, checkColumnMappingSchemaChangesOnStreamStartOnce (for reporting purposes)</li> </ul>","text":""},{"location":"SnapshotIterator/","title":"SnapshotIterator","text":"<p><code>SnapshotIterator</code> is an abstraction of iterators over indexed AddFile actions in a Delta log for DeltaSourceSnapshots.</p>"},{"location":"SnapshotIterator/#iterator-of-indexed-addfiles","title":"Iterator of Indexed AddFiles <pre><code>iterator(): Iterator[IndexedFile]\n</code></pre> <p><code>iterator</code> returns an <code>Iterator</code> (Scala) of <code>IndexedFile</code>s (AddFile actions in a Delta log with extra metadata) of filterFileList.</p> <p><code>iterator</code>\u00a0is used when:</p> <ul> <li><code>DeltaSource</code> is requested for the snapshot of a delta table at a given version</li> </ul>","text":""},{"location":"SnapshotIterator/#closing-iterator-cleaning-up-internal-resources","title":"Closing Iterator (Cleaning Up Internal Resources) <pre><code>close(\n  unpersistSnapshot: Boolean): Unit\n</code></pre> <p><code>close</code> is a no-op (and leaves proper operation to implementations).</p> <p><code>close</code>\u00a0is used when:</p> <ul> <li><code>DeltaSource</code> is requested to cleanUpSnapshotResources</li> </ul>","text":""},{"location":"SnapshotIterator/#implementations","title":"Implementations <ul> <li>DeltaSourceSnapshot</li> </ul>","text":""},{"location":"SnapshotManagement/","title":"SnapshotManagement","text":"<p><code>SnapshotManagement</code> is an extension for DeltaLog to manage Snapshots.</p>"},{"location":"SnapshotManagement/#current-snapshot","title":"Current Snapshot <p><code>SnapshotManagement</code> defines <code>currentSnapshot</code> registry with the recently-loaded Snapshot of the delta table.</p> <p><code>currentSnapshot</code> is the latest Snapshot initially and can be updated on demand (when installLogSegmentInternal and replaceSnapshot).</p> <p><code>currentSnapshot</code>...FIXME</p>  <p><code>currentSnapshot</code> is used when <code>SnapshotManagement</code> is requested for the following:</p> <ul> <li>unsafeVolatileSnapshot</li> <li>update (tryUpdate, updateInternal, installLogSegmentInternal, replaceSnapshot)</li> <li>updateAfterCommit</li> </ul>","text":""},{"location":"SnapshotManagement/#loading-latest-snapshot-at-initialization","title":"Loading Latest Snapshot at Initialization <pre><code>getSnapshotAtInit: Snapshot\n</code></pre> <p><code>getSnapshotAtInit</code> finds the LogSegment of the delta table (using the last checkpoint file if available)</p> <p>In the end, <code>getSnapshotAtInit</code> createSnapshotAtInitInternal.</p>","text":""},{"location":"SnapshotManagement/#createsnapshotatinitinternal","title":"createSnapshotAtInitInternal <pre><code>createSnapshotAtInitInternal(\n  initSegment: Option[LogSegment],\n  lastCheckpointOpt: Option[CheckpointMetaData],\n  timestamp: Long): CapturedSnapshot\n</code></pre> <p><code>createSnapshotAtInitInternal</code>...FIXME</p>","text":""},{"location":"SnapshotManagement/#fetching-log-files-for-version-checkpointed","title":"Fetching Log Files for Version Checkpointed <pre><code>getLogSegmentFrom(\n  startingCheckpoint: Option[CheckpointMetaData]): LogSegment\n</code></pre> <p><code>getLogSegmentFrom</code> fetches log files for the version (based on the optional <code>CheckpointMetaData</code> as the starting checkpoint version to start listing log files from).</p>","text":""},{"location":"SnapshotManagement/#fetching-latest-checkpoint-and-delta-log-files-for-version","title":"Fetching Latest Checkpoint and Delta Log Files for Version <pre><code>getLogSegmentForVersion(\n  startCheckpoint: Option[Long],\n  versionToLoad: Option[Long] = None): LogSegment\n</code></pre> <p><code>getLogSegmentForVersion</code> list all the files (in a transaction log) from the given <code>startCheckpoint</code> (or defaults to <code>0</code>).</p> <p><code>getLogSegmentForVersion</code> filters out unnecessary files and leaves checkpoint and delta files only.</p> <p><code>getLogSegmentForVersion</code> filters out checkpoint files of size <code>0</code>.</p> <p><code>getLogSegmentForVersion</code> takes all the files that are older than the requested <code>versionToLoad</code>.</p> <p><code>getLogSegmentForVersion</code> splits the files into checkpoint and delta files.</p> <p><code>getLogSegmentForVersion</code> finds the latest checkpoint from the list.</p> <p>In the end, <code>getLogSegmentForVersion</code> creates a LogSegment with the (checkpoint and delta) files.</p> <p><code>getLogSegmentForVersion</code> is used when:</p> <ul> <li><code>SnapshotManagement</code> is requested for getLogSegmentFrom, updateInternal and getSnapshotAt</li> </ul>","text":""},{"location":"SnapshotManagement/#listing-files-from-version-upwards","title":"Listing Files from Version Upwards <pre><code>listFrom(\n  startVersion: Long): Iterator[FileStatus]\n</code></pre> <p><code>listFrom</code>...FIXME</p>","text":""},{"location":"SnapshotManagement/#creating-snapshot","title":"Creating Snapshot <pre><code>createSnapshot(\n  segment: LogSegment,\n  minFileRetentionTimestamp: Long,\n  timestamp: Long): Snapshot\n</code></pre> <p><code>createSnapshot</code> readChecksum (for the version of the given LogSegment) and creates a Snapshot.</p> <p><code>createSnapshot</code> is used when:</p> <ul> <li><code>SnapshotManagement</code> is requested for getSnapshotAtInit, updateInternal and getSnapshotAt</li> </ul>","text":""},{"location":"SnapshotManagement/#last-successful-update-timestamp","title":"Last Successful Update Timestamp <p><code>SnapshotManagement</code> uses <code>lastUpdateTimestamp</code> internal registry for the timestamp of the last successful update.</p>","text":""},{"location":"SnapshotManagement/#updating-current-snapshot","title":"Updating Current Snapshot <pre><code>update(\n  stalenessAcceptable: Boolean = false): Snapshot\n</code></pre> <p><code>update</code> determines whether to do update asynchronously or not based on the input <code>stalenessAcceptable</code> flag and isSnapshotStale.</p> <p>With <code>stalenessAcceptable</code> flag turned off (the default value) and the state snapshot is not stale, <code>update</code> updates (with <code>isAsync</code> flag turned off).</p> <p><code>update</code>...FIXME</p>","text":""},{"location":"SnapshotManagement/#usage","title":"Usage <p><code>update</code> is used when:</p> <ul> <li><code>DeltaHistoryManager</code> is requested to getHistory, getActiveCommitAtTime, checkVersionExists</li> <li><code>DeltaLog</code> is requested to start a transaction</li> <li><code>OptimisticTransactionImpl</code> is requested to doCommit and getNextAttemptVersion</li> <li><code>DeltaTableV2</code> is requested for a Snapshot</li> <li><code>TahoeLogFileIndex</code> is requested for a Snapshot</li> <li><code>DeltaSource</code> is requested for the getStartingVersion</li> <li>In Delta commands...</li> </ul>","text":""},{"location":"SnapshotManagement/#issnapshotstale","title":"isSnapshotStale <pre><code>isSnapshotStale: Boolean\n</code></pre> <p><code>isSnapshotStale</code> reads spark.databricks.delta.stalenessLimit configuration property.</p> <p><code>isSnapshotStale</code> is enabled (<code>true</code>) when any of the following holds:</p> <ol> <li>spark.databricks.delta.stalenessLimit configuration property is <code>0</code> (the default)</li> <li>Internal lastUpdateTimestamp has never been updated (and is below <code>0</code>) or is at least spark.databricks.delta.stalenessLimit configuration property old</li> </ol>","text":""},{"location":"SnapshotManagement/#tryupdate","title":"tryUpdate <pre><code>tryUpdate(\n  isAsync: Boolean = false): Snapshot\n</code></pre> <p><code>tryUpdate</code>...FIXME</p>","text":""},{"location":"SnapshotManagement/#updateinternal","title":"updateInternal <pre><code>updateInternal(\n  isAsync: Boolean): Snapshot // (1)\n</code></pre> <ol> <li><code>isAsync</code> flag is not used</li> </ol> <p><code>updateInternal</code> requests the current Snapshot for the LogSegment that is in turn requested for the checkpointVersion to get the LogSegment for.</p> <p><code>updateInternal</code> installLogSegmentInternal.</p>","text":""},{"location":"SnapshotManagement/#installlogsegmentinternal","title":"installLogSegmentInternal <pre><code>installLogSegmentInternal(\n  previousSnapshot: Snapshot,\n  segmentOpt: Option[LogSegment],\n  updateTimestamp: Long,\n  isAsync: Boolean): Snapshot // (1)!\n</code></pre> <ol> <li><code>isAsync</code> is not used</li> </ol> <p><code>installLogSegmentInternal</code> gives the Snapshot (possibly an InitialSnapshot) of the delta table at the logPath.</p>  <p><code>installLogSegmentInternal</code>...FIXME</p> <p>With no LogSegment specified, <code>installLogSegmentInternal</code> prints out the following INFO message to the logs and replaceSnapshot with a new InitialSnapshot (for the logPath).</p> <pre><code>No delta log found for the Delta table at [logPath]\n</code></pre>","text":""},{"location":"SnapshotManagement/#replacing-snapshots","title":"Replacing Snapshots <pre><code>replaceSnapshot(\n  newSnapshot: Snapshot): Unit\n</code></pre> <p><code>replaceSnapshot</code> requests the currentSnapshot to uncache (and drop any cached data) and makes the given <code>newSnapshot</code> the current one.</p>","text":""},{"location":"SnapshotManagement/#updateaftercommit","title":"updateAfterCommit <pre><code>updateAfterCommit(\n  committedVersion: Long,\n  newChecksumOpt: Option[VersionChecksum],\n  preCommitLogSegment: LogSegment): Snapshot\n</code></pre> <p><code>updateAfterCommit</code>...FIXME</p>  <p><code>updateAfterCommit</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to attempt a commit</li> </ul>","text":""},{"location":"SnapshotManagement/#demo","title":"Demo <pre><code>import org.apache.spark.sql.delta.DeltaLog\nval log = DeltaLog.forTable(spark, dataPath)\n\nimport org.apache.spark.sql.delta.SnapshotManagement\nassert(log.isInstanceOf[SnapshotManagement], \"DeltaLog is a SnapshotManagement\")\n</code></pre> <pre><code>val snapshot = log.update(stalenessAcceptable = false)\n</code></pre> <pre><code>scala&gt; :type snapshot\norg.apache.spark.sql.delta.Snapshot\n\nassert(snapshot.version == 0)\n</code></pre>","text":""},{"location":"SnapshotManagement/#logging","title":"Logging <p>As an extension of DeltaLog, use DeltaLog logging to see what happens inside.</p>","text":""},{"location":"StagedDeltaTableV2/","title":"StagedDeltaTableV2","text":"<p><code>StagedDeltaTableV2</code> is a <code>StagedTable</code> (Spark SQL) that <code>SupportsWrite</code> (Spark SQL).</p>"},{"location":"StagedDeltaTableV2/#creating-instance","title":"Creating Instance","text":"<p><code>StagedDeltaTableV2</code> takes the following to be created:</p> <ul> <li> Identifier <li> Schema <li> Partitions (<code>Array[Transform]</code>) <li> Properties <li>Operation</li> <p><code>StagedDeltaTableV2</code> is created when:</p> <ul> <li><code>DeltaCatalog</code> is requested to stageReplace, stageCreateOrReplace or stageCreate</li> </ul>"},{"location":"StagedDeltaTableV2/#creationmode","title":"CreationMode <p><code>StagedDeltaTableV2</code> is given a <code>CreationMode</code> when created:</p> <ul> <li><code>Create</code> for stageCreate</li> <li><code>CreateOrReplace</code> for stageCreateOrReplace</li> <li><code>Replace</code> for stageReplace</li> </ul>","text":""},{"location":"StagedDeltaTableV2/#commitstagedchanges","title":"commitStagedChanges <pre><code>commitStagedChanges(): Unit\n</code></pre> <p><code>commitStagedChanges</code> is part of the <code>StagedTable</code> (Spark SQL) abstraction.</p>  <p><code>commitStagedChanges</code>...FIXME</p>","text":""},{"location":"StagedDeltaTableV2/#creating-writebuilder","title":"Creating WriteBuilder <pre><code>newWriteBuilder(\n  info: LogicalWriteInfo): V1WriteBuilder\n</code></pre> <p><code>newWriteBuilder</code>...FIXME</p> <p><code>newWriteBuilder</code> is part of the <code>SupportsWrite</code> (Spark SQL) abstraction.</p>","text":""},{"location":"State/","title":"State","text":""},{"location":"State/#creating-instance","title":"Creating Instance","text":"<p><code>State</code> takes the following to be created:</p> <ul> <li> <code>sizeInBytes</code> <li> <code>numOfSetTransactions</code> <li> <code>numOfFiles</code> <li> <code>numOfRemoves</code> <li> <code>numOfMetadata</code> <li> <code>numOfProtocol</code> <li> SetTransactions <li> Metadata <li> Protocol <li> (optional) FileSizeHistogram <p><code>State</code> is created when:</p> <ul> <li><code>InitialSnapshot</code> is requested for an initial state</li> </ul>"},{"location":"StateCache/","title":"StateCache","text":"<p><code>StateCache</code> is an abstraction of state caches that can cache a Dataset and uncache them all.</p>"},{"location":"StateCache/#contract","title":"Contract","text":""},{"location":"StateCache/#sparksession","title":"SparkSession <pre><code>spark: SparkSession\n</code></pre> <p><code>SparkSession</code> the cached RDDs belong to</p>","text":""},{"location":"StateCache/#implementations","title":"Implementations","text":"<ul> <li>DeltaSourceSnapshot</li> <li>Snapshot</li> </ul>"},{"location":"StateCache/#cached-rdds","title":"Cached RDDs <pre><code>cached: ArrayBuffer[RDD[_]]\n</code></pre> <p><code>StateCache</code> tracks cached RDDs in <code>cached</code> internal registry.</p> <p><code>cached</code> is given a new <code>RDD</code> when <code>StateCache</code> is requested to cache a Dataset.</p> <p><code>cached</code> is used when <code>StateCache</code> is requested to get a cached Dataset and uncache.</p>","text":""},{"location":"StateCache/#caching-dataset","title":"Caching Dataset <pre><code>cacheDS[A](\n  ds: Dataset[A],\n  name: String): CachedDS[A]\n</code></pre> <p><code>cacheDS</code> creates a new CachedDS.</p> <p><code>cacheDS</code> is used when:</p> <ul> <li><code>Snapshot</code> is requested for the cachedState</li> <li><code>DeltaSourceSnapshot</code> is requested for the initialFiles</li> <li><code>DataSkippingReaderBase</code> is requested for the withStatsCache</li> </ul>","text":""},{"location":"StateCache/#uncaching-all-cached-datasets","title":"Uncaching All Cached Datasets <pre><code>uncache[A](\n  ds: Dataset[A],\n  name: String): CachedDS[A]\n</code></pre> <p><code>uncache</code> uses the isCached internal flag to avoid multiple executions.</p> <p><code>uncache</code> is used when:</p> <ul> <li><code>DeltaLog</code> utility is used to access deltaLogCache and a cached entry expires</li> <li><code>SnapshotManagement</code> is requested to update state of a Delta table</li> <li><code>DeltaSourceSnapshot</code> is requested to close</li> </ul>","text":""},{"location":"StatisticsCollection/","title":"StatisticsCollection","text":"<p><code>StatisticsCollection</code> is an abstraction of statistics collectors.</p>"},{"location":"StatisticsCollection/#contract","title":"Contract","text":""},{"location":"StatisticsCollection/#data-schema","title":"Data Schema <pre><code>dataSchema: StructType\n</code></pre> <p>Schema (StructType) of the data files</p> <p>Used when:</p> <ul> <li><code>StatisticsCollection</code> is requested for the statCollectionSchema and the statsSchema</li> </ul>","text":""},{"location":"StatisticsCollection/#maximum-number-of-indexed-columns","title":"Maximum Number of Indexed Columns <pre><code>numIndexedCols: Int\n</code></pre> <p>Maximum number of leaf columns to collect stats on</p> <p>Used when:</p> <ul> <li><code>StatisticsCollection</code> is requested for statCollectionSchema and to collectStats</li> </ul>","text":""},{"location":"StatisticsCollection/#sparksession","title":"SparkSession <pre><code>spark: SparkSession\n</code></pre> <p>Used when:</p> <ul> <li><code>StatisticsCollection</code> is requested for statsCollector and statsSchema</li> </ul>","text":""},{"location":"StatisticsCollection/#implementations","title":"Implementations","text":"<ul> <li>writeFiles</li> <li>DataSkippingReaderBase</li> <li>Snapshot</li> </ul>"},{"location":"StatisticsCollection/#statsschema","title":"statsSchema <pre><code>statsSchema: StructType\n</code></pre> <p><code>statsSchema</code>...FIXME</p> <p><code>statsSchema</code> is used when:</p> <ul> <li><code>DataSkippingReaderBase</code> is requested for getStatsColumnOpt, withStatsInternal0, getStatsColumnOpt</li> </ul>","text":""},{"location":"StatisticsCollection/#statscollector-column","title":"statsCollector Column <pre><code>statsCollector: Column\n</code></pre> <p><code>statsCollector</code> takes the value of the spark.databricks.io.skipping.stringPrefixLength configuration property.</p> <p><code>statsCollector</code> creates a <code>Column</code> with <code>stats</code> name to be a <code>struct</code> of the following:</p> <ol> <li><code>count(*)</code> as <code>numRecords</code></li> <li>collectStats as <code>minValues</code></li> <li>collectStats as <code>maxValues</code></li> <li>collectStats as <code>nullCount</code></li> </ol>  Lazy Value <p><code>statsCollector</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>statsCollector</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested for the stats collector column (of the table at the snapshot within this transaction)</li> <li><code>TransactionalWrite</code> is requested to writeFiles</li> <li><code>StatisticsCollection</code> is requested for statsSchema</li> </ul>","text":""},{"location":"StatisticsCollection/#collectstats","title":"collectStats <pre><code>collectStats(\n  name: String,\n  schema: StructType)(\n  function: PartialFunction[(Column, StructField), Column]): Column\n</code></pre> <p><code>collectStats</code>...FIXME</p>","text":""},{"location":"StatisticsCollection/#statcollectionschema","title":"statCollectionSchema <pre><code>statCollectionSchema: StructType\n</code></pre> <p>For the number of leaf columns to collect stats on greater than or equal <code>0</code>, <code>statCollectionSchema</code> truncate the dataSchema. Otherwise, <code>statCollectionSchema</code> returns the dataSchema intact.</p>  Lazy Value <p><code>statCollectionSchema</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>","text":""},{"location":"StatisticsCollection/#truncateschema","title":"truncateSchema <pre><code>truncateSchema(\n  schema: StructType,\n  indexedCols: Int): (StructType, Int)\n</code></pre> <p><code>truncateSchema</code>...FIXME</p>","text":""},{"location":"StatisticsCollection/#recomputing-statistics","title":"Recomputing Statistics <pre><code>recompute(\n  spark: SparkSession,\n  deltaLog: DeltaLog,\n  predicates: Seq[Expression] = Seq(Literal(true)),\n  fileFilter: AddFile =&gt; Boolean = af =&gt; true): Unit\n</code></pre> <p><code>recompute</code>...FIXME</p>  <p><code>recompute</code> seems unused.</p>","text":""},{"location":"TahoeBatchFileIndex/","title":"TahoeBatchFileIndex","text":"<p><code>TahoeBatchFileIndex</code> is a file index of a delta table at a given version.</p>"},{"location":"TahoeBatchFileIndex/#creating-instance","title":"Creating Instance","text":"<p><code>TahoeBatchFileIndex</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li>Action Type</li> <li> AddFiles <li> DeltaLog <li> Data directory (as Hadoop Path) <li> Snapshot <p><code>TahoeBatchFileIndex</code> is created when:</p> <ul> <li><code>DeltaLog</code> is requested for a DataFrame for given AddFiles</li> <li>DeleteCommand and UpdateCommand are executed (and <code>DeltaCommand</code> is requested for a HadoopFsRelation)</li> </ul>"},{"location":"TahoeBatchFileIndex/#action-type","title":"Action Type <p><code>TahoeBatchFileIndex</code> is given an Action Type identifier when created:</p> <ul> <li>batch or streaming when <code>DeltaLog</code> is requested for a batch or streaming DataFrame for given AddFiles, respectively</li> <li>delete for DeleteCommand</li> <li>update for UpdateCommand</li> </ul>  <p>Important</p> <p>Action Type seems not to be used ever.</p>","text":""},{"location":"TahoeBatchFileIndex/#tableversion","title":"tableVersion <pre><code>tableVersion: Long\n</code></pre> <p><code>tableVersion</code> is part of the TahoeFileIndex abstraction.</p> <p><code>tableVersion</code> is always the version of the Snapshot.</p>","text":""},{"location":"TahoeBatchFileIndex/#matchingfiles","title":"matchingFiles <pre><code>matchingFiles(\n  partitionFilters: Seq[Expression],\n  dataFilters: Seq[Expression],\n  keepStats: Boolean = false): Seq[AddFile]\n</code></pre> <p><code>matchingFiles</code> is part of the TahoeFileIndex abstraction.</p> <p><code>matchingFiles</code> filterFileList (that gives a <code>DataFrame</code>) and collects the AddFiles (using <code>Dataset.collect</code>).</p>","text":""},{"location":"TahoeBatchFileIndex/#input-files","title":"Input Files <pre><code>inputFiles: Array[String]\n</code></pre> <p><code>inputFiles</code> is part of the <code>FileIndex</code> (Spark SQL) abstraction.</p> <p><code>inputFiles</code> returns the paths of all the given AddFiles.</p>","text":""},{"location":"TahoeBatchFileIndex/#partitions","title":"Partitions <pre><code>partitionSchema: StructType\n</code></pre> <p><code>partitionSchema</code> is part of the <code>FileIndex</code> (Spark SQL) abstraction.</p> <p><code>partitionSchema</code> requests the Snapshot for the metadata that is in turn requested for the partitionSchema.</p>","text":""},{"location":"TahoeBatchFileIndex/#estimated-size-of-relation","title":"Estimated Size of Relation <pre><code>sizeInBytes: Long\n</code></pre> <p><code>sizeInBytes</code> is part of the <code>FileIndex</code> (Spark SQL) abstraction.</p> <p><code>sizeInBytes</code> is a sum of the sizes of all the given AddFiles.</p>","text":""},{"location":"TahoeFileIndex/","title":"TahoeFileIndex","text":"<p><code>TahoeFileIndex</code>\u00a0is an extension of the <code>FileIndex</code> (Spark SQL) abstraction for file indices of delta tables that can list data files to scan (based on partition and data filters).</p> <p>The aim of <code>TahoeFileIndex</code> (and <code>FileIndex</code> in general) is to reduce usage of very expensive disk access for file-related information using Hadoop FileSystem API.</p>"},{"location":"TahoeFileIndex/#contract","title":"Contract","text":""},{"location":"TahoeFileIndex/#matchingfiles","title":"matchingFiles <pre><code>matchingFiles(\n  partitionFilters: Seq[Expression],\n  dataFilters: Seq[Expression]): Seq[AddFile]\n</code></pre> <p>AddFiles matching given partition and data filters (predicates)</p> <p>Used for listing data files</p>","text":""},{"location":"TahoeFileIndex/#implementations","title":"Implementations","text":"<ul> <li>PinnedTahoeFileIndex</li> <li>PreparedDeltaFileIndex</li> <li>TahoeBatchFileIndex</li> <li>TahoeLogFileIndex</li> </ul>"},{"location":"TahoeFileIndex/#creating-instance","title":"Creating Instance","text":"<p><code>TahoeFileIndex</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> <li> DeltaLog <li> Hadoop Path Abstract Class <p><code>TahoeFileIndex</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete TahoeFileIndexes.</p>"},{"location":"TahoeFileIndex/#root-paths","title":"Root Paths <pre><code>rootPaths: Seq[Path]\n</code></pre> <p><code>rootPaths</code> is the path only.</p> <p><code>rootPaths</code>\u00a0is part of the <code>FileIndex</code> (Spark SQL) abstraction.</p>","text":""},{"location":"TahoeFileIndex/#listing-files","title":"Listing Files <pre><code>listFiles(\n  partitionFilters: Seq[Expression],\n  dataFilters: Seq[Expression]): Seq[PartitionDirectory]\n</code></pre> <p><code>listFiles</code> is the path only.</p> <p><code>listFiles</code>\u00a0is part of the <code>FileIndex</code> (Spark SQL)abstraction.</p>","text":""},{"location":"TahoeFileIndex/#partitions","title":"Partitions <pre><code>partitionSchema: StructType\n</code></pre> <p><code>partitionSchema</code> is the partition schema of (the Metadata of the Snapshot) of the DeltaLog.</p> <p><code>partitionSchema</code>\u00a0is part of the <code>FileIndex</code> (Spark SQL) abstraction.</p>","text":""},{"location":"TahoeFileIndex/#version-of-delta-table","title":"Version of Delta Table <pre><code>tableVersion: Long\n</code></pre> <p><code>tableVersion</code> is the version of (the snapshot of) the DeltaLog.</p> <p><code>tableVersion</code>\u00a0is used when <code>TahoeFileIndex</code> is requested for the human-friendly textual representation.</p>","text":""},{"location":"TahoeFileIndex/#textual-representation","title":"Textual Representation <pre><code>toString: String\n</code></pre> <p><code>toString</code> returns the following text (using the version and the path of the Delta table):</p> <pre><code>Delta[version=[tableVersion], [truncatedPath]]\n</code></pre>","text":""},{"location":"TahoeLogFileIndex/","title":"TahoeLogFileIndex","text":"<p><code>TahoeLogFileIndex</code> is a file index.</p>"},{"location":"TahoeLogFileIndex/#creating-instance","title":"Creating Instance","text":"<p><code>TahoeLogFileIndex</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> DeltaLog <li> Data directory of the Delta table (as a Hadoop Path) <li> Snapshot at analysis <li> Partition Filters (as Catalyst expressions; default: empty) <li>isTimeTravelQuery flag (default: <code>false</code>)</li> <p><code>TahoeLogFileIndex</code> is created when:</p> <ul> <li><code>DeltaLog</code> is requested for an Insertable HadoopFsRelation</li> </ul>"},{"location":"TahoeLogFileIndex/#sparkdatabricksdeltachecklatestschemaonread","title":"spark.databricks.delta.checkLatestSchemaOnRead <p><code>TahoeLogFileIndex</code> uses the spark.databricks.delta.checkLatestSchemaOnRead configuration property when requested for a Snapshot.</p>","text":""},{"location":"TahoeLogFileIndex/#istimetravelquery-flag","title":"isTimeTravelQuery flag <p><code>TahoeLogFileIndex</code> is given a <code>isTimeTravelQuery</code> flag when created.</p> <p><code>isTimeTravelQuery</code> flag is <code>false</code> by default and can be different when <code>DeltaLog</code> is requested to create a BaseRelation (when <code>DeltaTableV2</code> is requested for a BaseRelation based on DeltaTimeTravelSpec).</p>","text":""},{"location":"TahoeLogFileIndex/#demo","title":"Demo <pre><code>val q = spark.read.format(\"delta\").load(\"/tmp/delta/users\")\nval plan = q.queryExecution.executedPlan\n\nimport org.apache.spark.sql.execution.FileSourceScanExec\nval scan = plan.collect { case e: FileSourceScanExec =&gt; e }.head\n\nimport org.apache.spark.sql.delta.files.TahoeLogFileIndex\nval index = scan.relation.location.asInstanceOf[TahoeLogFileIndex]\nscala&gt; println(index)\nDelta[version=1, file:/tmp/delta/users]\n</code></pre>","text":""},{"location":"TahoeLogFileIndex/#matchingfiles-method","title":"matchingFiles Method <pre><code>matchingFiles(\n  partitionFilters: Seq[Expression],\n  dataFilters: Seq[Expression],\n  keepStats: Boolean = false): Seq[AddFile]\n</code></pre> <p><code>matchingFiles</code> gets the snapshot (with <code>stalenessAcceptable</code> flag off) and requests it for the files to scan (for the index's partition filters, the given <code>partitionFilters</code> and <code>dataFilters</code>).</p>  <p>Note</p> <p>inputFiles and matchingFiles are similar. Both get the snapshot (of the delta table), but they use different filtering expressions and return value types.</p>  <p><code>matchingFiles</code> is part of the TahoeFileIndex abstraction.</p>","text":""},{"location":"TahoeLogFileIndex/#inputfiles-method","title":"inputFiles Method <pre><code>inputFiles: Array[String]\n</code></pre> <p><code>inputFiles</code> gets the snapshot (with <code>stalenessAcceptable</code> flag off) and requests it for the files to scan (for the index's partition filters only).</p>  <p>Note</p> <p>inputFiles and matchingFiles are similar. Both get the snapshot, but they use different filtering expressions and return value types.</p>  <p><code>inputFiles</code> is part of the <code>FileIndex</code> contract (Spark SQL).</p>","text":""},{"location":"TahoeLogFileIndex/#snapshot","title":"Snapshot <pre><code>getSnapshot: Snapshot\n</code></pre> <p><code>getSnapshot</code> returns the Snapshot to scan.</p>  <p>With checkSchemaOnRead enabled or the DeltaColumnMappingMode (of the Metadata of the Snapshot) set (different from <code>NoMapping</code>), <code>getSnapshot</code> makes sure that the schemas are read-compatible (and hasn't changed in an incompatible manner since analysis time)</p>  <p><code>getSnapshot</code> is used when:</p> <ul> <li><code>TahoeLogFileIndex</code> is requested for the matching files and the input files</li> </ul>","text":""},{"location":"TahoeLogFileIndex/#getsnapshottoscan","title":"getSnapshotToScan <pre><code>getSnapshotToScan: Snapshot\n</code></pre> <p><code>getSnapshot</code> returns the Snapshot with isTimeTravelQuery enabled or requests the DeltaLog to update and give one.</p>","text":""},{"location":"TahoeLogFileIndex/#internal-properties","title":"Internal Properties","text":""},{"location":"TahoeLogFileIndex/#historicalsnapshotopt","title":"historicalSnapshotOpt <p>Historical snapshot that is the Snapshot for the versionToUse if defined.</p> <p>Used when <code>TahoeLogFileIndex</code> is requested for the (historical or latest) snapshot and the schema of the partition columns</p>","text":""},{"location":"TransactionalWrite/","title":"TransactionalWrite","text":"<p><code>TransactionalWrite</code> is an abstraction of optimistic transactional writers that can write a structured query out to a Delta table.</p>"},{"location":"TransactionalWrite/#contract","title":"Contract","text":""},{"location":"TransactionalWrite/#deltalog","title":"DeltaLog <pre><code>deltaLog: DeltaLog\n</code></pre> <p>DeltaLog (of a delta table) that this transaction is changing</p> <p>Used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to prepare a commit, doCommit, checkAndRetry, and perform post-commit operations (and execute delta log checkpoint)</li> <li>ConvertToDeltaCommand is executed</li> <li><code>DeltaCommand</code> is requested to buildBaseRelation and commitLarge</li> <li>MergeIntoCommand is executed</li> <li><code>TransactionalWrite</code> is requested to write a structured query out to a delta table</li> <li>GenerateSymlinkManifest post-commit hook is executed</li> <li><code>ImplicitMetadataOperation</code> is requested to updateMetadata</li> <li><code>DeltaSink</code> is requested to addBatch</li> </ul>","text":""},{"location":"TransactionalWrite/#metadata","title":"Metadata <pre><code>metadata: Metadata\n</code></pre> <p>Metadata (of the delta table) that this transaction is changing</p>","text":""},{"location":"TransactionalWrite/#protocol","title":"Protocol <pre><code>protocol: Protocol\n</code></pre> <p>Protocol (of the delta table) that this transaction is changing</p> <p>Used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to updateMetadata, verifyNewMetadata and prepareCommit</li> <li>ConvertToDeltaCommand is executed</li> </ul>","text":""},{"location":"TransactionalWrite/#snapshot","title":"Snapshot <pre><code>snapshot: Snapshot\n</code></pre> <p>Snapshot (of the delta table) that this transaction is reading at</p>","text":""},{"location":"TransactionalWrite/#implementations","title":"Implementations","text":"<ul> <li>OptimisticTransaction</li> </ul>"},{"location":"TransactionalWrite/#sparkdatabricksdeltahistorymetricsenabled","title":"spark.databricks.delta.history.metricsEnabled <p>With spark.databricks.delta.history.metricsEnabled configuration property enabled, <code>TransactionalWrite</code> creates a <code>BasicWriteJobStatsTracker</code> (Spark SQL) and registers SQL metrics (when requested to write data out).</p>","text":""},{"location":"TransactionalWrite/#haswritten-flag","title":"hasWritten Flag <pre><code>hasWritten: Boolean = false\n</code></pre> <p><code>TransactionalWrite</code> uses <code>hasWritten</code> internal registry to prevent <code>OptimisticTransactionImpl</code> from updating metadata after having written out files.</p> <p><code>hasWritten</code> is initially <code>false</code> and changes to <code>true</code> after having data written out.</p>","text":""},{"location":"TransactionalWrite/#writing-data-out","title":"Writing Data Out <pre><code>writeFiles(\n  data: Dataset[_]): Seq[FileAction]  // (1)!\nwriteFiles(\n  data: Dataset[_],\n  writeOptions: Option[DeltaOptions]): Seq[FileAction]\nwriteFiles(\n  inputData: Dataset[_],\n  writeOptions: Option[DeltaOptions],\n  additionalConstraints: Seq[Constraint]): Seq[FileAction]\nwriteFiles(\n  data: Dataset[_],\n  additionalConstraints: Seq[Constraint]): Seq[FileAction]  // (2)!\n</code></pre> <ol> <li>Uses no <code>additionalConstraints</code></li> <li>Uses no <code>writeOptions</code></li> </ol> <p><code>writeFiles</code> writes the given <code>data</code> to a delta table and returns AddFiles with AddCDCFiles (from the DelayedCommitProtocol).</p>  <p><code>writeFiles</code> is used when:</p> <ul> <li><code>WriteIntoDelta</code> is requested to write</li> <li><code>DeleteCommand</code> is requested to rewriteFiles</li> <li><code>MergeIntoCommand</code> is requested to writeInsertsOnlyWhenNoMatchedClauses and writeAllChanges</li> <li><code>OptimizeExecutor</code> is requested to runOptimizeBinJob</li> <li><code>UpdateCommand</code> is requested to rewriteFiles</li> <li><code>DeltaSink</code> is requested to add a streaming micro-batch</li> </ul>  <p><code>writeFiles</code> creates a DeltaInvariantCheckerExec and a DelayedCommitProtocol to write out files to the data path (of the DeltaLog).</p>  FileFormatWriter <p><code>writeFiles</code> uses Spark SQL's <code>FileFormatWriter</code> utility to write out a result of a streaming query.</p> <p>Learn about FileFormatWriter in The Internals of Spark SQL online book.</p>  <p><code>writeFiles</code> is executed within <code>SQLExecution.withNewExecutionId</code>.</p>  SQLAppStatusListener <p><code>writeFiles</code> can be tracked using web UI or <code>SQLAppStatusListener</code> (using <code>SparkListenerSQLExecutionStart</code> and <code>SparkListenerSQLExecutionEnd</code> events).</p> <p>Learn about SQLAppStatusListener in The Internals of Spark SQL online book.</p>  <p>In the end, <code>writeFiles</code> returns the addedStatuses of the DelayedCommitProtocol committer.</p>  <p>Internally, <code>writeFiles</code> turns the hasWritten flag on (<code>true</code>).</p>  <p>Note</p> <p>After <code>writeFiles</code>, no metadata updates in the transaction are permitted.</p>  <p><code>writeFiles</code> performCDCPartition (into a possibly-augmented CDF-aware <code>DataFrame</code> and a corresponding schema with an additional CDF-aware __is_cdc column).</p> <p><code>writeFiles</code> normalize the (possibly-augmented CDF-aware) <code>DataFrame</code>.</p> <p><code>writeFiles</code> gets the partitioning columns based on the (possibly-augmented CDF-aware) partition schema.</p>","text":""},{"location":"TransactionalWrite/#delayedcommitprotocol-committer","title":"DelayedCommitProtocol Committer <p><code>writeFiles</code> creates a DelayedCommitProtocol committer for the data path (of the DeltaLog).</p>","text":""},{"location":"TransactionalWrite/#deltajobstatisticstracker","title":"DeltaJobStatisticsTracker <p><code>writeFiles</code> creates a DeltaJobStatisticsTracker if spark.databricks.delta.stats.collect configuration property is enabled.</p>","text":""},{"location":"TransactionalWrite/#constraints","title":"Constraints <p><code>writeFiles</code> collects constraints:</p> <ol> <li>From the table metadata (CHECK constraints and Column Invariants)</li> <li>Generated columns (after normalizeData)</li> <li>The given <code>additionalConstraints</code></li> </ol>","text":""},{"location":"TransactionalWrite/#deltatransactionalwrite-execution-id","title":"deltaTransactionalWrite Execution ID <p><code>writeFiles</code> requests a new Execution ID (that is used to track all Spark jobs of <code>FileFormatWriter.write</code> in Spark SQL) with the physical query plan after normalizeData and <code>deltaTransactionalWrite</code> name.</p>","text":""},{"location":"TransactionalWrite/#deltainvariantcheckerexec","title":"DeltaInvariantCheckerExec <p><code>writeFiles</code> creates a DeltaInvariantCheckerExec unary physical operator (with the executed plan of the normalized query execution as the child operator).</p>","text":""},{"location":"TransactionalWrite/#basicwritejobstatstracker","title":"BasicWriteJobStatsTracker <p><code>writeFiles</code> creates a <code>BasicWriteJobStatsTracker</code> (Spark SQL) if spark.databricks.delta.history.metricsEnabled configuration property is enabled.</p>","text":""},{"location":"TransactionalWrite/#write-options","title":"Write Options <p><code>writeFiles</code> filters out all the write options (from the given <code>writeOptions</code>) except the following:</p> <ol> <li>maxRecordsPerFile</li> <li>compression</li> </ol>","text":""},{"location":"TransactionalWrite/#fileformatwriter","title":"FileFormatWriter <p>As the last step under the new execution ID <code>writeFiles</code> writes out the data (using FileFormatWriter).</p>  <p>Tip</p> <p>Enable <code>ALL</code> logging level for org.apache.spark.sql.execution.datasources.FileFormatWriter logger to see what happens inside.</p>","text":""},{"location":"TransactionalWrite/#addfiles-and-addcdcfiles","title":"AddFiles and AddCDCFiles <p>In the end, <code>writeFiles</code> returns AddFiles and AddCDCFiles (from the DelayedCommitProtocol).</p>","text":""},{"location":"TransactionalWrite/#creating-filecommitprotocol-committer","title":"Creating FileCommitProtocol Committer <pre><code>getCommitter(\n  outputPath: Path): DelayedCommitProtocol\n</code></pre> <p><code>getCommitter</code> creates a new DelayedCommitProtocol with the <code>delta</code> job ID and the given <code>outputPath</code> (and no random prefix length).</p>  <p>Note</p> <p>The DelayedCommitProtocol is used for <code>FileFormatWriter</code> (Spark SQL) to write data out and, in the end, for the addedStatuses and changeFiles.</p>  <p><code>getCommitter</code> is used when:</p> <ul> <li><code>TransactionalWrite</code> is requested to write data out</li> </ul>","text":""},{"location":"TransactionalWrite/#getpartitioningcolumns","title":"getPartitioningColumns <pre><code>getPartitioningColumns(\n  partitionSchema: StructType,\n  output: Seq[Attribute],\n  colsDropped: Boolean): Seq[Attribute]\n</code></pre> <p><code>getPartitioningColumns</code>...FIXME</p>","text":""},{"location":"TransactionalWrite/#normalizedata","title":"normalizeData <pre><code>normalizeData(\n  deltaLog: DeltaLog,\n  data: Dataset[_]): (QueryExecution, Seq[Attribute], Seq[Constraint], Set[String])\n</code></pre> <p><code>normalizeData</code> normalizes the column names (using the table schema of the Metadata and the given <code>data</code>).</p> <p><code>normalizeData</code> tableHasDefaultExpr (using the Protocol and the Metadata).</p> <p><code>normalizeData</code>...FIXME</p>  <p><code>normalizeData</code> is used when:</p> <ul> <li><code>TransactionalWrite</code> is requested to write data out</li> </ul>","text":""},{"location":"TransactionalWrite/#makeoutputnullable","title":"makeOutputNullable <pre><code>makeOutputNullable(\n  output: Seq[Attribute]): Seq[Attribute]\n</code></pre> <p><code>makeOutputNullable</code>...FIXME</p>","text":""},{"location":"TransactionalWrite/#performcdcpartition","title":"performCDCPartition <pre><code>performCDCPartition(\n  inputData: Dataset[_]): (DataFrame, StructType)\n</code></pre> <p><code>performCDCPartition</code> returns the input <code>inputData</code> with or without __is_cdc extra column based on whether Change Data Feed is enabled for the table and _change_type column is available in the schema of the given <code>inputData</code> or not.</p> <p>The value of the __is_cdc extra column is as follows:</p> <ul> <li><code>true</code> for non-null <code>_change_type</code>s</li> <li><code>false</code> otherwise</li> </ul> <p>The schema (the <code>StructType</code> of the tuple to be returned) includes the __is_cdc extra column as the first column (followed by the physicalPartitionSchema).</p>","text":""},{"location":"VerifyChecksum/","title":"VerifyChecksum","text":"<p>= VerifyChecksum</p> <p><code>VerifyChecksum</code> is...FIXME</p> <p>== [[validateChecksum]] <code>validateChecksum</code> Method</p>"},{"location":"VerifyChecksum/#source-scala","title":"[source, scala]","text":""},{"location":"VerifyChecksum/#validatechecksumsnapshot-snapshot-unit","title":"validateChecksum(snapshot: Snapshot): Unit","text":"<p><code>validateChecksum</code>...FIXME</p> <p>NOTE: <code>validateChecksum</code> is used when...FIXME</p>"},{"location":"WriteIntoDeltaBuilder/","title":"WriteIntoDeltaBuilder","text":"<p><code>WriteIntoDeltaBuilder</code> is a <code>WriteBuilder</code> (Spark SQL) with support for the following capabilities:</p> <ul> <li><code>SupportsOverwrite</code> (Spark SQL)</li> <li><code>SupportsTruncate</code> (Spark SQL)</li> <li><code>V1WriteBuilder</code> (Spark SQL)</li> </ul>"},{"location":"WriteIntoDeltaBuilder/#creating-instance","title":"Creating Instance","text":"<p><code>WriteIntoDeltaBuilder</code> takes the following to be created:</p> <ul> <li> DeltaLog <li> Write-Specific Options <p><code>WriteIntoDeltaBuilder</code> is created\u00a0when:</p> <ul> <li><code>DeltaTableV2</code> is requested for a WriteBuilder</li> </ul>"},{"location":"WriteIntoDeltaBuilder/#buildforv1write","title":"buildForV1Write <pre><code>buildForV1Write(): InsertableRelation\n</code></pre> <p><code>buildForV1Write</code>\u00a0is part of the <code>V1WriteBuilder</code> (Spark SQL) abstraction.</p> <p><code>buildForV1Write</code> creates an <code>InsertableRelation</code> (Spark SQL) that does the following when requested to <code>insert</code>:</p> <ol> <li>Creates and executes a WriteIntoDelta command</li> <li>Re-cache all cached plans (by requesting the <code>CacheManager</code> to <code>recacheByPlan</code> for a <code>LogicalRelation</code> over the BaseRelation of the DeltaLog)</li> </ol>","text":""},{"location":"configuration-properties/","title":"Configuration Properties","text":""},{"location":"configuration-properties/#sparkdeltalogstoreclass","title":"spark.delta.logStore.class <p>The fully-qualified class name of a LogStore</p> <p>Default: HDFSLogStore</p> <p>Used when:</p> <ul> <li><code>LogStoreProvider</code> is requested for a LogStore</li> </ul>","text":""},{"location":"developer-api/","title":"Developer API","text":""},{"location":"developer-api/#developerapi","title":"DeveloperApi","text":"<ul> <li>DeltaColumnBuilder</li> <li>DeltaTable</li> <li>DeltaTableBuilder</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Installation of Delta Lake boils down to using spark-submit's <code>--packages</code> command-line option with the following configuration properties for DeltaSparkSessionExtension and DeltaCatalog:</p> <ul> <li><code>spark.sql.extensions</code> (Spark SQL)</li> <li><code>spark.sql.catalog.spark_catalog</code> (Spark SQL)</li> </ul> <p>Make sure that the version of Scala in Apache Spark should match Delta Lake's.</p>"},{"location":"installation/#spark-sql-application","title":"Spark SQL Application <pre><code>import org.apache.spark.sql.SparkSession\nval spark = SparkSession\n  .builder\n  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n  .getOrCreate\n</code></pre>","text":""},{"location":"installation/#spark-shell","title":"Spark Shell <pre><code>$ ./bin/spark-shell --version\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.0\n      /_/\n\nUsing Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.16\n</code></pre> <pre><code>./bin/spark-shell \\\n  --packages io.delta:delta-core_2.12:2.2.0 \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n</code></pre>","text":""},{"location":"installation/#version","title":"Version <p><code>io.delta.VERSION</code> can be used to show the version of Delta Lake installed.</p> <pre><code>assert(io.delta.VERSION == \"2.2.0\")\n</code></pre> <p>It is also possible to use DESCRIBE HISTORY and check out the engineInfo column.</p>","text":""},{"location":"options/","title":"Options","text":"<p>Delta Lake comes with options to fine-tune its uses. They can be defined using <code>option</code> method of the following:</p> <ul> <li><code>DataFrameReader</code> (Spark SQL) and <code>DataFrameWriter</code> (Spark SQL) for batch queries</li> <li><code>DataStreamReader</code> (Spark Structured Streaming) and <code>DataStreamWriter</code> (Spark Structured Streaming) for streaming queries</li> <li>SQL queries</li> </ul>"},{"location":"options/#accessing-options","title":"Accessing Options <p>The options are available at runtime as DeltaOptions.</p> <pre><code>import org.apache.spark.sql.delta.DeltaOptions\n</code></pre> <pre><code>assert(DeltaOptions.OVERWRITE_SCHEMA_OPTION == \"overwriteSchema\")\n</code></pre> <pre><code>val options = new DeltaOptions(Map.empty[String, String], spark.sessionState.conf)\nassert(options.failOnDataLoss, \"failOnDataLoss should be enabled by default\")\n</code></pre> <pre><code>val options = new DeltaOptions(\n  Map(DeltaOptions.OVERWRITE_SCHEMA_OPTION -&gt; true.toString),\n  spark.sessionState.conf)\nassert(\n  options.canOverwriteSchema,\n  s\"${DeltaOptions.OVERWRITE_SCHEMA_OPTION} should be enabled\")\n</code></pre>","text":""},{"location":"options/#checkpointlocation","title":"checkpointLocation <p>Checkpoint directory for storing checkpoint data of streaming queries (Spark Structured Streaming).</p>","text":""},{"location":"options/#datachange","title":"dataChange <p>Whether to write new data to the table or just rearrange data that is already part of the table. This option declares that the data being written by this job does not change any data in the table and merely rearranges existing data. This makes sure streaming queries reading from this table will not see any new changes</p> <p>Used when:</p> <ul> <li><code>DeltaWriteOptionsImpl</code> is requested for rearrangeOnly</li> </ul>  Demo <p>Learn more in Demo: dataChange.</p>","text":""},{"location":"options/#excluderegex","title":"excludeRegex <p>scala.util.matching.Regex to filter out the paths of FileActions</p> <p>Default: (undefined)</p> <p>Use DeltaOptions.excludeRegex to access the value</p> <p>Used when:</p> <ul> <li><code>DeltaSourceBase</code> is requested for the data (for a given DeltaSourceOffset)</li> <li><code>DeltaSourceCDCSupport</code> is requested for the data</li> </ul>","text":""},{"location":"options/#failondataloss","title":"failOnDataLoss <p>Controls whether or not to fail loading a delta table when the earliest available version (in the <code>_delta_log</code> directory) is after the version requested</p> <p>Default: <code>true</code></p> <p>Use DeltaOptions.failOnDataLoss to access the value</p>","text":""},{"location":"options/#ignorechanges","title":"ignoreChanges","text":""},{"location":"options/#ignoredeletes","title":"ignoreDeletes","text":""},{"location":"options/#ignorefiledeletion","title":"ignoreFileDeletion","text":""},{"location":"options/#maxbytespertrigger","title":"maxBytesPerTrigger","text":""},{"location":"options/#maxfilespertrigger","title":"maxFilesPerTrigger <p>Maximum number of files (AddFiles) that DeltaSource is supposed to scan (read) in a streaming micro-batch (trigger)</p> <p>Default: <code>1000</code></p> <p>Must be at least <code>1</code></p>","text":""},{"location":"options/#mergeschema","title":"mergeSchema <p>Enables schema migration (and allows automatic schema merging during a write operation for WriteIntoDelta and DeltaSink)</p> <p>Equivalent SQL Session configuration: spark.databricks.delta.schema.autoMerge.enabled</p>","text":""},{"location":"options/#optimizewrite","title":"optimizeWrite <p>Enables...FIXME</p>","text":""},{"location":"options/#overwriteschema","title":"overwriteSchema <p>Enables overwriting schema or change partitioning of a delta table during an overwrite write operation</p> <p>Use DeltaOptions.canOverwriteSchema to access the value</p>  <p>Note</p> <p>The schema cannot be overwritten when using replaceWhere option.</p>","text":""},{"location":"options/#path","title":"path <p>(required) Directory on a Hadoop DFS-compliant file system with an optional time travel identifier</p> <p>Default: (undefined)</p>  <p>Note</p> <p>Can also be specified using <code>load</code> method of <code>DataFrameReader</code> and <code>DataStreamReader</code>.</p>","text":""},{"location":"options/#queryname","title":"queryName","text":""},{"location":"options/#readchangefeed","title":"readChangeFeed <p>Enables Change Data Feed while reading a delta table (CDC read)</p> <p>Use DeltaOptions.readChangeFeed to access the value</p> <p>Requires either startingVersion or startingTimestamp option</p>","text":""},{"location":"options/#replacewhere","title":"replaceWhere <p>Partition predicates (unless replaceWhere.dataColumns.enabled is enabled to allow for arbitrary non-partition data predicates)</p> <p>Available as DeltaWriteOptions.replaceWhere</p>  <p>Demo</p> <p>Learn more in Demo: replaceWhere.</p>","text":""},{"location":"options/#timestampasof","title":"timestampAsOf <p>Timestamp of the version of a delta table for Time Travel</p> <p>Mutually exclusive with versionAsOf option and the time travel identifier of the path option.</p> <p>Used when:</p> <ul> <li><code>DeltaDataSource</code> utility is used to get a DeltaTimeTravelSpec</li> </ul>","text":""},{"location":"options/#usermetadata","title":"userMetadata <p>Defines a user-defined commit metadata</p> <p>Take precedence over spark.databricks.delta.commitInfo.userMetadata</p> <p>Available by inspecting CommitInfos using DESCRIBE HISTORY or DeltaTable.history.</p>  <p>Demo</p> <p>Learn more in Demo: User Metadata for Labelling Commits.</p>","text":""},{"location":"options/#versionasof","title":"versionAsOf <p>Version of a delta table for Time Travel</p> <p>Must be castable to a <code>long</code> number</p> <p>Mutually exclusive with timestampAsOf option and the time travel identifier of the path option.</p> <p>Used when:</p> <ul> <li><code>DeltaDataSource</code> utility is used to get a DeltaTimeTravelSpec</li> </ul>","text":""},{"location":"overview/","title":"Delta Lake","text":"<p>Delta Lake is an open-source table format (storage layer) for cloud data lakes with ACID transactions, time travel and many more (you'd rather not wanna miss in your data-heavy architectures).</p> <p>Delta Lake allows you to store data on blob stores like HDFS, S3, Azure Data Lake, GCS, query from many processing engines including Apache Spark, Trino, Apache Hive, Apache Flink, and provides APIs for Scala, Java, Python, Rust, and Ruby.</p> <p>As it was well said: \"Delta is a storage format while Spark is an execution engine...to separate storage from compute.\" Yet, Delta Lake can run with other execution engines like Trino or Apache Flink.</p> <p>Delta Lake 2.2.0 supports Apache Spark 3.3.1 (cf. build.sbt).</p>"},{"location":"overview/#delta-tables","title":"Delta Tables","text":"<p>Delta tables are parquet tables with a transactional log.</p> <p>Changes to (the state of) a delta table are reflected as actions and persisted to the transactional log (in JSON format).</p>"},{"location":"overview/#optimistictransaction","title":"OptimisticTransaction","text":"<p>Delta Lake uses OptimisticTransaction for transactional writes. A commit is successful when the transaction can write the actions to a delta file (in the transactional log). In case the delta file for the commit version already exists, the transaction is retried.</p> <p>More importantly, multiple queries can write to the same delta table simultaneously (at exactly the same time).</p>"},{"location":"overview/#transactional-writers","title":"Transactional Writers","text":"<p>TransactionalWrite is an interface for writing out data to a delta table.</p> <p>The following commands and operations can transactionally write new data files out to a data directory of a delta table:</p> <ul> <li>DeleteCommand</li> <li>MergeIntoCommand</li> <li>OptimizeTableCommand</li> <li>UpdateCommand</li> <li>WriteIntoDelta</li> <li>DeltaSink</li> </ul>"},{"location":"overview/#developer-apis","title":"Developer APIs","text":"<p>Delta Lake provides the following Developer APIs for developers to interact with (and even extend) Delta Lake using a supported programming language:</p> <ul> <li>DeltaTable</li> <li>DeltaTableBuilder</li> <li>DeltaColumnBuilder</li> </ul>"},{"location":"overview/#structured-queries","title":"Structured Queries","text":"<p>Delta Lake supports batch and streaming queries (Spark SQL and Structured Streaming, respectively) using delta format.</p> <p>In order to fine tune queries over data in Delta Lake use options.</p> <p>Structured queries can write (transactionally) to a delta table using the following interfaces:</p> <ul> <li>WriteIntoDelta command for batch queries (Spark SQL)</li> <li>DeltaSink for streaming queries (Spark Structured Streaming)</li> </ul>"},{"location":"overview/#batch-queries","title":"Batch Queries","text":"<p>Delta Lake supports reading and writing in batch queries:</p> <ul> <li> <p>Batch reads (as a <code>RelationProvider</code>)</p> </li> <li> <p>Batch writes (as a <code>CreatableRelationProvider</code>)</p> </li> </ul>"},{"location":"overview/#streaming-queries","title":"Streaming Queries","text":"<p>Delta Lake supports reading and writing in streaming queries:</p> <ul> <li> <p>Stream reads (as a <code>Source</code>)</p> </li> <li> <p>Stream writes (as a <code>Sink</code>)</p> </li> </ul>"},{"location":"overview/#logstore","title":"LogStore","text":"<p>Delta Lake uses LogStore abstraction for reading and writing physical log files and checkpoints (using Hadoop FileSystem API).</p>"},{"location":"overview/#delta-tables-in-logical-query-plans","title":"Delta Tables in Logical Query Plans","text":"<p>Delta Table defines <code>DeltaTable</code> Scala extractor to find delta tables in a logical query plan. The extractor finds <code>LogicalRelation</code>s (Spark SQL) with <code>HadoopFsRelation</code> (Spark SQL) and TahoeFileIndex.</p> <p>Put simply, delta tables are <code>LogicalRelation</code>s with <code>HadoopFsRelation</code> with TahoeFileIndex in logical query plans.</p>"},{"location":"overview/#concurrent-blind-append-transactions","title":"Concurrent Blind Append Transactions","text":"<p>A transaction can be blind append when simply appends new data to a table with no reliance on existing data (and without reading or modifying it).</p> <p>Blind append transactions are marked in the commit info to distinguish them from read-modify-appends (deletes, merges or updates) and assume no conflict between concurrent transactions.</p> <p>Blind Append Transactions allow for concurrent updates.</p> <pre><code>df.format(\"delta\")\n.mode(\"append\")\n.save(...)\n</code></pre>"},{"location":"overview/#generated-columns","title":"Generated Columns","text":"<p>Delta Lake supports Generated Columns.</p>"},{"location":"overview/#table-constraints","title":"Table Constraints","text":"<p>Delta Lake introduces table constraints to ensure data quality and integrity (during writes).</p>"},{"location":"overview/#exception-public-api","title":"Exception Public API","text":"<p>Delta Lake introduces exceptions due to conflicts between concurrent operations as a public API.</p>"},{"location":"overview/#simplified-storage-configuration","title":"Simplified Storage Configuration","text":"<p>Storage</p>"},{"location":"overview/#delta-lake-120","title":"Delta Lake 1.2.0","text":""},{"location":"overview/#compacting-small-files-optimize","title":"Compacting Small Files (Optimize)","text":"<p>Delta Lake 1.2.0 introduces a new OPTIMIZE SQL command for compacting small files into larger ones.</p>"},{"location":"overview/#data-skipping","title":"Data Skipping","text":"<p>Delta Lake 1.2.0 introduces support for Data Skipping.</p>"},{"location":"spark-logging/","title":"Logging","text":"<p>Spark uses log4j for logging.</p> <p>Note</p> <p>Learn more on Spark Logging in The Internals of Apache Spark online book.</p>"},{"location":"table-properties/","title":"Table Properties","text":"<p>Delta Lake allows setting up table properties for a custom behaviour of a delta table.</p>"},{"location":"table-properties/#delta-prefix","title":"delta Prefix","text":"<p>All table properties start with <code>delta.</code> prefix.</p>"},{"location":"table-properties/#show-tblproperties","title":"SHOW TBLPROPERTIES","text":"<p>Table properties can be displayed using <code>SHOW TBLPROPERTIES</code> SQL command:</p> <pre><code>SHOW TBLPROPERTIES &lt;table_name&gt; [(comma-separated properties)]\n</code></pre> <pre><code>sql(\"SHOW TBLPROPERTIES delta.`/tmp/delta/t1`\").show(truncate = false)\n</code></pre> <pre><code>+----------------------+-----+\n|key                   |value|\n+----------------------+-----+\n|delta.minReaderVersion|1    |\n|delta.minWriterVersion|2    |\n+----------------------+-----+\n</code></pre> <pre><code>sql(\"SHOW TBLPROPERTIES delta.`/tmp/delta/t1` (delta.minReaderVersion)\").show(truncate = false)\n</code></pre> <pre><code>+----------------------+-----+\n|key                   |value|\n+----------------------+-----+\n|delta.minReaderVersion|1    |\n+----------------------+-----+\n</code></pre>"},{"location":"table-properties/#alter-table-set-tblproperties","title":"ALTER TABLE SET TBLPROPERTIES","text":"<p>Table properties can be set a value or unset using <code>ALTER TABLE</code> SQL command:</p> <pre><code>ALTER TABLE &lt;table_name&gt; SET TBLPROPERTIES (&lt;key&gt;=&lt;value&gt;)\n</code></pre> <pre><code>ALTER TABLE table1 UNSET TBLPROPERTIES [IF EXISTS] ('key1', 'key2', ...);\n</code></pre> <pre><code>sql(\"ALTER TABLE delta.`/tmp/delta/t1` SET TBLPROPERTIES (delta.enableExpiredLogCleanup=true)\")\n</code></pre> <pre><code>sql(\"SHOW TBLPROPERTIES delta.`/tmp/delta/t1` (delta.enableExpiredLogCleanup)\").show(truncate = false)\n</code></pre> <pre><code>+-----------------------------+-----+\n|key                          |value|\n+-----------------------------+-----+\n|delta.enableExpiredLogCleanup|true |\n+-----------------------------+-----+\n</code></pre>"},{"location":"change-data-feed/","title":"Change Data Feed","text":"<p>Change Data Feed (CDF) (aka Change Data Capture or CDC in short) is a feature of Delta Lake that allows tracking row-level changes between versions of a delta table.</p> <p>With so-called CDC-Aware Table Scan (CDC Read), loading a delta table gives data changes (not the data of a particular version of the delta table).</p> <p>As they put it (in this comment), CDCReader is the key class used for Change Data Feed (with DelayedCommitProtocol to handle it properly).</p> <p>Non-CDC data is written out to the base directory of a delta table, while CDC data is written out to the _change_data special folder.</p> <p>Change Data Feed is a new feature in Delta Lake 2.0.0 (that was tracked under Support for Change Data Feed in Delta Lake #1105).</p>"},{"location":"change-data-feed/#enabling-cdf-for-a-delta-table","title":"Enabling CDF for a Delta table","text":"<p>Enable CDF for a table using delta.enableChangeDataFeed table property.</p> <pre><code>ALTER TABLE delta_demo\nSET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n</code></pre> <pre><code>CREATE TABLE delta_demo (id INT, name STRING, age INT)\nUSING delta\nTBLPROPERTIES (delta.enableChangeDataFeed = true)\n</code></pre> <p>Additionally, this property can be set for all new tables by default.</p> <pre><code>SET spark.databricks.delta.properties.defaults.enableChangeDataFeed = true;\n</code></pre>"},{"location":"change-data-feed/#options","title":"Options <p>Change Data Feed is enabled in batch and streaming queries using readChangeFeed option.</p> Batch QueryStreaming Query   <pre><code>spark\n  .read\n  .format(\"delta\")\n  .option(\"readChangeFeed\", \"true\")\n  .option(\"startingVersion\", startingVersion)\n  .option(\"endingVersion\", endingVersion)\n  .table(\"source\")\n</code></pre>   <pre><code>spark\n  .readStream\n  .format(\"delta\")\n  .option(\"readChangeFeed\", \"true\")\n  .option(\"startingVersion\", startingVersion)\n  .table(\"source\")\n</code></pre>    <p><code>readChangeFeed</code> is used alongside the other CDC options:</p> <ul> <li>startingVersion</li> <li>startingTimestamp</li> <li>endingVersion</li> <li>endingTimestamp</li> </ul>","text":""},{"location":"change-data-feed/#_change_type-column","title":"_change_type Column <p>_change_type column represents a change type.</p>    _change_type Command     <code>delete</code> DeleteCommand   FIXME","text":""},{"location":"change-data-feed/#protocol","title":"Protocol <p>Change Data Feed requires the minimum protocol version to be 0 for readers and 4 for writers.</p>","text":""},{"location":"change-data-feed/#column-mapping-not-supported","title":"Column Mapping Not Supported <p>Change data feed reads are currently not supported on tables with column mapping enabled (and a DeltaUnsupportedOperationException is thrown).</p>","text":""},{"location":"change-data-feed/#demo","title":"Demo <p>Change Data Feed</p>","text":""},{"location":"change-data-feed/#learn-more","title":"Learn More <ol> <li>Delta Lake guide</li> </ol>","text":""},{"location":"change-data-feed/CDCReader/","title":"CDCReader","text":"<p><code>CDCReader</code> utility is the key class for CDF and CDC in DeltaLake (per this comment).</p>"},{"location":"change-data-feed/CDCReader/#getcdcrelation","title":"getCDCRelation <pre><code>getCDCRelation(\n  spark: SparkSession,\n  deltaLog: DeltaLog,\n  snapshotToUse: Snapshot,\n  partitionFilters: Seq[Expression],\n  conf: SQLConf,\n  options: CaseInsensitiveStringMap): BaseRelation\n</code></pre>  <p>Note</p> <p><code>partitionFilters</code> argument is not used.</p>  <p><code>getCDCRelation</code> getVersionForCDC (with the startingVersion and startingTimestamp for the version and timestamp keys, respectively).</p> <p><code>getCDCRelation</code>...FIXME</p> <p><code>getCDCRelation</code> is used when:</p> <ul> <li><code>DeltaLog</code> is requested to create a relation</li> </ul>","text":""},{"location":"change-data-feed/CDCReader/#resolving-version","title":"Resolving Version <pre><code>getVersionForCDC(\n  spark: SparkSession,\n  deltaLog: DeltaLog,\n  conf: SQLConf,\n  options: CaseInsensitiveStringMap,\n  versionKey: String,\n  timestampKey: String): Option[Long]\n</code></pre> <p><code>getVersionForCDC</code> uses the given <code>options</code> map to get the value of the given <code>versionKey</code> key, if available.</p>  <p>When <code>versionKey</code> and <code>timestampKey</code> are specified</p> <p><code>versionKey</code> and <code>timestampKey</code> are specified in the given <code>options</code> argument that is passed down through getCDCRelation unmodified when <code>DeltaLog</code> is requested to create a relation with non-empty <code>cdcOptions</code>.</p>  <p>Otherwise, <code>getVersionForCDC</code> uses the given <code>options</code> map to get the value of the given <code>timestampKey</code> key, if available. <code>getVersionForCDC</code>...FIXME</p> <p>If neither the given <code>versionKey</code> nor the <code>timestampKey</code> key is available in the <code>options</code> map, <code>getVersionForCDC</code> returns <code>None</code> (undefined value).</p>","text":""},{"location":"change-data-feed/CDCReader/#_change_data-directory","title":"_change_data Directory <p><code>CDCReader</code> defines <code>_change_data</code> as the name of the directory (under the data directory) where data changes of a delta table are written out (using DelayedCommitProtocol). This directory may contain partition directories.</p> <p>Used when:</p> <ul> <li><code>DelayedCommitProtocol</code> is requested for the newTaskTempFile</li> </ul>","text":""},{"location":"change-data-feed/CDCReader/#cdf-virtual-columns","title":"CDF Virtual Columns <pre><code>CDC_COLUMNS_IN_DATA: Seq[String]\n</code></pre> <p><code>CDCReader</code> defines a <code>CDC_COLUMNS_IN_DATA</code> collection with __is_cdc and _change_type CDF-specific column names.</p>","text":""},{"location":"change-data-feed/CDCReader/#__is_cdc-partition-column","title":"__is_cdc Partition Column <p><code>CDCReader</code> defines <code>__is_cdc</code> column name to partition on with Change Data Feed enabled.</p> <p><code>__is_cdc</code> column is added when <code>TransactionalWrite</code> is requested to performCDCPartition with CDF enabled on a delta table (and _change_type among the columns).</p> <p>If added, <code>__is_cdc</code> column becomes the first partitioning column. It is then \"consumed\" by DelayedCommitProtocol (to write changes to <code>cdc-</code>-prefixed files, not <code>part-</code>).</p> <p><code>__is_cdc</code> is one of the CDF Virtual Columns.</p> <p>Used when:</p> <ul> <li><code>DelayedCommitProtocol</code> is requested to getFileName and buildActionFromAddedFile</li> </ul>","text":""},{"location":"change-data-feed/CDCReader/#_change_type-column","title":"_change_type Column <p><code>CDCReader</code> defines <code>_change_type</code> column name for the column that represents a change type.</p> <p><code>_change_type</code> is one of the CDF Virtual Columns and among the columns in the CDF-aware read schema.</p> <p><code>CDC_TYPE_COLUMN_NAME</code> is used when:</p> <ul> <li><code>DeleteCommand</code> is requested to performDelete (and then rewriteFiles)</li> <li><code>MergeIntoCommand</code> is requested to writeAllChanges (to matchedClauseOutput and notMatchedClauseOutput)</li> <li><code>UpdateCommand</code> is requested to withUpdatedColumns</li> <li><code>WriteIntoDelta</code> is requested to write</li> <li><code>CdcAddFileIndex</code> is requested to matchingFiles</li> <li><code>TahoeRemoveFileIndex</code> is requested to matchingFiles</li> <li><code>TransactionalWrite</code> is requested to performCDCPartition</li> <li><code>SchemaUtils</code> utility is used to normalizeColumnNames</li> </ul>","text":""},{"location":"change-data-feed/CDCReader/#cdc_type_not_cdc","title":"CDC_TYPE_NOT_CDC <pre><code>CDC_TYPE_NOT_CDC: String\n</code></pre> <p><code>CDCReader</code> defines a <code>CDC_TYPE_NOT_CDC</code> value that is always <code>null</code>.</p> <p><code>CDC_TYPE_NOT_CDC</code> is used when:</p> <ul> <li><code>DeleteCommand</code> is requested to rewriteFiles</li> <li><code>MergeIntoCommand</code> is requested to writeAllChanges (to matchedClauseOutput and notMatchedClauseOutput)</li> <li><code>UpdateCommand</code> is requested to withUpdatedColumns</li> <li><code>WriteIntoDelta</code> is requested to write</li> </ul>","text":""},{"location":"change-data-feed/CDCReader/#cdc-aware-table-scan-cdc-read","title":"CDC-Aware Table Scan (CDC Read) <pre><code>isCDCRead(\n  options: CaseInsensitiveStringMap): Boolean\n</code></pre> <p><code>isCDCRead</code> is <code>true</code> when one of the following options is specified (in the given <code>options</code>):</p> <ol> <li>readChangeFeed with <code>true</code> value</li> <li>(legacy) readChangeData with <code>true</code> value</li> </ol> <p>Otherwise, <code>isCDCRead</code> is <code>false</code>.</p> <p><code>isCDCRead</code> is used when:</p> <ul> <li><code>DeltaRelation</code> utility is used to fromV2Relation</li> <li><code>DeltaTableV2</code> is requested to withOptions</li> <li><code>DeltaDataSource</code> is requested for the streaming source schema and to create a BaseRelation</li> </ul>","text":""},{"location":"change-data-feed/CDCReader/#cdf-aware-read-schema-adding-cdf-columns","title":"CDF-Aware Read Schema (Adding CDF Columns) <pre><code>cdcReadSchema(\n  deltaSchema: StructType): StructType\n</code></pre> <p><code>cdcReadSchema</code> adds the CDF columns to the given <code>deltaSchema</code>.</p>    Name Type     _change_type <code>StringType</code>   _commit_version <code>LongType</code>   _commit_timestamp <code>TimestampType</code>    <p><code>cdcReadSchema</code> is used when:</p> <ul> <li><code>CDCReader</code> utility is used to getCDCRelation and scanIndex</li> <li><code>DeltaRelation</code> utility is used to fromV2Relation</li> <li><code>OptimisticTransactionImpl</code> is requested to performCdcMetadataCheck</li> <li><code>CdcAddFileIndex</code> is requested for the partitionSchema</li> <li><code>TahoeRemoveFileIndex</code> is requested for the partitionSchema</li> <li><code>DeltaDataSource</code> is requested for the sourceSchema</li> <li><code>DeltaSourceBase</code> is requested for the schema</li> <li><code>DeltaSourceCDCSupport</code> is requested to filterCDCActions</li> </ul>","text":""},{"location":"change-data-feed/CDCReader/#changestodf","title":"changesToDF <pre><code>changesToDF(\n  deltaLog: DeltaLog,\n  start: Long,\n  end: Long,\n  changes: Iterator[(Long, Seq[Action])],\n  spark: SparkSession,\n  isStreaming: Boolean = false): CDCVersionDiffInfo\n</code></pre> <p><code>changesToDF</code>...FIXME</p> <p><code>changesToDF</code> is used when:</p> <ul> <li><code>CDCReader</code> is requested to changesToBatchDF</li> <li><code>DeltaSourceCDCSupport</code> is requested to getCDCFileChangesAndCreateDataFrame</li> </ul>","text":""},{"location":"change-data-feed/CDCReader/#deltaunsupportedoperationexception","title":"DeltaUnsupportedOperationException <p><code>changesToDF</code> makes sure that the DeltaColumnMappingMode is NoMapping or throws a <code>DeltaUnsupportedOperationException</code>:</p> <pre><code>Change data feed (CDF) reads are currently not supported on tables with column mapping enabled.\n</code></pre>","text":""},{"location":"change-data-feed/CDCReader/#scanindex","title":"scanIndex <pre><code>scanIndex(\n  spark: SparkSession,\n  index: TahoeFileIndex,\n  metadata: Metadata,\n  isStreaming: Boolean = false): DataFrame\n</code></pre> <p><code>scanIndex</code> creates a <code>LogicalRelation</code> (Spark SQL) with a <code>HadoopFsRelation</code> (Spark SQL) (with the given TahoeFileIndex, cdcReadSchema, no bucketing, DeltaParquetFileFormat).</p> <p>In the end, <code>scanIndex</code> wraps it up as a <code>DataFrame</code>.</p>","text":""},{"location":"change-data-feed/CDCReader/#iscdcenabledontable","title":"isCDCEnabledOnTable <pre><code>isCDCEnabledOnTable(\n  metadata: Metadata): Boolean\n</code></pre> <p><code>isCDCEnabledOnTable</code> is the value of the delta.enableChangeDataFeed table property.</p> <p><code>isCDCEnabledOnTable</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to performCdcMetadataCheck and performCdcColumnMappingCheck</li> <li><code>WriteIntoDelta</code> is requested to write</li> <li><code>CDCReader</code> is requested to changesToDF</li> <li><code>TransactionalWrite</code> is requested to performCDCPartition</li> </ul>","text":""},{"location":"change-data-feed/CDCReader/#insert-change-type","title":"insert Change Type <p><code>CDCReader</code> defines <code>insert</code> value for the value of the _change_type column in the following:</p> <ul> <li>notMatchedClauseOutput with cdcEnabled (when writeAllChanges)</li> <li><code>WriteIntoDelta</code> is requested to write data out (with isCDCEnabledOnTable)</li> <li><code>CdcAddFileIndex</code> is requested to matchingFiles</li> </ul>","text":""},{"location":"change-data-feed/CdcAddFileIndex/","title":"CdcAddFileIndex","text":"<p><code>CdcAddFileIndex</code> is...FIXME</p>"},{"location":"change-data-feed/DeltaSourceCDCSupport/","title":"DeltaSourceCDCSupport","text":"<p><code>DeltaSourceCDCSupport</code> is an abstraction to bring CDC support to DeltaSource.</p>"},{"location":"change-data-feed/DeltaSourceCDCSupport/#getcdcfilechangesandcreatedataframe","title":"getCDCFileChangesAndCreateDataFrame <pre><code>getCDCFileChangesAndCreateDataFrame(\n  startVersion: Long,\n  startIndex: Long,\n  isStartingVersion: Boolean,\n  endOffset: DeltaSourceOffset): DataFrame\n</code></pre> <p><code>getCDCFileChangesAndCreateDataFrame</code> changesToDF with the following:</p> <ul> <li>getFileChangesForCDC (with no <code>AdmissionLimits</code>) for the versions and their FileActions</li> <li><code>isStreaming</code> enabled</li> </ul> <p>In the end, <code>getCDCFileChangesAndCreateDataFrame</code> returns the <code>DataFrame</code> with the file changes (out of the <code>CDCVersionDiffInfo</code>).</p>  <p><code>getCDCFileChangesAndCreateDataFrame</code> is used when:</p> <ul> <li><code>DeltaSourceBase</code> is requested to createDataFrameBetweenOffsets (and to getFileChangesAndCreateDataFrame) for <code>DeltaSource</code> for a streaming DataFrame (with data between the start and end offsets) with the readChangeFeed option enabled</li> </ul>","text":""},{"location":"change-data-feed/DeltaSourceCDCSupport/#getfilechangesforcdc","title":"getFileChangesForCDC <pre><code>getFileChangesForCDC(\n  fromVersion: Long,\n  fromIndex: Long,\n  isStartingVersion: Boolean,\n  limits: Option[AdmissionLimits],\n  endOffset: Option[DeltaSourceOffset]): Iterator[(Long, Iterator[IndexedFile])]\n</code></pre> <p>With isStartingVersion on (<code>true</code>), <code>getFileChangesForCDC</code> gets the snapshot at the <code>fromVersion</code> version and turns<code>dataChange</code> on for all AddFiles. <code>getFileChangesForCDC</code> creates a IndexedChangeFileSeq (with the snapshot and <code>isInitialSnapshot</code> flag enabled). <code>getFileChangesForCDC</code>...FIXME</p> <p>With isStartingVersion off (<code>false</code>), <code>getFileChangesForCDC</code> filterAndIndexDeltaLogs for the <code>fromVersion</code> version.</p> <p>That gives a collection of a version and IndexedChangeFileSeq pairs.</p> <p>In the end, <code>getFileChangesForCDC</code> requests all the <code>IndexedChangeFileSeq</code>s to filterFiles (with <code>fromVersion</code>, <code>fromIndex</code>, <code>limits</code> and <code>endOffset</code> arguments).</p>  <p><code>getFileChangesForCDC</code> is used when:</p> <ul> <li><code>DeltaSourceBase</code> is requested to getFileChangesWithRateLimit</li> <li><code>DeltaSourceCDCSupport</code> is requested to getCDCFileChangesAndCreateDataFrame</li> </ul>","text":""},{"location":"change-data-feed/DeltaSourceCDCSupport/#isstartingversion","title":"isStartingVersion <p><code>getFileChangesForCDC</code> is given <code>isStartingVersion</code> flag when executed:</p> <ul> <li> <p><code>true</code> for the following:</p> <ul> <li><code>DeltaSource</code> when getStartingVersion is undefined (returns <code>None</code>)</li> <li><code>DeltaSource</code> when getBatch with <code>startOffsetOption</code> and getStartingVersion both undefined (<code>None</code>s)</li> </ul> </li> <li> <p><code>false</code> for the following:</p> <ul> <li><code>DeltaSource</code> when getBatch with <code>startOffsetOption</code> undefined but getStartingVersion specified</li> </ul> </li> <li> <p><code>true</code> or <code>false</code> for the following:</p> <ul> <li><code>DeltaSourceBase</code> when getNextOffsetFromPreviousOffset based on isStartingVersion (of the previous offset)</li> <li><code>DeltaSource</code> when getBatch with <code>startOffsetOption</code> specified and based on the isStartingVersion (of the start offset)</li> </ul> </li> </ul>","text":""},{"location":"change-data-feed/DeltaSourceCDCSupport/#filterandindexdeltalogs","title":"filterAndIndexDeltaLogs <pre><code>filterAndIndexDeltaLogs(\n  startVersion: Long): Iterator[(Long, IndexedChangeFileSeq)]\n</code></pre> <p><code>filterAndIndexDeltaLogs</code> requests the DeltaLog to get the changes at the given <code>startVersion</code> version and on (<code>Iterator[(Long, Seq[Action])]</code>).</p> <p><code>filterAndIndexDeltaLogs</code> uses failOnDataLoss option to get the changes.</p> <p><code>filterAndIndexDeltaLogs</code> filterCDCActions (across the actions across all the versions) and converts the AddFiles, AddCDCFiles and RemoveFiles to <code>IndexedFile</code>s.</p> <p>In the end, for every version, <code>filterAndIndexDeltaLogs</code> creates a IndexedChangeFileSeq with the <code>IndexedFile</code>s (and the isInitialSnapshot flag off).</p>","text":""},{"location":"change-data-feed/DeltaSourceCDCSupport/#filtercdcactions","title":"filterCDCActions <pre><code>filterCDCActions(\n  actions: Seq[Action],\n  version: Long): Seq[FileAction]\n</code></pre>  <p>Note</p> <p><code>version</code> argument is ignored.</p>  <p><code>filterCDCActions</code> collects the AddCDCFile actions from the given <code>actions</code> (if there are any).</p> <p>Otherwise, <code>filterCDCActions</code> collects AddFiles and RemoveFiles with <code>dataChange</code> enabled.</p>","text":""},{"location":"change-data-feed/IndexedChangeFileSeq/","title":"IndexedChangeFileSeq","text":""},{"location":"change-data-feed/IndexedChangeFileSeq/#creating-instance","title":"Creating Instance","text":"<p><code>IndexedChangeFileSeq</code> takes the following to be created:</p> <ul> <li> <code>IndexedFile</code>s (<code>Iterator[IndexedFile]</code>) <li>isInitialSnapshot flag</li> <p><code>IndexedChangeFileSeq</code> is created when:</p> <ul> <li><code>DeltaSourceCDCSupport</code> is requested to getFileChangesForCDC</li> </ul>"},{"location":"change-data-feed/IndexedChangeFileSeq/#isinitialsnapshot-flag","title":"isInitialSnapshot Flag <p><code>IndexedChangeFileSeq</code> is given <code>isInitialSnapshot</code> flag when created:</p> <ul> <li><code>true</code> for <code>DeltaSourceCDCSupport</code> when getFileChangesForCDC with isStartingVersion flag on</li> <li><code>false</code> for <code>DeltaSourceCDCSupport</code> when filterAndIndexDeltaLogs (while getFileChangesForCDC)</li> </ul>","text":""},{"location":"change-data-feed/IndexedChangeFileSeq/#filterfiles","title":"filterFiles <pre><code>filterFiles(\n  fromVersion: Long,\n  fromIndex: Long,\n  limits: Option[AdmissionLimits],\n  endOffset: Option[DeltaSourceOffset] = None): Iterator[IndexedFile]\n</code></pre> <p><code>filterFiles</code>...FIXME</p> <p><code>filterFiles</code> is used when:</p> <ul> <li><code>DeltaSourceCDCSupport</code> is requested to getFileChangesForCDC</li> </ul>","text":""},{"location":"change-data-feed/IndexedChangeFileSeq/#isvalidindexedfile","title":"isValidIndexedFile <pre><code>isValidIndexedFile(\n  indexedFile: IndexedFile,\n  fromVersion: Long,\n  fromIndex: Long,\n  endOffset: Option[DeltaSourceOffset]): Boolean\n</code></pre> <p><code>isValidIndexedFile</code>...FIXME</p> <p><code>isValidIndexedFile</code> is used when:</p> <ul> <li><code>IndexedChangeFileSeq</code> is requested to filterFiles</li> </ul>","text":""},{"location":"change-data-feed/IndexedChangeFileSeq/#morethanfrom","title":"moreThanFrom <pre><code>moreThanFrom(\n  indexedFile: IndexedFile,\n  fromVersion: Long,\n  fromIndex: Long): Boolean\n</code></pre> <p><code>moreThanFrom</code>...FIXME</p>","text":""},{"location":"change-data-feed/TahoeRemoveFileIndex/","title":"TahoeRemoveFileIndex","text":"<p><code>TahoeRemoveFileIndex</code> is...FIXME</p>"},{"location":"check-constraints/","title":"CHECK Constraints","text":"<p>CHECK Constraints are named SQL expressions that are used to enforce data quality at table level (at write time).</p> <p>CHECK constraints are registered (added) using the following high-level operator:</p> <ul> <li>ALTER TABLE ADD CONSTRAINT SQL statement</li> </ul> <p>CHECK constraints are de-registered (dropped) using the following high-level operator:</p> <ul> <li>ALTER TABLE DROP CONSTRAINT SQL statement</li> </ul> <p>DeltaCatalog is responsible to manage CHECK constraints of a delta table (using AddConstraint and DropConstraint).</p>"},{"location":"check-constraints/#protocol","title":"Protocol <p>CHECK constraints require the Minimum Writer Version (of a delta table) to be at least 3.</p>","text":""},{"location":"check-constraints/#deltaconstraints","title":"delta.constraints <p>CHECK constraints are stored in the table configuration (of a table metadata) as <code>delta.constraints.</code>-keyed entries.</p>","text":""},{"location":"check-constraints/#altertablecommands-deltacatalog-and-delta-commands","title":"AlterTableCommands, DeltaCatalog and Delta Commands <p>At execution, <code>AlterTableCommand</code> (Spark SQL) is planned as a <code>AlterTableExec</code> (Spark SQL) that requests the active <code>TableCatalog</code> (Spark SQL) to alter a table.</p> <p>That is when DeltaCatalog kicks in to alter a table as AlterTableAddConstraintDeltaCommand.</p>","text":""},{"location":"check-constraints/AddConstraint/","title":"AddConstraint","text":"<p><code>AddConstraint</code> is a <code>TableChange</code> (Spark SQL) of AlterTableAddConstraint command.</p>"},{"location":"check-constraints/AddConstraint/#creating-instance","title":"Creating Instance","text":"<p><code>AddConstraint</code> takes the following to be created:</p> <ul> <li> Constraint Name <li> Constraint SQL Expression (text) <p><code>AddConstraint</code> is created\u00a0when:</p> <ul> <li><code>AlterTableAddConstraint</code> command is requested for the table changes</li> </ul>"},{"location":"check-constraints/AddConstraint/#query-execution","title":"Query Execution","text":"<p><code>AddConstraint</code> is resolved to AlterTableAddConstraintDeltaCommand and immediately executed when <code>DeltaCatalog</code> is requested to alter a delta table.</p>"},{"location":"check-constraints/AlterTableAddConstraint/","title":"AlterTableAddConstraint","text":"<p><code>AlterTableAddConstraint</code> is an <code>AlterTableCommand</code> (Spark SQL) that represents ALTER TABLE ADD CONSTRAINT SQL command for CHECK constraints.</p>"},{"location":"check-constraints/AlterTableAddConstraint/#creating-instance","title":"Creating Instance","text":"<p><code>AlterTableAddConstraint</code> takes the following to be created:</p> <ul> <li> Table (<code>LogicalPlan</code>) <li> Constraint Name <li> Constraint SQL Expression (text) <p><code>AlterTableAddConstraint</code> is created when:</p> <ul> <li><code>DeltaSqlAstBuilder</code> is requested to parse ALTER TABLE ADD CONSTRAINT SQL command</li> </ul>"},{"location":"check-constraints/AlterTableAddConstraint/#table-changes","title":"Table Changes <pre><code>changes: Seq[TableChange]\n</code></pre> <p><code>changes</code> is part of the <code>AlterTableCommand</code> (Spark SQL) abstraction.</p>  <p><code>changes</code> gives a single-element collection with an AddConstraint (with the constraintName and expr).</p>","text":""},{"location":"check-constraints/AlterTableDropConstraint/","title":"AlterTableDropConstraint","text":"<p><code>AlterTableDropConstraint</code> is an <code>AlterTableCommand</code> (Spark SQL) for ALTER TABLE DROP CONSTRAINT SQL command for CHECK constraints.</p>"},{"location":"check-constraints/AlterTableDropConstraint/#creating-instance","title":"Creating Instance","text":"<p><code>AlterTableDropConstraint</code> takes the following to be created:</p> <ul> <li> Table (<code>LogicalPlan</code>) <li> Constraint Name <li> <code>ifExists</code> flag <p><code>AlterTableDropConstraint</code> is created when:</p> <ul> <li><code>DeltaSqlAstBuilder</code> is requested to parse ALTER TABLE DROP CONSTRAINT SQL command</li> </ul>"},{"location":"check-constraints/AlterTableDropConstraint/#table-changes","title":"Table Changes <pre><code>changes: Seq[TableChange]\n</code></pre> <p><code>changes</code> is part of the <code>AlterTableCommand</code> (Spark SQL) abstraction.</p>  <p><code>changes</code> gives a single-element collection with an DropConstraint (with the constraintName and ifExists flag).</p>","text":""},{"location":"check-constraints/DropConstraint/","title":"DropConstraint","text":"<p><code>DropConstraint</code> is...FIXME</p>"},{"location":"column-invariants/","title":"Column Invariants","text":"<p>Note</p> <p>As per this comment, column invariants are old-style and users should use CHECK constraints:</p> <p>Utilities for handling constraints. Right now this includes:</p> <ul> <li>Column-level invariants (including both NOT NULL constraints and an old style of CHECK constraint specified in the column metadata)</li> <li>Table-level CHECK constraints</li> </ul> <p>Column Invariants are SQL expressions that are used to enforce data quality at column level (at write time using DeltaInvariantCheckerExec).</p> <p>Column Invariants are associated with any top-level or nested columns. If with a nested column, all parent columns have to be non-nullable (by NotNull constraints).</p> <p>Column invariants are column-level (not table-wide) yet use the same Check constraint as CHECK constraints. In other words, column invariants are single-column CHECK constraints (i.e., limited to a single column).</p>"},{"location":"column-invariants/#deltainvariants","title":"delta.invariants","text":"<p>Column invariants are stored in the table schema (of a table metadata) as JSON-encoded SQL expressions as delta.invariants metadata of a column.</p>"},{"location":"column-invariants/Invariants/","title":"Invariants Utility","text":""},{"location":"column-invariants/Invariants/#deltainvariants","title":"delta.invariants <p><code>Invariants</code> defines <code>delta.invariants</code> for the column invariants of a delta table.</p> <p><code>delta.invariants</code> contains a JSON-encoded SQL expression.</p>","text":""},{"location":"column-invariants/Invariants/#extracting-constraints-from-schema","title":"Extracting Constraints from Schema <pre><code>getFromSchema(\n  schema: StructType,\n  spark: SparkSession): Seq[Constraint]\n</code></pre> <p><code>getFromSchema</code> finds columns (top-level or nested) that are non-nullable and have delta.invariants metadata.</p> <p>For every parent of the columns, <code>getFromSchema</code> creates NotNull constraints.</p> <p>For the columns themselves, <code>getFromSchema</code> creates Check constraints.</p>  <p><code>getFromSchema</code>\u00a0is used when:</p> <ul> <li><code>Constraints</code> utility is used to extract table constraints</li> <li><code>Protocol</code> utility is used to determine the required minimum protocol</li> </ul>","text":""},{"location":"column-invariants/Invariants/#rule","title":"Rule <p><code>Invariants</code> utility defines a <code>Rule</code> abstraction.</p> <p><code>Rule</code> has a name.</p>","text":""},{"location":"column-invariants/Invariants/#arbitraryexpression","title":"ArbitraryExpression <p><code>ArbitraryExpression</code> is a Rule with the following:</p> <ul> <li><code>EXPRESSION([expression])</code> name</li> <li>An <code>Expression</code> (Spark SQL)</li> </ul>","text":""},{"location":"column-mapping/","title":"Column Mapping","text":"<p>Column Mapping allows mapping physical columns names to their logical ones for reading and writing parquet data files.</p> <p>Column Mapping is enabled for a delta table using delta.columnMapping.mode table property.</p> <p>Column Mapping turns ALTER TABLE RENAME COLUMN and ALTER TABLE CHANGE COLUMN commands into logical changes at metadata level (leading to no changes to the physical column names in parquet files and a mere transactional metadata update in a transaction log).</p> <p>Column Mapping uses the metadata of a <code>StructField</code> (Spark SQL) to  store the logical column name under the <code>delta.columnMapping.physicalName</code> metadata key.</p> <p>Column Mapping is available as of Delta Lake 1.2.0.</p>"},{"location":"column-mapping/#minimum-required-protocol","title":"Minimum Required Protocol","text":"<p>Column Mapping requires the minimum protocol version to be 2 for readers and 5 for writers.</p>"},{"location":"column-mapping/#demo","title":"Demo","text":"<p>Demo: Column Mapping</p>"},{"location":"column-mapping/#learn-more","title":"Learn More","text":"<ol> <li>Delta Lake 1.2 - More Speed, Efficiency and Extensibility Than Ever</li> </ol>"},{"location":"column-mapping/DeltaColumnMappingBase/","title":"DeltaColumnMappingBase (DeltaColumnMapping)","text":"<p><code>DeltaColumnMappingBase</code> is an abstraction of DeltaColumnMappings.</p>"},{"location":"column-mapping/DeltaColumnMappingBase/#implementations","title":"Implementations","text":"<ul> <li>DeltaColumnMapping</li> </ul>"},{"location":"column-mapping/DeltaColumnMappingBase/#compatible-protocol","title":"Compatible Protocol <p><code>DeltaColumnMappingBase</code> defines a Protocol (with MIN_READER_VERSION and MIN_WRITER_VERSION) as the minimum protocol version for the readers and writers to delta tables with column mapping.</p> <ul> <li><code>Protocol</code> utility is used for requiredMinimumProtocol</li> <li>delta.columnMapping.mode configuration property</li> <li>delta.columnMapping.maxColumnId configuration property</li> <li><code>DeltaErrors</code> is requested to changeColumnMappingModeOnOldProtocol (for error reporting)</li> </ul>","text":""},{"location":"column-mapping/DeltaColumnMappingBase/#minimum-reader-version","title":"Minimum Reader Version <p><code>DeltaColumnMappingBase</code> defines <code>MIN_READER_VERSION</code> constant as <code>2</code> for the minimum version of the compatible readers of delta tables to satisfyColumnMappingProtocol.</p>","text":""},{"location":"column-mapping/DeltaColumnMappingBase/#minimum-writer-version","title":"Minimum Writer Version <p><code>DeltaColumnMappingBase</code> defines <code>MIN_WRITER_VERSION</code> constant as <code>5</code> for the minimum version of the compatible writers to delta tables to satisfyColumnMappingProtocol.</p>","text":""},{"location":"column-mapping/DeltaColumnMappingBase/#createphysicalschema","title":"createPhysicalSchema <pre><code>createPhysicalSchema(\n  schema: StructType,\n  referenceSchema: StructType,\n  columnMappingMode: DeltaColumnMappingMode,\n  checkSupportedMode: Boolean = true): StructType\n</code></pre> <p><code>createPhysicalSchema</code>...FIXME</p> <p><code>createPhysicalSchema</code> is used when:</p> <ul> <li><code>DeltaColumnMappingBase</code> is requested to checkColumnIdAndPhysicalNameAssignments and createPhysicalAttributes</li> <li><code>DeltaParquetFileFormat</code> is requested to prepare a schema</li> </ul>","text":""},{"location":"column-mapping/DeltaColumnMappingBase/#renamecolumns","title":"renameColumns <pre><code>renameColumns(\n  schema: StructType): StructType\n</code></pre> <p><code>renameColumns</code>...FIXME</p> <p><code>renameColumns</code> is used when:</p> <ul> <li><code>Metadata</code> is requested for the physicalPartitionSchema</li> </ul>","text":""},{"location":"column-mapping/DeltaColumnMappingBase/#requiresnewprotocol","title":"requiresNewProtocol <pre><code>requiresNewProtocol(\n  metadata: Metadata): Boolean\n</code></pre> <p><code>requiresNewProtocol</code> is <code>true</code> when the DeltaColumnMappingMode (of this delta table per the given Metadata) is either IdMapping or NameMapping. Otherwise, <code>requiresNewProtocol</code> is <code>false</code></p> <p><code>requiresNewProtocol</code> is used when:</p> <ul> <li><code>Protocol</code> utility is used to determine the required minimum protocol.</li> </ul>","text":""},{"location":"column-mapping/DeltaColumnMappingBase/#checkcolumnidandphysicalnameassignments","title":"checkColumnIdAndPhysicalNameAssignments <pre><code>checkColumnIdAndPhysicalNameAssignments(\n  schema: StructType,\n  mode: DeltaColumnMappingMode): Unit\n</code></pre> <p><code>checkColumnIdAndPhysicalNameAssignments</code>...FIXME</p> <p><code>checkColumnIdAndPhysicalNameAssignments</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to verify the new metadata</li> </ul>","text":""},{"location":"column-mapping/DeltaColumnMappingBase/#dropcolumnmappingmetadata","title":"dropColumnMappingMetadata <pre><code>dropColumnMappingMetadata(\n  schema: StructType): StructType\n</code></pre> <p><code>dropColumnMappingMetadata</code>...FIXME</p> <p><code>dropColumnMappingMetadata</code> is used when:</p> <ul> <li><code>DeltaLog</code> is requested for a BaseRelation and for a DataFrame</li> <li><code>DeltaTableV2</code> is requested for the tableSchema</li> <li>AlterTableSetLocationDeltaCommand command is executed</li> <li>CreateDeltaTableCommand command is executed</li> <li><code>ImplicitMetadataOperation</code> is requested to update the metadata</li> </ul>","text":""},{"location":"column-mapping/DeltaColumnMappingBase/#mapping-virtual-to-physical-field-name","title":"Mapping Virtual to Physical Field Name <pre><code>getPhysicalName(\n  field: StructField): String\n</code></pre> <p><code>getPhysicalName</code> requests the given <code>StructField</code> (Spark SQL) for the <code>Metadata</code> to extract <code>delta.columnMapping.physicalName</code> key, if available (for column mapping). Otherwise, <code>getPhysicalName</code> returns the name of the given <code>StructField</code> (with no name changes).</p> <p><code>getPhysicalName</code> is used when:</p> <ul> <li><code>CheckpointV2</code> utility is used to extractPartitionValues</li> <li><code>ConflictChecker</code> is requested to getPrettyPartitionMessage</li> <li><code>DeltaColumnMappingBase</code> is requested to renameColumns, assignPhysicalNames and createPhysicalSchema</li> <li><code>DeltaLog</code> utility is used to rewritePartitionFilters</li> <li>AlterTableChangeColumnDeltaCommand is executed</li> <li><code>ConvertToDeltaCommand</code> utility is used to create an AddFile</li> <li><code>TahoeFileIndex</code> is requested to makePartitionDirectories</li> <li><code>DataSkippingReaderBase</code> is requested to getStatsColumnOpt</li> <li><code>StatisticsCollection</code> is requested to collect statistics</li> </ul>","text":""},{"location":"column-mapping/DeltaColumnMappingBase/#verifyandupdatemetadatachange","title":"verifyAndUpdateMetadataChange <pre><code>verifyAndUpdateMetadataChange(\n  oldProtocol: Protocol,\n  oldMetadata: Metadata,\n  newMetadata: Metadata,\n  isCreatingNewTable: Boolean): Metadata\n</code></pre> <p><code>verifyAndUpdateMetadataChange</code>...FIXME</p> <p>In the end, <code>verifyAndUpdateMetadataChange</code> tryFixMetadata with the given <code>newMetadata</code> and <code>oldMetadata</code> metadata.</p> <p><code>verifyAndUpdateMetadataChange</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to updateMetadataInternal</li> </ul>","text":""},{"location":"column-mapping/DeltaColumnMappingBase/#tryfixmetadata","title":"tryFixMetadata <pre><code>tryFixMetadata(\n  oldMetadata: Metadata,\n  newMetadata: Metadata,\n  isChangingModeOnExistingTable: Boolean): Metadata\n</code></pre> <p><code>tryFixMetadata</code> reads columnMapping.mode table property from the given <code>newMetadata</code> table metadata.</p> <p>If the DeltaColumnMappingMode is IdMapping or NameMapping, <code>tryFixMetadata</code> assignColumnIdAndPhysicalName with the given <code>newMetadata</code> and <code>oldMetadata</code> metadata and <code>isChangingModeOnExistingTable</code> flag.</p> <p>For <code>NoMapping</code>, <code>tryFixMetadata</code> does nothing and returns the given <code>newMetadata</code>.</p>","text":""},{"location":"column-mapping/DeltaColumnMappingBase/#satisfycolumnmappingprotocol","title":"satisfyColumnMappingProtocol <pre><code>satisfyColumnMappingProtocol(\n  protocol: Protocol): Boolean\n</code></pre> <p><code>satisfyColumnMappingProtocol</code> returns <code>true</code> when all the following hold true:</p> <ol> <li>minWriterVersion of the given <code>Protocol</code> is at least 5</li> <li>minReaderVersion of the given <code>Protocol</code> is at least 2</li> </ol>","text":""},{"location":"column-mapping/DeltaColumnMappingBase/#allowed-mapping-mode-change","title":"Allowed Mapping Mode Change <pre><code>allowMappingModeChange(\n  oldMode: DeltaColumnMappingMode,\n  newMode: DeltaColumnMappingMode): Boolean\n</code></pre> <p><code>allowMappingModeChange</code> is <code>true</code> when either of the following holds true:</p> <ol> <li>There is no mode change (and the old and new modes are the same)</li> <li>There is a mode change from NoMapping old mode to NameMapping</li> </ol> <p>Otherwise, <code>allowMappingModeChange</code> is <code>false</code>.</p>","text":""},{"location":"column-mapping/DeltaColumnMappingBase/#deltacolumnmapping","title":"DeltaColumnMapping <p><code>DeltaColumnMapping</code> is the only DeltaColumnMappingBase.</p>","text":""},{"location":"column-mapping/DeltaColumnMappingBase/#supported-column-mapping-modes","title":"Supported Column Mapping Modes <pre><code>supportedModes: Set[DeltaColumnMappingMode]\n</code></pre> <p><code>DeltaColumnMappingBase</code> defines <code>supportedModes</code> value with NoMapping and NameMapping column mapping modes.</p> <p><code>supportedModes</code> is used when:</p> <ul> <li><code>DeltaColumnMappingBase</code> is requested to verifyAndUpdateMetadataChange and createPhysicalSchema</li> </ul>","text":""},{"location":"column-mapping/DeltaColumnMappingBase/#getcolumnmappingmetadata","title":"getColumnMappingMetadata <pre><code>getColumnMappingMetadata(\n  field: StructField,\n  mode: DeltaColumnMappingMode): Metadata\n</code></pre>  <p>Note</p> <p><code>getColumnMappingMetadata</code> returns Spark SQL's Metadata not Delta Lake's.</p>  <p><code>getColumnMappingMetadata</code>...FIXME</p> <p><code>getColumnMappingMetadata</code> is used when:</p> <ul> <li><code>DeltaColumnMappingBase</code> is requested to setColumnMetadata and createPhysicalSchema</li> </ul>","text":""},{"location":"column-mapping/DeltaColumnMappingMode/","title":"DeltaColumnMappingMode","text":"<p><code>DeltaColumnMappingMode</code> is an abstraction of the column mapping modes in Column Mapping.</p>"},{"location":"column-mapping/DeltaColumnMappingMode/#contract","title":"Contract","text":""},{"location":"column-mapping/DeltaColumnMappingMode/#name","title":"Name <pre><code>name: String\n</code></pre> <p>Human-friendly name of this <code>DeltaColumnMappingMode</code> (for error reporting)</p> <p>Used when:</p> <ul> <li><code>DeltaErrors</code> utility is used to create a DeltaColumnMappingUnsupportedException and a <code>ColumnMappingException</code> (for missingColumnId, missingPhysicalName, duplicatedColumnId, duplicatedPhysicalName)</li> <li><code>DeltaColumnMappingBase</code> is requested to verifyAndUpdateMetadataChange, getColumnMappingMetadata and createPhysicalSchema</li> </ul>","text":""},{"location":"column-mapping/DeltaColumnMappingMode/#implementations","title":"Implementations","text":"Sealed Trait <p><code>DeltaColumnMappingMode</code> is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).</p> <p>Learn more in the Scala Language Specification.</p>"},{"location":"column-mapping/DeltaColumnMappingMode/#idmapping","title":"IdMapping <p>This mode uses the column ID as the identifier of a column.</p> <p>Name: <code>id</code></p> <p>Used for tables converted from Apache Iceberg.</p> <p>This mode requires a new protocol</p>","text":""},{"location":"column-mapping/DeltaColumnMappingMode/#namemapping","title":"NameMapping <p>Name: <code>name</code></p> <p><code>NameMapping</code> is the only allowed mapping mode change (from NoMapping)</p> <p>This mode requires a new protocol</p> <p><code>NameMapping</code> is among the supportedModes</p> <p><code>NameMapping</code> is used when:</p> <ul> <li><code>DeltaColumnMappingBase</code> is requested for the column mapping metadata, tryFixMetadata, getPhysicalNameFieldMap</li> </ul>","text":""},{"location":"column-mapping/DeltaColumnMappingMode/#nomapping","title":"NoMapping <p>No column mapping and the display name of a column is the only valid identifier to read and write data.</p> <p>Name: <code>none</code></p> <p>This mode does not require a new protocol</p> <p><code>NoMapping</code> is among the supportedModes</p>","text":""},{"location":"column-mapping/DeltaColumnMappingMode/#creating-deltacolumnmappingmode","title":"Creating DeltaColumnMappingMode <pre><code>apply(\n  name: String): DeltaColumnMappingMode\n</code></pre> <p><code>apply</code> returns the DeltaColumnMappingMode for the given <code>name</code> (if defined) or throws a ColumnMappingUnsupportedException:</p> <pre><code>The column mapping mode `[mode]` is not supported for this Delta version.\nPlease upgrade if you want to use this mode.\n</code></pre> <p><code>apply</code> is used when:</p> <ul> <li><code>DeltaConfigsBase</code> is requested to build delta.columnMapping.mode configuration property</li> </ul>","text":""},{"location":"commands/","title":"Commands","text":""},{"location":"commands/CreateDeltaTableCommand/","title":"CreateDeltaTableCommand","text":"<p><code>CreateDeltaTableCommand</code> is a leaf <code>RunnableCommand</code> (Spark SQL) to create a delta table (for DeltaCatalog).</p>"},{"location":"commands/CreateDeltaTableCommand/#creating-instance","title":"Creating Instance","text":"<p><code>CreateDeltaTableCommand</code> takes the following to be created:</p> <ul> <li> <code>CatalogTable</code> (Spark SQL) <li> Existing <code>CatalogTable</code> (if available) <li> <code>SaveMode</code> <li> Optional Data Query (<code>LogicalPlan</code>) <li>CreationMode</li> <li> <code>tableByPath</code> flag (default: <code>false</code>) <li> Output attributes (default: empty) <p><code>CreateDeltaTableCommand</code> is created when:</p> <ul> <li><code>DeltaCatalog</code> is requested to create a delta table</li> </ul>"},{"location":"commands/CreateDeltaTableCommand/#creationmode","title":"CreationMode <p><code>CreateDeltaTableCommand</code> can be given a <code>CreationMode</code> when created:</p> <ul> <li><code>Create</code> (default)</li> <li><code>CreateOrReplace</code></li> <li><code>Replace</code></li> </ul> <p><code>CreationMode</code> is <code>Create</code> by default or specified by DeltaCatalog.</p>","text":""},{"location":"commands/CreateDeltaTableCommand/#executing-command","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p>  <p><code>run</code>...FIXME</p>","text":""},{"location":"commands/CreateDeltaTableCommand/#updatecatalog","title":"updateCatalog <pre><code>updateCatalog(\n  spark: SparkSession,\n  table: CatalogTable): Unit\n</code></pre> <p><code>updateCatalog</code> uses the given <code>SparkSession</code> to access <code>SessionCatalog</code> to <code>createTable</code> or <code>alterTable</code> when the tableByPath flag is off. Otherwise, <code>updateCatalog</code> does nothing.</p>","text":""},{"location":"commands/CreateDeltaTableCommand/#getoperation","title":"getOperation <pre><code>getOperation(\n  metadata: Metadata,\n  isManagedTable: Boolean,\n  options: Option[DeltaOptions]): DeltaOperations.Operation\n</code></pre> <p><code>getOperation</code>...FIXME</p>","text":""},{"location":"commands/CreateDeltaTableCommand/#replacemetadataifnecessary","title":"replaceMetadataIfNecessary <pre><code>replaceMetadataIfNecessary(\n  txn: OptimisticTransaction,\n  tableDesc: CatalogTable,\n  options: DeltaOptions,\n  schema: StructType): Unit\n</code></pre>  <p>Unused argument</p> <p><code>tableDesc</code> argument is not used.</p>  <p><code>replaceMetadataIfNecessary</code> determines whether or not it is a replace operation (i.e., <code>CreateOrReplace</code> or <code>Replace</code> based on the CreationMode).</p> <p><code>replaceMetadataIfNecessary</code> determines whether or not it is supposed not to overwrite the schema of a Delta table (based on the overwriteSchema option in the input DeltaOptions).</p> <p>In the end, only for an <code>CreateOrReplace</code> or <code>Replace</code> operation on an existing delta table with overwriteSchema option enabled, <code>replaceMetadataIfNecessary</code> updates the metadata (on the given OptimisticTransaction) with the given <code>schema</code>.</p>","text":""},{"location":"commands/CreateDeltaTableCommand/#deltaillegalargumentexception","title":"DeltaIllegalArgumentException <p><code>replaceMetadataIfNecessary</code> throws an <code>DeltaIllegalArgumentException</code> for a <code>CreateOrReplace</code> or <code>Replace</code> operation with <code>overwriteSchema</code> option enabled:</p> <pre><code>The usage of overwriteSchema is not allowed when replacing a Delta table.\n</code></pre>","text":""},{"location":"commands/DeltaCommand/","title":"DeltaCommand","text":"<p><code>DeltaCommand</code> is a marker interface for delta commands.</p>"},{"location":"commands/DeltaCommand/#implementations","title":"Implementations","text":"<ul> <li>AlterDeltaTableCommand</li> <li>ConvertToDeltaCommand</li> <li>DeleteCommand</li> <li>MergeIntoCommand</li> <li>OptimizeExecutor</li> <li>OptimizeTableCommandBase</li> <li>RestoreTableCommand</li> <li>StatisticsCollection</li> <li>UpdateCommand</li> <li>VacuumCommandImpl</li> <li>WriteIntoDelta</li> </ul>"},{"location":"commands/DeltaCommand/#converting-predicate-text-to-catalyst-expression","title":"Converting Predicate Text to Catalyst Expression <pre><code>parsePredicates(\n  spark: SparkSession,\n  predicate: String): Seq[Expression]\n</code></pre> <p><code>parsePredicates</code> converts the given <code>predicate</code> text to an <code>Expression</code> (Spark SQL).</p>  <p><code>parsePredicates</code> requests the given <code>SparkSession</code> (Spark SQL) for the session <code>ParserInterface</code> (Spark SQL) to <code>parseExpression</code> the given <code>predicate</code> text.</p>  <p><code>parsePredicates</code> is used when:</p> <ul> <li>OptimizeTableCommand is executed (to convert the partitionPredicate)</li> <li><code>WriteIntoDelta</code> command is requested to write (to convert the replaceWhere option with predicates)</li> </ul>","text":""},{"location":"commands/DeltaCommand/#verifying-partition-predicates","title":"Verifying Partition Predicates <pre><code>verifyPartitionPredicates(\n  spark: SparkSession,\n  partitionColumns: Seq[String],\n  predicates: Seq[Expression]): Unit\n</code></pre> <p><code>verifyPartitionPredicates</code> asserts that the given <code>predicates</code> expressions are as follows:</p> <ul> <li>Contain no subqueries</li> <li>Reference partition columns only (<code>partitionColumns</code>)</li> </ul> <p><code>verifyPartitionPredicates</code> is used when:</p> <ul> <li>OptimizeTableCommand is executed (to verify the partitionPredicate if defined)</li> <li><code>WriteIntoDelta</code> command is requested to write (to verify the replaceWhere option for SaveMode.Overwrite mode)</li> <li><code>StatisticsCollection</code> utility is used to recompute statistics of a delta table</li> </ul>","text":""},{"location":"commands/DeltaCommand/#generatecandidatefilemap","title":"generateCandidateFileMap <pre><code>generateCandidateFileMap(\n  basePath: Path,\n  candidateFiles: Seq[AddFile]): Map[String, AddFile]\n</code></pre> <p><code>generateCandidateFileMap</code>...FIXME</p> <p><code>generateCandidateFileMap</code> is used when...FIXME</p>","text":""},{"location":"commands/DeltaCommand/#removefilesfrompaths","title":"removeFilesFromPaths <pre><code>removeFilesFromPaths(\n  deltaLog: DeltaLog,\n  nameToAddFileMap: Map[String, AddFile],\n  filesToRewrite: Seq[String],\n  operationTimestamp: Long): Seq[RemoveFile]\n</code></pre> <p><code>removeFilesFromPaths</code>...FIXME</p> <p><code>removeFilesFromPaths</code> is used when:</p> <ul> <li>DeleteCommand and UpdateCommand commands are executed</li> </ul>","text":""},{"location":"commands/DeltaCommand/#creating-hadoopfsrelation-with-tahoebatchfileindex","title":"Creating HadoopFsRelation (with TahoeBatchFileIndex) <pre><code>buildBaseRelation(\n  spark: SparkSession,\n  txn: OptimisticTransaction,\n  actionType: String,\n  rootPath: Path,\n  inputLeafFiles: Seq[String],\n  nameToAddFileMap: Map[String, AddFile]): HadoopFsRelation\n</code></pre> <p><code>buildBaseRelation</code> converts the given <code>inputLeafFiles</code> to AddFiles (with the given <code>rootPath</code> and <code>nameToAddFileMap</code>).</p> <p><code>buildBaseRelation</code> creates a TahoeBatchFileIndex for the <code>AddFile</code>s (with the input <code>actionType</code> and <code>rootPath</code>).</p> <p>In the end, <code>buildBaseRelation</code> creates a <code>HadoopFsRelation</code> (Spark SQL) with the <code>TahoeBatchFileIndex</code> (and the other properties based on the metadata of the given OptimisticTransaction).</p> <p><code>buildBaseRelation</code> is used when:</p> <ul> <li>DeleteCommand and UpdateCommand commands are executed (with <code>delete</code> and <code>update</code> action types, respectively)</li> </ul>","text":""},{"location":"commands/DeltaCommand/#gettouchedfile","title":"getTouchedFile <pre><code>getTouchedFile(\n  basePath: Path,\n  filePath: String,\n  nameToAddFileMap: Map[String, AddFile]): AddFile\n</code></pre> <p><code>getTouchedFile</code>...FIXME</p> <p><code>getTouchedFile</code> is used when:</p> <ul> <li> <p><code>DeltaCommand</code> is requested to removeFilesFromPaths and create a HadoopFsRelation (for DeleteCommand and UpdateCommand commands)</p> </li> <li> <p>MergeIntoCommand is executed</p> </li> </ul>","text":""},{"location":"commands/DeltaCommand/#iscatalogtable","title":"isCatalogTable <pre><code>isCatalogTable(\n  analyzer: Analyzer,\n  tableIdent: TableIdentifier): Boolean\n</code></pre> <p><code>isCatalogTable</code>...FIXME</p> <p><code>isCatalogTable</code> is used when:</p> <ul> <li><code>ConvertToDeltaCommandBase</code> is requested to isCatalogTable</li> </ul>","text":""},{"location":"commands/DeltaCommand/#ispathidentifier","title":"isPathIdentifier <pre><code>isPathIdentifier(\n  tableIdent: TableIdentifier): Boolean\n</code></pre> <p><code>isPathIdentifier</code>...FIXME</p> <p><code>isPathIdentifier</code> is used when...FIXME</p>","text":""},{"location":"commands/DeltaCommand/#commitlarge","title":"commitLarge <pre><code>commitLarge(\n  spark: SparkSession,\n  txn: OptimisticTransaction,\n  actions: Iterator[Action],\n  op: DeltaOperations.Operation,\n  context: Map[String, String],\n  metrics: Map[String, String]): Long\n</code></pre> <p><code>commitLarge</code>...FIXME</p> <p><code>commitLarge</code> is used when:</p> <ul> <li>ConvertToDeltaCommand command is executed (and requested to performConvert)</li> <li>RestoreTableCommand command is executed</li> </ul>","text":""},{"location":"commands/DeltaCommand/#updateandcheckpoint","title":"updateAndCheckpoint <pre><code>updateAndCheckpoint(\n  spark: SparkSession,\n  deltaLog: DeltaLog,\n  commitSize: Int,\n  attemptVersion: Long): Unit\n</code></pre> <p><code>updateAndCheckpoint</code> requests the given DeltaLog to update.</p> <p><code>updateAndCheckpoint</code> prints out the following INFO message to the logs:</p> <pre><code>Committed delta #[attemptVersion] to [logPath]. Wrote [commitSize] actions.\n</code></pre> <p>In the end, <code>updateAndCheckpoint</code> requests the given DeltaLog to checkpoint the current snapshot.</p>","text":""},{"location":"commands/DeltaCommand/#illegalstateexception","title":"IllegalStateException <p><code>updateAndCheckpoint</code> throws an <code>IllegalStateException</code> if the version after update does not match the assumed <code>attemptVersion</code>:</p> <pre><code>The committed version is [attemptVersion] but the current version is [currentSnapshot].\n</code></pre>","text":""},{"location":"commands/DeltaCommand/#logging","title":"Logging <p><code>DeltaCommand</code> is an abstract class and logging is configured using the logger of the implementations.</p>","text":""},{"location":"commands/WriteIntoDelta/","title":"WriteIntoDelta Command","text":"<p><code>WriteIntoDelta</code> is a Delta command that can write data(frame) transactionally into a delta table.</p> <p><code>WriteIntoDelta</code> is a <code>RunnableCommand</code> (Spark SQL) logical operator.</p>"},{"location":"commands/WriteIntoDelta/#creating-instance","title":"Creating Instance","text":"<p><code>WriteIntoDelta</code> takes the following to be created:</p> <ul> <li> DeltaLog <li> <code>SaveMode</code> <li> DeltaOptions <li> Names of the partition columns <li>Configuration</li> <li> Data (<code>DataFrame</code>) <p><code>WriteIntoDelta</code> is created\u00a0when:</p> <ul> <li><code>DeltaLog</code> is requested to create an insertable HadoopFsRelation (when <code>DeltaDataSource</code> is requested to create a relation as a CreatableRelationProvider or a RelationProvider)</li> <li><code>DeltaCatalog</code> is requested to createDeltaTable</li> <li><code>WriteIntoDeltaBuilder</code> is requested to buildForV1Write</li> <li><code>CreateDeltaTableCommand</code> command is executed</li> <li><code>DeltaDataSource</code> is requested to create a relation (for writing) (as a CreatableRelationProvider)</li> </ul>"},{"location":"commands/WriteIntoDelta/#configuration","title":"Configuration <p><code>WriteIntoDelta</code> is given a <code>configuration</code> when created as follows:</p> <ul> <li>Always empty for DeltaLog</li> <li>Always empty for DeltaDataSource</li> <li>Existing properties of a delta table in DeltaCatalog (with the <code>comment</code> key based on the value in the catalog)</li> <li>Existing configuration (of the Metadata of the Snapshot of the DeltaLog) for WriteIntoDeltaBuilder</li> <li>Existing properties of a delta table for CreateDeltaTableCommand (with the <code>comment</code> key based on the value in the catalog)</li> </ul>","text":""},{"location":"commands/WriteIntoDelta/#implicitmetadataoperation","title":"ImplicitMetadataOperation","text":"<p><code>WriteIntoDelta</code> is an operation that can update metadata (schema and partitioning) of the delta table.</p>"},{"location":"commands/WriteIntoDelta/#executing-command","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code>\u00a0is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p> <p><code>run</code> requests the DeltaLog to start a new transaction.</p> <p><code>run</code> writes and requests the <code>OptimisticTransaction</code> to commit (with <code>DeltaOperations.Write</code> operation with the SaveMode, partition columns, replaceWhere and userMetadata).</p>","text":""},{"location":"commands/WriteIntoDelta/#write","title":"write <pre><code>write(\n  txn: OptimisticTransaction,\n  sparkSession: SparkSession): Seq[Action]\n</code></pre> <p><code>write</code> checks out whether the write operation is to a delta table that already exists. If so (i.e. the readVersion of the transaction is above <code>-1</code>), <code>write</code> branches per the SaveMode:</p> <ul> <li> <p>For <code>ErrorIfExists</code>, <code>write</code> throws an <code>AnalysisException</code>.</p> <pre><code>[path] already exists.\n</code></pre> </li> <li> <p>For <code>Ignore</code>, <code>write</code> does nothing and returns back with no Actions.</p> </li> <li> <p>For <code>Overwrite</code>, <code>write</code> requests the DeltaLog to assert being removable</p> </li> </ul> <p><code>write</code> updateMetadata (with rearrangeOnly option).</p> <p><code>write</code>...FIXME</p> <p><code>write</code>\u00a0is used when:</p> <ul> <li>CreateDeltaTableCommand is executed</li> <li><code>WriteIntoDelta</code> is executed</li> </ul>","text":""},{"location":"commands/WriteIntoDelta/#demo","title":"Demo <pre><code>import org.apache.spark.sql.delta.commands.WriteIntoDelta\nimport org.apache.spark.sql.delta.DeltaLog\nimport org.apache.spark.sql.SaveMode\nimport org.apache.spark.sql.delta.DeltaOptions\nval tableName = \"/tmp/delta/t1\"\nval data = spark.range(5).toDF\nval writeCmd = WriteIntoDelta(\n  deltaLog = DeltaLog.forTable(spark, tableName),\n  mode = SaveMode.Overwrite,\n  options = new DeltaOptions(Map.empty[String, String], spark.sessionState.conf),\n  partitionColumns = Seq.empty[String],\n  configuration = Map.empty[String, String],\n  data)\n\n// Review web UI @ http://localhost:4040\n\nwriteCmd.run(spark)\n</code></pre>","text":""},{"location":"commands/WriteIntoDelta/#canoverwriteschema","title":"canOverwriteSchema <pre><code>canOverwriteSchema: Boolean\n</code></pre> <p><code>canOverwriteSchema</code> is part of the ImplicitMetadataOperation abstraction.</p>  <p><code>canOverwriteSchema</code> is <code>true</code> when all the following hold:</p> <ol> <li>canOverwriteSchema is enabled (<code>true</code>) (in the DeltaOptions)</li> <li>This <code>WriteIntoDelta</code> is overwrite operation</li> <li>replaceWhere option is not defined (in the DeltaOptions)</li> </ol>","text":""},{"location":"commands/WriteIntoDelta/#isoverwriteoperation","title":"isOverwriteOperation <pre><code>isOverwriteOperation: Boolean\n</code></pre> <p><code>isOverwriteOperation</code> is <code>true</code> for the SaveMode to be <code>SaveMode.Overwrite</code>.</p> <p><code>isOverwriteOperation</code> is used when:</p> <ul> <li><code>WriteIntoDelta</code> is requested for the canOverwriteSchema and to write</li> </ul>","text":""},{"location":"commands/WriteIntoDelta/#extractconstraints","title":"extractConstraints <pre><code>extractConstraints(\n  sparkSession: SparkSession,\n  expr: Seq[Expression]): Seq[Constraint]\n</code></pre> <p><code>extractConstraints</code>...FIXME</p> <p><code>extractConstraints</code> is used when:</p> <ul> <li><code>WriteIntoDelta</code> is requested to write data out (with <code>SaveMode.Overwrite</code> mode with replaceWhere option)</li> </ul>","text":""},{"location":"commands/alter/","title":"ALTER TABLE Commands","text":"<p>Delta Lake supports altering delta tables using <code>ALTER TABLE</code> high-level operators:</p> <ul> <li>ADD COLUMNS</li> <li>ADD CONSTRAINT</li> <li>CHANGE COLUMN</li> <li>AlterTableDropColumnsDeltaCommand</li> <li>DROP CONSTRAINT</li> <li>AlterTableReplaceColumnsDeltaCommand</li> <li>AlterTableSetPropertiesDeltaCommand</li> <li>AlterTableUnsetPropertiesDeltaCommand</li> </ul>"},{"location":"commands/alter/AlterDeltaTableCommand/","title":"AlterDeltaTableCommand","text":"<p><code>AlterDeltaTableCommand</code> is an extension of the DeltaCommand abstraction for Delta commands that alter a DeltaTableV2.</p>"},{"location":"commands/alter/AlterDeltaTableCommand/#contract","title":"Contract","text":""},{"location":"commands/alter/AlterDeltaTableCommand/#table","title":"table <pre><code>table: DeltaTableV2\n</code></pre> <p>DeltaTableV2</p> <p>Used when:</p> <ul> <li><code>AlterDeltaTableCommand</code> is requested to startTransaction</li> </ul>","text":""},{"location":"commands/alter/AlterDeltaTableCommand/#implementations","title":"Implementations","text":"<ul> <li>AlterTableAddColumnsDeltaCommand</li> <li>AlterTableAddConstraintDeltaCommand</li> <li>AlterTableChangeColumnDeltaCommand</li> <li>AlterTableDropConstraintDeltaCommand</li> <li>AlterTableReplaceColumnsDeltaCommand</li> <li>AlterTableSetLocationDeltaCommand</li> <li>AlterTableSetPropertiesDeltaCommand</li> <li>AlterTableUnsetPropertiesDeltaCommand</li> </ul>"},{"location":"commands/alter/AlterDeltaTableCommand/#starttransaction","title":"startTransaction <pre><code>startTransaction(): OptimisticTransaction\n</code></pre> <p><code>startTransaction</code> simply requests the DeltaTableV2 for the DeltaLog that in turn is requested to startTransaction.</p>","text":""},{"location":"commands/alter/AlterDeltaTableCommand/#checking-dependent-expressions","title":"Checking Dependent Expressions <pre><code>checkDependentExpressions(\n  sparkSession: SparkSession,\n  columnParts: Seq[String],\n  newMetadata: actions.Metadata,\n  protocol: Protocol,\n  operationName: String): Unit\n</code></pre> <p><code>checkDependentExpressions</code> skips execution when spark.databricks.delta.alterTable.changeColumn.checkExpressions configuration property is disabled (<code>false</code>).</p> <p><code>checkDependentExpressions</code> checks if the column to change (<code>columnParts</code>) is referenced by check constraints or generated columns (and throws an <code>AnalysisException</code> if there are any).</p>  <p><code>checkDependentExpressions</code> is used when:</p> <ul> <li>AlterTableDropColumnsDeltaCommand and AlterTableChangeColumnDeltaCommand are executed</li> </ul>","text":""},{"location":"commands/alter/AlterDeltaTableCommand/#check-constraints","title":"Check Constraints <p><code>checkDependentExpressions</code> findDependentConstraints (with the given<code>columnParts</code> and the newMetadata) and throws an AnalysisException if there are any:</p> <pre><code>Cannot [operationName] column [columnName] because this column is referenced by the following check constraint(s):\n    [constraints]\n</code></pre>","text":""},{"location":"commands/alter/AlterDeltaTableCommand/#generated-columns","title":"Generated Columns <p><code>checkDependentExpressions</code> findDependentGeneratedColumns (with the given<code>columnParts</code>, the newMetadata and protocol) and throws an AnalysisException if there are any:</p> <pre><code>Cannot [operationName] column [columnName] because this column is referenced by the following generated column(s):\n    [fieldNames]\n</code></pre>","text":""},{"location":"commands/alter/AlterTableAddColumnsDeltaCommand/","title":"AlterTableAddColumnsDeltaCommand","text":"<p><code>AlterTableAddColumnsDeltaCommand</code> is a <code>LeafRunnableCommand</code> (Spark SQL) that represents <code>ALTER TABLE ADD COLUMNS</code> SQL command.</p> <p>Note</p> <p><code>AlterTableAddColumnsDeltaCommand</code> is a variant of Spark SQL's AlterTableAddColumnsCommand for Delta Lake to support <code>ALTER TABLE ADD COLUMNS</code> command.</p> <p>Otherwise, Spark SQL would throw an AnalysisException.</p> <p><code>AlterTableAddColumnsDeltaCommand</code> is an AlterDeltaTableCommand.</p>"},{"location":"commands/alter/AlterTableAddColumnsDeltaCommand/#creating-instance","title":"Creating Instance","text":"<p><code>AlterTableAddColumnsDeltaCommand</code> takes the following to be created:</p> <ul> <li> DeltaTableV2 <li> Columns to Add <p><code>AlterTableAddColumnsDeltaCommand</code> is created when:</p> <ul> <li><code>DeltaCatalog</code> is requested to alter a table</li> </ul>"},{"location":"commands/alter/AlterTableAddColumnsDeltaCommand/#ignorecacheddata","title":"IgnoreCachedData <p><code>AlterTableAddColumnsDeltaCommand</code> is an <code>IgnoreCachedData</code> (Spark SQL) logical operator.</p>","text":""},{"location":"commands/alter/AlterTableAddColumnsDeltaCommand/#executing-command","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> starts a transaction and requests it for the current Metadata (of the DeltaTableV2).</p> <p><code>run</code> alters the current schema (creates a new metadata) and notifies the transaction.</p> <p>In the end, <code>run</code> commits the transaction (as ADD COLUMNS operation).</p>  <p><code>run</code> is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p>","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/","title":"AlterTableAddConstraintDeltaCommand","text":"<p><code>AlterTableAddConstraintDeltaCommand</code> is a transactional AlterDeltaTableCommand to register a new CHECK constraint (when altering a delta table with AddConstraint table changes).</p> <p><code>AlterTableAddConstraintDeltaCommand</code> represents the ALTER TABLE ADD CONSTRAINT SQL command.</p>"},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#creating-instance","title":"Creating Instance","text":"<p><code>AlterTableAddConstraintDeltaCommand</code> takes the following to be created:</p> <ul> <li> DeltaTableV2 <li> Constraint Name <li> Constraint SQL Expression (text) <p><code>AlterTableAddConstraintDeltaCommand</code> is created\u00a0when:</p> <ul> <li><code>DeltaCatalog</code> is requested to alter a delta table (with AddConstraint table changes)</li> </ul>"},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#executing-command","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code>\u00a0is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p>","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#begin-transaction","title":"Begin Transaction <p><code>run</code> starts a transaction.</p>","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#update-metadata","title":"Update Metadata <p><code>run</code> creates a new Metadata with the configuration altered to include the name and the given exprText.</p>","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#check-existing-data-full-scan","title":"Check Existing Data (Full Scan) <p><code>run</code> prints out the following INFO message to the logs:</p> <pre><code>Checking that [exprText] is satisfied for existing data.\nThis will require a full table scan.\n</code></pre> <p><code>run</code> creates a <code>DataFrame</code> that represents the version of the delta table (based on the Snapshot and the AddFiles). <code>run</code> counts the records that satisfy the <code>WHERE</code> clause:</p> <pre><code>((NOT [expr]) OR ([expr] IS UNKNOWN))\n</code></pre>","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#commit-transaction","title":"Commit Transaction <p>With no rows violating the check constraint, <code>run</code> commits the transaction with the new Metadata and <code>ADD CONSTRAINT</code> operation (with the given name and exprText).</p> <p>In the end, <code>run</code> returns an empty collection.</p>","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#analysisexceptions","title":"AnalysisExceptions","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#illegal-constraint-name","title":"Illegal Constraint Name <p><code>run</code> throws an <code>AnalysisException</code> when the name of the table constraint is <code>__CHAR_VARCHAR_STRING_LENGTH_CHECK__</code>:</p> <pre><code>Cannot use '__CHAR_VARCHAR_STRING_LENGTH_CHECK__' as the name of a CHECK constraint.\n</code></pre>","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#constraint-already-exists","title":"Constraint Already Exists <p><code>run</code> throws an <code>AnalysisException</code> when the given name is already in use (by an existing constraint):</p> <pre><code>Constraint '[name]' already exists as a CHECK constraint. Please delete the old constraint first.\nOld constraint:\n[oldExpr]\n</code></pre>","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#constraint-non-boolean","title":"Constraint Non-Boolean <p><code>run</code> throws an <code>AnalysisException</code> when the given exprText does not produce a <code>BooleanType</code> value:</p> <pre><code>CHECK constraint '[name]' ([expr]) should be a boolean expression.\n</code></pre>","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#constraint-violation","title":"Constraint Violation <p><code>run</code> throws an <code>AnalysisException</code> when there are rows that violate the new constraint (with the name of the delta table and the exprText):</p> <pre><code>[num] rows in [name] violate the new CHECK constraint ([exprText])\n</code></pre>","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#runnablecommand","title":"RunnableCommand <p><code>AlterTableAddConstraintDeltaCommand</code> is a <code>LeafRunnableCommand</code> (Spark SQL) logical operator.</p>","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#ignorecacheddata","title":"IgnoreCachedData <p><code>AlterTableAddConstraintDeltaCommand</code> is a <code>IgnoreCachedData</code> (Spark SQL) logical operator.</p>","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#demo","title":"Demo","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#create-table","title":"Create Table <pre><code>sql(\"\"\"\nDROP TABLE IF EXISTS delta_demo;\n\"\"\")\n</code></pre> <pre><code>sql(\"\"\"\nCREATE TABLE delta_demo (id INT)\nUSING delta;\n\"\"\")\n</code></pre>","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#adding-constraint","title":"Adding Constraint <p>Enable logging to see the following INFO message in the logs:</p> <pre><code>Checking that id &gt; 5 is satisfied for existing data.\nThis will require a full table scan.\n</code></pre> <pre><code>sql(\"\"\"\nALTER TABLE delta_demo\nADD CONSTRAINT demo_check_constraint CHECK (id &gt; 5);\n\"\"\")\n</code></pre>","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#desc-history","title":"DESC HISTORY <pre><code>sql(\"\"\"\nDESC HISTORY delta_demo;\n\"\"\")\n  .select('version, 'operation, 'operationParameters)\n  .show(truncate = false)\n</code></pre> <pre><code>+-------+--------------+-----------------------------------------------------------------------------+\n|version|operation     |operationParameters                                                          |\n+-------+--------------+-----------------------------------------------------------------------------+\n|1      |ADD CONSTRAINT|{name -&gt; demo_check_constraint, expr -&gt; id &gt; 5}                              |\n|0      |CREATE TABLE  |{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}|\n+-------+--------------+-----------------------------------------------------------------------------+\n</code></pre>","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#deltainvariantviolationexception","title":"DeltaInvariantViolationException <pre><code>sql(\"\"\"\nINSERT INTO delta_demo VALUES 3;\n\"\"\")\n</code></pre> <pre><code>DeltaInvariantViolationException: CHECK constraint demo_check_constraint (id &gt; 5) violated by row with values:\n - id : 3\n</code></pre>","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#metadata","title":"Metadata <p>CHECK constraints are stored in Metadata of a delta table.</p> <pre><code>sql(\"\"\"\nSHOW TBLPROPERTIES delta_demo;\n\"\"\")\n  .where('key startsWith \"delta.constraints.\")\n  .show(truncate = false)\n</code></pre> <pre><code>+---------------------------------------+------+\n|key                                    |value |\n+---------------------------------------+------+\n|delta.constraints.demo_check_constraint|id &gt; 5|\n+---------------------------------------+------+\n</code></pre>","text":""},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.commands.AlterTableAddConstraintDeltaCommand</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.delta.commands.AlterTableAddConstraintDeltaCommand=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"commands/alter/AlterTableChangeColumnDeltaCommand/","title":"AlterTableChangeColumnDeltaCommand","text":"<p><code>AlterTableChangeColumnDeltaCommand</code> is an AlterDeltaTableCommand to change (alter) the name, the comment, the nullability, the position and the data type of a column (of a DeltaTableV2).</p> <p><code>AlterTableChangeColumnDeltaCommand</code> is used when DeltaCatalog.alterTable is requested to execute <code>ColumnChange</code>s (Spark SQL).</p> ColumnChange SQL <code>RenameColumn</code> (Spark SQL) <code>ALTER TABLE RENAME COLUMN</code> (Spark SQL) <code>UpdateColumnComment</code> <code>ALTER TABLE CHANGE COLUMN COMMENT</code> <code>UpdateColumnNullability</code> <code>ALTER TABLE CHANGE COLUMN (SET | DROP) NOT NULL</code> <code>UpdateColumnPosition</code> <code>ALTER TABLE CHANGE COLUMN (FIRST | AFTER)</code> <code>UpdateColumnType</code> <code>ALTER TABLE CHANGE COLUMN TYPE</code>"},{"location":"commands/alter/AlterTableChangeColumnDeltaCommand/#creating-instance","title":"Creating Instance","text":"<p><code>AlterTableChangeColumnDeltaCommand</code> takes the following to be created:</p> <ul> <li> DeltaTableV2 <li> Column Path <li> Column Name <li> New Column (as StructField) <li> <code>ColumnPosition</code> (optional) <li> (unused) <code>syncIdentity</code> flag <p><code>AlterTableChangeColumnDeltaCommand</code> is created when:</p> <ul> <li><code>DeltaCatalog</code> is requested to alter a table</li> </ul>"},{"location":"commands/alter/AlterTableChangeColumnDeltaCommand/#executing-command","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> starts a transaction.</p> <p><code>run</code> asserts that the column is available (in the schema of the metadata of the transaction).</p> <p><code>run</code> transforms the current (old) schema to a new one based on the column changes (given by newColumn). As part of the changes, for every column, <code>run</code> checks whether column mapping is used to determine the column name.</p> <p><code>run</code>...FIXME</p> <p><code>run</code> updates the metadata (for the active transaction).</p> <p><code>run</code> commits the transaction with the following operation (and  no actions):</p> <ol> <li>RENAME COLUMN when the name of the given <code>newColumn</code> is different from the given <code>columnName</code></li> <li>CHANGE COLUMN, otherwise</li> </ol> <p>In the end, <code>run</code> returns an empty collection.</p>","text":""},{"location":"commands/alter/AlterTableChangeColumnDeltaCommand/#updating-metadata","title":"Updating Metadata <p>In the end, <code>run</code> requests the <code>OptimisticTransaction</code> to update the Metadata (with the new <code>newMetadata</code>) and commit (with no Actions and ChangeColumn operation).</p>  <p><code>run</code> is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p>","text":""},{"location":"commands/alter/AlterTableChangeColumnDeltaCommand/#leafrunnablecommand","title":"LeafRunnableCommand <p><code>AlterTableChangeColumnDeltaCommand</code> is a <code>LeafRunnableCommand</code> (Spark SQL).</p>","text":""},{"location":"commands/alter/AlterTableChangeColumnDeltaCommand/#ignorecacheddata","title":"IgnoreCachedData <p><code>AlterTableChangeColumnDeltaCommand</code> is a <code>IgnoreCachedData</code> (Spark SQL).</p>","text":""},{"location":"commands/alter/AlterTableDropColumnsDeltaCommand/","title":"AlterTableDropColumnsDeltaCommand","text":"<p><code>AlterTableDropColumnsDeltaCommand</code> is...FIXME</p>"},{"location":"commands/alter/AlterTableDropConstraintDeltaCommand/","title":"AlterTableDropConstraintDeltaCommand","text":"<p><code>AlterTableDropConstraintDeltaCommand</code> is...FIXME</p>"},{"location":"commands/alter/AlterTableReplaceColumnsDeltaCommand/","title":"AlterTableReplaceColumnsDeltaCommand","text":"<p><code>AlterTableReplaceColumnsDeltaCommand</code> is an AlterDeltaTableCommand.</p> <p>Danger</p> <p><code>AlterTableReplaceColumnsDeltaCommand</code> seems to be no longer used and obsolete by AlterTableChangeColumnDeltaCommand that handles all <code>ColumnChange</code>s (incl. <code>RenameColumn</code>).</p>"},{"location":"commands/alter/AlterTableReplaceColumnsDeltaCommand/#creating-instance","title":"Creating Instance","text":"<p><code>AlterTableReplaceColumnsDeltaCommand</code> takes the following to be created:</p> <ul> <li> DeltaTableV2 <li> Columns (as StructFields)"},{"location":"commands/alter/AlterTableReplaceColumnsDeltaCommand/#leafrunnablecommand","title":"LeafRunnableCommand <p><code>AlterTableReplaceColumnsDeltaCommand</code> is a <code>LeafRunnableCommand</code> (Spark SQL).</p>","text":""},{"location":"commands/alter/AlterTableReplaceColumnsDeltaCommand/#ignorecacheddata","title":"IgnoreCachedData <p><code>AlterTableReplaceColumnsDeltaCommand</code> is a <code>IgnoreCachedData</code> (Spark SQL).</p>","text":""},{"location":"commands/alter/AlterTableSetLocationDeltaCommand/","title":"AlterTableSetLocationDeltaCommand","text":"<p><code>AlterTableSetLocationDeltaCommand</code> is...FIXME</p>"},{"location":"commands/alter/AlterTableSetPropertiesDeltaCommand/","title":"AlterTableSetPropertiesDeltaCommand","text":"<p><code>AlterTableSetPropertiesDeltaCommand</code> is a transactional AlterDeltaTableCommand for <code>SetProperty</code> table changes (when altering a delta table).</p>"},{"location":"commands/alter/AlterTableSetPropertiesDeltaCommand/#creating-instance","title":"Creating Instance","text":"<p><code>AlterTableSetPropertiesDeltaCommand</code> takes the following to be created:</p> <ul> <li> DeltaTableV2 <li> Configuration (<code>Map[String, String]</code>) <p><code>AlterTableSetPropertiesDeltaCommand</code> is created\u00a0when:</p> <ul> <li><code>DeltaCatalog</code> is requested to alterTable (with <code>SetProperty</code> table changes)</li> </ul>"},{"location":"commands/alter/AlterTableSetPropertiesDeltaCommand/#executing-command","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p>","text":""},{"location":"commands/alter/AlterTableSetPropertiesDeltaCommand/#begin-transaction","title":"Begin Transaction <p><code>run</code> starts a transaction.</p>","text":""},{"location":"commands/alter/AlterTableSetPropertiesDeltaCommand/#update-metadata","title":"Update Metadata <p><code>run</code> requests the transaction for the table metadata and does sanity check to prevent illegal changes (on reserved properties):</p> <ul> <li>Throws <code>AnalysisException</code>s for the following:<ul> <li><code>delta.constraints.</code>-prefixed properties (handled by ALTER TABLE ADD CONSTRAINT)</li> <li><code>location</code></li> <li><code>provider</code></li> </ul> </li> <li>Filters out <code>comment</code> property changes (as it is used later for the description)</li> </ul> <p><code>run</code> creates a new Metadata with the following:</p> <ul> <li>Overwrites description with <code>comment</code> property if defined in the given configuration</li> <li>Overwrites the current configuration of the delta table with the property changes</li> </ul> <p><code>run</code> updates the metadata (of the transaction).</p>","text":""},{"location":"commands/alter/AlterTableSetPropertiesDeltaCommand/#commit-transaction","title":"Commit Transaction <p><code>run</code> commits the transaction with <code>SET TBLPROPERTIES</code> operation (with the given configuration) and no actions.</p> <p>In the end, <code>run</code> returns an empty collection.</p>","text":""},{"location":"commands/alter/AlterTableSetPropertiesDeltaCommand/#runnablecommand","title":"RunnableCommand <p><code>AlterTableSetPropertiesDeltaCommand</code> is a <code>LeafRunnableCommand</code> (Spark SQL) logical operator.</p>","text":""},{"location":"commands/alter/AlterTableSetPropertiesDeltaCommand/#ignorecacheddata","title":"IgnoreCachedData <p><code>AlterTableSetPropertiesDeltaCommand</code> is a <code>IgnoreCachedData</code> (Spark SQL) logical operator.</p>","text":""},{"location":"commands/alter/AlterTableSetPropertiesDeltaCommand/#demo","title":"Demo <pre><code>sql(\"\"\"\nDROP TABLE IF EXISTS delta_demo;\n\"\"\")\n</code></pre> <pre><code>sql(\"\"\"\nCREATE TABLE delta_demo (id INT)\nUSING delta;\n\"\"\")\n</code></pre> <pre><code>sql(\"\"\"\nDESC HISTORY delta_demo;\n\"\"\")\n  .select('version, 'operation, 'operationParameters)\n  .show(truncate = false)\n</code></pre> <pre><code>+-------+------------+-----------------------------------------------------------------------------+\n|version|operation   |operationParameters                                                          |\n+-------+------------+-----------------------------------------------------------------------------+\n|0      |CREATE TABLE|{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}|\n+-------+------------+-----------------------------------------------------------------------------+\n</code></pre> <pre><code>sql(\"\"\"\nALTER TABLE delta_demo\nSET TBLPROPERTIES (delta.enableChangeDataFeed = true);\n\"\"\")\n</code></pre> <pre><code>sql(\"\"\"\nDESC HISTORY delta_demo;\n\"\"\")\n  .select('version, 'operation, 'operationParameters)\n  .show(truncate = false)\n</code></pre> <pre><code>+-------+-----------------+-----------------------------------------------------------------------------+\n|version|operation        |operationParameters                                                          |\n+-------+-----------------+-----------------------------------------------------------------------------+\n|1      |SET TBLPROPERTIES|{properties -&gt; {\"delta.enableChangeDataFeed\":\"true\"}}                        |\n|0      |CREATE TABLE     |{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}|\n+-------+-----------------+-----------------------------------------------------------------------------+\n</code></pre>","text":""},{"location":"commands/alter/AlterTableUnsetPropertiesDeltaCommand/","title":"AlterTableUnsetPropertiesDeltaCommand","text":"<p><code>AlterTableUnsetPropertiesDeltaCommand</code> is...FIXME</p>"},{"location":"commands/convert/","title":"Convert to Delta Command","text":"<p>Delta Lake supports converting (importing) parquet tables to delta format using the following high-level operators:</p> <ul> <li>CONVERT TO DELTA SQL command</li> <li>DeltaTable.convertToDelta</li> </ul>"},{"location":"commands/convert/ConvertTargetTable/","title":"ConvertTargetTable","text":"<p><code>ConvertTargetTable</code> is...FIXME</p>"},{"location":"commands/convert/ConvertToDeltaCommand/","title":"ConvertToDeltaCommand (ConvertToDeltaCommandBase)","text":"<p><code>ConvertToDeltaCommand</code> is a DeltaCommand that converts a parquet table to delta format (imports it into Delta).</p> <p><code>ConvertToDeltaCommand</code> is a <code>RunnableCommand</code> (Spark SQL).</p> <p><code>ConvertToDeltaCommand</code> requires that the partition schema matches the partitions of the parquet table (or an AnalysisException is thrown)</p>"},{"location":"commands/convert/ConvertToDeltaCommand/#creating-instance","title":"Creating Instance","text":"<p><code>ConvertToDeltaCommand</code> takes the following to be created:</p> <ul> <li> Parquet table (<code>TableIdentifier</code>) <li> Partition schema (<code>Option[StructType]</code>) <li> Delta Path (<code>Option[String]</code>) <p><code>ConvertToDeltaCommand</code> is created when:</p> <ul> <li>CONVERT TO DELTA statement is used (and <code>DeltaSqlAstBuilder</code> is requested to visitConvert)</li> <li>DeltaTable.convertToDelta utility is used (and <code>DeltaConvert</code> utility is used to executeConvert)</li> </ul>"},{"location":"commands/convert/ConvertToDeltaCommand/#executing-command","title":"Executing Command <pre><code>run(\n  spark: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> is part of the <code>RunnableCommand</code> (Spark SQL) contract.</p> <p><code>run</code> creates a ConvertProperties from the TableIdentifier (with the given <code>SparkSession</code>).</p> <p><code>run</code> makes sure that the (data source) provider (the database part of the TableIdentifier) is either <code>delta</code> or <code>parquet</code>. For all other data source providers, <code>run</code> throws an <code>AnalysisException</code>:</p> <pre><code>CONVERT TO DELTA only supports parquet tables, but you are trying to convert a [sourceName] source: [ident]\n</code></pre> <p>For <code>delta</code> data source provider, <code>run</code> simply prints out the following message to standard output and returns.</p> <pre><code>The table you are trying to convert is already a delta table\n</code></pre> <p>For <code>parquet</code> data source provider, <code>run</code> uses <code>DeltaLog</code> utility to create a DeltaLog. <code>run</code> then requests <code>DeltaLog</code> to update and start a new transaction. In the end, <code>run</code> performConvert.</p> <p>In case the readVersion of the new transaction is greater than <code>-1</code>, <code>run</code> simply prints out the following message to standard output and returns.</p> <pre><code>The table you are trying to convert is already a delta table\n</code></pre>","text":""},{"location":"commands/convert/ConvertToDeltaCommand/#performconvert","title":"performConvert <pre><code>performConvert(\n  spark: SparkSession,\n  txn: OptimisticTransaction,\n  convertProperties: ConvertTarget): Seq[Row]\n</code></pre> <p><code>performConvert</code> makes sure that the directory exists (from the given <code>ConvertProperties</code> which is the table part of the TableIdentifier of the command).</p> <p><code>performConvert</code> requests the <code>OptimisticTransaction</code> for the DeltaLog that is then requested to ensureLogDirectoryExist.</p> <p><code>performConvert</code> creates a Dataset to recursively list directories and files in the directory and leaves only files (by filtering out directories using <code>WHERE</code> clause).</p>  <p>Note</p> <p><code>performConvert</code> uses <code>Dataset</code> API to build a distributed computation to query files.</p>  <p> <code>performConvert</code> caches the <code>Dataset</code> of file names. <p> <code>performConvert</code> uses spark.databricks.delta.import.batchSize.schemaInference configuration property for the number of files per batch for schema inference. <code>performConvert</code> mergeSchemasInParallel for every batch of files and then mergeSchemas. <p><code>performConvert</code> constructTableSchema using the inferred table schema and the partitionSchema (if specified).</p> <p><code>performConvert</code> creates a new Metadata using the table schema and the partitionSchema (if specified).</p> <p><code>performConvert</code> requests the <code>OptimisticTransaction</code> to update the metadata.</p> <p> <code>performConvert</code> uses spark.databricks.delta.import.batchSize.statsCollection configuration property for the number of files per batch for stats collection. <code>performConvert</code> creates an AddFile (in the data path of the DeltaLog of the <code>OptimisticTransaction</code>) for every file in a batch. <p> In the end, <code>performConvert</code> streamWrite (with the <code>OptimisticTransaction</code>, the <code>AddFile</code>s, and Convert operation) and unpersists the <code>Dataset</code> of file names.","text":""},{"location":"commands/convert/ConvertToDeltaCommand/#checkcolumnmapping","title":"checkColumnMapping <pre><code>checkColumnMapping(\n  txnMetadata: Metadata,\n  convertTargetTable: ConvertTargetTable): Unit\n</code></pre> <p><code>checkColumnMapping</code> throws a DeltaColumnMappingUnsupportedException when the requiredColumnMappingMode of the given ConvertTargetTable is not DeltaColumnMappingMode of the given Metadata.</p>","text":""},{"location":"commands/convert/ConvertToDeltaCommand/#streamwrite","title":"streamWrite <pre><code>streamWrite(\n  spark: SparkSession,\n  txn: OptimisticTransaction,\n  addFiles: Iterator[AddFile],\n  op: DeltaOperations.Operation,\n  numFiles: Long): Long\n</code></pre> <p><code>streamWrite</code>...FIXME</p>","text":""},{"location":"commands/convert/ConvertToDeltaCommand/#createaddfile","title":"createAddFile <pre><code>createAddFile(\n  file: SerializableFileStatus,\n  basePath: Path,\n  fs: FileSystem,\n  conf: SQLConf): AddFile\n</code></pre> <p><code>createAddFile</code> creates an AddFile action.</p> <p>Internally, <code>createAddFile</code>...FIXME</p> <p> <code>createAddFile</code> throws an <code>AnalysisException</code> if the number of fields in the given partition schema does not match the number of partitions found (at partition discovery phase): <pre><code>Expecting [size] partition column(s): [expectedCols], but found [size] partition column(s): [parsedCols] from parsing the file name: [path]\n</code></pre>","text":""},{"location":"commands/convert/ConvertToDeltaCommand/#mergeschemasinparallel","title":"mergeSchemasInParallel <pre><code>mergeSchemasInParallel(\n  sparkSession: SparkSession,\n  filesToTouch: Seq[FileStatus],\n  serializedConf: SerializableConfiguration): Option[StructType]\n</code></pre> <p><code>mergeSchemasInParallel</code>...FIXME</p>","text":""},{"location":"commands/convert/ConvertToDeltaCommand/#constructtableschema","title":"constructTableSchema <pre><code>constructTableSchema(\n  spark: SparkSession,\n  dataSchema: StructType,\n  partitionFields: Seq[StructField]): StructType\n</code></pre> <p><code>constructTableSchema</code>...FIXME</p>","text":""},{"location":"commands/convert/ConvertToDeltaCommand/#converttodeltacommandbase","title":"ConvertToDeltaCommandBase <p><code>ConvertToDeltaCommandBase</code> is the base of <code>ConvertToDeltaCommand</code>-like commands with the only known implementation being <code>ConvertToDeltaCommand</code> itself.</p>","text":""},{"location":"commands/convert/ConvertToDeltaCommand/#iscatalogtable","title":"isCatalogTable <pre><code>isCatalogTable(\n  analyzer: Analyzer,\n  tableIdent: TableIdentifier): Boolean\n</code></pre> <p><code>isCatalogTable</code>...FIXME</p> <p><code>isCatalogTable</code> is part of the DeltaCommand abstraction.</p>","text":""},{"location":"commands/convert/ConvertToDeltaCommand/#gettargettable","title":"getTargetTable <pre><code>getTargetTable(\n  spark: SparkSession,\n  target: ConvertTarget): ConvertTargetTable\n</code></pre> <p><code>getTargetTable</code>...FIXME</p> <p><code>getTargetTable</code> is used when:</p> <ul> <li><code>ConvertToDeltaCommandBase</code> is executed</li> </ul>","text":""},{"location":"commands/convert/DeltaConvert/","title":"DeltaConvert Utility","text":""},{"location":"commands/convert/DeltaConvert/#executeconvert","title":"executeConvert <pre><code>executeConvert(\n  spark: SparkSession,\n  tableIdentifier: TableIdentifier,\n  partitionSchema: Option[StructType],\n  deltaPath: Option[String]): DeltaTable\n</code></pre> <p><code>executeConvert</code> converts a parquet table to a delta table.</p> <p><code>executeConvert</code> executes a new ConvertToDeltaCommand.</p> <p>In the end, <code>executeConvert</code> creates a DeltaTable.</p>  <p>Note</p> <p><code>executeConvert</code> can convert a Spark table (to Delta) that is registered in a metastore.</p>  <p><code>executeConvert</code> is used when:</p> <ul> <li>DeltaTable.convertToDelta utility is used</li> </ul>","text":""},{"location":"commands/convert/DeltaConvert/#demo","title":"Demo <pre><code>import org.apache.spark.sql.SparkSession\nassert(spark.isInstanceOf[SparkSession])\n\n// CONVERT TO DELTA only supports parquet tables\n// TableIdentifier should be parquet.`users`\nimport org.apache.spark.sql.catalyst.TableIdentifier\nval table = TableIdentifier(table = \"users\", database = Some(\"parquet\"))\n\nimport org.apache.spark.sql.types.{StringType, StructField, StructType}\nval partitionSchema: Option[StructType] = Some(\n  new StructType().add(StructField(\"country\", StringType)))\n\nval deltaPath: Option[String] = None\n\n// Use web UI to monitor execution, e.g. http://localhost:4040\nimport io.delta.tables.execution.DeltaConvert\nDeltaConvert.executeConvert(spark, table, partitionSchema, deltaPath)\n</code></pre>","text":""},{"location":"commands/convert/FileManifest/","title":"FileManifest","text":"<p><code>FileManifest</code> is an abstraction of file manifests for ConvertToDeltaCommand.</p> <p><code>FileManifest</code> is <code>Closeable</code> (Java).</p>"},{"location":"commands/convert/FileManifest/#contract","title":"Contract","text":""},{"location":"commands/convert/FileManifest/#basepath","title":"basePath <pre><code>basePath: String\n</code></pre> <p>The base path of a delta table</p>","text":""},{"location":"commands/convert/FileManifest/#getfiles","title":"getFiles <pre><code>getFiles: Iterator[SerializableFileStatus]\n</code></pre> <p>The active files of a delta table</p> <p>Used when:</p> <ul> <li><code>ConvertToDeltaCommand</code> is requested to createDeltaActions and performConvert</li> </ul>","text":""},{"location":"commands/convert/FileManifest/#implementations","title":"Implementations","text":"<ul> <li>ManualListingFileManifest</li> <li>MetadataLogFileManifest</li> </ul>"},{"location":"commands/convert/ManualListingFileManifest/","title":"ManualListingFileManifest","text":"<p><code>ManualListingFileManifest</code> is a FileManifest.</p>"},{"location":"commands/convert/ManualListingFileManifest/#getfiles","title":"getFiles <pre><code>getFiles: Iterator[SerializableFileStatus]\n</code></pre> <p><code>getFiles</code>\u00a0is part of the FileManifest abstraction.</p> <p><code>getFiles</code>...FIXME</p>","text":""},{"location":"commands/convert/ManualListingFileManifest/#close","title":"close <pre><code>close\n</code></pre> <p><code>close</code>\u00a0is part of the <code>Closeable</code> (Java) abstraction.</p> <p><code>close</code>...FIXME</p>","text":""},{"location":"commands/convert/ManualListingFileManifest/#hdfs-filestatus-list-dataset","title":"HDFS FileStatus List Dataset <pre><code>list: Dataset[SerializableFileStatus]\n</code></pre> <p><code>list</code> creates a HDFS FileStatus dataset and marks it to be cached (once an action is executed).</p>  <p>Scala lazy value</p> <p><code>list</code> is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged.</p>  <p><code>list</code>\u00a0is used when:</p> <ul> <li><code>ManualListingFileManifest</code> is requested to getFiles and close</li> </ul>","text":""},{"location":"commands/convert/ManualListingFileManifest/#dolist","title":"doList <pre><code>doList(): Dataset[SerializableFileStatus]\n</code></pre> <p><code>doList</code>...FIXME</p> <p><code>doList</code>\u00a0is used when:</p> <ul> <li><code>ManualListingFileManifest</code> is requested for file status dataset</li> </ul>","text":""},{"location":"commands/convert/MetadataLogFileManifest/","title":"MetadataLogFileManifest","text":"<p><code>MetadataLogFileManifest</code> is...FIXME</p>"},{"location":"commands/convert/ParquetTable/","title":"ParquetTable","text":""},{"location":"commands/convert/ParquetTable/#converttargettable","title":"ConvertTargetTable <p><code>ParquetTable</code> is a ConvertTargetTable.</p>","text":""},{"location":"commands/convert/ParquetTable/#creating-instance","title":"Creating Instance <p><code>ParquetTable</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> <li> Base Path <li> Partition Schema (<code>Option[StructType]</code>)  <p><code>ParquetTable</code> is created when:</p> <ul> <li><code>ConvertToDeltaCommand</code> is requested to getTargetTable</li> </ul>","text":""},{"location":"commands/convert/ParquetTable/#numfiles","title":"numFiles <pre><code>numFiles: Long\n</code></pre> <p><code>numFiles</code> inferSchema when _numFiles registry is uninitialized.</p> <p>In the end, <code>numFiles</code> returns the value of _numFiles registry.</p> <p><code>numFiles</code> is part of the ConvertTargetTable abstraction.</p>","text":""},{"location":"commands/convert/ParquetTable/#_numfiles","title":"_numFiles <pre><code>_numFiles: Option[Long]\n</code></pre> <p><code>ParquetTable</code> defines <code>_numFiles</code> internal registry.</p> <p><code>_numFiles</code> is <code>None</code> (uninitialized) when <code>ParquetTable</code> is created.</p> <p><code>_numFiles</code> is initialized once when <code>ParquetTable</code> is requested for the numFiles (and inferSchema).</p> <p><code>_numFiles</code> is used for the numFiles.</p>","text":""},{"location":"commands/convert/ParquetTable/#inferschema","title":"inferSchema <pre><code>inferSchema(): Unit\n</code></pre> <p><code>inferSchema</code>...FIXME</p> <p><code>inferSchema</code> is used when:</p> <ul> <li><code>ParquetTable</code> is requested for the numFiles and tableSchema</li> </ul>","text":""},{"location":"commands/convert/ParquetTable/#getschemaforbatch","title":"getSchemaForBatch <pre><code>getSchemaForBatch(\n  spark: SparkSession,\n  batch: Seq[SerializableFileStatus],\n  serializedConf: SerializableConfiguration): StructType\n</code></pre> <p><code>getSchemaForBatch</code>...FIXME</p>","text":""},{"location":"commands/convert/ParquetTable/#mergeschemasinparallel","title":"mergeSchemasInParallel <pre><code>mergeSchemasInParallel(\n  sparkSession: SparkSession,\n  filesToTouch: Seq[FileStatus],\n  serializedConf: SerializableConfiguration): Option[StructType]\n</code></pre> <p><code>mergeSchemasInParallel</code>...FIXME</p>","text":""},{"location":"commands/delete/","title":"Delete Command","text":"<p>Delta Lake supports deleting records from delta tables using the following high-level operators:</p> <ul> <li>DELETE FROM SQL command</li> <li>DeltaTable.delete</li> </ul>"},{"location":"commands/delete/DeleteCommand/","title":"DeleteCommand","text":"<p><code>DeleteCommand</code> is a DeltaCommand that represents DeltaDelete logical command at execution.</p> <p><code>DeleteCommand</code> is a <code>LeafRunnableCommand</code> (Spark SQL) logical operator.</p>"},{"location":"commands/delete/DeleteCommand/#creating-instance","title":"Creating Instance","text":"<p><code>DeleteCommand</code> takes the following to be created:</p> <ul> <li> TahoeFileIndex <li> Target Data (LogicalPlan) <li> Condition (Expression) <p><code>DeleteCommand</code> is created (also using apply factory utility) when:</p> <ul> <li>PreprocessTableDelete logical resolution rule is executed (and resolves a DeltaDelete logical command)</li> </ul>"},{"location":"commands/delete/DeleteCommand/#performance-metrics","title":"Performance Metrics","text":"Signature <pre><code>metrics: Map[String, SQLMetric]\n</code></pre> <p><code>metrics</code> is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p> <p><code>metrics</code> creates the performance metrics.</p>"},{"location":"commands/delete/DeleteCommand/#executing-command","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p> <p><code>run</code> requests the TahoeFileIndex for the DeltaLog (and asserts that the table is removable).</p> <p><code>run</code> requests the <code>DeltaLog</code> to start a new transaction for performDelete.</p> <p>In the end, <code>run</code> re-caches all cached plans (incl. this relation itself) by requesting the <code>CacheManager</code> (Spark SQL) to recache the target.</p>","text":""},{"location":"commands/delete/DeleteCommand/#performdelete","title":"performDelete <pre><code>performDelete(\n  sparkSession: SparkSession,\n  deltaLog: DeltaLog,\n  txn: OptimisticTransaction): Unit\n</code></pre> <p><code>performDelete</code> is used when:</p> <ul> <li>DeleteCommand is executed</li> <li><code>WriteIntoDelta</code> is requested to removeFiles</li> </ul>","text":""},{"location":"commands/delete/DeleteCommand/#number-of-table-files","title":"Number of Table Files <p><code>performDelete</code> requests the given DeltaLog for the current Snapshot that is in turn requested for the number of files in the delta table.</p>","text":""},{"location":"commands/delete/DeleteCommand/#finding-delete-actions","title":"Finding Delete Actions <p><code>performDelete</code> branches off based on the optional condition:</p> <ol> <li>No condition to delete the whole table</li> <li>Condition defined on metadata only</li> <li>Other conditions</li> </ol>","text":""},{"location":"commands/delete/DeleteCommand/#delete-condition-undefined","title":"Delete Condition Undefined <p><code>performDelete</code>...FIXME</p>","text":""},{"location":"commands/delete/DeleteCommand/#metadata-only-delete-condition","title":"Metadata-Only Delete Condition <p><code>performDelete</code>...FIXME</p>","text":""},{"location":"commands/delete/DeleteCommand/#other-delete-conditions","title":"Other Delete Conditions <p><code>performDelete</code>...FIXME</p>","text":""},{"location":"commands/delete/DeleteCommand/#delete-actions-available","title":"Delete Actions Available <p><code>performDelete</code>...FIXME</p>","text":""},{"location":"commands/delete/DeleteCommand/#rewritefiles","title":"rewriteFiles <pre><code>rewriteFiles(\n  txn: OptimisticTransaction,\n  baseData: DataFrame,\n  filterCondition: Expression,\n  numFilesToRewrite: Long): Seq[FileAction]\n</code></pre> <p><code>rewriteFiles</code> reads the delta.enableChangeDataFeed table property of the delta table (from the Metadata of the given OptimisticTransaction).</p> <p><code>rewriteFiles</code> creates a <code>numTouchedRows</code> metric and a <code>numTouchedRowsUdf</code> UDF to count the number of rows that have been touched.</p> <p><code>rewriteFiles</code> creates a <code>DataFrame</code> to write (with the <code>numTouchedRowsUdf</code> UDF and the <code>filterCondition</code> column). The <code>DataFrame</code> can also include _change_type column (with <code>null</code> or <code>delete</code> values based on the <code>filterCondition</code>).</p> <p>In the end, <code>rewriteFiles</code> requests the given OptimisticTransaction to write the DataFrame.</p>","text":""},{"location":"commands/delete/DeleteCommand/#creating-deletecommand","title":"Creating DeleteCommand <pre><code>apply(\n  delete: DeltaDelete): DeleteCommand\n</code></pre> <p><code>apply</code> creates a DeleteCommand.</p>","text":""},{"location":"commands/delete/DeleteCommandMetrics/","title":"DeleteCommandMetrics","text":"<p><code>DeleteCommandMetrics</code> is a marker extension of the <code>LeafRunnableCommand</code> (Spark SQL) abstraction for delete commands with performance metrics.</p>"},{"location":"commands/delete/DeleteCommandMetrics/#performance-metrics","title":"Performance Metrics <pre><code>createMetrics: Map[String, SQLMetric]\n</code></pre>    Name web UI     <code>numRemovedFiles</code> number of files removed.   <code>numAddedFiles</code> number of files added.   <code>numDeletedRows</code> number of rows deleted.   <code>numFilesBeforeSkipping</code> number of files before skipping   <code>numBytesBeforeSkipping</code> number of bytes before skipping   <code>numFilesAfterSkipping</code> number of files after skipping   <code>numBytesAfterSkipping</code> number of bytes after skipping   <code>numPartitionsAfterSkipping</code> number of partitions after skipping   <code>numPartitionsAddedTo</code> number of partitions added   <code>numPartitionsRemovedFrom</code> number of partitions removed   <code>numCopiedRows</code> number of rows copied   <code>numBytesAdded</code> number of bytes added   <code>numBytesRemoved</code> number of bytes removed   <code>executionTimeMs</code> time taken to execute the entire operation   <code>scanTimeMs</code> time taken to scan the files for matches   <code>rewriteTimeMs</code> time taken to rewrite the matched files   <code>numAddedChangeFiles</code> number of change data capture files generated   <code>changeFileBytes</code> total size of change data capture files generated   <code>numTouchedRows</code> number of rows touched    <p><code>createMetrics</code> is used when:</p> <ul> <li><code>DeleteCommand</code> is requested for the performance metrics</li> </ul>","text":""},{"location":"commands/delete/DeleteCommandMetrics/#getdeletedrowsfromaddfilesandupdatemetrics","title":"getDeletedRowsFromAddFilesAndUpdateMetrics <pre><code>getDeletedRowsFromAddFilesAndUpdateMetrics(\n  files: Seq[AddFile]): Option[Long]\n</code></pre> <p><code>getDeletedRowsFromAddFilesAndUpdateMetrics</code>...FIXME</p>  <p><code>getDeletedRowsFromAddFilesAndUpdateMetrics</code> is used when:</p> <ul> <li><code>DeleteCommand</code> is requested to performDelete</li> </ul>","text":""},{"location":"commands/delete/DeltaDelete/","title":"DeltaDelete Unary Logical Command","text":"<p><code>DeltaDelete</code> is an unary logical operator (Spark SQL) that represents <code>DeleteFromTable</code>s (Spark SQL) at execution.</p> <p>As per the comment:</p> <p>Needs to be compatible with DBR 6 and can't use the new class added in Spark 3.0: <code>DeleteFromTable</code>.</p>"},{"location":"commands/delete/DeltaDelete/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaDelete</code> takes the following to be created:</p> <ul> <li> Child <code>LogicalPlan</code> (Spark SQL) <li> Condition Expression (Spark SQL) <p><code>DeltaDelete</code> is created\u00a0when:</p> <ul> <li>DeltaAnalysis logical resolution rule is executed and resolves <code>DeleteFromTable</code>s</li> </ul>"},{"location":"commands/delete/DeltaDelete/#logical-resolution","title":"Logical Resolution","text":"<p><code>DeltaDelete</code> is resolved to a DeleteCommand when PreprocessTableDelete post-hoc logical resolution rule is executed.</p>"},{"location":"commands/describe-detail/","title":"DESCRIBE DETAIL Command","text":"<p>Delta Lake supports displaying details of delta tables using the following high-level operator:</p> <ul> <li>DESCRIBE DETAIL SQL command</li> </ul>"},{"location":"commands/describe-detail/DescribeDeltaDetailCommand/","title":"DescribeDeltaDetailCommand (DescribeDeltaDetailCommandBase)","text":"<p><code>DescribeDeltaDetailCommand</code> is a <code>RunnableCommand</code> (Spark SQL) for DESCRIBE DETAIL SQL command.</p> <pre><code>(DESC | DESCRIBE) DETAIL (path | table)\n</code></pre>"},{"location":"commands/describe-detail/DescribeDeltaDetailCommand/#creating-instance","title":"Creating Instance","text":"<p><code>DescribeDeltaDetailCommand</code> takes the following to be created:</p> <ul> <li> (optional) Table Path <li> (optional) Table identifier <p><code>DescribeDeltaDetailCommand</code> is created\u00a0when:</p> <ul> <li><code>DeltaSqlAstBuilder</code> is requested to parse DESCRIBE DETAIL SQL command)</li> </ul>"},{"location":"commands/describe-detail/DescribeDeltaDetailCommand/#run","title":"run <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code>\u00a0is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p> <p><code>run</code>...FIXME</p>","text":""},{"location":"commands/describe-detail/DescribeDeltaDetailCommand/#demo","title":"Demo <pre><code>val q = sql(\"DESCRIBE DETAIL '/tmp/delta/users'\")\n</code></pre> <pre><code>scala&gt; q.show\n+------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+\n|format|                  id|name|description|            location|           createdAt|       lastModified|partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|\n+------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+\n| delta|3799b291-dbfa-4f8...|null|       null|file:/tmp/delta/u...|2020-01-06 17:08:...|2020-01-06 17:12:28| [city, country]|       4|       2581|        []|               1|               2|\n+------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+\n</code></pre>","text":""},{"location":"commands/describe-history/","title":"Describe History Command","text":"<p>Delta Lake supports displaying versions (history) of delta tables using the following high-level operators:</p> <ul> <li>DESCRIBE HISTORY SQL command</li> <li>DeltaTable.history</li> </ul>"},{"location":"commands/describe-history/DescribeDeltaHistoryCommand/","title":"DescribeDeltaHistoryCommand","text":"<p><code>DescribeDeltaHistoryCommand</code> is a <code>RunnableCommand</code> (Spark SQL) that uses DeltaHistoryManager for the commit history of a delta table.</p> <p><code>DescribeDeltaHistoryCommand</code> is used for DESCRIBE HISTORY SQL command.</p>"},{"location":"commands/describe-history/DescribeDeltaHistoryCommand/#creating-instance","title":"Creating Instance","text":"<p><code>DescribeDeltaHistoryCommand</code> takes the following to be created:</p> <ul> <li> (optional) Directory <li> (optional) <code>TableIdentifier</code> <li> (optional) Number of commits to display <li> Output Attributes (default: CommitInfo) <p><code>DescribeDeltaHistoryCommand</code> is created for:</p> <ul> <li>DESCRIBE HISTORY SQL command (that uses <code>DeltaSqlAstBuilder</code> to parse DESCRIBE HISTORY SQL command)</li> </ul>"},{"location":"commands/describe-history/DescribeDeltaHistoryCommand/#executing-command","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code>\u00a0is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p> <p><code>run</code> creates a Hadoop <code>Path</code> to (the location of) the delta table (based on DeltaTableIdentifier).</p> <p><code>run</code> creates a DeltaLog for the delta table.</p> <p><code>run</code> requests the <code>DeltaLog</code> for the DeltaHistoryManager that is requested for the commit history.</p>","text":""},{"location":"commands/generate/","title":"Generate Command","text":"<p>Delta Lake supports executing generator functions on delta tables using the following high-level operators:</p> <ul> <li>GENERATE SQL command</li> <li>DeltaTable.generate</li> </ul> <p>Only symlink_format_manifest mode is supported.</p>"},{"location":"commands/generate/DeltaGenerateCommand/","title":"DeltaGenerateCommand","text":"<p><code>DeltaGenerateCommand</code> is a <code>RunnableCommand</code> (Spark SQL) to execute a generate function on a delta table.</p> <p><code>DeltaGenerateCommand</code> is used for the following:</p> <ul> <li>GENERATE SQL command</li> <li>DeltaTable.generate operation</li> </ul> <p><code>DeltaGenerateCommand</code> supports symlink_format_manifest mode name only.</p>"},{"location":"commands/generate/DeltaGenerateCommand/#demo","title":"Demo","text":"<pre><code>val path = \"/tmp/delta/d01\"\nval tid = s\"delta.`$path`\"\nval q = s\"GENERATE symlink_format_manifest FOR TABLE $tid\"\nsql(q).collect\n</code></pre>"},{"location":"commands/generate/DeltaGenerateCommand/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaGenerateCommand</code> takes the following to be created:</p> <ul> <li>Mode Name</li> <li> <code>TableIdentifier</code> (Spark SQL) <p><code>DeltaGenerateCommand</code> is created for:</p> <ul> <li>GENERATE SQL command (that uses <code>DeltaSqlAstBuilder</code> to parse GENERATE SQL command)</li> <li>DeltaTable.generate operator (that uses <code>DeltaTableOperations</code> to executeGenerate)</li> </ul>"},{"location":"commands/generate/DeltaGenerateCommand/#generate-mode-name","title":"Generate Mode Name <p><code>DeltaGenerateCommand</code> is given a mode name when created.</p> <p><code>DeltaGenerateCommand</code> uses a lookup table of the supported generation functions by mode name (yet supports just <code>symlink_format_manifest</code>).</p>    Mode Name Generation Function     <code>symlink_format_manifest</code> generateFullManifest","text":""},{"location":"commands/generate/DeltaGenerateCommand/#executing-command","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code>\u00a0is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p> <p><code>run</code> creates a Hadoop <code>Path</code> to (the location of) the delta table (based on DeltaTableIdentifier).</p> <p><code>run</code> creates a DeltaLog for the delta table.</p> <p><code>run</code> executes the generation function for the mode name.</p> <p><code>run</code> returns no rows (an empty collection).</p>","text":""},{"location":"commands/generate/DeltaGenerateCommand/#illegalargumentexception","title":"IllegalArgumentException <p><code>run</code> throws an <code>IllegalArgumentException</code> when executed with an unsupported mode name:</p> <pre><code>Specified mode '[modeName]' is not supported. Supported modes are: [supportedModes]\n</code></pre>","text":""},{"location":"commands/generate/DeltaGenerateCommand/#analysisexception","title":"AnalysisException <p><code>run</code> throws an <code>AnalysisException</code> when executed for a non-delta table:</p> <pre><code>GENERATE is only supported for Delta tables.\n</code></pre>","text":""},{"location":"commands/merge/","title":"Merge Command","text":"<p>Delta Lake supports merging records into a delta table using the following high-level operators:</p> <ul> <li>MERGE INTO SQL command (Spark SQL)</li> <li>DeltaTable.merge</li> </ul>"},{"location":"commands/merge/#single-insert-only-merges","title":"Single INSERT-only MERGEs","text":"<p>There is a special handling of single INSERT-only MERGEs.</p> SQL <pre><code>MERGE INTO merge_demo to\nUSING merge_demo_source from\nON to.id = from.id\nWHEN NOT MATCHED THEN INSERT *;\n</code></pre>"},{"location":"commands/merge/#demo","title":"Demo","text":"<ul> <li>Demo: Merge Operation</li> </ul>"},{"location":"commands/merge/DeltaMergeAction/","title":"DeltaMergeAction","text":"<p><code>DeltaMergeAction</code> is a <code>UnaryExpression</code> (Spark SQL) that represents a single action in MERGE command.</p>"},{"location":"commands/merge/DeltaMergeAction/#deltamergeintoclause","title":"DeltaMergeIntoClause <p><code>DeltaMergeAction</code>s are returned when <code>DeltaMergeIntoClause</code> is requested to resolvedActions (for PreprocessTableMerge and MergeIntoCommand).</p>","text":""},{"location":"commands/merge/DeltaMergeAction/#unevaluable","title":"Unevaluable <p><code>DeltaMergeAction</code> is a <code>Unevaluable</code> expression that is resolved when <code>DeltaMergeInto</code> utility is used to resolveReferencesAndSchema.</p>","text":""},{"location":"commands/merge/DeltaMergeAction/#creating-instance","title":"Creating Instance <p><code>DeltaMergeAction</code> takes the following to be created:</p> <ul> <li> Target Column Names (<code>Seq[String]</code>) <li> <code>Expression</code> <li> <code>targetColNameResolved</code> flag (default: <code>false</code>)  <p><code>DeltaMergeAction</code> is created when:</p> <ul> <li><code>DeltaMergeIntoClause</code> utility is used to toActions</li> <li><code>DeltaMergeInto</code> utility is used to resolveReferencesAndSchema</li> <li><code>PreprocessTableMerge</code> logical resolution rule is executed</li> </ul>","text":""},{"location":"commands/merge/DeltaMergeBuilder/","title":"DeltaMergeBuilder","text":"<p><code>DeltaMergeBuilder</code> is a builder interface to describe how to merge data from a source DataFrame into the target delta table (using whenMatched and whenNotMatched conditions).</p> <p>In the end, <code>DeltaMergeBuilder</code> is supposed to be executed to take action. <code>DeltaMergeBuilder</code> creates a DeltaMergeInto logical command that is resolved to a MergeIntoCommand runnable logical command (using PreprocessTableMerge logical resolution rule).</p>"},{"location":"commands/merge/DeltaMergeBuilder/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaMergeBuilder</code> takes the following to be created:</p> <ul> <li> Target DeltaTable <li> Source <code>DataFrame</code> <li> Condition <code>Column</code> <li> When Clauses <p><code>DeltaMergeBuilder</code> is created using DeltaTable.merge operator.</p>"},{"location":"commands/merge/DeltaMergeBuilder/#operators","title":"Operators","text":""},{"location":"commands/merge/DeltaMergeBuilder/#whenmatched","title":"whenMatched <pre><code>whenMatched(): DeltaMergeMatchedActionBuilder\nwhenMatched(\n  condition: Column): DeltaMergeMatchedActionBuilder\nwhenMatched(\n  condition: String): DeltaMergeMatchedActionBuilder\n</code></pre> <p>Creates a DeltaMergeMatchedActionBuilder (for the <code>DeltaMergeBuilder</code> and a condition)</p>","text":""},{"location":"commands/merge/DeltaMergeBuilder/#whennotmatched","title":"whenNotMatched <pre><code>whenNotMatched(): DeltaMergeNotMatchedActionBuilder\nwhenNotMatched(\n  condition: Column): DeltaMergeNotMatchedActionBuilder\nwhenNotMatched(\n  condition: String): DeltaMergeNotMatchedActionBuilder\n</code></pre> <p>Creates a DeltaMergeNotMatchedActionBuilder (for the <code>DeltaMergeBuilder</code> and a condition)</p>","text":""},{"location":"commands/merge/DeltaMergeBuilder/#executing-merge","title":"Executing Merge <pre><code>execute(): Unit\n</code></pre> <p><code>execute</code> creates a merge plan (that is DeltaMergeInto logical command) and resolves column references.</p> <p><code>execute</code> runs PreprocessTableMerge logical resolution rule on the <code>DeltaMergeInto</code> logical command (that gives MergeIntoCommand runnable logical command).</p> <p>In the end, <code>execute</code> executes the MergeIntoCommand logical command.</p>","text":""},{"location":"commands/merge/DeltaMergeBuilder/#creating-logical-plan-for-merge","title":"Creating Logical Plan for Merge <pre><code>mergePlan: DeltaMergeInto\n</code></pre> <p><code>mergePlan</code> creates a DeltaMergeInto logical command.</p> <p><code>mergePlan</code> is used when <code>DeltaMergeBuilder</code> is requested to execute.</p>","text":""},{"location":"commands/merge/DeltaMergeBuilder/#creating-deltamergebuilder","title":"Creating DeltaMergeBuilder <pre><code>apply(\n  targetTable: DeltaTable,\n  source: DataFrame,\n  onCondition: Column): DeltaMergeBuilder\n</code></pre> <p><code>apply</code> utility creates a new <code>DeltaMergeBuilder</code> for the given parameters and no DeltaMergeIntoClauses.</p> <p><code>apply</code> is used for DeltaTable.merge operator.</p>","text":""},{"location":"commands/merge/DeltaMergeBuilder/#adding-deltamergeintoclause","title":"Adding DeltaMergeIntoClause <pre><code>withClause(\n  clause: DeltaMergeIntoClause): DeltaMergeBuilder\n</code></pre> <p><code>withClause</code> creates a new <code>DeltaMergeBuilder</code> (based on the existing properties, e.g. the DeltaTable) with the given DeltaMergeIntoClause added to the existing DeltaMergeIntoClauses (to create a more refined <code>DeltaMergeBuilder</code>).</p> <p><code>withClause</code> is used when:</p> <ul> <li>DeltaMergeMatchedActionBuilder is requested to updateAll, delete and addUpdateClause</li> <li>DeltaMergeNotMatchedActionBuilder is requested to insertAll and addInsertClause</li> </ul>","text":""},{"location":"commands/merge/DeltaMergeInto/","title":"DeltaMergeInto Logical Command","text":"<p><code>DeltaMergeInto</code> is a logical <code>Command</code> (Spark SQL).</p>"},{"location":"commands/merge/DeltaMergeInto/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaMergeInto</code> takes the following to be created:</p> <ul> <li> Target Table (Spark SQL) <li> Source Table or Subquery (Spark SQL) <li> Condition Expression <li> DeltaMergeIntoMatchedClauses <li> Non-Matched DeltaMergeIntoInsertClauses <li>migrateSchema flag</li> <p>When created, <code>DeltaMergeInto</code> verifies the actions in the matchedClauses and notMatchedClauses clauses.</p> <p><code>DeltaMergeInto</code> is created (using apply and resolveReferences utilities) when:</p> <ul> <li><code>DeltaMergeBuilder</code> is requested to execute</li> <li>DeltaAnalysis logical resolution rule is executed</li> </ul>"},{"location":"commands/merge/DeltaMergeInto/#logical-resolution","title":"Logical Resolution","text":"<p><code>DeltaMergeInto</code> is resolved to MergeIntoCommand by PreprocessTableMerge logical resolution rule.</p>"},{"location":"commands/merge/DeltaMergeInto/#migrateschema-flag","title":"migrateSchema Flag <p><code>DeltaMergeInto</code> is given <code>migrateSchema</code> flag when created:</p> <ul> <li>apply uses <code>false</code> always</li> <li>resolveReferences is <code>true</code> only with the spark.databricks.delta.schema.autoMerge.enabled configuration property enabled and <code>*</code>s only (in matched and not-matched clauses)</li> </ul> <p><code>migrateSchema</code> is used when:</p> <ul> <li>PreprocessTableMerge logical resolution rule is executed</li> </ul>","text":""},{"location":"commands/merge/DeltaMergeInto/#supportssubquery","title":"SupportsSubquery <p><code>DeltaMergeInto</code> is a <code>SupportsSubquery</code> (Spark SQL)</p>","text":""},{"location":"commands/merge/DeltaMergeInto/#creating-deltamergeinto","title":"Creating DeltaMergeInto <pre><code>apply(\n  target: LogicalPlan,\n  source: LogicalPlan,\n  condition: Expression,\n  whenClauses: Seq[DeltaMergeIntoClause]): DeltaMergeInto\n</code></pre> <p><code>apply</code> collects DeltaMergeIntoInsertClauses and DeltaMergeIntoMatchedClauses from the given <code>whenClauses</code> and creates a <code>DeltaMergeInto</code> command (with migrateSchema flag off).</p> <p><code>apply</code> is used when:</p> <ul> <li><code>DeltaMergeBuilder</code> is requested to execute (when mergePlan)</li> <li>DeltaAnalysis logical resolution rule is executed (and resolves <code>MergeIntoTable</code> logical command)</li> </ul>","text":""},{"location":"commands/merge/DeltaMergeInto/#analysisexceptions","title":"AnalysisExceptions <p><code>apply</code> throws an <code>AnalysisException</code> for the <code>whenClauses</code> empty:</p> <pre><code>There must be at least one WHEN clause in a MERGE statement\n</code></pre> <p><code>apply</code> throws an <code>AnalysisException</code> if there is a matched clause with no condition (except the last matched clause):</p> <pre><code>When there are more than one MATCHED clauses in a MERGE statement,\nonly the last MATCHED clause can omit the condition.\n</code></pre> <p><code>apply</code> throws an <code>AnalysisException</code> if there is an insert clause with no condition (except the last matched clause):</p> <pre><code>When there are more than one NOT MATCHED clauses in a MERGE statement,\nonly the last NOT MATCHED clause can omit the condition.\n</code></pre>","text":""},{"location":"commands/merge/DeltaMergeInto/#resolvereferencesandschema","title":"resolveReferencesAndSchema <pre><code>resolveReferencesAndSchema(\n  merge: DeltaMergeInto,\n  conf: SQLConf)(\n  resolveExpr: (Expression, LogicalPlan) =&gt; Expression): DeltaMergeInto\n</code></pre> <p><code>resolveReferencesAndSchema</code>...FIXME</p> <p><code>resolveReferencesAndSchema</code> is used when:</p> <ul> <li><code>DeltaMergeBuilder</code> is requested to execute</li> <li>DeltaAnalysis logical resolution rule is executed (to resolve <code>MergeIntoTable</code> logical command)</li> </ul>","text":""},{"location":"commands/merge/DeltaMergeIntoClause/","title":"DeltaMergeIntoClause","text":"<p><code>DeltaMergeIntoClause</code>\u00a0is an extension of the <code>Expression</code> (Spark SQL) abstraction for WHEN clauses.</p>"},{"location":"commands/merge/DeltaMergeIntoClause/#contract","title":"Contract","text":""},{"location":"commands/merge/DeltaMergeIntoClause/#actions","title":"Actions <pre><code>actions: Seq[Expression]\n</code></pre>","text":""},{"location":"commands/merge/DeltaMergeIntoClause/#condition","title":"Condition <pre><code>condition: Option[Expression]\n</code></pre>","text":""},{"location":"commands/merge/DeltaMergeIntoClause/#implementations","title":"Implementations","text":"<ul> <li>DeltaMergeIntoInsertClause</li> <li>DeltaMergeIntoMatchedClause</li> </ul> Sealed Trait <p><code>DeltaMergeIntoClause</code> is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).</p>"},{"location":"commands/merge/DeltaMergeIntoClause/#verifing-actions","title":"Verifing Actions <pre><code>verifyActions(): Unit\n</code></pre> <p><code>verifyActions</code> goes over the actions and makes sure that they are either <code>UnresolvedStar</code>s (Spark SQL) or DeltaMergeActions.</p> <p>For unsupported actions, <code>verifyActions</code> throws an <code>IllegalArgumentException</code>:</p> <pre><code>Unexpected action expression [action] in [this]\n</code></pre> <p><code>verifyActions</code>\u00a0is used when:</p> <ul> <li><code>DeltaMergeInto</code> is created</li> </ul>","text":""},{"location":"commands/merge/DeltaMergeIntoClause/#toactions","title":"toActions <pre><code>toActions(\n  assignments: Seq[Assignment]): Seq[Expression]\ntoActions(\n  colNames: Seq[UnresolvedAttribute],\n  exprs: Seq[Expression],\n  isEmptySeqEqualToStar: Boolean = true): Seq[Expression]\n</code></pre> <p><code>toActions</code> requires that the numbers of <code>colNames</code> and <code>exprs</code> are the same.</p> <p><code>toActions</code> creates one of the following expressions based on the given <code>colNames</code> and <code>isEmptySeqEqualToStar</code> flag:</p> <ol> <li><code>UnresolvedStar</code> for no <code>colNames</code> and <code>isEmptySeqEqualToStar</code> flag enabled</li> <li>DeltaMergeActions (for every pair of column name and expression based on <code>colNames</code> and <code>exprs</code>, respectively)</li> </ol> <p><code>toActions</code> is used when:</p> <ul> <li><code>DeltaAnalysis</code> logical resolution rule is executed</li> <li><code>DeltaMergeMatchedActionBuilder</code> is requested to updateAll, addUpdateClause, insertAll and addInsertClause</li> <li><code>DeltaMergeIntoUpdateClause</code> is created</li> </ul>","text":""},{"location":"commands/merge/DeltaMergeIntoClause/#resolvedactions","title":"resolvedActions <pre><code>resolvedActions: Seq[DeltaMergeAction]\n</code></pre> <p><code>resolvedActions</code>...FIXME</p> <p><code>resolvedActions</code> is used when:</p> <ul> <li>PreprocessTableMerge logical resolution rule is executed</li> <li>MergeIntoCommand is executed (and requested to writeInsertsOnlyWhenNoMatchedClauses and writeAllChanges)</li> </ul>","text":""},{"location":"commands/merge/DeltaMergeIntoDeleteClause/","title":"DeltaMergeIntoDeleteClause","text":"<p><code>DeltaMergeIntoDeleteClause</code> is...FIXME</p>"},{"location":"commands/merge/DeltaMergeIntoInsertClause/","title":"DeltaMergeIntoInsertClause","text":"<p><code>DeltaMergeIntoInsertClause</code> is a DeltaMergeIntoClause for the following:</p> <ul> <li><code>InsertAction</code> not-matched actions in <code>MergeIntoTable</code> (Spark SQL) logical command</li> <li>DeltaMergeNotMatchedActionBuilder.insertAll, DeltaMergeNotMatchedActionBuilder.insert and DeltaMergeNotMatchedActionBuilder.insertExpr operators</li> </ul>"},{"location":"commands/merge/DeltaMergeIntoInsertClause/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaMergeIntoInsertClause</code> takes the following to be created:</p> <ul> <li> (optional) Condition <code>Expression</code> (Spark SQL) <li> Action <code>Expression</code>s (Spark SQL) <p><code>DeltaMergeIntoInsertClause</code> is created\u00a0when:</p> <ul> <li><code>DeltaMergeNotMatchedActionBuilder</code> is requested to insertAll and addInsertClause</li> <li>DeltaAnalysis logical resolution rule is executed (and resolves <code>MergeIntoTable</code> logical command with <code>InsertAction</code> not-matched actions)</li> </ul>"},{"location":"commands/merge/DeltaMergeIntoMatchedClause/","title":"DeltaMergeIntoMatchedClause","text":"<p><code>DeltaMergeIntoMatchedClause</code>\u00a0is an extension of the DeltaMergeIntoClause abstraction for WHEN MATCHED clauses.</p>"},{"location":"commands/merge/DeltaMergeIntoMatchedClause/#implementations","title":"Implementations","text":"<ul> <li>DeltaMergeIntoDeleteClause</li> <li>DeltaMergeIntoUpdateClause</li> </ul> Sealed Trait <p><code>DeltaMergeIntoMatchedClause</code> is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).</p>"},{"location":"commands/merge/DeltaMergeIntoUpdateClause/","title":"DeltaMergeIntoUpdateClause","text":"<p><code>DeltaMergeIntoUpdateClause</code> is a DeltaMergeIntoMatchedClause for the following:</p> <ul> <li><code>UpdateAction</code> matched actions in <code>MergeIntoTable</code> (Spark SQL) logical command</li> <li>DeltaMergeMatchedActionBuilder.updateAll, DeltaMergeMatchedActionBuilder.update and DeltaMergeMatchedActionBuilder.updateExpr operators</li> </ul>"},{"location":"commands/merge/DeltaMergeIntoUpdateClause/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaMergeIntoUpdateClause</code> takes the following to be created:</p> <ul> <li> (optional) Condition <code>Expression</code> (Spark SQL) <li> Action <code>Expression</code>s (Spark SQL) <p><code>DeltaMergeIntoUpdateClause</code> is created\u00a0when:</p> <ul> <li><code>DeltaMergeMatchedActionBuilder</code> is requested to updateAll and addUpdateClause</li> <li>DeltaAnalysis logical resolution rule is executed (and resolves <code>MergeIntoTable</code> logical command with <code>UpdateAction</code> matched actions)</li> </ul>"},{"location":"commands/merge/DeltaMergeMatchedActionBuilder/","title":"DeltaMergeMatchedActionBuilder","text":"<p><code>DeltaMergeMatchedActionBuilder</code> is a builder interface for DeltaMergeBuilder.whenMatched operator.</p>"},{"location":"commands/merge/DeltaMergeMatchedActionBuilder/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaMergeMatchedActionBuilder</code> takes the following to be created:</p> <ul> <li> DeltaMergeBuilder <li> Optional match condition <p><code>DeltaMergeMatchedActionBuilder</code> is created when:</p> <ul> <li><code>DeltaMergeBuilder</code> is requested to whenMatched (using apply factory method)</li> </ul>"},{"location":"commands/merge/DeltaMergeMatchedActionBuilder/#operators","title":"Operators","text":""},{"location":"commands/merge/DeltaMergeMatchedActionBuilder/#delete","title":"delete <pre><code>delete(): DeltaMergeBuilder\n</code></pre> <p>Adds a <code>DeltaMergeIntoDeleteClause</code> (with the matchCondition) to the DeltaMergeBuilder.</p>","text":""},{"location":"commands/merge/DeltaMergeMatchedActionBuilder/#update","title":"update <pre><code>update(\n  set: Map[String, Column]): DeltaMergeBuilder\n</code></pre>","text":""},{"location":"commands/merge/DeltaMergeMatchedActionBuilder/#updateall","title":"updateAll <pre><code>updateAll(): DeltaMergeBuilder\n</code></pre>","text":""},{"location":"commands/merge/DeltaMergeMatchedActionBuilder/#updateexpr","title":"updateExpr <pre><code>updateExpr(\n  set: Map[String, String]): DeltaMergeBuilder\n</code></pre>","text":""},{"location":"commands/merge/DeltaMergeMatchedActionBuilder/#creating-deltamergematchedactionbuilder","title":"Creating DeltaMergeMatchedActionBuilder <pre><code>apply(\n  mergeBuilder: DeltaMergeBuilder,\n  matchCondition: Option[Column]): DeltaMergeMatchedActionBuilder\n</code></pre> <p><code>apply</code> creates a <code>DeltaMergeMatchedActionBuilder</code> (for the given parameters).</p> <p><code>apply</code> is used when:</p> <ul> <li><code>DeltaMergeBuilder</code> is requested to whenMatched</li> </ul>","text":""},{"location":"commands/merge/DeltaMergeMatchedActionBuilder/#addupdateclause","title":"addUpdateClause <pre><code>addUpdateClause(\n  set: Map[String, Column]): DeltaMergeBuilder\n</code></pre> <p><code>addUpdateClause</code>...FIXME</p> <p><code>addUpdateClause</code> is used when:</p> <ul> <li><code>DeltaMergeMatchedActionBuilder</code> is requested to update and updateExpr</li> </ul>","text":""},{"location":"commands/merge/DeltaMergeNotMatchedActionBuilder/","title":"DeltaMergeNotMatchedActionBuilder","text":"<p><code>DeltaMergeNotMatchedActionBuilder</code> is a builder interface for DeltaMergeBuilder.whenNotMatched operator.</p>"},{"location":"commands/merge/DeltaMergeNotMatchedActionBuilder/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaMergeNotMatchedActionBuilder</code> takes the following to be created:</p> <ul> <li> DeltaMergeBuilder <li> (optional) Not-Match Condition (Spark SQL) <p><code>DeltaMergeNotMatchedActionBuilder</code> is created\u00a0when:</p> <ul> <li><code>DeltaMergeBuilder</code> is requested to whenNotMatched</li> </ul>"},{"location":"commands/merge/DeltaMergeNotMatchedActionBuilder/#operators","title":"Operators","text":""},{"location":"commands/merge/DeltaMergeNotMatchedActionBuilder/#insert","title":"insert <pre><code>insert(\n  values: Map[String, Column]): DeltaMergeBuilder\n</code></pre> <p><code>insert</code> adds an insert clause (with the <code>values</code>).</p>","text":""},{"location":"commands/merge/DeltaMergeNotMatchedActionBuilder/#insertall","title":"insertAll <pre><code>insertAll(): DeltaMergeBuilder\n</code></pre> <p><code>insertAll</code> requests the DeltaMergeBuilder to register a new DeltaMergeIntoInsertClause.</p>","text":""},{"location":"commands/merge/DeltaMergeNotMatchedActionBuilder/#insertexpr","title":"insertExpr <pre><code>insertExpr(\n  values: Map[String, String]): DeltaMergeBuilder\n</code></pre> <p><code>insertExpr</code> adds an insert clause (with the <code>values</code>).</p>","text":""},{"location":"commands/merge/DeltaMergeNotMatchedActionBuilder/#registering-new-deltamergeintoinsertclause","title":"Registering New DeltaMergeIntoInsertClause <pre><code>addInsertClause(\n  setValues: Map[String, Column]): DeltaMergeBuilder\n</code></pre> <p><code>addInsertClause</code> requests the DeltaMergeBuilder to register a new DeltaMergeIntoInsertClause (similarly to insertAll but with the given <code>setValues</code>).</p> <p><code>addInsertClause</code> is used when:</p> <ul> <li><code>DeltaMergeNotMatchedActionBuilder</code> is requested to insert and insertExpr</li> </ul>","text":""},{"location":"commands/merge/JoinedRowProcessor/","title":"JoinedRowProcessor","text":"<p><code>JoinedRowProcessor</code> is used to process partitions for <code>MergeIntoCommand</code> to write out merged data.</p>"},{"location":"commands/merge/JoinedRowProcessor/#creating-instance","title":"Creating Instance","text":"<p><code>JoinedRowProcessor</code> takes the following to be created:</p> <ul> <li> <code>targetRowHasNoMatch</code> Expression <li> <code>sourceRowHasNoMatch</code> Expression <li> <code>matchedConditions</code> Expressions <li> <code>matchedOutputs</code> (<code>Seq[Seq[Seq[Expression]]]</code>) <li> <code>notMatchedConditions</code> Expressions <li> <code>notMatchedOutputs</code> (<code>Seq[Seq[Seq[Expression]]]</code>) <li> <code>noopCopyOutput</code> Expressions <li> <code>deleteRowOutput</code> Expressions <li> <code>joinedAttributes</code> Attributes <li> <code>joinedRowEncoder</code> (<code>ExpressionEncoder[Row]</code>) <li> <code>outputRowEncoder</code> (<code>ExpressionEncoder[Row]</code>) <p><code>JoinedRowProcessor</code> is created\u00a0when:</p> <ul> <li><code>MergeIntoCommand</code> is requested to write out merged data</li> </ul>"},{"location":"commands/merge/JoinedRowProcessor/#processing-partition","title":"Processing Partition <pre><code>processPartition(\n  rowIterator: Iterator[Row]): Iterator[Row]\n</code></pre> <p><code>processPartition</code>...FIXME</p> <p><code>processPartition</code>\u00a0is used when:</p> <ul> <li><code>MergeIntoCommand</code> is requested to write out merged data</li> </ul>","text":""},{"location":"commands/merge/MergeIntoCommand/","title":"MergeIntoCommand","text":"<p><code>MergeIntoCommand</code> is a transactional DeltaCommand that represents a DeltaMergeInto logical command at execution.</p>"},{"location":"commands/merge/MergeIntoCommand/#performance-metrics","title":"Performance Metrics","text":"Name web UI <code>numSourceRows</code> number of source rows <code>numSourceRowsInSecondScan</code> number of source rows (during repeated scan) <code>numTargetRowsCopied</code> number of target rows rewritten unmodified <code>numTargetRowsInserted</code> number of inserted rows <code>numTargetRowsUpdated</code> number of updated rows <code>numTargetRowsDeleted</code> number of deleted rows <code>numTargetFilesBeforeSkipping</code> number of target files before skipping <code>numTargetFilesAfterSkipping</code> number of target files after skipping <code>numTargetFilesRemoved</code> number of files removed to target <code>numTargetFilesAdded</code> number of files added to target <code>numTargetChangeFilesAdded</code> number of change data capture files generated <code>numTargetChangeFileBytes</code> total size of change data capture files generated <code>numTargetBytesBeforeSkipping</code> number of target bytes before skipping <code>numTargetBytesAfterSkipping</code> number of target bytes after skipping <code>numTargetBytesRemoved</code> number of target bytes removed <code>numTargetBytesAdded</code> number of target bytes added <code>numTargetPartitionsAfterSkipping</code> number of target partitions after skipping <code>numTargetPartitionsRemovedFrom</code> number of target partitions from which files were removed <code>numTargetPartitionsAddedTo</code> number of target partitions to which files were added <code>executionTimeMs</code> time taken to execute the entire operation <code>scanTimeMs</code> time taken to scan the files for matches <code>rewriteTimeMs</code> time taken to rewrite the matched files"},{"location":"commands/merge/MergeIntoCommand/#number-of-target-rows-rewritten-unmodified","title":"number of target rows rewritten unmodified <p><code>numTargetRowsCopied</code> performance metric (like the other metrics) is turned into a non-deterministic user-defined function (UDF).</p> <p><code>numTargetRowsCopied</code> becomes <code>incrNoopCountExpr</code> UDF.</p> <p><code>incrNoopCountExpr</code> UDF is resolved on a joined plan and used to create a JoinedRowProcessor for processing partitions of the joined plan <code>Dataset</code>.</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#creating-instance","title":"Creating Instance","text":"<p><code>MergeIntoCommand</code> takes the following to be created:</p> <ul> <li>Source Data</li> <li> Target Data (LogicalPlan) <li> TahoeFileIndex <li> Merge Condition (Expression) <li> Matched Clauses <li> Non-Matched Insert Clauses <li>Migrated Schema</li> <p><code>MergeIntoCommand</code> is created when:</p> <ul> <li>PreprocessTableMerge logical resolution rule is executed (to resolve a DeltaMergeInto logical command)</li> </ul>"},{"location":"commands/merge/MergeIntoCommand/#source-data","title":"Source Data <pre><code>source: LogicalPlan\n</code></pre> <p>When created, <code>MergeIntoCommand</code> is given a <code>LogicalPlan</code> (Spark SQL) for the source data to merge from (internally referred to as source).</p> <p>The source is used twice:</p> <ul> <li>Firstly, in one of the following:<ul> <li>An inner join (in findTouchedFiles) that is <code>count</code> in web UI</li> <li>A leftanti join (in writeInsertsOnlyWhenNoMatchedClauses)</li> </ul> </li> <li>Secondly, in right or full outer join (in writeAllChanges)</li> </ul>  <p>Tip</p> <p>Enable <code>DEBUG</code> logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger to see the inner-workings of writeAllChanges.</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#migrated-schema","title":"Migrated Schema <pre><code>migratedSchema: Option[StructType]\n</code></pre> <p><code>MergeIntoCommand</code> can be given a <code>migratedSchema</code> (Spark SQL).</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#target-delta-table","title":"Target Delta Table <pre><code>targetDeltaLog: DeltaLog\n</code></pre> <p><code>targetDeltaLog</code> is the DeltaLog (of the TahoeFileIndex) of the target delta table.</p> <p><code>targetDeltaLog</code> is used for the following:</p> <ul> <li>Start a new transaction when executed</li> <li>To access the Data Path when finding files to rewrite</li> </ul>  Lazy Value <p><code>targetDeltaLog</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards.</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#executing-command","title":"Executing Command <pre><code>run(\n  spark: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p>  <p><code>run</code> is a transactional operation that is made up of the following steps:</p> <ol> <li>Begin Transaction<ol> <li>schema.autoMerge.enabled</li> <li>FileActions</li> <li>Register Metrics</li> </ol> </li> <li>Commit Transaction</li> <li>Re-Cache Target Delta Table</li> <li>Post Metric Updates</li> </ol>","text":""},{"location":"commands/merge/MergeIntoCommand/#begin-transaction","title":"Begin Transaction <p><code>run</code> starts a new transaction (on the target delta table).</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#schemaautomergeenabled","title":"schema.autoMerge.enabled <p>Only when spark.databricks.delta.schema.autoMerge.enabled configuration property is enabled, <code>run</code> updates the metadata (of the transaction) with the following:</p> <ul> <li>migratedSchema (if defined) or the schema of the target</li> <li><code>isOverwriteMode</code> flag off</li> <li><code>rearrangeOnly</code> flag off</li> </ul>","text":""},{"location":"commands/merge/MergeIntoCommand/#fileactions","title":"FileActions <p><code>run</code> determines FileActions.</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#single-insert-only-merge","title":"Single Insert-Only Merge <p>For a single insert-only merge with spark.databricks.delta.merge.optimizeInsertOnlyMerge.enabled configuration property enabled, <code>run</code> writeInsertsOnlyWhenNoMatchedClauses.</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#other-merges","title":"Other Merges <p>Otherwise, <code>run</code> finds the files to rewrite (i.e., AddFiles with the rows that satisfy the merge condition) and uses them to write out merge changes.</p> <p>The <code>AddFile</code>s are converted into RemoveFiles.</p> <p><code>run</code> gives the <code>RemoveFile</code>s and the written-out FileActions.</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#register-metrics","title":"Register Metrics <p><code>run</code> registers the SQL metrics (with the current transaction).</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#commit-transaction","title":"Commit Transaction <p><code>run</code> commits the current transaction (with the FileActions and <code>MERGE</code> operation).</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#re-cache-target-delta-table","title":"Re-Cache Target Delta Table <p><code>run</code> requests the <code>CacheManager</code> to re-cache the target plan.</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#post-metric-updates","title":"Post Metric Updates <p>In the end, <code>run</code> posts the SQL metric updates (as a <code>SparkListenerDriverAccumUpdates</code> (Apache Spark) Spark event) to <code>SparkListener</code>s (incl. Spark UI).</p>  <p>Note</p> <p>Use <code>SparkListener</code> (Apache Spark) to intercept <code>SparkListenerDriverAccumUpdates</code> events.</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#building-target-logical-query-plan-for-addfiles","title":"Building Target Logical Query Plan for AddFiles <pre><code>buildTargetPlanWithFiles(\n  deltaTxn: OptimisticTransaction,\n  files: Seq[AddFile]): LogicalPlan\n</code></pre> <p><code>buildTargetPlanWithFiles</code> creates a DataFrame to represent the given AddFiles to access the analyzed logical query plan. <code>buildTargetPlanWithFiles</code> requests the given OptimisticTransaction for the DeltaLog to create a DataFrame (for the Snapshot and the given AddFiles).</p> <p>In the end, <code>buildTargetPlanWithFiles</code> creates a <code>Project</code> logical operator with <code>Alias</code> expressions so the output columns of the analyzed logical query plan (of the <code>DataFrame</code> of the <code>AddFiles</code>) reference the target's output columns (by name).</p>  <p>Note</p> <p>The output columns of the target delta table are associated with a OptimisticTransaction as the Metadata.</p> <pre><code>deltaTxn.metadata.schema\n</code></pre>","text":""},{"location":"commands/merge/MergeIntoCommand/#exceptions","title":"Exceptions <p><code>run</code> throws an <code>AnalysisException</code> when the target schema is different than the delta table's (has changed after analysis phase):</p> <pre><code>The schema of your Delta table has changed in an incompatible way since your DataFrame or DeltaTable object was created. Please redefine your DataFrame or DeltaTable object. Changes:\n[schemaDiff]\nThis check can be turned off by setting the session configuration key spark.databricks.delta.checkLatestSchemaOnRead to false.\n</code></pre>","text":""},{"location":"commands/merge/MergeIntoCommand/#writing-out-single-insert-only-merge","title":"Writing Out Single Insert-Only Merge <pre><code>writeInsertsOnlyWhenNoMatchedClauses(\n  spark: SparkSession,\n  deltaTxn: OptimisticTransaction): Seq[FileAction]\n</code></pre> <p>In the end, <code>writeInsertsOnlyWhenNoMatchedClauses</code> returns the FileActions (from writing out data).</p> <p><code>writeInsertsOnlyWhenNoMatchedClauses</code> is used when:</p> <ul> <li><code>MergeIntoCommand</code> is executed (for single insert-only merge with spark.databricks.delta.merge.optimizeInsertOnlyMerge.enabled enabled)</li> </ul>","text":""},{"location":"commands/merge/MergeIntoCommand/#target-output-columns","title":"Target Output Columns <p><code>writeInsertsOnlyWhenNoMatchedClauses</code> gets the names of the output (target) columns.</p> <p><code>writeInsertsOnlyWhenNoMatchedClauses</code> creates a collection of output columns with the target names and the resolved DeltaMergeActions of a single DeltaMergeIntoInsertClause (as <code>Alias</code> expressions).</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#source-dataframe","title":"Source DataFrame <p><code>writeInsertsOnlyWhenNoMatchedClauses</code> creates a UDF to update numSourceRows metric.</p> <p><code>writeInsertsOnlyWhenNoMatchedClauses</code> creates a source <code>DataFrame</code> for the source data with <code>Dataset.filter</code>s with the UDF and the condition of the DeltaMergeIntoInsertClause (if defined) or <code>Literal.TrueLiteral</code>.</p>  <p>Use condition for filter pushdown optimization</p> <p>The condition of this single DeltaMergeIntoInsertClause is pushed down to the source when Spark SQL optimizes the query.</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#data-skipped-addfiles","title":"Data-Skipped AddFiles <p><code>writeInsertsOnlyWhenNoMatchedClauses</code> splits conjunctive predicates (<code>And</code> expressions) in the merge condition and determines a so-called targetOnlyPredicates (predicates with the target columns only). <code>writeInsertsOnlyWhenNoMatchedClauses</code> requests the given OptimisticTransaction to filterFiles matching the target-only predicates (that gives AddFiles).</p>  <p>Merge Condition and Data Skipping</p> <p>The merge condition of this MergeIntoCommand is used for Data Skipping.</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#target-dataframe","title":"Target DataFrame <p><code>writeInsertsOnlyWhenNoMatchedClauses</code> creates a target <code>DataFrame</code> for the data-skipped <code>AddFile</code>s.</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#insert-dataframe","title":"Insert DataFrame <p><code>writeInsertsOnlyWhenNoMatchedClauses</code> creates a UDF to update numTargetRowsInserted metric.</p> <p><code>writeInsertsOnlyWhenNoMatchedClauses</code> left-anti joins the source <code>DataFrame</code> with the target <code>DataFrame</code> on the merge condition. <code>writeInsertsOnlyWhenNoMatchedClauses</code> selects the output columns and uses <code>Dataset.filter</code> with the UDF.</p>  Demo: Left-Anti Join <pre><code>val source = Seq(0, 1, 2, 3).toDF(\"id\") // (1)!\nval target = Seq(3, 4, 5).toDF(\"id\") // (2)!\nval usingColumns = Seq(\"id\")\nval q = source.join(target, usingColumns, \"leftanti\")\n</code></pre> <ol> <li> <p>Equivalent to <code>spark.range(4)</code></p> <pre><code>+---+\n| id|\n+---+\n|  0|\n|  1|\n|  2|\n|  3|\n+---+\n</code></pre> </li> <li> <p>Equivalent to <code>spark.range(3, 6)</code></p> <pre><code>+---+\n| id|\n+---+\n|  3|\n|  4|\n|  5|\n+---+\n</code></pre> </li> </ol> <pre><code>scala&gt; q.show\n+---+\n| id|\n+---+\n|  0|\n|  1|\n|  2|\n+---+\n</code></pre>","text":""},{"location":"commands/merge/MergeIntoCommand/#writefiles","title":"writeFiles <p><code>writeInsertsOnlyWhenNoMatchedClauses</code> requests the given OptimisticTransaction to write out the insertDf (possibly repartitionIfNeeded on the partitionColumns of the target delta table).</p>  <p>Note</p> <p>This step triggers a Spark write job (and this active transaction is marked as hasWritten).</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#update-metrics","title":"Update Metrics <p>In the end, <code>writeInsertsOnlyWhenNoMatchedClauses</code> updates the metrics.</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#writing-out-merged-data","title":"Writing Out Merged Data <pre><code>writeAllChanges(\n  spark: SparkSession,\n  deltaTxn: OptimisticTransaction,\n  filesToRewrite: Seq[AddFile]): Seq[FileAction]\n</code></pre> <p>In the end, <code>writeAllChanges</code> returns the FileActions (from writing out data).</p>  <p><code>writeAllChanges</code> is used when:</p> <ul> <li><code>MergeIntoCommand</code> is executed (that is neither a single insert-only merge nor spark.databricks.delta.merge.optimizeInsertOnlyMerge.enabled configuration property is enabled for which writeInsertsOnlyWhenNoMatchedClauses is used instead)</li> </ul>","text":""},{"location":"commands/merge/MergeIntoCommand/#targetoutputcols","title":"targetOutputCols <p><code>writeAllChanges</code> builds the target output schema (possibly with some <code>null</code>s for the target columns that are not in the current schema).</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#newtarget-query-plan","title":"newTarget Query Plan <p><code>writeAllChanges</code> builds a target logical query plan for the given <code>filesToRewrite</code> AddFiles.</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#join-type","title":"Join Type <p><code>writeAllChanges</code> determines the join type to use:</p> <ul> <li><code>rightOuter</code> for matched-only merge with spark.databricks.delta.merge.optimizeMatchedOnlyMerge.enabled configuration property enabled</li> <li><code>fullOuter</code> otherwise</li> </ul> <p><code>writeAllChanges</code> prints out the following DEBUG message to the logs:</p> <pre><code>writeAllChanges using [joinType] join:\nsource.output: [outputSet]\ntarget.output: [outputSet]\ncondition: [condition]\nnewTarget.output: [outputSet]\n</code></pre>","text":""},{"location":"commands/merge/MergeIntoCommand/#source-dataframe_1","title":"Source DataFrame <p><code>writeAllChanges</code> creates a source <code>DataFrame</code> for the source query plan and an extra column:</p> <ul> <li><code>_source_row_present_</code> with the value of an UDF that increments the numSourceRowsInSecondScan metric</li> </ul>","text":""},{"location":"commands/merge/MergeIntoCommand/#new-target-dataframe","title":"New Target DataFrame <p><code>writeAllChanges</code> creates a target <code>DataFrame</code> for the newTarget query plan and an extra column:</p> <ul> <li><code>_target_row_present_</code> with <code>true</code> literal</li> </ul>","text":""},{"location":"commands/merge/MergeIntoCommand/#joined-dataframe","title":"Joined DataFrame <p><code>writeAllChanges</code> creates a joined <code>DataFrame</code> that is a <code>Dataset.join</code> of the source and new target <code>DataFrame</code>s with the given join condition and the join type.</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#output-schema","title":"Output Schema <p><code>writeAllChanges</code> creates an output schema based on cdcEnabled:</p> <ul> <li>FIXME</li> </ul>","text":""},{"location":"commands/merge/MergeIntoCommand/#create-joinedrowprocessor","title":"Create JoinedRowProcessor <p><code>writeAllChanges</code> creates a JoinedRowProcessor (that is then used to map over partitions of the joined DataFrame).</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#output-dataframe","title":"Output DataFrame <p><code>writeAllChanges</code> creates a <code>DataFrame</code> for the <code>joinedPlan</code> and uses <code>Dataset.mapPartitions</code> operator to let the JoinedRowProcessor to processPartition.</p> <p><code>writeAllChanges</code> drops row_dropped and incr_row_count columns.</p> <p><code>writeAllChanges</code> prints out the following DEBUG message to the logs:</p> <pre><code>writeAllChanges: join output plan:\n[outputDF.queryExecution]\n</code></pre>","text":""},{"location":"commands/merge/MergeIntoCommand/#writefiles_1","title":"writeFiles <p><code>writeAllChanges</code> requests the given OptimisticTransaction to write out the insertDf (possibly repartitioning if needed on the partition columns).</p>  <p>Note</p> <p>This step triggers a Spark write job (and this active transaction is marked as hasWritten).</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#update-metrics_1","title":"Update Metrics <p>In the end, <code>writeAllChanges</code> updates the metrics.</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#single-insert-only-merge_1","title":"Single Insert-Only Merge <pre><code>isSingleInsertOnly: Boolean\n</code></pre> <p><code>isSingleInsertOnly</code> holds true when this <code>MERGE</code> command is a single <code>WHEN NOT MATCHED THEN INSERT</code>:</p> <ol> <li>No MATCHED clauses</li> <li>There is just a single notMatchedClauses</li> </ol> Example: Single Insert-Only Merge<pre><code>MERGE INTO merge_demo to\nUSING merge_demo_source from\nON to.id = from.id\nWHEN NOT MATCHED THEN INSERT *;\n</code></pre>","text":""},{"location":"commands/merge/MergeIntoCommand/#matched-only-merge","title":"Matched-Only Merge <pre><code>isMatchedOnly: Boolean\n</code></pre> <p><code>isMatchedOnly</code> holds true when this <code>MERGE</code> command is <code>WHEN MATCHED THEN</code>-only:</p> <ol> <li>No NOT MATCHED clauses</li> <li>MATCHED clauses only</li> </ol> <pre><code>MERGE INTO merge_demo to\nUSING merge_demo_source from\nON to.id = from.id\nWHEN MATCHED AND to.id &lt; 3 THEN DELETE\nWHEN MATCHED THEN UPDATE SET *;\n</code></pre>","text":""},{"location":"commands/merge/MergeIntoCommand/#creating-metric-update-udf","title":"Creating Metric Update UDF <pre><code>makeMetricUpdateUDF(\n  name: String): Expression\n</code></pre> <p><code>makeMetricUpdateUDF</code> looks up the performance metric (by <code>name</code>) in the metrics.</p> <p>In the end, <code>makeMetricUpdateUDF</code> defines a non-deterministic UDF to increment the metric (when executed).</p>  <p>Note</p> <p>This <code>Expression</code> is used to increment the following performance metrics:</p> <ul> <li>number of source rows</li> <li>numSourceRowsInSecondScan</li> <li>number of target rows rewritten unmodified</li> <li>number of deleted rows</li> <li>number of inserted rows</li> <li>number of updated rows</li> </ul>  <p><code>makeMetricUpdateUDF</code> is used when:</p> <ul> <li><code>MergeIntoCommand</code> is requested to findTouchedFiles, writeInsertsOnlyWhenNoMatchedClauses, writeAllChanges</li> </ul>","text":""},{"location":"commands/merge/MergeIntoCommand/#notmatchedclauseoutput","title":"notMatchedClauseOutput <pre><code>notMatchedClauseOutput(\n  clause: DeltaMergeIntoInsertClause): Seq[Seq[Expression]]\n</code></pre> <p><code>notMatchedClauseOutput</code> returns the main data output followed by the CDF data output when cdcEnabled:</p> <pre><code>(mainDataOutput)\n</code></pre> <p>or</p> <pre><code>(mainDataOutput, insertCdcOutput)\n</code></pre>  <p><code>notMatchedClauseOutput</code> creates the output (resolved expressions) of the main data based on the following:</p> <ol> <li>The <code>Expression</code>s from the DeltaMergeActions of the given DeltaMergeIntoInsertClause</li> <li><code>FalseLiteral</code></li> <li>The UDF to increment the numTargetRowsInserted metric</li> <li>null literal (to indicate the main data not CDF's)</li> </ol> <p>With cdcEnabled, <code>notMatchedClauseOutput</code> creates the output (resolved expressions) of the CDF data based on the following:</p> <ol> <li>The <code>Expression</code>s from the DeltaMergeActions of the given DeltaMergeIntoInsertClause</li> <li><code>FalseLiteral</code></li> <li><code>TrueLiteral</code></li> <li>insert literal</li> </ol>  <p>Note</p> <p>The first two expressions are the same in the main data and CDF data outputs.</p>   <p><code>notMatchedClauseOutput</code> is used when:</p> <ul> <li><code>MergeIntoCommand</code> is requested to write out merged data (for notMatchedOutputs to create a JoinedRowProcessor based on the non-matched insert clauses)</li> </ul>","text":""},{"location":"commands/merge/MergeIntoCommand/#finding-files-to-rewrite","title":"Finding Files to Rewrite <pre><code>findTouchedFiles(\n  spark: SparkSession,\n  deltaTxn: OptimisticTransaction): Seq[AddFile]\n</code></pre>  <p>Important</p> <p><code>findTouchedFiles</code> is such a fine piece of art (a Delta gem). It uses a custom accumulator, a UDF (to use this accumulator to record touched file names) and <code>input_file_name()</code> standard function for the names of the files read.</p> <p>It is always worth keeping in mind that Delta Lake uses files for data storage and that is why <code>input_file_name()</code> standard function works. It would not work for non-file-based data sources.</p>   Example 1: Understanding the Internals of <code>findTouchedFiles</code> <p>The following query writes out a 10-element dataset using the default parquet data source to <code>/tmp/parquet</code> directory:</p> <pre><code>val target = \"/tmp/parquet\"\nspark.range(10).write.save(target)\n</code></pre> <p>The number of parquet part files varies based on the number of partitions (CPU cores).</p> <p>The following query loads the parquet dataset back alongside <code>input_file_name()</code> standard function to mimic <code>findTouchedFiles</code>'s behaviour.</p> <pre><code>val FILE_NAME_COL = \"_file_name_\"\nval dataFiles = spark.read.parquet(target).withColumn(FILE_NAME_COL, input_file_name())\n</code></pre> <pre><code>scala&gt; dataFiles.show(truncate = false)\n+---+---------------------------------------------------------------------------------------+\n|id |_file_name_                                                                            |\n+---+---------------------------------------------------------------------------------------+\n|4  |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n|0  |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n|3  |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n|6  |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n|1  |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n|8  |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n|2  |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n|7  |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n|5  |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n|9  |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n+---+---------------------------------------------------------------------------------------+\n</code></pre> <p>As you may have thought, not all part files have got data and so they are not included in the dataset. That is when <code>findTouchedFiles</code> uses <code>groupBy</code> operator and <code>count</code> action to calculate match frequency.</p> <pre><code>val counts = dataFiles.groupBy(FILE_NAME_COL).count()\nscala&gt; counts.show(truncate = false)\n+---------------------------------------------------------------------------------------+-----+\n|_file_name_                                                                            |count|\n+---------------------------------------------------------------------------------------+-----+\n|file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n|file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n|file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n|file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n|file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n|file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n|file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n|file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n|file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n|file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n+---------------------------------------------------------------------------------------+-----+\n</code></pre> <p>Let's load all the part files in the <code>/tmp/parquet</code> directory and find which file(s) have no data.</p> <pre><code>import scala.sys.process._\nval cmd = (s\"ls $target\" #| \"grep .parquet\").lineStream\nval allFiles = cmd.toArray.toSeq.toDF(FILE_NAME_COL)\n  .select(concat(lit(s\"file://$target/\"), col(FILE_NAME_COL)) as FILE_NAME_COL)\nval joinType = \"left_anti\" // MergeIntoCommand uses inner as it wants data file\nval noDataFiles = allFiles.join(dataFiles, Seq(FILE_NAME_COL), joinType)\n</code></pre> <p>Mind that the data vs non-data datasets could be different, but that should not \"interfere\" with the main reasoning flow.</p> <pre><code>scala&gt; noDataFiles.show(truncate = false)\n+---------------------------------------------------------------------------------------+\n|_file_name_                                                                            |\n+---------------------------------------------------------------------------------------+\n|file:///tmp/parquet/part-00000-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n+---------------------------------------------------------------------------------------+\n</code></pre>  <p><code>findTouchedFiles</code> registers an accumulator to collect all the distinct files that need to be rewritten (touched files).</p>  <p>Note</p> <p>The name of the accumulator is internal.metrics.MergeIntoDelta.touchedFiles and <code>internal.metrics</code> part is supposed to hide it from web UI as potentially large (set of file names to be rewritten).</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#recordtouchedfilename","title":"recordTouchedFileName <p><code>findTouchedFiles</code> defines a nondeterministic UDF that adds the file names to the accumulator (recordTouchedFileName).</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#dataskippedfiles","title":"dataSkippedFiles <p><code>findTouchedFiles</code> splits conjunctive predicates (<code>And</code> binary expressions) in the condition expression and collects the predicates that use the target's columns (targetOnlyPredicates). <code>findTouchedFiles</code> requests the given OptimisticTransaction for the files that match the target-only predicates (and creates a <code>dataSkippedFiles</code> collection of AddFiles).</p>  <p>Note</p> <p>This step looks similar to filter predicate pushdown.</p>  <p><code>findTouchedFiles</code> creates one <code>DataFrame</code> for the source data (using Spark SQL utility).</p> <p><code>findTouchedFiles</code> builds a logical query plan for the files (matching the predicates) and creates another <code>DataFrame</code> for the target data. <code>findTouchedFiles</code> adds two columns to the target dataframe:</p> <ol> <li><code>_row_id_</code> for <code>monotonically_increasing_id()</code> standard function</li> <li><code>_file_name_</code> for <code>input_file_name()</code> standard function</li> </ol> <p><code>findTouchedFiles</code> creates (a <code>DataFrame</code> that is) an INNER JOIN of the source and target <code>DataFrame</code>s using the condition expression.</p> <p><code>findTouchedFiles</code> takes the joined dataframe and selects <code>_row_id_</code> column and the recordTouchedFileName UDF on the <code>_file_name_</code> column as <code>one</code>. The <code>DataFrame</code> is internally known as <code>collectTouchedFiles</code>.</p> <p><code>findTouchedFiles</code> uses <code>groupBy</code> operator on <code>_row_id_</code> to calculate a sum of all the values in the <code>one</code> column (as <code>count</code> column) in the two-column <code>collectTouchedFiles</code> dataset. The <code>DataFrame</code> is internally known as <code>matchedRowCounts</code>.</p>  <p>Note</p> <p>No Spark job has been submitted yet. <code>findTouchedFiles</code> is still in \"query preparation\" mode.</p>  <p><code>findTouchedFiles</code> uses <code>filter</code> on the <code>count</code> column (in the <code>matchedRowCounts</code> dataset) with values greater than <code>1</code>. If there are any, <code>findTouchedFiles</code> throws an <code>UnsupportedOperationException</code> exception:</p> <pre><code>Cannot perform MERGE as multiple source rows matched and attempted to update the same\ntarget row in the Delta table. By SQL semantics of merge, when multiple source rows match\non the same target row, the update operation is ambiguous as it is unclear which source\nshould be used to update the matching target row.\nYou can preprocess the source table to eliminate the possibility of multiple matches.\n</code></pre>  <p>Note</p> <p>Since <code>findTouchedFiles</code> uses <code>count</code> action there should be a Spark SQL query reported (and possibly Spark jobs) in web UI.</p>  <p><code>findTouchedFiles</code> requests the <code>touchedFilesAccum</code> accumulator for the touched file names.</p>  Example 2: Understanding the Internals of <code>findTouchedFiles</code> <pre><code>val TOUCHED_FILES_ACCUM_NAME = \"MergeIntoDelta.touchedFiles\"\nval touchedFilesAccum = spark.sparkContext.collectionAccumulator[String](TOUCHED_FILES_ACCUM_NAME)\nval recordTouchedFileName = udf { (fileName: String) =&gt; {\n  touchedFilesAccum.add(fileName)\n  1\n}}.asNondeterministic()\n</code></pre> <pre><code>val target = \"/tmp/parquet\"\nspark.range(10).write.save(target)\n</code></pre> <pre><code>val FILE_NAME_COL = \"_file_name_\"\nval dataFiles = spark.read.parquet(target).withColumn(FILE_NAME_COL, input_file_name())\nval collectTouchedFiles = dataFiles.select(col(FILE_NAME_COL), recordTouchedFileName(col(FILE_NAME_COL)).as(\"one\"))\n</code></pre> <pre><code>scala&gt; collectTouchedFiles.show(truncate = false)\n+---------------------------------------------------------------------------------------+---+\n|_file_name_                                                                            |one|\n+---------------------------------------------------------------------------------------+---+\n|file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n|file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n|file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n|file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n|file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n|file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n|file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n|file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n|file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n|file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n+---------------------------------------------------------------------------------------+---+    \n</code></pre> <pre><code>import scala.collection.JavaConverters._\nval touchedFileNames = touchedFilesAccum.value.asScala.toSeq\n</code></pre> <p>Use the Stages tab in web UI to review the accumulator values.</p>  <p><code>findTouchedFiles</code> prints out the following TRACE message to the logs:</p> <pre><code>findTouchedFiles: matched files:\n  [touchedFileNames]\n</code></pre> <p><code>findTouchedFiles</code> generateCandidateFileMap for the files that match the target-only predicates.</p> <p><code>findTouchedFiles</code> getTouchedFile for every touched file name.</p> <p><code>findTouchedFiles</code> updates the following performance metrics:</p> <ul> <li>numTargetFilesBeforeSkipping and adds the numOfFiles of the Snapshot of the given OptimisticTransaction</li> <li>numTargetFilesAfterSkipping and adds the number of the files that match the target-only predicates</li> <li>numTargetFilesRemoved and adds the number of the touched files</li> </ul> <p>In the end, <code>findTouchedFiles</code> gives the touched files (as AddFiles).</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#repartitionifneeded","title":"repartitionIfNeeded <pre><code>repartitionIfNeeded(\n  spark: SparkSession,\n  df: DataFrame,\n  partitionColumns: Seq[String]): DataFrame\n</code></pre> <p><code>repartitionIfNeeded</code> repartitions the given <code>DataFrame</code> by the <code>partitionColumns</code> (using <code>Dataset.repartition</code> operation) when all the following hold:</p> <ol> <li>There is at least one partition column (among the given <code>partitionColumns</code>)</li> <li>spark.databricks.delta.merge.repartitionBeforeWrite.enabled configuration property is <code>true</code></li> </ol>  <p><code>repartitionIfNeeded</code> is used when:</p> <ul> <li><code>MergeIntoCommand</code> is executed (and writes data out for Single Insert-Only Merge and other merges)</li> </ul>","text":""},{"location":"commands/merge/MergeIntoCommand/#leafrunnablecommand","title":"LeafRunnableCommand <p><code>MergeIntoCommand</code> is a <code>LeafRunnableCommand</code> (Spark SQL) logical operator.</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#demo","title":"Demo <p>Demo: Merge Operation.</p>","text":""},{"location":"commands/merge/MergeIntoCommand/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.commands.MergeIntoCommand</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.delta.commands.MergeIntoCommand=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"commands/optimize/","title":"OPTIMIZE Command","text":"<p>From Optimize performance with file management:</p> <p>To improve query speed, Delta Lake on Databricks supports the ability to optimize the layout of data stored in cloud storage. Delta Lake on Databricks supports two layout algorithms: bin-packing and Z-Ordering.</p> <p>As of Delta Lake 2.0.0, the above quote applies to the open source version, too.</p> <p><code>OPTIMIZE</code> command can be executed as follows:</p> <ul> <li>OPTIMIZE SQL Command</li> <li>DeltaTable.optimize operator</li> </ul>"},{"location":"commands/optimize/#optimize-sql-command","title":"OPTIMIZE SQL Command","text":"<p>OPTIMIZE SQL command</p>"},{"location":"commands/optimize/#bin-packing","title":"bin-packing","text":"<p>In <code>bin-packing</code> (aka. file compaction) mode, <code>OPTIMIZE</code> command compacts files together (that are smaller than spark.databricks.delta.optimize.minFileSize to files of spark.databricks.delta.optimize.maxFileSize size).</p>"},{"location":"commands/optimize/#z-ordering","title":"Z-Ordering","text":"<p><code>OPTIMIZE</code> can specify <code>ZORDER BY</code> columns for multi-dimensional clustering.</p>"},{"location":"commands/optimize/#optimizemaxthreads","title":"optimize.maxThreads","text":"<p><code>OPTIMIZE</code> command uses spark.databricks.delta.optimize.maxThreads threads for compaction.</p>"},{"location":"commands/optimize/#demo","title":"Demo","text":"<p>Demo: Optimize</p>"},{"location":"commands/optimize/#learning-more","title":"Learning More","text":"<p>There seems so many articles and academic papers about space filling curve based clustering algorithms. I'm hoping that one day I'll have read enough to develop my own intuition about z-order multi-dimensional optimization. If you know good articles about this space (pun intended), let me know. I'll collect them here for future reference (for others to learn along).</p> <p>Thank you! \ud83d\ude4f</p> <ol> <li>Z-order curve</li> </ol>"},{"location":"commands/optimize/InterleaveBits/","title":"InterleaveBits Expression","text":"<p><code>InterleaveBits</code> is an <code>Expression</code> (Spark SQL) for interleave_bits function.</p>"},{"location":"commands/optimize/InterleaveBits/#creating-instance","title":"Creating Instance","text":"<p><code>InterleaveBits</code> takes the following to be created:</p> <ul> <li> Child <code>Expression</code>s (Spark SQL) <p><code>InterleaveBits</code> is created when:</p> <ul> <li><code>MultiDimClusteringFunctions</code> utility is used to interleave_bits</li> </ul>"},{"location":"commands/optimize/InterleaveBits/#interpreted-expression-evaluation","title":"Interpreted Expression Evaluation <pre><code>eval(\n  input: InternalRow): Any\n</code></pre> <p><code>eval</code> is part of the <code>Expression</code> (Spark SQL) abstraction.</p>  <p><code>eval</code> requests all the child expressions to <code>eval</code> (with the given <code>InternalRow</code>). <code>eval</code> replaces <code>null</code> values with <code>0</code>, keeps <code>Int</code>-typed results and throws an <code>IllegalArgumentException</code> for others:</p> <pre><code>[name] expects only inputs of type Int, but got: [any] of type [type]\n</code></pre> <p><code>eval</code> updates the <code>ints</code> array with the values.</p> <p><code>eval</code> computes Z-values for all the multidimensional data points (cf. Z-order curve).</p> <p>In the end, <code>eval</code> returns a byte array (4 times larger than the number of children expressions).</p>","text":""},{"location":"commands/optimize/InterleaveBits/#expectsinputtypes","title":"ExpectsInputTypes <p><code>InterleaveBits</code> is an <code>ExpectsInputTypes</code> (Spark SQL).</p>","text":""},{"location":"commands/optimize/InterleaveBits/#inputtypes","title":"inputTypes <pre><code>inputTypes: Seq[DataType]\n</code></pre> <p><code>inputTypes</code> is part of the <code>ExpectsInputTypes</code> (Spark SQL) abstraction.</p>  <p><code>inputTypes</code> is <code>IntegerType</code> for all the child expressions.</p>","text":""},{"location":"commands/optimize/InterleaveBits/#codegenfallback","title":"CodegenFallback <p><code>InterleaveBits</code> is an <code>CodegenFallback</code> (Spark SQL).</p>","text":""},{"location":"commands/optimize/InterleaveBits/#evaluation-result-datatype","title":"Evaluation Result DataType <pre><code>dataType: DataType\n</code></pre> <p><code>dataType</code> is part of the <code>Expression</code> (Spark SQL) abstraction.</p>  <p><code>dataType</code> is always <code>BinaryType</code>.</p>","text":""},{"location":"commands/optimize/InterleaveBits/#nullable","title":"nullable <pre><code>nullable: Boolean\n</code></pre> <p><code>nullable</code> is part of the <code>Expression</code> (Spark SQL) abstraction.</p> <p><code>nullable</code> is always <code>false</code>.</p>","text":""},{"location":"commands/optimize/MultiDimClustering/","title":"MultiDimClustering","text":"<p><code>MultiDimClustering</code> is an abstraction of multi-dimensional clustering algorithms (for changing the data layout).</p>"},{"location":"commands/optimize/MultiDimClustering/#contract","title":"Contract","text":""},{"location":"commands/optimize/MultiDimClustering/#cluster","title":"cluster <pre><code>cluster(\n  df: DataFrame,\n  colNames: Seq[String],\n  approxNumPartitions: Int): DataFrame\n</code></pre>  <p>Note</p> <p>It can be surprising to find out that this method is never really used. The reason is that there is a companion object <code>MultiDimClustering</code> with the cluster utility of the same signature that simply ZOrderClustering.cluster (bypassing any virtual calls as if there were multiple implementations yet there is just one).</p>","text":""},{"location":"commands/optimize/MultiDimClustering/#implementations","title":"Implementations","text":"<ul> <li>SpaceFillingCurveClustering</li> </ul>"},{"location":"commands/optimize/MultiDimClustering/#cluster_1","title":"cluster <pre><code>cluster(\n  df: DataFrame,\n  approxNumPartitions: Int,\n  colNames: Seq[String]): DataFrame\n</code></pre> <p><code>cluster</code> does Z-Order clustering.</p> <p><code>cluster</code> is used when:</p> <ul> <li><code>OptimizeExecutor</code> is requested to runOptimizeBinJob (with isMultiDimClustering flag enabled)</li> </ul>","text":""},{"location":"commands/optimize/MultiDimClustering/#assertionerror","title":"AssertionError <p><code>cluster</code> throws an <code>AssertionError</code> when there are no <code>colNames</code> specified:</p> <pre><code>Cannot cluster by zero columns!\n</code></pre>","text":""},{"location":"commands/optimize/MultiDimClusteringFunctions/","title":"MultiDimClusteringFunctions","text":"<p><code>MultiDimClusteringFunctions</code> utility offers Spark SQL functions for multi-dimensional clustering.</p>"},{"location":"commands/optimize/MultiDimClusteringFunctions/#range_partition_id","title":"range_partition_id <pre><code>range_partition_id(\n  col: Column,\n  numPartitions: Int): Column\n</code></pre> <p><code>range_partition_id</code> creates a <code>Column</code> (Spark SQL) with RangePartitionId unary expression (for the given arguments).</p> <p><code>range_partition_id</code> is used when:</p> <ul> <li><code>ZOrderClustering</code> utility is used for the clustering expression</li> </ul>","text":""},{"location":"commands/optimize/MultiDimClusteringFunctions/#interleave_bits","title":"interleave_bits <pre><code>interleave_bits(\n  cols: Column*): Column\n</code></pre> <p><code>interleave_bits</code> creates a <code>Column</code> (Spark SQL) with InterleaveBits expression (for the expressions of the given columns).</p> <p><code>interleave_bits</code> is used when:</p> <ul> <li><code>ZOrderClustering</code> utility is used for the clustering expression</li> </ul>","text":""},{"location":"commands/optimize/OptimizeExecutor/","title":"OptimizeExecutor","text":"<p><code>OptimizeExecutor</code> is a DeltaCommand for OptimizeTableCommand to optimize a delta table.</p> <p><code>OptimizeExecutor</code> is a SQLMetricsReporting.</p>"},{"location":"commands/optimize/OptimizeExecutor/#creating-instance","title":"Creating Instance","text":"<p><code>OptimizeExecutor</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> DeltaLog (of the Delta table to be optimized) <li> Partition predicate expressions (Spark SQL) <li> Z-OrderBy Columns (Names) <p><code>OptimizeExecutor</code> is created when:</p> <ul> <li><code>OptimizeTableCommand</code> is requested to run</li> </ul>"},{"location":"commands/optimize/OptimizeExecutor/#optimize","title":"optimize <pre><code>optimize(): Seq[Row]\n</code></pre> <p><code>optimize</code> is used when:</p> <ul> <li><code>OptimizeTableCommand</code> is requested to run</li> </ul>  <p><code>optimize</code> reads the following configuration properties:</p> <ul> <li>spark.databricks.delta.optimize.minFileSize for the threshold of the size of files to be grouped together and rewritten as larger ones</li> <li>spark.databricks.delta.optimize.maxFileSize for the maximum desired file size (in bytes) of the compaction output files</li> </ul> <p><code>optimize</code> requests the DeltaLog to startTransaction.</p> <p><code>optimize</code> requests the <code>OptimisticTransaction</code> for the files matching the partition predicates.</p> <p><code>optimize</code> finds the files of the size below the spark.databricks.delta.optimize.minFileSize threshold (that are the files considered for compacting) and groups them by partition values.</p> <p><code>optimize</code> group the files into bins (of the <code>spark.databricks.delta.optimize.maxFileSize</code> size).</p>  <p>Note</p> <p>A bin is a group of files, whose total size does not exceed the desired size. They will be coalesced into a single output file.</p>  <p><code>optimize</code> creates a <code>ForkJoinPool</code> with spark.databricks.delta.optimize.maxThreads threads (with the <code>OptimizeJob</code> thread prefix). The task pool is then used to parallelize the submission of runCompactBinJob optimization jobs to Spark.</p> <p>Once the compaction jobs are done, <code>optimize</code> tries to commit the transaction (the given actions to the log) when there were any AddFiles.</p> <p>In the end, <code>optimize</code> returns a <code>Row</code> with the data path (of the Delta table) and the optimize statistics.</p>","text":""},{"location":"commands/optimize/OptimizeExecutor/#runoptimizebinjob","title":"runOptimizeBinJob <pre><code>runOptimizeBinJob(\n  txn: OptimisticTransaction,\n  partition: Map[String, String],\n  bin: Seq[AddFile],\n  maxFileSize: Long): Seq[FileAction]\n</code></pre> <p><code>runOptimizeBinJob</code> creates an input <code>DataFrame</code> to represent data described by the given AddFiles. <code>runOptimizeBinJob</code> requests the deltaLog (of the given OptimisticTransaction) to create the DataFrame with <code>Optimize</code> action type.</p> <p>For Z-Ordering (isMultiDimClustering flag is enabled), <code>runOptimizeBinJob</code> does the following:</p> <ol> <li>Calculates the approximate number of files (as the total size of all the given <code>AddFile</code>s divided by the given <code>maxFileSize</code>)</li> <li>Repartitions the <code>DataFrame</code> to as many partitions as the approximate number of files using multi-dimensional clustering for the z-orderby columns</li> </ol> <p>Otherwise, <code>runOptimizeBinJob</code> coalesces the <code>DataFrame</code> to <code>1</code> partition (using <code>DataFrame.coalesce</code> operator).</p> <p><code>runOptimizeBinJob</code> sets a custom description for the job group (for all future Spark jobs started by this thread).</p> <p><code>runOptimizeBinJob</code> writes out the repartitioned <code>DataFrame</code>. <code>runOptimizeBinJob</code> requests the given OptimisticTransaction to writeFiles.</p> <p><code>runOptimizeBinJob</code> marks all the AddFiles (as the result of writeFiles) as not dataChange. No other FileActions are expected or <code>runOptimizeBinJob</code> throws an <code>IllegalStateException</code>:</p> <pre><code>Unexpected action [other] with type [class].\nFile compaction job output should only have AddFiles\n</code></pre> <p><code>runOptimizeBinJob</code> creates RemoveFiles for all the given AddFiles. <code>runOptimizeBinJob</code> uses the current timestamp and the <code>dataChange</code> flag is disabled (as was earlier with the <code>AddFile</code>s).</p> <p>In the end, <code>runOptimizeBinJob</code> returns the AddFiles and RemoveFiles.</p>","text":""},{"location":"commands/optimize/OptimizeExecutor/#commitandretry","title":"commitAndRetry <pre><code>commitAndRetry(\n  txn: OptimisticTransaction,\n  optimizeOperation: Operation,\n  actions: Seq[Action],\n  metrics: Map[String, SQLMetric])(f: OptimisticTransaction =&gt; Boolean): Unit\n</code></pre> <p><code>commitAndRetry</code>...FIXME</p>","text":""},{"location":"commands/optimize/OptimizeExecutor/#ismultidimclustering-flag","title":"isMultiDimClustering Flag <p><code>OptimizeExecutor</code> defines <code>isMultiDimClustering</code> flag based on whether there are zOrderByColumns specified or not. In other words, <code>isMultiDimClustering</code> is <code>true</code> for OPTIMIZE ZORDER.</p> <p>The most important use of <code>isMultiDimClustering</code> flag is for multi-dimensional clustering while runOptimizeBinJob.</p> <p><code>OptimizeExecutor</code> uses it also for the following:</p> <ul> <li>Determine data (parquet) files to optimize (that in fact keeps all the files by partition)</li> <li>Create a <code>ZOrderStats</code> at the end of optimize</li> <li>Keep all the files of a partition in a single bin in groupFilesIntoBins</li> </ul>","text":""},{"location":"commands/optimize/OptimizeMetrics/","title":"OptimizeMetrics","text":"<p><code>OptimizeMetrics</code> is the metrics of the OPTIMIZE command:</p> <ul> <li> numFilesAdded <li> numFilesRemoved <li> <code>FileSizeMetrics</code> of the files added <li> <code>FileSizeMetrics</code> of the files removed <li> partitionsOptimized <li> zOrderStats <li> numBatches <li> totalConsideredFiles <li> totalFilesSkipped <li> preserveInsertionOrder <li> numFilesSkippedToReduceWriteAmplification <li> numBytesSkippedToReduceWriteAmplification <li> startTimeMs <li> endTimeMs <p><code>OptimizeMetrics</code> is created when <code>OptimizeStats</code> is requested for the OptimizeMetrics.</p>"},{"location":"commands/optimize/OptimizeStats/","title":"OptimizeStats","text":"<p><code>OptimizeStats</code> holds the statistics of OptimizeExecutor (when requested to optimize).</p>"},{"location":"commands/optimize/OptimizeStats/#creating-instance","title":"Creating Instance","text":"<p><code>OptimizeStats</code> takes the following to be created:</p> <ul> <li> addedFilesSizeStats <li> removedFilesSizeStats <li> numPartitionsOptimized <li> <code>ZOrderStats</code> <li> numBatches <li> totalConsideredFiles <li> totalFilesSkipped <li> preserveInsertionOrder <li> numFilesSkippedToReduceWriteAmplification <li> numBytesSkippedToReduceWriteAmplification <li> startTimeMs <li> endTimeMs <li> totalClusterParallelism <li> totalScheduledTasks <li> <code>AutoCompactParallelismStats</code> <p><code>OptimizeStats</code> is created when:</p> <ul> <li><code>OptimizeExecutor</code> is requested to optimize</li> </ul>"},{"location":"commands/optimize/OptimizeStats/#filesizestats","title":"FileSizeStats <p><code>FileSizeStats</code> holds the following metrics:</p> <ul> <li> minFileSize <li> maxFileSize <li> totalFiles <li> totalSize  <p><code>FileSizeStats</code> is created and used to represent the following <code>OptimizeStats</code> metrics:</p> <ul> <li>addedFilesSizeStats</li> <li>removedFilesSizeStats</li> </ul>","text":""},{"location":"commands/optimize/OptimizeStats/#creating-optimizemetrics","title":"Creating OptimizeMetrics <pre><code>toOptimizeMetrics: OptimizeMetrics\n</code></pre> <p><code>toOptimizeMetrics</code> converts this <code>OptimizeStats</code> to a OptimizeMetrics.</p>  <p><code>toOptimizeMetrics</code> is used when:</p> <ul> <li><code>OptimizeExecutor</code> is requested to optimize</li> </ul>","text":""},{"location":"commands/optimize/OptimizeTableCommand/","title":"OptimizeTableCommand","text":"<p><code>OptimizeTableCommand</code> is a <code>LeafRunnableCommand</code> (Spark SQL) for the following high-level operators:</p> <ul> <li>OPTIMIZE SQL command</li> <li>DeltaOptimizeBuilder.executeCompaction</li> <li>DeltaOptimizeBuilder.executeZOrderBy</li> </ul> <p><code>OptimizeTableCommand</code> is an OptimizeTableCommandBase.</p> <p>Once executed, <code>OptimizeTableCommand</code> returns a <code>DataFrame</code> with the following:</p> <ul> <li>dataPath</li> <li>OptimizeMetrics</li> </ul>"},{"location":"commands/optimize/OptimizeTableCommand/#creating-instance","title":"Creating Instance","text":"<p><code>OptimizeTableCommand</code> takes the following to be created:</p> <ul> <li> Table Path <li> <code>TableIdentifier</code> <li> Partition Predicates <li> Options <li> <code>zOrderBy</code> attributes (aka interleaveBy attributes) <p><code>OptimizeTableCommand</code> is created when:</p> <ul> <li><code>DeltaSqlAstBuilder</code> is requested to parse OPTIMIZE SQL statement</li> <li><code>DeltaOptimizeBuilder</code> is requested to execute</li> </ul>"},{"location":"commands/optimize/OptimizeTableCommand/#executing-command","title":"Executing Command  Signature <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p>  <p><code>run</code> gets the DeltaLog of the delta table (by the given path or TableIdentifier).</p> <p><code>run</code> validates the zOrderBy columns (that may throw <code>DeltaIllegalArgumentException</code> or <code>DeltaAnalysisException</code> exceptions and so break the command execution).</p> <p>In the end, <code>run</code> creates an OptimizeExecutor that is in turn requested to optimize.</p>","text":""},{"location":"commands/optimize/OptimizeTableCommandBase/","title":"OptimizeTableCommandBase","text":"<p><code>OptimizeTableCommandBase</code> is a (marker) extension of the DeltaCommand abstraction for optimize commands.</p> <p><code>OptimizeTableCommandBase</code> is a <code>RunnableCommand</code> (Spark SQL).</p>"},{"location":"commands/optimize/OptimizeTableCommandBase/#implementations","title":"Implementations","text":"<ul> <li>OptimizeTableCommand</li> </ul>"},{"location":"commands/optimize/OptimizeTableCommandBase/#output-attributes","title":"Output Attributes <pre><code>output: Seq[Attribute]\n</code></pre> <p><code>output</code> is part of the <code>Command</code> (Spark SQL) abstraction.</p>    Name DataType     <code>path</code> <code>StringType</code>   <code>metrics</code> OptimizeMetrics","text":""},{"location":"commands/optimize/OptimizeTableCommandBase/#validating-zorderby-columns","title":"Validating zOrderBy Columns <pre><code>validateZorderByColumns(\n  spark: SparkSession,\n  deltaLog: DeltaLog,\n  unresolvedZOrderByCols: Seq[UnresolvedAttribute]): Unit\n</code></pre>  <p>Note</p> <p>Since <code>validateZorderByColumns</code> returns <code>Unit</code> (no value to work with), I'm sure you have already figured out that it is mainly to throw an exception when things are not as expected for the OPTIMIZE command.</p>  <p><code>validateZorderByColumns</code> does nothing (and returns) when there is no <code>unresolvedZOrderByCols</code> columns specified.</p> <p><code>validateZorderByColumns</code> makes sure that no <code>unresolvedZOrderByCols</code> column violates the following requirements (or throws <code>DeltaIllegalArgumentException</code> or <code>DeltaAnalysisException</code>):</p> <ol> <li>It is part of data schema</li> <li>Column statistics are available for the column (when spark.databricks.delta.optimize.zorder.checkStatsCollection.enabled enabled)</li> <li>It is not a partition column (as Z-Ordering can only be performed on data columns)</li> </ol> <p><code>validateZorderByColumns</code> is used when:</p> <ul> <li><code>OptimizeTableCommand</code> is executed</li> </ul>","text":""},{"location":"commands/optimize/PartitionerExpr/","title":"PartitionerExpr Unary Expression","text":"<p><code>PartitionerExpr</code> is a <code>UnaryExpression</code> (Spark SQL) that represents RangePartitionId unary expression at execution (after RangePartitionIdRewrite optimization rule).</p>"},{"location":"commands/optimize/PartitionerExpr/#creating-instance","title":"Creating Instance","text":"<p><code>PartitionerExpr</code> takes the following to be created:</p> <ul> <li> Child <code>Expression</code> (Spark SQL) <li> <code>Partitioner</code> (Spark Core) <p><code>PartitionerExpr</code> is created when:</p> <ul> <li><code>RangePartitionIdRewrite</code> optimization rule is executed (on a <code>LogicalPlan</code> with RangePartitionId expressions)</li> </ul>"},{"location":"commands/optimize/PartitionerExpr/#interpreted-expression-evaluation","title":"Interpreted Expression Evaluation <pre><code>eval(\n  input: InternalRow): Any\n</code></pre> <p><code>eval</code> is part of the <code>Expression</code> (Spark SQL) abstraction.</p>  <p><code>eval</code> requests the child expression to <code>eval</code> for the given <code>InternalRow</code> and uses the evaluation result to update the GenericInternalRow at <code>0</code>th position.</p> <p>In the end, <code>eval</code> requests the Partitioner to <code>getPartition</code> for the just-updated GenericInternalRow.</p>","text":""},{"location":"commands/optimize/PartitionerExpr/#code-generated-expression-evaluation","title":"Code-Generated Expression Evaluation <pre><code>doGenCode(\n  ctx: CodegenContext,\n  ev: ExprCode): ExprCode\n</code></pre> <p><code>doGenCode</code>...FIXME</p> <p><code>doGenCode</code> is part of the <code>Expression</code> (Spark SQL) abstraction.</p>","text":""},{"location":"commands/optimize/PartitionerExpr/#evaluation-result-datatype","title":"Evaluation Result DataType <pre><code>dataType: DataType\n</code></pre> <p><code>dataType</code> is part of the <code>Expression</code> (Spark SQL) abstraction.</p>  <p><code>dataType</code> is always <code>IntegerType</code>.</p>","text":""},{"location":"commands/optimize/PartitionerExpr/#nullable","title":"nullable <pre><code>nullable: Boolean\n</code></pre> <p><code>nullable</code> is part of the <code>Expression</code> (Spark SQL) abstraction.</p>  <p><code>nullable</code> is always <code>false</code>.</p>","text":""},{"location":"commands/optimize/RangePartitionId/","title":"RangePartitionId Unary Expression","text":"<p><code>RangePartitionId</code> is a <code>UnaryExpression</code> (Spark SQL) that represents range_partition_id function in a logical query plan (until it is resolved into PartitionerExpr by RangePartitionIdRewrite optimization).</p>"},{"location":"commands/optimize/RangePartitionId/#creating-instance","title":"Creating Instance","text":"<p><code>RangePartitionId</code> takes the following to be created:</p> <ul> <li> Child <code>Expression</code> (Spark SQL) <li> Number of partitions <p><code>RangePartitionId</code> requires the number of partitions to be greater than <code>0</code>.</p> <p><code>RangePartitionId</code> is created when:</p> <ul> <li>range_partition_id function is used</li> </ul>"},{"location":"commands/optimize/RangePartitionId/#unevaluable","title":"Unevaluable <p><code>RangePartitionId</code> is <code>Unevaluable</code> (Spark SQL) that is rewritten by RangePartitionIdRewrite to PartitionerExpr.</p>","text":""},{"location":"commands/optimize/RangePartitionId/#checkinputdatatypes","title":"checkInputDataTypes <pre><code>checkInputDataTypes(): TypeCheckResult\n</code></pre> <p><code>checkInputDataTypes</code> is part of the <code>Expression</code> (Spark SQL) abstraction.</p>  <p><code>checkInputDataTypes</code> is successful when the <code>DataType</code> (Spark SQL) of the child expression is row-orderable (Spark SQL). Otherwise, <code>checkInputDataTypes</code> is negative.</p>","text":""},{"location":"commands/optimize/RangePartitionId/#evaluation-result-datatype","title":"Evaluation Result DataType <pre><code>dataType: DataType\n</code></pre> <p><code>dataType</code> is part of the <code>Expression</code> (Spark SQL) abstraction.</p>  <p><code>dataType</code> is always <code>IntegerType</code>.</p>","text":""},{"location":"commands/optimize/RangePartitionId/#nullable","title":"nullable <pre><code>nullable: Boolean\n</code></pre> <p><code>nullable</code> is part of the <code>Expression</code> (Spark SQL) abstraction.</p>  <p><code>nullable</code> is always <code>false</code>.</p>","text":""},{"location":"commands/optimize/RangePartitionIdRewrite/","title":"RangePartitionIdRewrite Optimization Rule","text":"<p><code>RangePartitionIdRewrite</code> is an optimization rule (<code>Rule[LogicalPlan]</code>) to rewrite RangePartitionIds to PartitionerExprs.</p>"},{"location":"commands/optimize/RangePartitionIdRewrite/#creating-instance","title":"Creating Instance","text":"<p><code>RangePartitionIdRewrite</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <p><code>RangePartitionIdRewrite</code> is created when:</p> <ul> <li><code>DeltaSparkSessionExtension</code> is requested to register delta extensions</li> </ul>"},{"location":"commands/optimize/RangePartitionIdRewrite/#samplesizeperpartition-hint","title":"sampleSizePerPartition Hint <p><code>RangePartitionIdRewrite</code> uses <code>spark.sql.execution.rangeExchange.sampleSizePerPartition</code> (Spark SQL) configuration property for <code>samplePointsPerPartitionHint</code> for a <code>RangePartitioner</code> (Spark Core) when executed.</p>","text":""},{"location":"commands/optimize/RangePartitionIdRewrite/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> is part of the <code>Rule</code> (Spark SQL) abstraction.</p>  <p><code>apply</code> transforms <code>UnaryNode</code>s with RangePartitionId unary expressions in the given <code>LogicalPlan</code> (from the children first and up).</p> <p>For every <code>RangePartitionId</code>, <code>apply</code> creates a new logical query plan for sampling (based on the child expression of the <code>RangePartitionId</code>).</p> <pre><code>import org.apache.spark.sql.functions.lit\nval expr = lit(5).expr\n\nimport org.apache.spark.sql.catalyst.expressions.Alias\nval aliasedExpr = Alias(expr, \"__RPI_child_col__\")()\n</code></pre> <p><code>apply</code> changes the current call site to the following:</p> <pre><code>RangePartitionId([childExpr], [numPartitions]) sampling\n</code></pre>  <p>Call Site</p> <p>A call site is used in web UI and <code>SparkListener</code>s for all jobs submitted from the current and child threads.</p>  <p><code>apply</code> creates a <code>RangePartitioner</code> (Spark Core) (with the number of partition of the <code>RangePartitionId</code>, the RDD of the query plan for sampling, <code>ascending</code> flag enabled, and the sampleSizePerPartition hint).</p> <p>In the end, <code>apply</code> creates a PartitionerExpr with the child expression (of the <code>RangePartitionId</code>) and the <code>RangePartitioner</code>.</p>","text":""},{"location":"commands/optimize/SpaceFillingCurveClustering/","title":"SpaceFillingCurveClustering","text":"<p><code>SpaceFillingCurveClustering</code> is an extension of the MultiDimClustering abstraction for space filling curve based clustering algorithms.</p>"},{"location":"commands/optimize/SpaceFillingCurveClustering/#contract","title":"Contract","text":""},{"location":"commands/optimize/SpaceFillingCurveClustering/#getclusteringexpression","title":"getClusteringExpression <pre><code>getClusteringExpression(\n  cols: Seq[Column],\n  numRanges: Int): Column\n</code></pre> <p>Used when:</p> <ul> <li><code>SpaceFillingCurveClustering</code> is requested to execute multi-dimensional clustering</li> </ul>","text":""},{"location":"commands/optimize/SpaceFillingCurveClustering/#implementations","title":"Implementations","text":"<ul> <li>ZOrderClustering</li> </ul>"},{"location":"commands/optimize/SpaceFillingCurveClustering/#multi-dimensional-clustering","title":"Multi-Dimensional Clustering <pre><code>cluster(\n  df: DataFrame,\n  colNames: Seq[String],\n  approxNumPartitions: Int): DataFrame\n</code></pre> <p><code>cluster</code> is part of the MultiDimClustering abstraction.</p> <p><code>cluster</code> converts the given <code>colNames</code> into <code>Column</code>s (using the given <code>df</code> dataframe).</p> <p><code>cluster</code> adds two column expressions (to the given <code>DataFrame</code>):</p> <ol> <li><code>[randomUUID]-rpKey1</code> for a clustering expression (with the <code>colNames</code> and the spark.databricks.io.skipping.mdc.rangeId.max configuration property)</li> <li><code>[randomUUID]-rpKey2</code> for an extra noise (for an independent and identically distributed samples uniformly distributed in <code>[0.0, 1.0)</code> using <code>rand</code> standard function)</li> </ol> <p><code>cluster</code> uses <code>rpKey2</code> column only with spark.databricks.io.skipping.mdc.addNoise enabled.</p> <p><code>cluster</code> repartitions the given <code>DataFrame</code> by the <code>rpKey1</code> and <code>rpKey2</code> partitioning expressions into the <code>approxNumPartitions</code> partitions (using Dataset.repartitionByRange operator).</p> <p>In the end, <code>cluster</code> returns the repartitioned <code>DataFrame</code> (with the two columns to be dropped).</p>","text":""},{"location":"commands/optimize/SpaceFillingCurveClustering/#demo","title":"Demo <pre><code>import org.apache.spark.sql.delta.skipping.ZOrderClustering\nval df = spark.range(5).toDF\nval colNames = \"id\" :: Nil\nval approxNumPartitions = 3\nval repartByRangeDf = ZOrderClustering.cluster(df, colNames, approxNumPartitions)\n</code></pre> <pre><code>println(repartByRangeDf.queryExecution.executedPlan.numberedTreeString)\n</code></pre> <pre><code>00 AdaptiveSparkPlan isFinalPlan=false\n01 +- Project [id#0L]\n02    +- Exchange rangepartitioning(1075d2d7-0cd9-4ce2-be11-ff345ff45047-rpKey1#3 ASC NULLS FIRST, f311bca4-4b5a-4bd8-b9e6-5fb05570c8e1-rpKey2#6 ASC NULLS FIRST, 3), REPARTITION_BY_NUM, [id=#21]\n03       +- Project [id#0L, cast(interleavebits(partitionerexpr(id#0L, org.apache.spark.RangePartitioner@442452be)) as string) AS 1075d2d7-0cd9-4ce2-be11-ff345ff45047-rpKey1#3, cast(((rand(5298108963717326846) * 255.0) - 128.0) as tinyint) AS f311bca4-4b5a-4bd8-b9e6-5fb05570c8e1-rpKey2#6]\n04          +- Range (0, 5, step=1, splits=16)\n</code></pre> <pre><code>assert(repartByRangeDf.rdd.getNumPartitions == 3)\n</code></pre> <p>I'm sure that the demo begs for some more love but let's explore the data in the parquet data files anyway. The dataset is very basic (yet I'm hoping someone will take it from there).</p> <pre><code>repartByRangeDf.write.format(\"delta\").save(\"/tmp/zorder\")\n</code></pre> <pre><code>$ tree /tmp/zorder\n/tmp/zorder\n\u251c\u2500\u2500 _delta_log\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 00000000000000000000.json\n\u251c\u2500\u2500 part-00000-5677c9b7-7439-4365-8233-8f0e6184dcf3-c000.snappy.parquet\n\u251c\u2500\u2500 part-00001-9dfb628f-c225-4d23-acb8-0946f0c3617d-c000.snappy.parquet\n\u2514\u2500\u2500 part-00002-e1172c78-4d47-4947-bd37-f67c72701062-c000.snappy.parquet\n</code></pre> <pre><code>scala&gt; spark.read.load(\"/tmp/zorder/part-00000-*\").show\n+---+\n| id|\n+---+\n|  0|\n|  1|\n+---+\n\n\nscala&gt; spark.read.load(\"/tmp/zorder/part-00001-*\").show\n+---+\n| id|\n+---+\n|  2|\n|  3|\n+---+\n\n\nscala&gt; spark.read.load(\"/tmp/zorder/part-00002-*\").show\n+---+\n| id|\n+---+\n|  4|\n+---+\n</code></pre>","text":""},{"location":"commands/optimize/ZOrderClustering/","title":"ZOrderClustering","text":"<p><code>ZOrderClustering</code> is a SpaceFillingCurveClustering for MultiDimClustering.cluster utility.</p>"},{"location":"commands/optimize/ZOrderClustering/#getclusteringexpression","title":"getClusteringExpression <pre><code>getClusteringExpression(\n  cols: Seq[Column],\n  numRanges: Int): Column\n</code></pre> <p><code>getClusteringExpression</code> is part of the SpaceFillingCurveClustering abstraction.</p>  <p><code>getClusteringExpression</code> creates a range_partition_id function (with the given <code>numRanges</code> for the number of partitions) for every <code>Column</code> (in the given <code>cols</code>).</p> <p>In the end, <code>getClusteringExpression</code> interleave_bits with the <code>range_partition_id</code> columns and casts the (evaluation) result to <code>StringType</code>.</p>","text":""},{"location":"commands/optimize/ZOrderClustering/#demo","title":"Demo <p>For some reason, getClusteringExpression is <code>protected[skipping]</code> so let's hop over the fence with the following hack.</p> <p>Paste the following to <code>spark-shell</code> in <code>:paste -raw</code> mode:</p> <pre><code>package org.apache.spark.sql.delta.skipping\nobject protectedHack {\n  import org.apache.spark.sql.Column\n  def getClusteringExpression(\n    cols: Seq[Column], numRanges: Int): Column = {\n      ZOrderClustering.getClusteringExpression(cols, numRanges)\n    }\n}\n</code></pre> <pre><code>import org.apache.spark.sql.delta.skipping.protectedHack\nval clusterExpr = protectedHack.getClusteringExpression(cols = Seq($\"x\", $\"y\"), numRanges = 3)\n</code></pre> <pre><code>scala&gt; println(clusterExpr.expr.numberedTreeString)\n00 cast(interleavebits(rangepartitionid('x, 3), rangepartitionid('y, 3)) as string)\n01 +- interleavebits(rangepartitionid('x, 3), rangepartitionid('y, 3))\n02    :- rangepartitionid('x, 3)\n03    :  +- 'x\n04    +- rangepartitionid('y, 3)\n05       +- 'y\n</code></pre>","text":""},{"location":"commands/restore/","title":"RESTORE Command","text":"<p>Delta Lake supports restoring delta tables to a specified version or timestamp using the following high-level operators:</p> <ul> <li>RESTORE TABLE SQL statement</li> <li>DeltaTable.restoreToVersion operation</li> <li>DeltaTable.restoreToTimestamp operation</li> </ul>"},{"location":"commands/restore/#demo","title":"Demo","text":"<p>Demo: Rolling Back Table Changes (Restore Command)</p>"},{"location":"commands/restore/RestoreTableCommand/","title":"RestoreTableCommand","text":"<p><code>RestoreTableCommand</code> is a DeltaCommand to restore a delta table to a specified version or timestamp.</p> <p><code>RestoreTableCommand</code> is transactional (and starts a new transaction when executed).</p>"},{"location":"commands/restore/RestoreTableCommand/#creating-instance","title":"Creating Instance","text":"<p><code>RestoreTableCommand</code> takes the following to be created:</p> <ul> <li> DeltaTableV2 <li> <code>TableIdentifier</code> of the delta table <p><code>RestoreTableCommand</code> is created when:</p> <ul> <li><code>DeltaAnalysis</code> logical resolution rule is executed (to resolve a RestoreTableStatement)</li> </ul>"},{"location":"commands/restore/RestoreTableCommand/#restoretablecommandbase","title":"RestoreTableCommandBase <p><code>RestoreTableCommand</code> is a RestoreTableCommandBase.</p>","text":""},{"location":"commands/restore/RestoreTableCommand/#leafrunnablecommand","title":"LeafRunnableCommand <p><code>RestoreTableCommand</code> is a <code>LeafRunnableCommand</code> (Spark SQL).</p>","text":""},{"location":"commands/restore/RestoreTableCommand/#output-attributes","title":"Output Attributes <pre><code>output: Seq[Attribute]\n</code></pre> <p><code>output</code> is the outputSchema.</p> <p><code>output</code> is part of the <code>Command</code> (Spark SQL) abstraction.</p>","text":""},{"location":"commands/restore/RestoreTableCommand/#executing-command","title":"Executing Command <pre><code>run(\n  spark: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> requests the DeltaTableV2 for the DeltaLog.</p> <p><code>run</code> requests the DeltaTableV2 for the DeltaTimeTravelSpec to access the version.</p>  <p>Note</p> <p><code>RestoreTableCommand</code> does not even bother itself to check if the optional DeltaTimeTravelSpec (of the DeltaTableV2) is defined or not. It is assumed that it is available and so is the version.</p> <p>It is unlike the timestamp. Why?!</p>  <p><code>run</code> retrieves the timestamp (if defined).</p> <p><code>run</code> determines the version to restore based on the version or the timestamp (whatever defined). For the timestamp, <code>run</code> resolves the version by requesting the <code>DeltaLog</code> for the DeltaHistoryManager that in turn is requested for the active commit at that timestamp (with the <code>canReturnLastCommit</code> flag enabled).</p>  Either the version or timestamp should be provided for restore <p><code>run</code> asserts that either the version or timestamp are provided or throws an <code>IllegalArgumentException</code>:</p> <pre><code>Either the version or timestamp should be provided for restore\n</code></pre>  <p><code>run</code> requests the <code>DeltaLog</code> to update to find the latest version.</p>  Version to restore should be less then last available version <p><code>run</code> requires that the latest version of the delta table is higher than the requested version to restore or throws an <code>IllegalArgumentException</code>:</p> <pre><code>Version to restore ([versionToRestore])should be less then last available version ([latestVersion])\n</code></pre> <p>NB: You're right that there are a few typos in the exception message.</p>  <p><code>run</code> requests the <code>DeltaLog</code> to start a new transaction and does the heavy lifting.</p> <p><code>run</code> requests the <code>OptimisticTransaction</code> for the latest (current) Snapshot.</p> <p><code>run</code> requests the <code>DeltaLog</code> for the Snapshot at the version to restore (snapshot to restore to) and reconciles them using all the files (in the snapshots).</p>  Dataset[AddFile] <p>All the files are <code>Dataset[AddFile]</code>s.</p>  <p><code>run</code> determines <code>filesToAdd</code> (as <code>Dataset[AddFile]</code>). <code>run</code> uses <code>left_anti</code> join on the <code>Dataset[AddFile]</code> of the snapshot to restore to with the current snapshot on <code>path</code> column. <code>run</code> marks the <code>AddFile</code>s (in the joined <code>Dataset[AddFile]</code>) as dataChange.</p>  No Spark job started yet <p>No Spark job is started yet as <code>run</code> is only preparing it.</p>  <p><code>run</code> determines <code>filesToRemove</code> (as <code>Dataset[RemoveFile]</code>). <code>run</code> uses <code>left_anti</code> join on the <code>Dataset[AddFile]</code> of the current snapshot with the snapshot to restore to on <code>path</code> column. <code>run</code> marks the <code>AddFile</code>s (in the joined <code>Dataset[AddFile]</code>) to be removed with the current timestamp.</p>  No Spark job started yet <p>No Spark job is started yet as <code>run</code> is only preparing it.</p>  <p><code>run</code> checkSnapshotFilesAvailability when <code>spark.sql.files.ignoreMissingFiles</code> (Spark SQL) configuration property is enabled.</p>  spark.sql.files.ignoreMissingFiles <p><code>spark.sql.files.ignoreMissingFiles</code> is disabled by default.</p>  <p><code>run</code> computeMetrics (as a metrics Spark job).</p>  2 Spark Jobs Submitted <p>computeMetrics submits 2 aggregations that give 2 Spark jobs submitted.</p>  <p><code>run</code> creates a local (Scala) iterator over the <code>filesToAdd</code> dataset (as a add actions Spark job).</p>  Local Scala Iterator is Multiple Spark Jobs <p><code>run</code> uses <code>Dataset.toLocalIterator</code> (Spark SQL) action that triggers multiple jobs.</p>  <p><code>run</code> creates a local (Scala) iterator over the <code>filesToRemove</code> dataset (as a remove actions Spark job).</p> <p><code>run</code> requests the <code>OptimisticTransaction</code> to update the metadata to the Metadata of the snapshot to restore to.</p> <p><code>run</code> commitLarge the current transaction with the following:</p> <ul> <li>AddFiles and RemoveFiles</li> <li><code>DeltaOperations.Restore</code></li> </ul> <p>In the end, <code>run</code> returns the metrics.</p>  <p><code>run</code> is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p>","text":""},{"location":"commands/restore/RestoreTableCommand/#gettimestamp","title":"getTimestamp <pre><code>getTimestamp(): Option[String]\n</code></pre> <p><code>getTimestamp</code>...FIXME</p>","text":""},{"location":"commands/restore/RestoreTableCommand/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.commands.RestoreTableCommand</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.delta.commands.RestoreTableCommand=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"commands/restore/RestoreTableCommandBase/","title":"RestoreTableCommandBase","text":"<p><code>RestoreTableCommandBase</code> is an abstraction of RestoreTableCommands with pre-defined output schema and metrics.</p>"},{"location":"commands/restore/RestoreTableCommandBase/#output-attributes","title":"Output Attributes <pre><code>outputSchema: Seq[Attribute]\n</code></pre> <p><code>outputSchema</code> is a collection of <code>AttributeReference</code>s (Spark SQL).</p>    Name Data Type     <code>table_size_after_restore</code> <code>LongType</code>   <code>num_of_files_after_restore</code> <code>LongType</code>   <code>num_removed_files</code> <code>LongType</code>   <code>num_restored_files</code> <code>LongType</code>   <code>removed_files_size</code> <code>LongType</code>   <code>restored_files_size</code> <code>LongType</code>    <p><code>outputSchema</code> is used when:</p> <ul> <li><code>RestoreTableCommand</code> is requested for the output attributes</li> </ul>","text":""},{"location":"commands/restore/RestoreTableStatement/","title":"RestoreTableStatement Unary Logical Operator","text":"<p><code>RestoreTableStatement</code> is a unary logical operator (Spark SQL) that represents the following:</p> <ul> <li>RESTORE TABLE SQL statement</li> <li>DeltaTable.restoreToVersion operation</li> <li>DeltaTable.restoreToTimestamp operation</li> </ul>"},{"location":"commands/restore/RestoreTableStatement/#creating-instance","title":"Creating Instance","text":"<p><code>RestoreTableStatement</code> takes the following to be created:</p> <ul> <li> TimeTravel specification <p><code>RestoreTableStatement</code> is created when:</p> <ul> <li><code>DeltaSqlAstBuilder</code> is requested to parse RESTORE SQL statement</li> <li><code>DeltaTableOperations</code> is requested to executeRestore</li> <li>PreprocessTableRestore logical resolution rule is executed (to resolve delta tables)</li> </ul>"},{"location":"commands/restore/RestoreTableStatement/#analysis-phase","title":"Analysis Phase","text":"<p><code>RestoreTableStatement</code> is resolved to RestoreTableCommand (by DeltaAnalysis logical resolution rule).</p>"},{"location":"commands/restore/TimeTravel/","title":"TimeTravel","text":"<p><code>TimeTravel</code> is a leaf logical operator (LeafNode) for RESTORE command to time travel the child relation to the given timestamp or version.</p>"},{"location":"commands/restore/TimeTravel/#creating-instance","title":"Creating Instance","text":"<p><code>TimeTravel</code> takes the following to be created:</p> <ul> <li> Relation (as a LogicalPlan) <li> Timestamp (Expression) <li> Version <li>Creation Source ID</li> <p><code>TimeTravel</code> is created when:</p> <ul> <li><code>DeltaSqlAstBuilder</code> is requested to parse RESTORE command</li> <li><code>DeltaTableOperations</code> is requested to executeRestore</li> <li><code>PreprocessTableRestore</code> is requested to resolve RestoreTableStatement logical operators</li> </ul>"},{"location":"commands/restore/TimeTravel/#creation-source-id","title":"Creation Source ID <p><code>TimeTravel</code> is given a Creation Source ID when created.</p> <p>The Creation Source ID indicates the API used to time travel:</p> <ul> <li><code>sql</code> when <code>DeltaSqlAstBuilder</code> is requested to parse RESTORE command</li> <li><code>deltaTable</code> when <code>DeltaTableOperations</code> is requested to executeRestore</li> </ul>","text":""},{"location":"commands/restore/TimeTravel/#analysis-phase","title":"Analysis Phase <p><code>TimeTravel</code> is resolved to DeltaTimeTravelSpec when DeltaAnalysis logical resolution rule is resolving RestoreTableStatement unary logical operator.</p>","text":""},{"location":"commands/update/","title":"Update Command","text":"<p>Delta Lake supports updating (records in) delta tables using the following high-level operators:</p> <ul> <li>UPDATE TABLE SQL command</li> <li>DeltaTable.update or DeltaTable.updateExpr</li> </ul>"},{"location":"commands/update/DeltaUpdateTable/","title":"DeltaUpdateTable Unary Logical Operator","text":"<p><code>DeltaUpdateTable</code> is an unary logical operator (Spark SQL) that represents <code>UpdateTable</code> (Spark SQL) at execution.</p>"},{"location":"commands/update/DeltaUpdateTable/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaUpdateTable</code> takes the following to be created:</p> <ul> <li> Child <code>LogicalPlan</code> (Spark SQL) <li> Update Column Expressions (Spark SQL) <li> Update Expressions (Spark SQL) <li> (optional) Condition Expression (Spark SQL) <p><code>DeltaUpdateTable</code> is created\u00a0when:</p> <ul> <li>DeltaAnalysis logical resolution rule is executed and resolves <code>UpdateTable</code>s</li> </ul>"},{"location":"commands/update/DeltaUpdateTable/#logical-resolution","title":"Logical Resolution","text":"<p><code>DeltaUpdateTable</code> is resolved to a UpdateCommand when PreprocessTableUpdate post-hoc logical resolution rule is executed.</p>"},{"location":"commands/update/UpdateCommand/","title":"UpdateCommand","text":"<p><code>UpdateCommand</code> is a DeltaCommand that represents DeltaUpdateTable logical command at execution.</p> <p><code>UpdateCommand</code> is a <code>RunnableCommand</code> (Spark SQL) logical operator.</p>"},{"location":"commands/update/UpdateCommand/#creating-instance","title":"Creating Instance","text":"<p><code>UpdateCommand</code> takes the following to be created:</p> <ul> <li> TahoeFileIndex <li> Target Data (LogicalPlan) <li> Update Expressions (Spark SQL) <li> (optional) Condition Expression (Spark SQL) <p><code>UpdateCommand</code> is created when:</p> <ul> <li>PreprocessTableUpdate logical resolution rule is executed (and resolves a DeltaUpdateTable logical command)</li> </ul>"},{"location":"commands/update/UpdateCommand/#performance-metrics","title":"Performance Metrics","text":"Name web UI <code>numAddedFiles</code> number of files added. <code>numRemovedFiles</code> number of files removed. <code>numUpdatedRows</code> number of rows updated. <code>executionTimeMs</code> time taken to execute the entire operation <code>scanTimeMs</code> time taken to scan the files for matches <code>rewriteTimeMs</code> time taken to rewrite the matched files"},{"location":"commands/update/UpdateCommand/#executing-command","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p> <p><code>run</code>...FIXME</p>","text":""},{"location":"commands/update/UpdateCommand/#performupdate","title":"performUpdate <pre><code>performUpdate(\n  sparkSession: SparkSession,\n  deltaLog: DeltaLog,\n  txn: OptimisticTransaction): Unit\n</code></pre> <p><code>performUpdate</code>...FIXME</p>","text":""},{"location":"commands/update/UpdateCommand/#rewritefiles","title":"rewriteFiles <pre><code>rewriteFiles(\n  spark: SparkSession,\n  txn: OptimisticTransaction,\n  rootPath: Path,\n  inputLeafFiles: Seq[String],\n  nameToAddFileMap: Map[String, AddFile],\n  condition: Expression): Seq[FileAction]\n</code></pre> <p><code>rewriteFiles</code>...FIXME</p>","text":""},{"location":"commands/update/UpdateCommand/#buildupdatedcolumns","title":"buildUpdatedColumns <pre><code>buildUpdatedColumns(\n  condition: Expression): Seq[Column]\n</code></pre> <p><code>buildUpdatedColumns</code>...FIXME</p>","text":""},{"location":"commands/vacuum/","title":"Vacuum Command","text":"<p>Vacuum command allows for garbage collection of a delta table.</p> <p>Vacuum command can be executed as VACUUM SQL command or DeltaTable.vacuum operator.</p>"},{"location":"commands/vacuum/#demo","title":"Demo","text":""},{"location":"commands/vacuum/#vacuum-sql-command","title":"VACUUM SQL Command","text":"<pre><code>val q = sql(\"VACUUM delta.`/tmp/delta/t1`\")\n</code></pre> <pre><code>scala&gt; q.show\nDeleted 0 files and directories in a total of 2 directories.\n+------------------+\n|              path|\n+------------------+\n|file:/tmp/delta/t1|\n+------------------+\n</code></pre>"},{"location":"commands/vacuum/#deltatablevacuum","title":"DeltaTable.vacuum","text":"<pre><code>import io.delta.tables.DeltaTable\nDeltaTable.forPath(\"/tmp/delta/t1\").vacuum\n</code></pre>"},{"location":"commands/vacuum/#dry-run","title":"Dry Run","text":"<p>Visit Demo: Vacuum.</p>"},{"location":"commands/vacuum/VacuumCommand/","title":"VacuumCommand Utility","text":"<p><code>VacuumCommand</code> is a concrete VacuumCommandImpl for garbage collection of a delta table for the following:</p> <ul> <li> <p>DeltaTable.vacuum operator (as <code>DeltaTableOperations</code> is requested to execute vacuum command)</p> </li> <li> <p>VACUUM SQL command (as VacuumTableCommand is executed)</p> </li> </ul>"},{"location":"commands/vacuum/VacuumCommand/#garbage-collection-of-delta-table","title":"Garbage Collection of Delta Table <pre><code>gc(\n  spark: SparkSession,\n  deltaLog: DeltaLog,\n  dryRun: Boolean = true,\n  retentionHours: Option[Double] = None,\n  clock: Clock = new SystemClock): DataFrame\n</code></pre> <p><code>gc</code> requests the given <code>DeltaLog</code> to update (and hence give the latest Snapshot of the delta table).</p>","text":""},{"location":"commands/vacuum/VacuumCommand/#retentionmillis","title":"retentionMillis <p><code>gc</code> converts the retention hours to milliseconds and checkRetentionPeriodSafety (with deletedFileRetentionDuration table configuration).</p>","text":""},{"location":"commands/vacuum/VacuumCommand/#timestamp-to-delete-files-before","title":"Timestamp to Delete Files Before <p><code>gc</code> determines the timestamp to delete files before based on the retentionMillis (if defined) or defaults to minFileRetentionTimestamp table configuration.</p> <p><code>gc</code> prints out the following INFO message to the logs (with the path of the given DeltaLog):</p> <pre><code>Starting garbage collection (dryRun = [dryRun]) of untracked files older than [deleteBeforeTimestamp] in [path]\n</code></pre>","text":""},{"location":"commands/vacuum/VacuumCommand/#valid-files","title":"Valid Files <p><code>gc</code> requests the <code>Snapshot</code> for the state dataset and maps over partitions (<code>Dataset.mapPartitions</code>) with a map function that does the following (for every Action in a partition of SingleActions):</p> <ol> <li>Skips RemoveFiles with the deletion timestamp after the timestamp to delete files before</li> <li>Adds the path of FileActions (that live inside the directory of the table) with all subdirectories</li> <li>Skips other actions</li> </ol> <p><code>gc</code> converts the mapped state dataset into a <code>DataFrame</code> with a single <code>path</code> column.</p>  <p>Note</p> <p>There is no DataFrame action executed so no processing yet (using Spark).</p>","text":""},{"location":"commands/vacuum/VacuumCommand/#all-files-and-directories-dataset","title":"All Files and Directories Dataset <p><code>gc</code> finds all the files and directories (recursively) in the data path (with <code>spark.sql.sources.parallelPartitionDiscovery.parallelism</code> number of file listing tasks).</p>","text":""},{"location":"commands/vacuum/VacuumCommand/#caching-all-files-and-directories-dataset","title":"Caching All Files and Directories Dataset <p><code>gc</code> caches the allFilesAndDirs dataset.</p>","text":""},{"location":"commands/vacuum/VacuumCommand/#number-of-directories","title":"Number of Directories <p><code>gc</code> counts the number of directories (as the count of the rows with <code>isDir</code> column being <code>true</code> in the allFilesAndDirs dataset).</p>  <p>Note</p> <p>This step submits a Spark job for <code>Dataset.count</code>.</p>","text":""},{"location":"commands/vacuum/VacuumCommand/#paths-dataset","title":"Paths Dataset <p><code>gc</code> creates a Spark SQL query to count <code>path</code>s of the allFilesAndDirs with files with the <code>modificationTime</code> ealier than the deleteBeforeTimestamp and the directories (<code>isDir</code>s). That creates a <code>DataFrame</code> of <code>path</code> and <code>count</code> columns.</p> <p><code>gc</code> uses left-anti join of the counted path <code>DataFrame</code> with the validFiles on <code>path</code>.</p> <p><code>gc</code> filters out paths with <code>count</code> more than <code>1</code> and selects <code>path</code>.</p>","text":""},{"location":"commands/vacuum/VacuumCommand/#dry-run","title":"Dry Run <p><code>gc</code> counts the rows in the paths Dataset for the number of files and directories that are safe to delete (numFiles).</p>  <p>Note</p> <p>This step submits a Spark job for <code>Dataset.count</code>.</p>  <p><code>gc</code> prints out the following message to the console (with the dirCounts):</p> <pre><code>Found [numFiles] files and directories in a total of [dirCounts] directories that are safe to delete.\n</code></pre> <p>In the end, <code>gc</code> converts the paths to Hadoop DFS format and creates a <code>DataFrame</code> with a single <code>path</code> column.</p>","text":""},{"location":"commands/vacuum/VacuumCommand/#deleting-files-and-directories","title":"Deleting Files and Directories <p><code>gc</code> prints out the following INFO message to the logs:</p> <pre><code>Deleting untracked files and empty directories in [path]\n</code></pre> <p><code>gc</code> deletes the untracked files and empty directories (with parallel delete enabled flag based on spark.databricks.delta.vacuum.parallelDelete.enabled configuration property).</p> <p><code>gc</code> prints out the following message to standard output (with the dirCounts):</p> <pre><code>Deleted [filesDeleted] files and directories in a total of [dirCounts] directories.\n</code></pre> <p>In the end, <code>gc</code> creates a <code>DataFrame</code> with a single <code>path</code> column with just the data path of the delta table to vacuum.</p>","text":""},{"location":"commands/vacuum/VacuumCommand/#unpersist-all-files-and-directories-dataset","title":"Unpersist All Files and Directories Dataset <p><code>gc</code> unpersists the allFilesAndDirs dataset.</p>","text":""},{"location":"commands/vacuum/VacuumCommand/#checkretentionperiodsafety","title":"checkRetentionPeriodSafety <pre><code>checkRetentionPeriodSafety(\n  spark: SparkSession,\n  retentionMs: Option[Long],\n  configuredRetention: Long): Unit\n</code></pre> <p><code>checkRetentionPeriodSafety</code>...FIXME</p>","text":""},{"location":"commands/vacuum/VacuumCommand/#logging","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.commands.VacuumCommand</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.delta.commands.VacuumCommand=ALL\n</code></pre> <p>Refer to Logging.</p>","text":""},{"location":"commands/vacuum/VacuumCommandImpl/","title":"VacuumCommandImpl","text":"<p><code>VacuumCommandImpl</code>\u00a0is a DeltaCommand.</p> <p>Note</p> <p><code>VacuumCommandImpl</code>\u00a0is a Scala trait just to let Databricks provide a commercial version of vacuum command.</p>"},{"location":"commands/vacuum/VacuumCommandImpl/#delete","title":"delete <pre><code>delete(\n  diff: Dataset[String],\n  spark: SparkSession,\n  basePath: String,\n  hadoopConf: Broadcast[SerializableConfiguration],\n  parallel: Boolean): Long\n</code></pre> <p><code>delete</code>...FIXME</p> <p><code>delete</code>\u00a0is used when:</p> <ul> <li><code>VacuumCommand</code> is requested to gc</li> </ul>","text":""},{"location":"commands/vacuum/VacuumTableCommand/","title":"VacuumTableCommand","text":"<p><code>VacuumTableCommand</code> is a runnable command (Spark SQL) for VACUUM SQL command.</p>"},{"location":"commands/vacuum/VacuumTableCommand/#creating-instance","title":"Creating Instance","text":"<p><code>VacuumTableCommand</code> takes the following to be created:</p> <ul> <li> Path <li> <code>TableIdentifier</code> <li> Optional Horizon Hours <li> <code>dryRun</code> flag <p><code>VacuumTableCommand</code> requires that either the table or the path is defined and it is the root directory of a delta table. Partition directories are not supported.</p> <p><code>VacuumTableCommand</code> is created\u00a0when:</p> <ul> <li><code>DeltaSqlAstBuilder</code> is requested to parse VACUUM SQL command</li> </ul>"},{"location":"commands/vacuum/VacuumTableCommand/#executing-command","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p> <p><code>run</code> takes the path to vacuum (either the table or the path) and finds the root directory of the delta table.</p> <p><code>run</code> creates a DeltaLog instance for the delta table and gc it (passing in the <code>DeltaLog</code> instance, the dryRun and the horizonHours options).</p> <p><code>run</code> throws an <code>AnalysisException</code> when executed for a non-root directory of a delta table:</p> <pre><code>Please provide the base path ([baseDeltaPath]) when Vacuuming Delta tables. Vacuuming specific partitions is currently not supported.\n</code></pre> <p><code>run</code> throws an <code>AnalysisException</code> when executed for a <code>DeltaLog</code> with the snapshot version being <code>-1</code>:</p> <pre><code>[deltaTableIdentifier] is not a Delta table. VACUUM is only supported for Delta tables.\n</code></pre>","text":""},{"location":"commands/vacuum/VacuumTableCommand/#output-schema","title":"Output Schema <p>The output schema of <code>VacuumTableCommand</code> is a single <code>path</code> column (of type <code>StringType</code>).</p>","text":""},{"location":"constraints/","title":"Table Constraints","text":"<p>Table constraints can be one of the following:</p> <ul> <li>Column Invariants</li> <li>CHECK Constraints</li> <li>Generated Columns</li> </ul>"},{"location":"constraints/#references","title":"References","text":"<ul> <li>Constraints</li> </ul>"},{"location":"constraints/CheckDeltaInvariant/","title":"CheckDeltaInvariant","text":"<p><code>CheckDeltaInvariant</code> is a <code>UnaryExpression</code> (Spark SQL) for DeltaInvariantCheckerExec physical operator.</p>"},{"location":"constraints/CheckDeltaInvariant/#creating-instance","title":"Creating Instance","text":"<p><code>CheckDeltaInvariant</code> takes the following to be created:</p> <ul> <li> Child Expression (Spark SQL) <li> Column Extractors (<code>Map[String, Expression]</code>) <li> Constraint <p><code>CheckDeltaInvariant</code> is created using withBoundReferences and\u00a0when:</p> <ul> <li><code>DeltaInvariantCheckerExec</code> physical operator is executed</li> </ul>"},{"location":"constraints/CheckDeltaInvariant/#evaluating-expression","title":"Evaluating Expression <pre><code>eval(\n  input: InternalRow): Any\n</code></pre> <p><code>eval</code>\u00a0is part of the <code>Expression</code> (Spark SQL) abstraction.</p> <p><code>eval</code> asserts the constraint on the input <code>InternalRow</code>.</p>","text":""},{"location":"constraints/CheckDeltaInvariant/#asserts-constraint","title":"Asserts Constraint <pre><code>assertRule(\n  input: InternalRow): Unit\n</code></pre> <p><code>assertRule</code>...FIXME</p>","text":""},{"location":"constraints/CheckDeltaInvariant/#creating-checkdeltainvariant-with-boundreferences","title":"Creating CheckDeltaInvariant with BoundReferences <pre><code>withBoundReferences(\n  input: AttributeSeq): CheckDeltaInvariant\n</code></pre> <p><code>withBoundReferences</code>...FIXME</p> <p><code>withBoundReferences</code>\u00a0is used when:</p> <ul> <li><code>DeltaInvariantCheckerExec</code> physical operator is executed</li> </ul>","text":""},{"location":"constraints/Constraint/","title":"Constraint","text":"<p><code>Constraint</code> is an abstraction of table constraints that writers have to assert before writing.</p>"},{"location":"constraints/Constraint/#contract","title":"Contract","text":""},{"location":"constraints/Constraint/#name","title":"Name <pre><code>name: String\n</code></pre> <p>Used when:</p> <ul> <li><code>InvariantViolationException</code> utility is used to create an InvariantViolationException for a check constraint</li> </ul>","text":""},{"location":"constraints/Constraint/#implementations","title":"Implementations","text":"Sealed Trait <p><code>Constraint</code> is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).</p>"},{"location":"constraints/Constraint/#check","title":"Check <p>A constraint with a SQL expression (Spark SQL) to check for when writing out data</p>","text":""},{"location":"constraints/Constraint/#notnull","title":"NotNull <p>A constraint on a column to be not null</p> <p>Name: <code>NOT NULL</code></p>","text":""},{"location":"constraints/Constraints/","title":"Constraints Utility","text":""},{"location":"constraints/Constraints/#extracting-all-constraints","title":"Extracting All Constraints <pre><code>getAll(\n  metadata: Metadata,\n  spark: SparkSession): Seq[Constraint]\n</code></pre> <p><code>getAll</code> extracts CHECK constraints (from the given table metadata).</p> <p><code>getAll</code> extracts invariants (from the schema of the given table metadata).</p> <p>In the end, <code>getAll</code> returns the CHECK constraints and invariants.</p>  <p><code>getAll</code>\u00a0is used when:</p> <ul> <li><code>TransactionalWrite</code> is requested to write data out</li> </ul>","text":""},{"location":"constraints/Constraints/#extracting-check-constraints-from-table-metadata","title":"Extracting Check Constraints from Table Metadata <pre><code>getCheckConstraints(\n  metadata: Metadata,\n  spark: SparkSession): Seq[Constraint]\n</code></pre> <p><code>getCheckConstraints</code> extracts Check constraints from the <code>delta.constraints.</code>-keyed entries in the configuration of the given Metadata:</p> <ul> <li>The name is the key without the <code>delta.constraints.</code> prefix</li> <li>The expression is the value parsed</li> </ul> <p><code>getCheckConstraints</code>\u00a0is used when:</p> <ul> <li><code>Constraints</code> utility is used to extract all constraints</li> <li><code>Protocol</code> utility is used to determine the required minimum protocol</li> </ul>","text":""},{"location":"constraints/DeltaInvariantChecker/","title":"DeltaInvariantChecker Unary Logical Operator","text":"<p><code>DeltaInvariantChecker</code> is a <code>UnaryNode</code> (Spark SQL).</p>"},{"location":"constraints/DeltaInvariantChecker/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaInvariantChecker</code> takes the following to be created:</p> <ul> <li> Child <code>LogicalPlan</code> (Spark SQL) <li> Constraints <p>Note</p> <p><code>DeltaInvariantChecker</code> does not seem to be created\u00a0at all.</p>"},{"location":"constraints/DeltaInvariantChecker/#execution-planning","title":"Execution Planning","text":"<p><code>DeltaInvariantChecker</code> is planned as DeltaInvariantCheckerExec unary physical operator by DeltaInvariantCheckerStrategy execution planning strategy.</p>"},{"location":"constraints/DeltaInvariantCheckerExec/","title":"DeltaInvariantCheckerExec Unary Physical Operator","text":"<p><code>DeltaInvariantCheckerExec</code> is an <code>UnaryExecNode</code> (Spark SQL) to assert constraints.</p>"},{"location":"constraints/DeltaInvariantCheckerExec/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaInvariantCheckerExec</code> takes the following to be created:</p> <ul> <li> Child <code>SparkPlan</code> (Spark SQL) <li> Constraints <p><code>DeltaInvariantCheckerExec</code> is created\u00a0when:</p> <ul> <li><code>TransactionalWrite</code> is requested to write data out</li> <li>DeltaInvariantCheckerStrategy execution planning strategy is executed</li> </ul>"},{"location":"constraints/DeltaInvariantCheckerExec/#executing-physical-operator","title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code>\u00a0is part of the <code>SparkPlan</code> (Spark SQL) abstraction.</p> <p><code>doExecute</code> builds invariants for the given constraints and applies (evaluates) them to every row from the child physical operator.</p> <p><code>doExecute</code> simply requests the child physical operator to execute (and becomes a noop) for no constraints.</p>","text":""},{"location":"constraints/DeltaInvariantCheckerExec/#building-invariants","title":"Building Invariants <pre><code>buildInvariantChecks(\n  output: Seq[Attribute],\n  constraints: Seq[Constraint],\n  spark: SparkSession): Seq[CheckDeltaInvariant]\n</code></pre> <p><code>buildInvariantChecks</code> converts the given Constraints into CheckDeltaInvariants.</p>","text":""},{"location":"constraints/DeltaInvariantCheckerStrategy/","title":"DeltaInvariantCheckerStrategy Execution Planning Strategy","text":"<p><code>DeltaInvariantCheckerStrategy</code> is a <code>SparkStrategy</code> (Spark SQL) to plan a DeltaInvariantChecker unary logical operator (with constraints attached) into a DeltaInvariantCheckerExec for execution.</p> <p>Danger</p> <p><code>DeltaInvariantCheckerStrategy</code> does not seem to be used at all.</p>"},{"location":"constraints/DeltaInvariantCheckerStrategy/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaInvariantCheckerStrategy</code> is a Scala object and takes no input arguments to be created.</p>"},{"location":"constraints/DeltaInvariantCheckerStrategy/#executing-rule","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): Seq[SparkPlan]\n</code></pre> <p><code>apply</code>\u00a0is part of the <code>SparkStrategy</code> (Spark SQL) abstraction.</p> <p>For a given DeltaInvariantChecker unary logical operator with constraints attached, <code>apply</code> creates a DeltaInvariantCheckerExec unary physical operator. Otherwise, <code>apply</code> does nothing (noop).</p>","text":""},{"location":"constraints/Invariant/","title":"Invariant","text":"<p><code>Invariant</code> represents a Rule associated with a column.</p>"},{"location":"constraints/Invariant/#creating-instance","title":"Creating Instance","text":"<p><code>Invariant</code> takes the following to be created:</p> <ul> <li> Column Name <li> <code>Rule</code>"},{"location":"constraints/InvariantViolationException/","title":"InvariantViolationException","text":"<p><code>InvariantViolationException</code> is a <code>RuntimeException</code> (Java) that is reported when data does not match the rules of a table (using Constraints).</p>"},{"location":"constraints/InvariantViolationException/#creating-instance","title":"Creating Instance","text":"<p><code>InvariantViolationException</code> takes the following to be created:</p> <ul> <li> Error Message <p><code>InvariantViolationException</code> is created\u00a0when:</p> <ul> <li><code>DeltaErrors</code> utility is used to notNullColumnMissingException</li> <li><code>InvariantViolationException</code> utility is used to apply</li> </ul>"},{"location":"constraints/InvariantViolationException/#creating-invariantviolationexception","title":"Creating InvariantViolationException <p><code>apply</code> creates a InvariantViolationException for the given constraint: Check or NotNull.</p>","text":""},{"location":"constraints/InvariantViolationException/#check","title":"Check <pre><code>apply(\n  constraint: Constraints.Check,\n  values: Map[String, Any]): InvariantViolationException\n</code></pre> <p>Check</p> <pre><code>CHECK constraint [name] [sql] violated by row with values:\n - [column] : [value]\n</code></pre>","text":""},{"location":"constraints/InvariantViolationException/#notnull","title":"NotNull <pre><code>apply(\n  constraint: Constraints.NotNull): InvariantViolationException\n</code></pre> <p>NotNull</p> <pre><code>NOT NULL constraint violated for column: [name]\n</code></pre>","text":""},{"location":"constraints/InvariantViolationException/#usage","title":"Usage <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>CheckDeltaInvariant</code> is used to eval (and assertRule)</li> </ul>","text":""},{"location":"contenders/","title":"Contenders","text":"<p>As it happens in the open source software world, Delta Lake is not alone in the area of Data Lakes on top of Apache Spark. The following is a list of some other open source projects that seems to compete or cover the same use cases.</p>"},{"location":"contenders/#apache-iceberg","title":"Apache Iceberg","text":"<p>Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to Presto and Spark that use a high-performance format that works just like a SQL table.</p>"},{"location":"contenders/#videos","title":"Videos","text":"<ul> <li>ACID ORC, Iceberg, and Delta Lake\u2014An Overview of Table Formats for Large Scale Storage and Analytics</li> <li>Introducing Iceberg Tables designed for object stores</li> <li>Introducing Apache Iceberg: Tables Designed for Object Stores</li> <li>Iceberg: a fast table format for S3</li> </ul>"},{"location":"contenders/#apache-hudi","title":"Apache Hudi","text":"<p>Apache Hudi ingests and manages storage of large analytical datasets over DFS (HDFS or cloud stores) and provides three logical views for query access.</p>"},{"location":"contenders/#videos_1","title":"Videos","text":"<ul> <li>Hoodie: An Open Source Incremental Processing Framework From Uber</li> <li>Powering Uber's global network analytics pipelines in real-time with Apache Hudi</li> </ul>"},{"location":"data-skipping/","title":"Data Skipping","text":"<p>Data Skipping is an optimization of queries with filter clauses that uses data skipping column statistics to find the set of parquet data files that need to be queried (and prune away files that do not match the filters and contain no rows the query cares about). That means that no filters effectively skips data skipping.</p> <p>Data Skipping is enabled using spark.databricks.delta.stats.skipping configuration property.</p>"},{"location":"data-skipping/#limit-pushdown","title":"LIMIT Pushdown","text":"<p>LIMIT Pushdown</p>"},{"location":"data-skipping/#internals","title":"Internals","text":"<p>Data Skipping uses DataSkippingReaderBase as the main abstraction for scanning parquet data files (with getDataSkippedFiles being the crucial part).</p>"},{"location":"data-skipping/#demo","title":"Demo","text":"<p>Data Skipping</p>"},{"location":"data-skipping/#learn-more","title":"Learn More","text":"<ol> <li>Delta Lake 1.2 - More Speed, Efficiency and Extensibility Than Ever</li> </ol>"},{"location":"data-skipping/ColumnPredicateBuilder/","title":"ColumnPredicateBuilder","text":"<p><code>ColumnPredicateBuilder</code> is a DataSkippingPredicateBuilder that UsesMetadataFields.</p>"},{"location":"data-skipping/ColumnPredicateBuilder/#equalto","title":"equalTo  Signature <pre><code>equalTo(\n  statsProvider: StatsProvider,\n  colPath: Seq[String],\n  value: Column): Option[DataSkippingPredicate]\n</code></pre> <p><code>equalTo</code> is part of the DataSkippingPredicateBuilder abstraction.</p>  <p><code>equalTo</code> requests the given StatsProvider for getPredicateWithStatTypes for the given <code>colPath</code> and the following metadata:</p> <ul> <li>minValues</li> <li>maxValues</li> </ul> <p><code>equalTo</code> builds a Catalyst expression to match files with the requested <code>value</code>:</p> <pre><code>min &lt;= value &amp;&amp; value &lt;= max\n</code></pre>","text":""},{"location":"data-skipping/ColumnPredicateBuilder/#greaterthan","title":"greaterThan  Signature <pre><code>greaterThan(\n  statsProvider: StatsProvider,\n  colPath: Seq[String],\n  value: Column): Option[DataSkippingPredicate]\n</code></pre> <p><code>greaterThan</code> is part of the DataSkippingPredicateBuilder abstraction.</p>  <p><code>greaterThan</code> requests the given StatsProvider for getPredicateWithStatType for the given <code>colPath</code> and the following metadata:</p> <ul> <li>maxValues</li> </ul> <p><code>greaterThan</code> builds a Catalyst expression to match files with the requested <code>value</code>:</p> <pre><code>c &gt; value\n</code></pre>","text":""},{"location":"data-skipping/CountStarDeltaTable/","title":"CountStarDeltaTable Scala Extractor","text":"<p><code>CountStarDeltaTable</code> is an extractor object (Scala) to extractGlobalCount for an Aggregate logical operator.</p>"},{"location":"data-skipping/CountStarDeltaTable/#matching-aggregate","title":"Matching Aggregate <pre><code>unapply(\n  plan: Aggregate): Option[Long]\n</code></pre> <p><code>unapply</code> extractGlobalCount for an <code>Aggregate</code> (Spark SQL) logical operator with the following:</p> <ul> <li>No grouping expressions</li> <li><code>Alias(AggregateExpression(Count(Seq(Literal(1, _))), Complete, false, None, _), _)</code> aggregate expression</li> <li>DeltaTable over a TahoeLogFileIndex with no partition filters</li> </ul> <p>Otherwise, <code>unapply</code> gives <code>None</code> (to indicate no match).</p>  <p><code>unapply</code> is used when:</p> <ul> <li><code>OptimizeMetadataOnlyDeltaQuery</code> is requested to optimizeQueryWithMetadata</li> </ul>","text":""},{"location":"data-skipping/CountStarDeltaTable/#extractglobalcount","title":"extractGlobalCount <pre><code>extractGlobalCount(\n  tahoeLogFileIndex: TahoeLogFileIndex): Option[Long]\n</code></pre> <p><code>extractGlobalCount</code> gets a DeltaScanGenerator for the given TahoeLogFileIndex.</p> <p><code>extractGlobalCount</code> requests the <code>DeltaScanGenerator</code> to filesWithStatsForScan (that gives a <code>DataFrame</code>) to execute the following aggregations on:</p> <ul> <li><code>sum(\"stats.numRecords\")</code></li> <li><code>count(when(col(\"stats.numRecords\").isNull, 1))</code></li> </ul>","text":""},{"location":"data-skipping/DataFiltersBuilder/","title":"DataFiltersBuilder","text":"<p><code>DataFiltersBuilder</code> builds data filters for Data Skipping.</p> <p><code>DataFiltersBuilder</code> used when <code>DataSkippingReaderBase</code> is requested for the filesForScan with data filters and spark.databricks.delta.stats.skipping enabled.</p>"},{"location":"data-skipping/DataFiltersBuilder/#creating-instance","title":"Creating Instance","text":"<p><code>DataFiltersBuilder</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> DeltaDataSkippingType <p><code>DataFiltersBuilder</code> is created when:</p> <ul> <li><code>DataSkippingReaderBase</code> is requested to filesForScan (with data filters and spark.databricks.delta.stats.skipping enabled)</li> </ul>"},{"location":"data-skipping/DataFiltersBuilder/#statsprovider","title":"StatsProvider <p><code>DataFiltersBuilder</code> creates a StatsProvider (for the getStatsColumnOpt) when created.</p>","text":""},{"location":"data-skipping/DataFiltersBuilder/#creating-dataskippingpredicate","title":"Creating DataSkippingPredicate <pre><code>apply(\n  dataFilter: Expression): Option[DataSkippingPredicate]\n</code></pre> <p><code>apply</code> constructDataFilters for the given <code>dataFilter</code> expression.</p>  <p><code>apply</code> is used when:</p> <ul> <li><code>DataSkippingReaderBase</code> is requested to filesForScan (with data filters and spark.databricks.delta.stats.skipping enabled)</li> </ul>","text":""},{"location":"data-skipping/DataFiltersBuilder/#constructdatafilters","title":"constructDataFilters <pre><code>constructDataFilters(\n  dataFilter: Expression): Option[DataSkippingPredicate]\n</code></pre> <p><code>constructDataFilters</code> creates a <code>DataSkippingPredicate</code> for expression types that can be used for data skipping.</p>  <p><code>constructDataFilters</code>...FIXME</p> <p>For <code>IsNull</code> with a skipping-eligible column, <code>constructDataFilters</code> requests the StatsProvider for the getPredicateWithStatType for nullCount to build a Catalyst expression to match files with null count larger than zero.</p> <pre><code>nullCount &gt; Literal(0)\n</code></pre> <p>For <code>IsNotNull</code> with a skipping-eligible column, <code>constructDataFilters</code> creates <code>StatsColumn</code>s for the following:</p> <ul> <li>nullCount</li> <li>numRecords</li> </ul> <p><code>constructDataFilters</code> requests the StatsProvider for the getPredicateWithStatsColumns for the two <code>StatsColumn</code>s to build a Catalyst expression to match files with null count less than the row count.</p> <pre><code>nullCount &lt; numRecords\n</code></pre> <p><code>constructDataFilters</code>...FIXME</p>","text":""},{"location":"data-skipping/DataFiltersBuilder/#constructliteralinlistdatafilters","title":"constructLiteralInListDataFilters <pre><code>constructLiteralInListDataFilters(\n  a: Expression,\n  possiblyNullValues: Seq[Any]): Option[DataSkippingPredicate]\n</code></pre> <p><code>constructLiteralInListDataFilters</code>...FIXME</p>","text":""},{"location":"data-skipping/DataSkippingPredicateBuilder/","title":"DataSkippingPredicateBuilder","text":"<p><code>DataSkippingPredicateBuilder</code> is an abstraction of predicate builders in Data Skipping.</p>"},{"location":"data-skipping/DataSkippingPredicateBuilder/#contract-subset","title":"Contract (Subset)","text":""},{"location":"data-skipping/DataSkippingPredicateBuilder/#equalto","title":"equalTo <pre><code>equalTo(\n  statsProvider: StatsProvider,\n  colPath: Seq[String],\n  value: Column): Option[DataSkippingPredicate]\n</code></pre> <p><code>DataSkippingPredicate</code> that matches files (based on their min/max range) which contains the requested point</p> <p>See:</p> <ul> <li>ColumnPredicateBuilder</li> </ul> <p>Used when:</p> <ul> <li><code>DataFiltersBuilder</code> is requested to constructDataFilters for <code>EqualTo</code> predicate</li> </ul>","text":""},{"location":"data-skipping/DataSkippingPredicateBuilder/#greaterthan","title":"greaterThan <pre><code>greaterThan(\n  statsProvider: StatsProvider,\n  colPath: Seq[String],\n  value: Column): Option[DataSkippingPredicate]\n</code></pre> <p><code>DataSkippingPredicate</code> that matches files (based on their min/max range) which contains values larger than the requested lower bound</p> <p>See:</p> <ul> <li>ColumnPredicateBuilder</li> </ul> <p>Used when:</p> <ul> <li><code>DataFiltersBuilder</code> is requested to constructDataFilters for <code>GreaterThan</code> predicate</li> </ul>","text":""},{"location":"data-skipping/DataSkippingPredicateBuilder/#implementations","title":"Implementations","text":"<ul> <li>ColumnPredicateBuilder</li> </ul>"},{"location":"data-skipping/DataSkippingReaderBase/","title":"DataSkippingReaderBase","text":"<p><code>DataSkippingReaderBase</code> is an extension of the DeltaScanGenerator abstraction for DeltaScan generators.</p> <p>The heart of <code>DataSkippingReaderBase</code> (and Data Skipping in general) is the withStats DataFrame.</p>"},{"location":"data-skipping/DataSkippingReaderBase/#contract","title":"Contract","text":""},{"location":"data-skipping/DataSkippingReaderBase/#allfiles-dataset-of-addfiles","title":"allFiles Dataset (of AddFiles) <pre><code>allFiles: Dataset[AddFile]\n</code></pre> <p><code>Dataset</code> of AddFiles</p> <p>See:</p> <ul> <li>Snapshot</li> </ul> <p>Used when:</p> <ul> <li><code>DataSkippingReaderBase</code> is requested to withStatsInternal0, withNoStats, getAllFiles, filterOnPartitions, getSpecificFilesWithStats</li> </ul>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#deltalog","title":"DeltaLog <pre><code>deltaLog: DeltaLog\n</code></pre> <p>DeltaLog</p> <p>Used when:</p> <ul> <li><code>DataSkippingReaderBase</code> is requested to filesForScan</li> </ul>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#metadata","title":"Metadata <pre><code>metadata: Metadata\n</code></pre> <p>Metadata</p> <p>Used when:</p> <ul> <li><code>DataSkippingReaderBase</code> is requested for the columnMappingMode, and to getStatsColumnOpt, filesWithStatsForScan, constructPartitionFilters, filterOnPartitions, filesForScan</li> </ul>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#numoffilesopt","title":"numOfFilesOpt <pre><code>numOfFilesOpt: Option[Long]\n</code></pre> <p>See:</p> <ul> <li>Snapshot</li> </ul> <p>Used when:</p> <ul> <li><code>DataSkippingReaderBase</code> is requested to filesForScan</li> </ul>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#path","title":"Path <pre><code>path: Path\n</code></pre> <p>Hadoop Path</p>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#redacted-path","title":"Redacted Path <pre><code>redactedPath: String\n</code></pre> <p>Used when:</p> <ul> <li><code>DataSkippingReaderBase</code> is requested to withStatsCache</li> </ul>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#schema","title":"Schema <pre><code>schema: StructType\n</code></pre> <p>See:</p> <ul> <li>SnapshotDescriptor</li> </ul> <p>Used when:</p> <ul> <li><code>DataSkippingReaderBase</code> is requested to filesForScan</li> </ul>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#sizeinbytes","title":"sizeInBytes <pre><code>sizeInBytes: Long\n</code></pre> <p>Used when:</p> <ul> <li><code>DataSkippingReaderBase</code> is requested to filesForScan</li> </ul>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#version","title":"version <pre><code>version: Long\n</code></pre> <p>Used when:</p> <ul> <li><code>DataSkippingReaderBase</code> is requested to withStatsCache, filesForScan</li> </ul>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#implementations","title":"Implementations","text":"<ul> <li>Snapshot</li> </ul>"},{"location":"data-skipping/DataSkippingReaderBase/#statsskipping","title":"stats.skipping <p><code>DataSkippingReaderBase</code> uses spark.databricks.delta.stats.skipping configuration property for filesForScan.</p>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#withstats-dataframe","title":"withStats DataFrame <pre><code>withStats: DataFrame\n</code></pre> <p><code>withStats</code> withStatsInternal.</p>  Final Method <p><code>withStats</code> is a Scala final method and may not be overridden in subclasses.</p> <p>Learn more in the Scala Language Specification.</p>   <p><code>withStats</code> is used when:</p> <ul> <li><code>DataSkippingReaderBase</code> is requested to filesWithStatsForScan, getAllFiles, filterOnPartitions, filterOnPartitions, getDataSkippedFiles, filesForScan</li> </ul>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#withstatsinternal-dataframe","title":"withStatsInternal DataFrame <pre><code>withStatsInternal: DataFrame\n</code></pre> <p><code>withStatsInternal</code> requests the withStatsCache for the DS.</p>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#withstatscache","title":"withStatsCache <pre><code>withStatsCache: CachedDS[Row]\n</code></pre>  Lazy Value <p><code>withStatsCache</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>withStatsCache</code> caches the withStatsInternal0 DataFrame under the following name (with the version and the redactedPath):</p> <pre><code>Delta Table State with Stats #[version] - [redactedPath]\n</code></pre>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#withstatsinternal0-dataframe","title":"withStatsInternal0 DataFrame <pre><code>withStatsInternal0: DataFrame\n</code></pre> <p><code>withStatsInternal0</code> is the allFiles Dataset with the statistics parsed (the <code>stats</code> column decoded from JSON).</p>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#filesforscan","title":"filesForScan  Signature <pre><code>filesForScan(\n  limit: Long): DeltaScan\nfilesForScan(\n  limit: Long,\n  partitionFilters: Seq[Expression]): DeltaScan\nfilesForScan(\n  filters: Seq[Expression],\n  keepNumRecords: Boolean): DeltaScan\n</code></pre> <p><code>filesForScan</code> is part of the DeltaScanGenerator abstraction.</p>  <p><code>filesForScan</code> branches off based on the given <code>filters</code> expressions and the schema.</p> <p>If the given <code>filters</code> expressions are either <code>TrueLiteral</code> or empty, or the schema is empty, <code>filesForScan</code> executes delta.skipping.none code path.</p> <p>If there are partition-based filter expressions only (among the <code>filters</code> expressions), <code>filesForScan</code> executes delta.skipping.partition code path. Otherwise, <code>filesForScan</code> executes delta.skipping.data code path.</p>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#no-data-skipping","title":"No Data Skipping <p><code>filesForScan</code>...FIXME</p>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#deltaskippingpartition","title":"delta.skipping.partition <p><code>filesForScan</code>...FIXME</p>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#deltaskippingdata","title":"delta.skipping.data <p><code>filesForScan</code> constructs the final partition filters with the partition filters (of the given <code>filters</code> expressions).</p> <p>With spark.databricks.delta.stats.skipping configuration property enabled, <code>filesForScan</code> creates a file skipping predicate expression for every data filter.</p> <p><code>filesForScan</code> getDataSkippedFiles for the final partition-only and data skipping filters (that leverages data skipping statistics to find the set of parquet files that need to be queried).</p> <p>In the end, creates a DeltaScan (with the files and sizes, and <code>dataSkippingOnlyV1</code> or <code>dataSkippingAndPartitionFilteringV1</code> data skipping types).</p>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#buildsizecollectorfilter","title":"buildSizeCollectorFilter <pre><code>buildSizeCollectorFilter(): (ArrayAccumulator, Column =&gt; Column)\n</code></pre> <p><code>buildSizeCollectorFilter</code>...FIXME</p>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#verifystatsforfilter","title":"verifyStatsForFilter <pre><code>verifyStatsForFilter(\n  referencedStats: Set[StatsColumn]): Column\n</code></pre> <p><code>verifyStatsForFilter</code>...FIXME</p>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#prunefilesbylimit","title":"pruneFilesByLimit <pre><code>pruneFilesByLimit(\n  df: DataFrame,\n  limit: Long): ScanAfterLimit\n</code></pre> <p><code>pruneFilesByLimit</code>...FIXME</p>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#getfilesandnumrecords","title":"getFilesAndNumRecords <pre><code>getFilesAndNumRecords(\n  df: DataFrame): Iterator[(AddFile, NumRecords)]\n</code></pre> <p><code>getFilesAndNumRecords</code> gets AddFiles and the number of records within each file (based on <code>numRecords</code> statistic) for pruneFilesByLimit.</p>  <p><code>getFilesAndNumRecords</code> adds the following columns to the given <code>DataFrame</code>:</p>    Name Expression     <code>numPhysicalRecords</code> numRecords   <code>numLogicalRecords</code> numRecords   <code>stats</code> <code>null</code> string literal    <p><code>getFilesAndNumRecords</code> projects the <code>DataFrame</code> (using <code>DataFrame.select</code>) to create a <code>DataFrame</code> of AddFiles and <code>NumRecords</code>s (out of the <code>numPhysicalRecords</code> and <code>numLogicalRecords</code> columns).</p>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#column-mapping-mode","title":"Column Mapping Mode <pre><code>columnMappingMode: DeltaColumnMappingMode\n</code></pre> <p><code>columnMappingMode</code> is the value of columnMapping.mode table property (in the Metadata).</p>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#getstatscolumnopt","title":"getStatsColumnOpt <pre><code>getStatsColumnOpt(\n  stat: StatsColumn): Option[Column] // (1)!\ngetStatsColumnOpt(\n  statType: String,\n  pathToColumn: Seq[String] = Nil): Option[Column]\n</code></pre> <ol> <li>Uses <code>statType</code> and <code>pathToColumn</code> of the given <code>StatsColumn</code></li> </ol> <p><code>getStatsColumnOpt</code> resolves the given <code>pathToColumn</code> to a <code>Column</code> to access a requested <code>statType</code> statistics.</p>  <p><code>getStatsColumnOpt</code> looks up the <code>statType</code> in the statistics schema (by name). If not available, <code>getStatsColumnOpt</code> returns <code>None</code> (an undefined value) immediately.</p> <p><code>getStatsColumnOpt</code>...FIXME</p> <p><code>getStatsColumnOpt</code> filters out non-leaf <code>StructType</code> columns as they lack statistics and skipping predicates can't use them.</p> <p>Due to a JSON truncation of timestamps to milliseconds, for maxValues statistic of <code>TimestampType</code>s, <code>getStatsColumnOpt</code> adjusts 1 millisecond upwards (to include records that differ in microsecond precision).</p>  <p><code>getStatsColumnOpt</code> is used when:</p> <ul> <li><code>DataSkippingReaderBase</code> is requested to getStatsColumnOrNullLiteral and getStatsColumnOpt</li> <li><code>DataFiltersBuilder</code> is created</li> </ul>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#getstatscolumnornullliteral","title":"getStatsColumnOrNullLiteral <pre><code>getStatsColumnOrNullLiteral(\n  stat: StatsColumn): Column // (1)!\ngetStatsColumnOrNullLiteral(\n  statType: String,\n  pathToColumn: Seq[String] = Nil): Column\n</code></pre> <ol> <li>Uses <code>statType</code> and <code>pathToColumn</code> of the given <code>StatsColumn</code></li> </ol> <p><code>getStatsColumnOrNullLiteral</code> getStatsColumnOpt (for the <code>statType</code> and <code>pathToColumn</code>), if available, or falls back to <code>lit(null)</code>.</p>  <p><code>getStatsColumnOrNullLiteral</code> is used when:</p> <ul> <li><code>DataSkippingReaderBase</code> is requested to verifyStatsForFilter, buildSizeCollectorFilter</li> </ul>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#getdataskippedfiles","title":"getDataSkippedFiles <pre><code>getDataSkippedFiles(\n  partitionFilters: Column,\n  dataFilters: DataSkippingPredicate,\n  keepNumRecords: Boolean): (Seq[AddFile], Seq[DataSize])\n</code></pre> <p><code>getDataSkippedFiles</code> builds the size collectors and the filter functions:</p>    Size Collector <code>Column =&gt; Column</code> Filter Function     totalSize totalFilter   partitionSize partitionFilter   scanSize scanFilter     Size Collectors are Accumulators <p>The size collectors are <code>ArrayAccumulator</code>s that are <code>AccumulatorV2</code>s (Spark Core).</p> <pre><code>class ArrayAccumulator(val size: Int)\nextends AccumulatorV2[(Int, Long), Array[Long]]\n</code></pre>  <p><code>getDataSkippedFiles</code> takes the withStats DataFrame and adds the following <code>WHERE</code> clauses (and creates a <code>filteredFiles</code> dataset):</p> <ol> <li><code>totalFilter</code> with <code>Literal.TrueLiteral</code></li> <li><code>partitionFilter</code> with the given <code>partitionFilters</code></li> <li><code>scanFilter</code> with the given <code>dataFilters</code> or a negation of verifyStatsForFilter (with the referenced statistics of the <code>dataFilters</code>)</li> </ol>  <p>Note</p> <p>At this point, <code>getDataSkippedFiles</code> has built a <code>DataFrame</code> that is a filtered withStats DataFrame.</p>  <p>With the given <code>keepNumRecords</code> flag enabled, <code>getDataSkippedFiles</code> adds JSON-encoded <code>numRecords</code> column (based on <code>stats.numRecords</code> column).</p> <pre><code>to_json(struct(col(\"stats.numRecords\") as 'numRecords))\n</code></pre>  keepNumRecords flag is always disabled <p>The given <code>keepNumRecords</code> flag is always off (<code>false</code>) per the default value of filesForScan.</p>  <p>In the end, <code>getDataSkippedFiles</code> converts the filtered DataFrame to AddFiles and the <code>DataSize</code>s based on the following <code>ArrayAccumulator</code>s:</p> <ol> <li><code>totalSize</code></li> <li><code>partitionSize</code></li> <li><code>scanSize</code></li> </ol>","text":""},{"location":"data-skipping/DataSkippingReaderBase/#convertdataframetoaddfiles","title":"convertDataFrameToAddFiles <pre><code>convertDataFrameToAddFiles(\n  df: DataFrame): Array[AddFile]\n</code></pre> <p><code>convertDataFrameToAddFiles</code> converts the given <code>DataFrame</code> (a <code>Dataset[Row]</code>) to a <code>Dataset[AddFile]</code> and executes <code>Dataset.collect</code> operator.</p>  <p>web UI</p> <p><code>Dataset.collect</code> is an action and can be tracked in web UI.</p>   <p><code>convertDataFrameToAddFiles</code> is used when:</p> <ul> <li><code>DataSkippingReaderBase</code> is requested to getAllFiles, filterOnPartitions, getDataSkippedFiles, getSpecificFilesWithStats</li> </ul>","text":""},{"location":"data-skipping/DeltaDataSkippingType/","title":"DeltaDataSkippingType","text":"<p><code>DeltaDataSkippingType</code> is...FIXME</p>"},{"location":"data-skipping/DeltaScan/","title":"DeltaScan","text":""},{"location":"data-skipping/DeltaScan/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaScan</code> takes the following to be created:</p> <ul> <li> Version <li> AddFiles <li> Total <code>DataSize</code> <li> Partition <code>DataSize</code> <li> Scanned <code>DataSize</code> <li> Partition filters (<code>ExpressionSet</code>) <li> Data filters (<code>ExpressionSet</code>) <li> Unused filters (<code>ExpressionSet</code>) <li> Projection (<code>AttributeSet</code>) <li> Scan duration (in millis) <li> <code>DeltaDataSkippingType</code> <p><code>DeltaScan</code> is created when:</p> <ul> <li><code>PartitionFiltering</code> is requested for the files to scan</li> <li><code>DataSkippingReaderBase</code> is requested for the files to scan</li> </ul>"},{"location":"data-skipping/DeltaScan/#prepareddeltafileindex","title":"PreparedDeltaFileIndex <p><code>DeltaScan</code> is used to create a PreparedDeltaFileIndex.</p>","text":""},{"location":"data-skipping/DeltaScan/#all-filters","title":"All Filters <pre><code>allFilters: ExpressionSet\n</code></pre> <p><code>allFilters</code> is a collection of the partitionFilters, the dataFilters, and the unusedFilters.</p>  <p><code>allFilters</code> is used when:</p> <ul> <li><code>PreparedDeltaFileIndex</code> is requested for the matching data files</li> </ul>","text":""},{"location":"data-skipping/DeltaScanGenerator/","title":"DeltaScanGenerator","text":"<p><code>DeltaScanGenerator</code> is an abstraction of delta table scan generators.</p>"},{"location":"data-skipping/DeltaScanGenerator/#contract","title":"Contract","text":""},{"location":"data-skipping/DeltaScanGenerator/#filesforscan","title":"filesForScan <pre><code>filesForScan(\n  limit: Long): DeltaScan\nfilesForScan(\n  limit: Long,\n  partitionFilters: Seq[Expression]): DeltaScan\nfilesForScan(\n  filters: Seq[Expression],\n  keepNumRecords: Boolean = false): DeltaScan\n</code></pre> <p>DeltaScan with the files to scan</p> <p>Used when:</p> <ul> <li><code>PrepareDeltaScanBase</code> is requested to filesForScan</li> </ul>","text":""},{"location":"data-skipping/DeltaScanGenerator/#fileswithstatsforscan","title":"filesWithStatsForScan <pre><code>filesWithStatsForScan(\n  partitionFilters: Seq[Expression]): DataFrame\n</code></pre> <p>Used when:</p> <ul> <li><code>OptimizeMetadataOnlyDeltaQuery</code> is requested to optimizeQueryWithMetadata</li> </ul>","text":""},{"location":"data-skipping/DeltaScanGenerator/#snapshot-to-scan","title":"Snapshot to Scan <pre><code>snapshotToScan: Snapshot\n</code></pre> <p>Snapshot to generate a table scan for</p> <p>Used when:</p> <ul> <li><code>DataSkippingReaderBase</code> is requested to filesForScan</li> <li><code>PrepareDeltaScanBase</code> is requested to filesForScan</li> </ul>","text":""},{"location":"data-skipping/DeltaScanGenerator/#implementations","title":"Implementations","text":"<ul> <li>DataSkippingReaderBase</li> <li>OptimisticTransactionImpl</li> </ul>"},{"location":"data-skipping/FileSizeHistogram/","title":"FileSizeHistogram","text":"<p>Note</p> <p><code>FileSizeHistogram</code> does not seem to be used in the current release.</p>"},{"location":"data-skipping/FileSizeHistogram/#creating-instance","title":"Creating Instance","text":"<p><code>FileSizeHistogram</code> takes the following to be created:</p> <ul> <li> Sorted Bin Boundaries <li> File Counts <li> Total Bytes <p><code>FileSizeHistogram</code> is created using apply.</p>"},{"location":"data-skipping/FileSizeHistogram/#creating-filesizehistogram","title":"Creating FileSizeHistogram <pre><code>apply(\n  sortedBinBoundaries: IndexedSeq[Long]): FileSizeHistogram\n</code></pre> <p><code>apply</code> creates a FileSizeHistogram with the given <code>sortedBinBoundaries</code> and the fileCounts and the totalBytes all <code>0</code>s (for every element in <code>sortedBinBoundaries</code>).</p>","text":""},{"location":"data-skipping/OptimizeMetadataOnlyDeltaQuery/","title":"OptimizeMetadataOnlyDeltaQuery","text":"<p><code>OptimizeMetadataOnlyDeltaQuery</code> is an abstraction of metadata-only PrepareDeltaScans.</p>"},{"location":"data-skipping/OptimizeMetadataOnlyDeltaQuery/#contract","title":"Contract","text":""},{"location":"data-skipping/OptimizeMetadataOnlyDeltaQuery/#getdeltascangenerator","title":"getDeltaScanGenerator <pre><code>getDeltaScanGenerator(\n  index: TahoeLogFileIndex): DeltaScanGenerator\n</code></pre> <p>DeltaScanGenerator for the given TahoeLogFileIndex</p> <p>See:</p> <ul> <li>PrepareDeltaScanBase</li> </ul> <p>Used when:</p> <ul> <li><code>CountStarDeltaTable</code> is requested to extractGlobalCount</li> </ul>","text":""},{"location":"data-skipping/OptimizeMetadataOnlyDeltaQuery/#implementations","title":"Implementations","text":"<ul> <li>PrepareDeltaScanBase</li> </ul>"},{"location":"data-skipping/OptimizeMetadataOnlyDeltaQuery/#optimizequerywithmetadata","title":"optimizeQueryWithMetadata <pre><code>optimizeQueryWithMetadata(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>optimizeQueryWithMetadata</code> uses stats.numRecords statistic for CountStarDeltaTable queries and so making them very fast by being metadata-only.</p>  <p><code>optimizeQueryWithMetadata</code> transforms the given <code>LogicalPlan</code> (Spark SQL) (with subqueries, children and every node itself) to replace a CountStarDeltaTable with a <code>LocalRelation</code> (Spark SQL).</p>  <p><code>optimizeQueryWithMetadata</code> is used when:</p> <ul> <li><code>PrepareDeltaScanBase</code> logical optimization is requested to execute</li> </ul>","text":""},{"location":"data-skipping/PrepareDeltaScan/","title":"PrepareDeltaScan Logical Optimization","text":"<p><code>PrepareDeltaScan</code> is a PrepareDeltaScanBase.</p>"},{"location":"data-skipping/PrepareDeltaScan/#creating-instance","title":"Creating Instance","text":"<p><code>PrepareDeltaScan</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <p><code>PrepareDeltaScan</code> is created when:</p> <ul> <li><code>DeltaSparkSessionExtension</code> is requested to register delta extensions (and injects pre-CBO optimizer rules)</li> </ul>"},{"location":"data-skipping/PrepareDeltaScan/#logging","title":"Logging","text":"<p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.stats.PrepareDeltaScan</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.delta.stats.PrepareDeltaScan=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"data-skipping/PrepareDeltaScanBase/","title":"PrepareDeltaScanBase Logical Optimizations","text":"<p><code>PrepareDeltaScanBase</code> is an extension of the <code>Rule[LogicalPlan]</code> (Spark SQL) abstraction for logical optimizations that prepareDeltaScan.</p>"},{"location":"data-skipping/PrepareDeltaScanBase/#implementations","title":"Implementations","text":"<ul> <li>PrepareDeltaScan</li> </ul>"},{"location":"data-skipping/PrepareDeltaScanBase/#predicatehelper","title":"PredicateHelper <p><code>PrepareDeltaScanBase</code> is a <code>PredicateHelper</code> (Spark SQL).</p>","text":""},{"location":"data-skipping/PrepareDeltaScanBase/#executing-rule","title":"Executing Rule <pre><code>apply(\n  _plan: LogicalPlan): LogicalPlan\n</code></pre> <p>With spark.databricks.delta.stats.skipping configuration property enabled, <code>apply</code> makes sure that the given <code>LogicalPlan</code> (Spark SQL) is neither a subquery (<code>Subquery</code> or <code>SupportsSubquery</code>) nor a <code>V2WriteCommand</code> (Spark SQL) and prepareDeltaScan.</p>  <p><code>apply</code> is part of the <code>Rule</code> (Spark SQL) abstraction.</p>","text":""},{"location":"data-skipping/PrepareDeltaScanBase/#preparedeltascan","title":"prepareDeltaScan <pre><code>prepareDeltaScan(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>prepareDeltaScan</code> finds delta table scans (i.e. DeltaTables with TahoeLogFileIndex).</p> <p>For a delta table scan, <code>prepareDeltaScan</code> finds a DeltaScanGenerator for the <code>TahoeLogFileIndex</code>.</p> <p><code>prepareDeltaScan</code> uses an internal <code>deltaScans</code> registry (of canonicalized logical scans and their Snapshots and DeltaScans) to look up the delta table scan or creates a new entry.</p> <p><code>prepareDeltaScan</code> creates a PreparedDeltaFileIndex.</p> <p>In the end, <code>prepareDeltaScan</code> optimizeGeneratedColumns.</p>","text":""},{"location":"data-skipping/PrepareDeltaScanBase/#getdeltascangenerator","title":"getDeltaScanGenerator <pre><code>getDeltaScanGenerator(\n  index: TahoeLogFileIndex): DeltaScanGenerator\n</code></pre> <p><code>getDeltaScanGenerator</code>...FIXME</p>","text":""},{"location":"data-skipping/PrepareDeltaScanBase/#getpreparedindex","title":"getPreparedIndex <pre><code>getPreparedIndex(\n  preparedScan: DeltaScan,\n  fileIndex: TahoeLogFileIndex): PreparedDeltaFileIndex\n</code></pre> <p><code>getPreparedIndex</code> creates a new PreparedDeltaFileIndex (for the DeltaScan and the TahoeLogFileIndex).</p> <p><code>getPreparedIndex</code> requires that the partitionFilters (of the TahoeLogFileIndex) are empty or throws an <code>AssertionError</code>:</p> <pre><code>assertion failed: Partition filters should have been extracted by DeltaAnalysis.\n</code></pre>","text":""},{"location":"data-skipping/PrepareDeltaScanBase/#filesforscan","title":"filesForScan <pre><code>filesForScan(\n  scanGenerator: DeltaScanGenerator,\n  limitOpt: Option[Int],\n  projection: Seq[Attribute],\n  filters: Seq[Expression],\n  delta: LogicalRelation): (Snapshot, DeltaScan)\n</code></pre>  <p>Note</p> <p>The given <code>limitOpt</code> argument is not used.</p>  <p><code>filesForScan</code> prints out the following INFO message to the logs:</p> <pre><code>DELTA: Filtering files for query\n</code></pre> <p><code>filesForScan</code> determines the filters for a scan based on generatedColumn.partitionFilterOptimization.enabled configuration property:</p> <ul> <li>If disabled, <code>filesForScan</code> uses the given <code>filters</code> expressions unchanged</li> <li>With generatedColumn.partitionFilterOptimization.enabled enabled, <code>filesForScan</code> generates the partition filters that are used alongside the given <code>filters</code> expressions</li> </ul> <p><code>filesForScan</code> requests the given DeltaScanGenerator for the Snapshot to scan and a DeltaScan (that are the return pair).</p> <p>In the end, <code>filesForScan</code> prints out the following INFO message to the logs:</p> <pre><code>DELTA: Done\n</code></pre>","text":""},{"location":"data-skipping/PrepareDeltaScanBase/#optimizegeneratedcolumns","title":"optimizeGeneratedColumns <pre><code>optimizeGeneratedColumns(\n  scannedSnapshot: Snapshot,\n  scan: LogicalPlan,\n  preparedIndex: PreparedDeltaFileIndex,\n  filters: Seq[Expression],\n  limit: Option[Int],\n  delta: LogicalRelation): LogicalPlan\n</code></pre> <p><code>optimizeGeneratedColumns</code>...FIXME</p>","text":""},{"location":"data-skipping/PrepareDeltaScanBase/#logging","title":"Logging <p><code>PrepareDeltaScanBase</code> is an abstract class and logging is configured using the logger of the implementations.</p>","text":""},{"location":"data-skipping/PreparedDeltaFileIndex/","title":"PreparedDeltaFileIndex","text":"<p><code>PreparedDeltaFileIndex</code> is a TahoeFileIndex that uses DeltaScan for all the work.</p>"},{"location":"data-skipping/PreparedDeltaFileIndex/#creating-instance","title":"Creating Instance","text":"<p><code>PreparedDeltaFileIndex</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> DeltaLog <li> Hadoop Path <li>DeltaScan</li> <li> Partition schema (StructType) <li> Version scanned <p><code>PreparedDeltaFileIndex</code> is created when:</p> <ul> <li><code>PrepareDeltaScanBase</code> logical optimization rule is executed</li> </ul>"},{"location":"data-skipping/PreparedDeltaFileIndex/#deltascan","title":"DeltaScan <p><code>PreparedDeltaFileIndex</code> is given a DeltaScan when created.</p> <p>The <code>DeltaScan</code> is used for all its methods.</p>","text":""},{"location":"data-skipping/PreparedDeltaFileIndex/#input-files","title":"Input Files <pre><code>inputFiles: Array[String]\n</code></pre> <p><code>inputFiles</code>...FIXME</p> <p><code>inputFiles</code> is part of the <code>FileIndex</code> (Spark SQL) abstraction.</p>","text":""},{"location":"data-skipping/PreparedDeltaFileIndex/#matching-data-files","title":"Matching Data Files <pre><code>matchingFiles(\n  partitionFilters: Seq[Expression],\n  dataFilters: Seq[Expression]): Seq[AddFile]\n</code></pre> <p><code>matchingFiles</code>...FIXME</p> <p><code>matchingFiles</code> is part of the TahoeFileIndex abstraction.</p>","text":""},{"location":"data-skipping/PreparedDeltaFileIndex/#estimated-size","title":"Estimated Size <pre><code>sizeInBytes: Long\n</code></pre> <p><code>sizeInBytes</code>...FIXME</p> <p><code>sizeInBytes</code> is part of the <code>FileIndex</code> (Spark SQL) abstraction.</p>","text":""},{"location":"data-skipping/ReadsMetadataFields/","title":"ReadsMetadataFields","text":"<p><code>ReadsMetadataFields</code> is...FIXME</p>"},{"location":"data-skipping/StatsProvider/","title":"StatsProvider","text":""},{"location":"data-skipping/StatsProvider/#creating-instance","title":"Creating Instance","text":"<p><code>StatsProvider</code> takes the following to be created:</p> <ul> <li>Statistic Column Function</li> </ul> <p><code>StatsProvider</code> is created alongside DataFiltersBuilder.</p>"},{"location":"data-skipping/StatsProvider/#statistic-column-function","title":"Statistic Column Function <p><code>StatsProvider</code> is given a Statistic Column Function when created.</p> <pre><code>StatsColumn =&gt; Option[Column]\n</code></pre> <p>The function is always getStatsColumnOpt.</p>","text":""},{"location":"data-skipping/StatsProvider/#getpredicatewithstattype","title":"getPredicateWithStatType <pre><code>getPredicateWithStatType(\n  pathToColumn: Seq[String],\n  statType: String)(\n  f: Column =&gt; Column): Option[DataSkippingPredicate]\n</code></pre> <p><code>getPredicateWithStatType</code> getPredicateWithStatsColumn with a <code>StatsColumn</code> for the given <code>statType</code> and <code>pathToColumn</code>, and the given <code>f</code> function.</p>  <p><code>getPredicateWithStatType</code> is used when:</p> <ul> <li><code>ColumnPredicateBuilder</code> is requested to lessThan, lessThanOrEqual, greaterThan, greaterThanOrEqual</li> <li><code>DataFiltersBuilder</code> is requested to constructDataFilters (for <code>IsNull</code> predicate)</li> </ul>","text":""},{"location":"data-skipping/StatsProvider/#getpredicatewithstatscolumn","title":"getPredicateWithStatsColumn <pre><code>getPredicateWithStatsColumn(\n  statCol: StatsColumn)(\n  f: Column =&gt; Column): Option[DataSkippingPredicate]\n</code></pre> <p><code>getPredicateWithStatsColumn</code> uses the getStat function to look up the given <code>statCol</code> and, if found (<code>stat</code>), creates a <code>DataSkippingPredicate</code> with the following:</p> <ul> <li>The result of applying the <code>f</code> function to <code>stat</code></li> <li>The given <code>statCol</code></li> </ul>","text":""},{"location":"data-skipping/StatsProvider/#getpredicatewithstattypes","title":"getPredicateWithStatTypes <pre><code>getPredicateWithStatTypes(\n  pathToColumn: Seq[String],\n  statType1: String,\n  statType2: String)(\n  f: (Column, Column) =&gt; Column): Option[DataSkippingPredicate]\ngetPredicateWithStatTypes(\n  pathToColumn: Seq[String],\n  statType1: String,\n  statType2: String,\n  statType3: String)(\n  f: (Column, Column, Column) =&gt; Column): Option[DataSkippingPredicate]\n</code></pre> <p><code>getPredicateWithStatTypes</code> getPredicateWithStatsColumns with a <code>StatsColumn</code> for all the given <code>statType</code>s and <code>pathToColumn</code>, and the given <code>f</code> function.</p>  <p><code>getPredicateWithStatTypes</code> is used when:</p> <ul> <li><code>ColumnPredicateBuilder</code> is requested to equalTo, notEqualTo</li> <li><code>DataFiltersBuilder</code> is requested to constructDataFilters (for <code>StartsWith</code> predicate)</li> </ul>","text":""},{"location":"data-skipping/UsesMetadataFields/","title":"UsesMetadataFields","text":"<p><code>UsesMetadataFields</code> is an abstraction of components that use metadata fields (from a delta table's transaction log).</p>"},{"location":"data-skipping/UsesMetadataFields/#contract","title":"Contract","text":""},{"location":"data-skipping/UsesMetadataFields/#maxvalues","title":"maxValues <p>The largest (possibly truncated) value for a column</p> <p>Used when:</p> <ul> <li><code>DataSkippingReaderBase</code> is requested to verifyStatsForFilter and getStatsColumnOpt</li> <li><code>ColumnPredicateBuilder</code> is requested to equalTo, notEqualTo, greaterThan, greaterThanOrEqual</li> <li><code>DataFiltersBuilder</code> is requested to constructDataFilters</li> <li><code>StatisticsCollection</code> is requested to statsCollector and statsSchema</li> </ul>","text":""},{"location":"data-skipping/UsesMetadataFields/#minvalues","title":"minValues <p>The smallest (possibly truncated) value for a column</p>","text":""},{"location":"data-skipping/UsesMetadataFields/#nullcount","title":"nullCount <p>The number of null values present for a column</p>","text":""},{"location":"data-skipping/UsesMetadataFields/#numrecords","title":"numRecords <p>The total number of records in the file</p>","text":""},{"location":"data-skipping/UsesMetadataFields/#implementations","title":"Implementations","text":"<ul> <li>ColumnPredicateBuilder</li> <li>ReadsMetadataFields</li> <li>StatisticsCollection</li> </ul>"},{"location":"demo/","title":"Demos","text":"<p>The following demos are available:</p> <ul> <li>Change Data Feed</li> <li>Generated Columns</li> <li>Column Mapping</li> <li>Rolling Back Table Changes (Restore Command)</li> <li>Optimize</li> <li>Data Skipping</li> <li>Time Travel</li> <li>Vacuum</li> <li>dataChange</li> <li>replaceWhere</li> <li>Merge Operation</li> <li>Converting Parquet Dataset Into Delta Format</li> <li>Stream Processing of Delta Table</li> <li>Using Delta Lake as Streaming Sink in Structured Streaming</li> <li>Debugging Delta Lake Using IntelliJ IDEA</li> <li>Observing Transaction Retries</li> <li>DeltaTable, DeltaLog And Snapshots</li> <li>Schema Evolution</li> <li>User Metadata for Labelling Commits</li> </ul>"},{"location":"demo/Converting-Parquet-Dataset-Into-Delta-Format/","title":"Demo: Converting Parquet Dataset Into Delta Format","text":"<pre><code>/*\nspark-shell \\\n  --packages io.delta:delta-core_2.12:2.2.0 \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.databricks.delta.snapshotPartitions=1\n*/\nassert(spark.version.matches(\"2.4.[2-6]\"), \"Delta Lake supports Spark 2.4.2+\")\n\nimport org.apache.spark.sql.SparkSession\nassert(spark.isInstanceOf[SparkSession])\n\nval deltaLake = \"/tmp/delta\"\n\n// Create parquet table\nval users = s\"$deltaLake/users\"\nimport spark.implicits._\nval data = Seq(\n  (0L, \"Agata\", \"Warsaw\", \"Poland\"),\n  (1L, \"Jacek\", \"Warsaw\", \"Poland\"),\n  (2L, \"Bartosz\", \"Paris\", \"France\")\n).toDF(\"id\", \"name\", \"city\", \"country\")\ndata\n  .write\n  .format(\"parquet\")\n  .partitionBy(\"city\", \"country\")\n  .mode(\"overwrite\")\n  .save(users)\n\n// TIP: Use git to version the users directory\n//      to track the changes for import\n\n// CONVERT TO DELTA only supports parquet tables\n// TableIdentifier should be parquet.`users`\n\n// Use TableIdentifier to refer to the parquet table\n// The path itself would work too\nval tableId = s\"parquet.`$users`\"\nval partitionSchema = \"city STRING, country STRING\"\n\n// Import users table into Delta Lake\n// Well, convert the parquet table into delta table\n// Use web UI to monitor execution, e.g. http://localhost:4040\n\nimport io.delta.tables.DeltaTable\nval dt = DeltaTable.convertToDelta(spark, tableId, partitionSchema)\nassert(dt.isInstanceOf[DeltaTable])\n\n// users table is now in delta format\nassert(DeltaTable.isDeltaTable(users))\n</code></pre>"},{"location":"demo/Debugging-Delta-Lake-Using-IntelliJ-IDEA/","title":"Demo: Debugging Delta Lake Using IntelliJ IDEA","text":"<p>Import Delta Lake's sources to IntelliJ IDEA.</p> <p>Configure a new Remote debug configuration in IntelliJ IDEA (e.g. Run &gt; Debug &gt; Edit Configurations...) and simply give it a name and save.</p> <p>Tip</p> <p>Use <code>Option+Ctrl+D</code> to access Debug menu on mac OS.</p> <p></p> <p>Run <code>spark-shell</code> as follows to enable remote JVM for debugging.</p> <pre><code>export SPARK_SUBMIT_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005\n</code></pre> <pre><code>spark-shell \\\n  --packages io.delta:delta-core_2.12:2.2.0 \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\\n  --conf spark.databricks.delta.snapshotPartitions=1\n</code></pre>"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/","title":"Demo: DeltaTable, DeltaLog And Snapshots","text":""},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#create-delta-table","title":"Create Delta Table","text":"<pre><code>import org.apache.spark.sql.SparkSession\nassert(spark.isInstanceOf[SparkSession])\n</code></pre> <pre><code>val tableName = \"users\"\n</code></pre> <pre><code>sql(s\"DROP TABLE IF EXISTS $tableName\")\nsql(s\"\"\"\n    | CREATE TABLE $tableName (id bigint, name string, city string, country string)\n    | USING delta\n    \"\"\".stripMargin)\n</code></pre> <pre><code>scala&gt; spark.catalog.listTables.show\n+-----+--------+-----------+---------+-----------+\n| name|database|description|tableType|isTemporary|\n+-----+--------+-----------+---------+-----------+\n|users| default|       null|  MANAGED|      false|\n+-----+--------+-----------+---------+-----------+\n</code></pre>"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#access-transaction-log-deltalog","title":"Access Transaction Log (DeltaLog)","text":"<pre><code>import org.apache.spark.sql.catalyst.TableIdentifier\nval tid = TableIdentifier(tableName)\n\nimport org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, tid)\n</code></pre> <p>Update the state of the delta table to the most recent version.</p> <pre><code>val snapshot = deltaLog.update()\nassert(snapshot.version == 0)\n</code></pre> <pre><code>val state = snapshot.state\n</code></pre> <pre><code>scala&gt; :type state\norg.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.SingleAction]\n</code></pre> <p>Review the cached RDD for the state snapshot in the Storage tab of the web UI (e.g. http://localhost:4040/storage/).</p> <p></p> <p>The \"version\" part of Delta Table State name of the cached RDD should match the version of the snapshot.</p> <p>Show the changes (actions).</p> <pre><code>scala&gt; state.show\n+----+----+------+--------------------+--------+----+----------+\n| txn| add|remove|            metaData|protocol| cdc|commitInfo|\n+----+----+------+--------------------+--------+----+----------+\n|null|null|  null|                null|  {1, 2}|null|      null|\n|null|null|  null|{90316970-5bf1-45...|    null|null|      null|\n+----+----+------+--------------------+--------+----+----------+\n</code></pre>"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#deltatable-as-dataframe","title":"DeltaTable as DataFrame","text":""},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#deltatable","title":"DeltaTable","text":"<pre><code>import io.delta.tables.DeltaTable\nval dt = DeltaTable.forName(tableName)\n</code></pre> <pre><code>val h = dt.history.select('version, 'operation, 'operationParameters, 'operationMetrics)\n</code></pre> <pre><code>scala&gt; h.show(truncate = false)\n+-------+------------+-----------------------------------------------------------------------------+----------------+\n|version|operation   |operationParameters                                                          |operationMetrics|\n+-------+------------+-----------------------------------------------------------------------------+----------------+\n|0      |CREATE TABLE|{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}|{}              |\n+-------+------------+-----------------------------------------------------------------------------+----------------+\n</code></pre>"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#converting-deltatable-into-dataframe","title":"Converting DeltaTable into DataFrame","text":"<pre><code>val users = dt.toDF\n</code></pre> <pre><code>scala&gt; users.show\n+---+----+----+-------+\n| id|name|city|country|\n+---+----+----+-------+\n+---+----+----+-------+\n</code></pre>"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#add-new-users","title":"Add new users","text":"<pre><code>val newUsers = Seq(\n  (0L, \"Agata\", \"Warsaw\", \"Poland\"),\n  (1L, \"Bartosz\", \"Paris\", \"France\")\n).toDF(\"id\", \"name\", \"city\", \"country\")\n</code></pre> <pre><code>scala&gt; newUsers.show\n+---+-------+------+-------+\n| id|   name|  city|country|\n+---+-------+------+-------+\n|  0|  Agata|Warsaw| Poland|\n|  1|Bartosz| Paris| France|\n+---+-------+------+-------+\n</code></pre> <pre><code>// newUsers.write.format(\"delta\").mode(\"append\").saveAsTable(name)\nnewUsers.writeTo(tableName).append\nassert(deltaLog.snapshot.version == 1)\n</code></pre> <p>Review the cached RDD for the state snapshot in the Storage tab of the web UI (e.g. http://localhost:4040/storage/).</p> <p>Note that the <code>DataFrame</code> variant of the delta table has automatically been refreshed (making <code>REFRESH TABLE</code> unnecessary).</p> <pre><code>scala&gt; users.show\n+---+-------+------+-------+\n| id|   name|  city|country|\n+---+-------+------+-------+\n|  1|Bartosz| Paris| France|\n|  0|  Agata|Warsaw| Poland|\n+---+-------+------+-------+\n</code></pre> <pre><code>val h = dt.history.select('version, 'operation, 'operationParameters, 'operationMetrics)\n</code></pre> <pre><code>scala&gt; h.show(truncate = false)\n+-------+------------+-----------------------------------------------------------------------------+-----------------------------------------------------------+\n|version|operation   |operationParameters                                                          |operationMetrics                                           |\n+-------+------------+-----------------------------------------------------------------------------+-----------------------------------------------------------+\n|1      |WRITE       |{mode -&gt; Append, partitionBy -&gt; []}                                          |{numFiles -&gt; 2, numOutputBytes -&gt; 2299, numOutputRows -&gt; 2}|\n|0      |CREATE TABLE|{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}|{}                                                         |\n+-------+------------+-----------------------------------------------------------------------------+-----------------------------------------------------------+\n</code></pre>"},{"location":"demo/Observing-Transaction-Retries/","title":"Demo: Observing Transaction Retries","text":"<p>Enable <code>ALL</code> logging level for org.apache.spark.sql.delta.OptimisticTransaction logger. You'll be looking for the following DEBUG message in the logs:</p> <pre><code>Attempting to commit version [version] with 13 actions with Serializable isolation level\n</code></pre> <p>Start with Debugging Delta Lake Using IntelliJ IDEA and place the following line breakpoints in <code>OptimisticTransactionImpl</code>:</p> <p>. In <code>OptimisticTransactionImpl.doCommit</code> when a transaction is about to <code>deltaLog.store.write</code> (line 388)</p> <p>. In <code>OptimisticTransactionImpl.doCommit</code> when a transaction is about to <code>checkAndRetry</code> after a <code>FileAlreadyExistsException</code> (line 433)</p> <p>. In <code>OptimisticTransactionImpl.checkAndRetry</code> when a transaction calculates <code>nextAttemptVersion</code> (line 453)</p> <p>In order to interfere with a transaction about to be committed, you will use ROOT:WriteIntoDelta.md[WriteIntoDelta] action (it is simple and does the work).</p> <p>Run the command (copy and paste the ROOT:WriteIntoDelta.md#demo[demo code] to <code>spark-shell</code> using paste mode). You should see the following messages in the logs:</p> <pre><code>scala&gt; writeCmd.run(spark)\nDeltaLog: DELTA: Updating the Delta table's state\nOptimisticTransaction: Attempting to commit version 6 with 13 actions with Serializable isolation level\n</code></pre> <p>That's when you \"commit\" another transaction (to simulate two competing transactional writes). Simply create a delta file for the transaction. The commit version in the message above is <code>6</code> so the name of the delta file should be <code>00000000000000000006.json</code>:</p> <pre><code>$ touch /tmp/delta/t1/_delta_log/00000000000000000006.json\n</code></pre> <p><code>F9</code> in IntelliJ IDEA to resume the <code>WriteIntoDelta</code> command. It should stop at <code>checkAndRetry</code> due to <code>FileAlreadyExistsException</code>. Press <code>F9</code> twice to resume.</p> <p>You should see the following messages in the logs:</p> <pre><code>OptimisticTransaction: No logical conflicts with deltas [6, 7), retrying.\nOptimisticTransaction: Attempting to commit version 7 with 13 actions with Serializable isolation level\n</code></pre> <p>Rinse and repeat. You know the drill already. Happy debugging!</p>"},{"location":"demo/Using-Delta-Lake-as-Streaming-Sink-in-Structured-Streaming/","title":"Demo: Using Delta Lake as Streaming Sink in Structured Streaming","text":"<pre><code>assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession])\nassert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\")\n\n// Input data \"format\"\ncase class User(id: Long, name: String, city: String)\n\n// Any streaming data source would work\n// Using memory data source\n// Gives control over the input stream\nimplicit val ctx = spark.sqlContext\nimport org.apache.spark.sql.execution.streaming.MemoryStream\nval usersIn = MemoryStream[User]\nval users = usersIn.toDF\n\nval deltaLake = \"/tmp/delta-lake\"\nval checkpointLocation = \"/tmp/delta-checkpointLocation\"\nval path = s\"$deltaLake/users\"\nval partitionBy = \"city\"\n\n// The streaming query that writes out to Delta Lake\nval sq = users\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", checkpointLocation)\n  .option(\"path\", path)\n  .partitionBy(partitionBy)\n  .start\n\n// TIP: You could use git to version the path directory\n//      and track the changes of every micro-batch\n\n// TIP: Use web UI to monitor execution, e.g. http://localhost:4040\n\n// FIXME: Use DESCRIBE HISTORY every micro-batch\n\nval batch1 = Seq(\n  User(0, \"Agata\", \"Warsaw\"),\n  User(1, \"Jacek\", \"Warsaw\"))\nval offset = usersIn.addData(batch1)\nsq.processAllAvailable()\n\nval history = s\"DESCRIBE HISTORY delta.`$path`\"\nval clmns = Seq($\"version\", $\"timestamp\", $\"operation\", $\"operationParameters\", $\"isBlindAppend\")\n\nval h = sql(history).select(clmns: _*).orderBy($\"version\".asc)\nscala&gt; h.show(truncate = false)\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+\n|version|timestamp          |operation       |operationParameters                                                                  |isBlindAppend|\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+\n|0      |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -&gt; Append, queryId -&gt; f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -&gt; 0]|true         |\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+\n\nval batch2 = Seq(\n  User(2, \"Bartek\", \"Paris\"),\n  User(3, \"Jacek\", \"Paris\"))\nval offset = usersIn.addData(batch2)\nsq.processAllAvailable()\n\n// You have to execute the history SQL command again\n// It materializes immediately with whatever data is available at the time\nval h = sql(history).select(clmns: _*).orderBy($\"version\".asc)\nscala&gt; h.show(truncate = false)\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+\n|version|timestamp          |operation       |operationParameters                                                                  |isBlindAppend|\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+\n|0      |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -&gt; Append, queryId -&gt; f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -&gt; 0]|true         |\n|1      |2019-12-06 10:13:27|STREAMING UPDATE|[outputMode -&gt; Append, queryId -&gt; f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -&gt; 1]|true         |\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+\n\nval batch3 = Seq(\n  User(4, \"Gorazd\", \"Ljubljana\"))\nval offset = usersIn.addData(batch3)\nsq.processAllAvailable()\n\n// Let's use DeltaTable API instead\n\nimport io.delta.tables.DeltaTable\nval usersDT = DeltaTable.forPath(path)\n\nval h = usersDT.history.select(clmns: _*).orderBy($\"version\".asc)\nscala&gt; h.show(truncate = false)\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+\n|version|timestamp          |operation       |operationParameters                                                                  |isBlindAppend|\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+\n|0      |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -&gt; Append, queryId -&gt; f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -&gt; 0]|true         |\n|1      |2019-12-06 10:13:27|STREAMING UPDATE|[outputMode -&gt; Append, queryId -&gt; f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -&gt; 1]|true         |\n|2      |2019-12-06 10:20:56|STREAMING UPDATE|[outputMode -&gt; Append, queryId -&gt; f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -&gt; 2]|true         |\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+\n</code></pre>"},{"location":"demo/change-data-feed/","title":"Demo: Change Data Feed","text":"<p>This demo shows Change Data Feed in action.</p>"},{"location":"demo/change-data-feed/#create-delta-table-with-change-data-feed-enabled","title":"Create Delta Table with Change Data Feed Enabled","text":"SQL <pre><code>CREATE TABLE cdf_demo (id INT, name STRING)\nUSING delta\nTBLPROPERTIES (delta.enableChangeDataFeed = true);\n</code></pre>"},{"location":"demo/change-data-feed/#insert-into","title":"INSERT INTO","text":"<pre><code>INSERT INTO cdf_demo VALUES (0, 'insert into');\n</code></pre> <pre><code>SELECT * FROM cdf_demo;\n</code></pre> <pre><code>+---+-----------+\n|id |name       |\n+---+-----------+\n|0  |insert into|\n+---+-----------+\n</code></pre>"},{"location":"demo/change-data-feed/#update","title":"UPDATE","text":"<p><code>UPDATE</code> is among commands supported by Change Data Feed.</p> <pre><code>UPDATE cdf_demo SET name = 'update' WHERE id = 0;\n</code></pre> <pre><code>SELECT * FROM cdf_demo;\n</code></pre> <pre><code>+---+------+\n|id |name  |\n+---+------+\n|0  |update|\n+---+------+\n</code></pre>"},{"location":"demo/change-data-feed/#_change_data","title":"_change_data","text":"<p>After executing the above <code>UPDATE</code> command, Delta Lake creates a <code>_change_data</code> directory (with <code>cdc</code> files).</p> <pre><code>$ tree spark-warehouse/cdf_demo\nspark-warehouse/cdf_demo\n\u251c\u2500\u2500 _change_data\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 cdc-00000-d5a2730f-de81-4bc7-8bb1-b6c0ff5fec37.c000.snappy.parquet\n\u251c\u2500\u2500 _delta_log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 00000000000000000000.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 00000000000000000001.json\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 00000000000000000002.json\n\u251c\u2500\u2500 part-00000-088e28d1-b95f-46e2-812a-5389ae58af28-c000.snappy.parquet\n\u2514\u2500\u2500 part-00000-0947d1e8-a398-4e2c-8afe-db734b84f6b4.c000.snappy.parquet\n\n2 directories, 6 files\n</code></pre>"},{"location":"demo/change-data-feed/#cdc-aware-batch-scan","title":"CDC-Aware Batch Scan","text":"<pre><code>val changes = spark\n.read\n.format(\"delta\")\n.option(\"readChangeFeed\", true)\n.option(\"startingVersion\", \"0\")\n.table(\"delta_demo\")\n</code></pre> <pre><code>changes.show(truncate = false)\n</code></pre> <pre><code>+---+-----------+----------------+---------------+-----------------------+\n|id |name       |_change_type    |_commit_version|_commit_timestamp      |\n+---+-----------+----------------+---------------+-----------------------+\n|0  |insert into|update_preimage |2              |2022-07-24 18:23:42.102|\n|0  |update     |update_postimage|2              |2022-07-24 18:23:42.102|\n|0  |insert into|insert          |1              |2022-07-24 18:15:48.892|\n+---+-----------+----------------+---------------+-----------------------+\n</code></pre>"},{"location":"demo/change-data-feed/#cdc-aware-streaming-query","title":"CDC-Aware Streaming Query","text":"<pre><code>spark\n.readStream\n.format(\"delta\")\n.option(\"readChangeFeed\", true)\n.table(\"delta_demo\")\n.writeStream\n.format(\"console\")\n.option(\"truncate\", false)\n.queryName(\"Change feed from delta_demo\")\n.start\n</code></pre> <pre><code>spark.table(\"delta_demo\").show\n</code></pre> <pre><code>+---+\n| id|\n+---+\n|  1|\n|  0|\n|  2|\n|  3|\n|  4|\n+---+\n</code></pre>"},{"location":"demo/change-data-feed/#single-insert-only-merge","title":"Single Insert-Only Merge","text":"<pre><code>sql(\"\"\"\n  MERGE INTO delta_demo target\n  USING (VALUES 5 source(id))\n  ON target.id = source.id\n  WHEN NOT MATCHED THEN INSERT *;\n\"\"\")\n</code></pre>"},{"location":"demo/change-data-feed/#streaming-micro-batch","title":"Streaming Micro-Batch","text":"<p>You should see the following output from the streaming query:</p> <pre><code>-------------------------------------------\nBatch: 1\n-------------------------------------------\n+---+------------+---------------+-----------------------+\n|id |_change_type|_commit_version|_commit_timestamp      |\n+---+------------+---------------+-----------------------+\n|5  |insert      |9              |2022-08-05 14:38:49.305|\n+---+------------+---------------+-----------------------+\n</code></pre>"},{"location":"demo/change-data-feed/#insert-into-and-streaming-query","title":"INSERT INTO and Streaming Query","text":"<pre><code>sql(\"\"\"\nINSERT INTO delta_demo VALUES (6);\n\"\"\")\n</code></pre> <p>You should see the following output from the streaming query.</p> <pre><code>-------------------------------------------\nBatch: 1\n-------------------------------------------\n+---+------------+---------------+-----------------------+\n|id |_change_type|_commit_version|_commit_timestamp      |\n+---+------------+---------------+-----------------------+\n|6  |insert      |10             |2022-08-05 16:35:08.657|\n+---+------------+---------------+-----------------------+\n</code></pre>"},{"location":"demo/change-data-feed/#review-and-merge","title":"Review and Merge","text":"<p>The following are loose notes (findings) while investigating CDF.</p>"},{"location":"demo/change-data-feed/#overwrite-save-mode","title":"overwrite Save Mode","text":"<pre><code>spark\n.range(5)\n.write\n.format(\"delta\")\n.mode(\"overwrite\")\n.save(\"/tmp/delta-xxx\")\n</code></pre> <pre><code>val startingVersion = 2\nval v2 = spark\n.read\n.format(\"delta\")\n.option(\"readChangeFeed\", true)\n.option(\"startingVersion\", startingVersion)\n.load(\"/tmp/delta-xxx\")\n</code></pre> <pre><code>v2.show(truncate = false)\n+---+------------+---------------+-----------------------+\n|id |_change_type|_commit_version|_commit_timestamp      |\n+---+------------+---------------+-----------------------+\n|0  |insert      |2              |2022-07-31 17:51:25.777|\n|1  |insert      |2              |2022-07-31 17:51:25.777|\n|2  |insert      |2              |2022-07-31 17:51:25.777|\n|3  |insert      |2              |2022-07-31 17:51:25.777|\n|4  |insert      |2              |2022-07-31 17:51:25.777|\n|1  |delete      |2              |2022-07-31 17:51:25.777|\n|0  |delete      |2              |2022-07-31 17:51:25.777|\n|2  |delete      |2              |2022-07-31 17:51:25.777|\n|3  |delete      |2              |2022-07-31 17:51:25.777|\n|4  |delete      |2              |2022-07-31 17:51:25.777|\n+---+------------+---------------+-----------------------+\n</code></pre> <pre><code>scala&gt; v2.orderBy('id).show(truncate = false)\n+---+------------+---------------+-----------------------+\n|id |_change_type|_commit_version|_commit_timestamp      |\n+---+------------+---------------+-----------------------+\n|0  |delete      |2              |2022-07-31 17:51:25.777|\n|0  |insert      |2              |2022-07-31 17:51:25.777|\n|1  |insert      |2              |2022-07-31 17:51:25.777|\n|1  |delete      |2              |2022-07-31 17:51:25.777|\n|2  |insert      |2              |2022-07-31 17:51:25.777|\n|2  |delete      |2              |2022-07-31 17:51:25.777|\n|3  |insert      |2              |2022-07-31 17:51:25.777|\n|3  |delete      |2              |2022-07-31 17:51:25.777|\n|4  |insert      |2              |2022-07-31 17:51:25.777|\n|4  |delete      |2              |2022-07-31 17:51:25.777|\n+---+------------+---------------+-----------------------+\n</code></pre>"},{"location":"demo/change-data-feed/#delete-from","title":"DELETE FROM","text":"<pre><code>sql(\"DELETE FROM delta.`/tmp/delta-xxx` WHERE id &gt; 3\").show\n</code></pre> <pre><code>val descHistory = sql(\"desc history delta.`/tmp/delta-xxx`\").select('version, 'operation, 'operationParameters)\n</code></pre> <pre><code>scala&gt; descHistory.show(truncate = false)\n+-------+-----------------+-----------------------------------------------------------------+\n|version|operation        |operationParameters                                              |\n+-------+-----------------+-----------------------------------------------------------------+\n|3      |DELETE           |{predicate -&gt; [\"(spark_catalog.delta.`/tmp/delta-xxx`.id &gt; 3L)\"]}|\n|2      |WRITE            |{mode -&gt; Overwrite, partitionBy -&gt; []}                           |\n|1      |SET TBLPROPERTIES|{properties -&gt; {\"delta.enableChangeDataFeed\":\"true\"}}            |\n|0      |WRITE            |{mode -&gt; ErrorIfExists, partitionBy -&gt; []}                       |\n+-------+-----------------+-----------------------------------------------------------------+\n</code></pre> <pre><code>val startingVersion = 3\nval v3 = spark\n.read\n.format(\"delta\")\n.option(\"readChangeFeed\", true)\n.option(\"startingVersion\", startingVersion)\n.load(\"/tmp/delta-xxx\")\nv3.orderBy('id).show(truncate = false)\n</code></pre> <pre><code>+---+------------+---------------+-----------------------+\n|id |_change_type|_commit_version|_commit_timestamp      |\n+---+------------+---------------+-----------------------+\n|4  |delete      |3              |2022-08-01 10:22:21.402|\n+---+------------+---------------+-----------------------+\n</code></pre>"},{"location":"demo/change-data-feed/#endingversion-option","title":"endingVersion Option","text":"<pre><code>val startingVersion = 3\nval endingVersion = 3\nval v3 = spark\n.read\n.format(\"delta\")\n.option(\"readChangeFeed\", true)\n.option(\"startingVersion\", startingVersion)\n.option(\"endingVersion\", endingVersion)\n.load(\"/tmp/delta-xxx\")\nv3.orderBy('id).show(truncate = false)\n</code></pre> <pre><code>+---+------------+---------------+-----------------------+\n|id |_change_type|_commit_version|_commit_timestamp      |\n+---+------------+---------------+-----------------------+\n|4  |delete      |3              |2022-08-01 10:22:21.402|\n+---+------------+---------------+-----------------------+\n</code></pre>"},{"location":"demo/column-mapping/","title":"Demo: Column Mapping","text":"<p>This demo shows Column Mapping in action.</p>"},{"location":"demo/column-mapping/#create-delta-table","title":"Create Delta Table","text":"<p>Let's create a delta table using a mixture of Scala and SQL.</p> ScalaSQL <pre><code>val tableName = \"delta101\"\n\nsql(s\"DROP TABLE IF EXISTS $tableName\")\nspark.range(1).writeTo(tableName).using(\"delta\").create\n</code></pre> <pre><code>DROP TABLE IF EXISTS delta101;\nCREATE TABLE delta101 USING delta VALUES (0) t(id)\n</code></pre>"},{"location":"demo/column-mapping/#enable-column-mapping-by-name","title":"Enable Column Mapping (by Name)","text":"ScalaSQL <pre><code>sql(s\"\"\"\n  ALTER TABLE $tableName SET TBLPROPERTIES (\n    'delta.columnMapping.mode' = 'name'\n  )\n  \"\"\")\n</code></pre> <pre><code>ALTER TABLE delta101 SET TBLPROPERTIES (\n'delta.columnMapping.mode' = 'name'\n)\n</code></pre> <p>The above command runs into the following issue (which is fairly self-explanatory):</p> <pre><code>org.apache.spark.sql.delta.DeltaColumnMappingUnsupportedException:\nYour current table protocol version does not support changing column mapping modes\nusing delta.columnMapping.mode.\n\nRequired Delta protocol version for column mapping:\nProtocol(2,5)\nYour table's current Delta protocol version:\nProtocol(1,2)\n\nPlease upgrade your table's protocol version using ALTER TABLE SET TBLPROPERTIES and try again.\n</code></pre> <p>Let's upgrade the table protocol and enable column mapping all in one go.</p> Scala <pre><code>sql(s\"\"\"\n  ALTER TABLE $tableName SET TBLPROPERTIES (\n    'delta.minReaderVersion' = '2',\n    'delta.minWriterVersion' = '5',\n    'delta.columnMapping.mode' = 'name'\n  )\n  \"\"\")\n</code></pre>"},{"location":"demo/column-mapping/#show-table-properties","title":"Show Table Properties","text":"Scala <pre><code>sql(s\"SHOW TBLPROPERTIES $tableName\").show(truncate = false)\n</code></pre> <pre><code>+-------------------------------+-------+\n|key                            |value  |\n+-------------------------------+-------+\n|Type                           |MANAGED|\n|delta.columnMapping.maxColumnId|1      |\n|delta.columnMapping.mode       |name   |\n|delta.minReaderVersion         |2      |\n|delta.minWriterVersion         |5      |\n+-------------------------------+-------+\n</code></pre>"},{"location":"demo/column-mapping/#review-schema","title":"Review Schema","text":"<p>At the current setup, the physical column names (there is only <code>id</code>, actually) are the only column names.</p> <p>You can use a Spark SQL way to access the schema.</p> Spark SQL <pre><code>println(spark.table(tableName).schema.prettyJson)\n</code></pre> <pre><code>{\n  \"type\" : \"struct\",\n  \"fields\" : [ {\n    \"name\" : \"id\",\n    \"type\" : \"integer\",\n    \"nullable\" : true,\n    \"metadata\" : { }\n  } ]\n}\n</code></pre> <p>A much elaborative way is to use Delta Lake API (that includes column mapping metadata).</p> Delta Lake <pre><code>import org.apache.spark.sql.catalyst.TableIdentifier\nval tid = TableIdentifier(tableName)\n\nimport org.apache.spark.sql.delta.DeltaTableIdentifier\nval dtId = DeltaTableIdentifier(table = Some(tid))\n\nimport org.apache.spark.sql.delta.DeltaLog\nval table = DeltaLog.forTable(spark, dtId)\n\nprintln(table.snapshot.metadata.schema.prettyJson)\n</code></pre> <pre><code>{\n  \"type\" : \"struct\",\n  \"fields\" : [ {\n    \"name\" : \"id\",\n    \"type\" : \"long\",\n    \"nullable\" : true,\n    \"metadata\" : {\n      \"delta.columnMapping.id\" : 1,\n      \"delta.columnMapping.physicalName\" : \"id\"\n    }\n  } ]\n}\n</code></pre>"},{"location":"demo/column-mapping/#rename-column","title":"Rename Column","text":"<p>Let's rename the <code>id</code> column using ALTER TABLE RENAME COLUMN command and review the schema again.</p> Scala <pre><code>sql(s\"ALTER TABLE $tableName RENAME COLUMN id TO new_id\")\n</code></pre>"},{"location":"demo/column-mapping/#describe-history","title":"Describe History","text":"<p><code>ALTER TABLE RENAME COLUMN</code> is a transactional change of the metadata of a delta table and is recorded in the transaction log.</p> Scala <pre><code>sql(\"desc history delta101\")\n.select('version, 'operation, 'operationParameters)\n.show(truncate = false)\n</code></pre> <pre><code>+-------+----------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n|version|operation             |operationParameters                                                                                                                        |\n+-------+----------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n|2      |CHANGE COLUMN         |{column -&gt; {\"name\":\"new_id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{\"delta.columnMapping.id\":1,\"delta.columnMapping.physicalName\":\"id\"}}}|\n|1      |SET TBLPROPERTIES     |{properties -&gt; {\"delta.minReaderVersion\":\"2\",\"delta.minWriterVersion\":\"5\",\"delta.columnMapping.mode\":\"name\"}}                              |\n|0      |CREATE TABLE AS SELECT|{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}                                                              |\n+-------+----------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"demo/column-mapping/#review-schema-after-column-mapping","title":"Review Schema (After Column Mapping)","text":""},{"location":"demo/column-mapping/#catalog-table-view","title":"Catalog Table View","text":"<p>Let's review the schema again.</p> Scala <pre><code>spark.table(tableName).printSchema\n</code></pre> <pre><code>root\n |-- new_id: long (nullable = true)\n</code></pre>"},{"location":"demo/column-mapping/#parquet-view","title":"Parquet View","text":"<p>This time you're going to read the data files without Delta Lake to know and review the schema (as is in the parquet files themselves). That will show you the physical column names as they are in parquet data files.</p> Scala <pre><code>spark.read.format(\"parquet\").load(s\"spark-warehouse/$tableName\").printSchema\n</code></pre> <pre><code>root\n|-- id: long (nullable = true)\n</code></pre> <p>So, parquet files with the data of a delta table know about <code>id</code> column while Delta Lake maps it over to <code>new_id</code> at load time.</p>"},{"location":"demo/column-mapping/#delta-lake-view","title":"Delta Lake View","text":"<p>In the end, let's have a look at the schema using Delta Lake API.</p> Delta Lake <pre><code>import org.apache.spark.sql.catalyst.TableIdentifier\nval tid = TableIdentifier(tableName)\n\nimport org.apache.spark.sql.delta.DeltaTableIdentifier\nval dtId = DeltaTableIdentifier(table = Some(tid))\n\nimport org.apache.spark.sql.delta.DeltaLog\nval table = DeltaLog.forTable(spark, dtId)\n\nprintln(table.snapshot.metadata.schema.prettyJson)\n</code></pre> <pre><code>{\n  \"type\" : \"struct\",\n  \"fields\" : [ {\n    \"name\" : \"new_id\",\n    \"type\" : \"long\",\n    \"nullable\" : true,\n    \"metadata\" : {\n      \"delta.columnMapping.id\" : 1,\n      \"delta.columnMapping.physicalName\" : \"id\"\n    }\n  } ]\n}\n</code></pre> <p>Note the new <code>name</code> (<code>new_id</code>) and the associated metadata.</p>"},{"location":"demo/data-skipping/","title":"Demo: Data Skipping","text":"<p>This demo shows Data Skipping in action.</p>"},{"location":"demo/data-skipping/#logging","title":"Logging","text":"<p>Enable logging for PrepareDeltaScan and the others used in data skipping.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.delta.stats=ALL\n</code></pre>"},{"location":"demo/data-skipping/#start-spark-shell","title":"Start Spark Shell","text":"<pre><code>./bin/spark-shell \\\n  --packages io.delta:delta-core_2.12:2.2.0 \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n</code></pre> <pre><code>import org.apache.spark.sql.delta.sources.DeltaSQLConf\nassert(spark.sessionState.conf.getConf(DeltaSQLConf.DELTA_STATS_SKIPPING), \"Data skipping should be enabled\")\n</code></pre>"},{"location":"demo/data-skipping/#create-delta-table","title":"Create Delta Table","text":"<pre><code>val tableName = \"d01\"\nsql(s\"DROP TABLE IF EXISTS $tableName\")\nspark.range(5).writeTo(tableName).using(\"delta\").create\n</code></pre>"},{"location":"demo/data-skipping/#show-column-statistics","title":"Show Column Statistics","text":"<pre><code>import org.apache.spark.sql.delta._\nimport org.apache.spark.sql.catalyst.TableIdentifier\nval d01 = DeltaLog.forTable(spark, TableIdentifier(tableName))\n</code></pre> <pre><code>val partitionFilters = Nil\nval filesWithStatsForScan = d01.snapshot.filesWithStatsForScan(partitionFilters)\nfilesWithStatsForScan.printSchema\n</code></pre> <pre><code>root\n |-- path: string (nullable = true)\n |-- partitionValues: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n |-- size: long (nullable = false)\n |-- modificationTime: long (nullable = false)\n |-- dataChange: boolean (nullable = false)\n |-- stats: struct (nullable = true)\n |    |-- numRecords: long (nullable = true)\n |    |-- minValues: struct (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |-- maxValues: struct (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |-- nullCount: struct (nullable = true)\n |    |    |-- id: long (nullable = true)\n |-- tags: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n</code></pre> <pre><code>val tableStats = filesWithStatsForScan.select('path, 'size, $\"stats.*\")\ntableStats.orderBy('path).show\n</code></pre> <pre><code>+--------------------+----+----------+---------+---------+---------+\n|                path|size|numRecords|minValues|maxValues|nullCount|\n+--------------------+----+----------+---------+---------+---------+\n|part-00000-43b9e4...| 296|         0|   {null}|   {null}|   {null}|\n|part-00003-2685fb...| 478|         1|      {0}|      {0}|      {0}|\n|part-00006-815e72...| 478|         1|      {1}|      {1}|      {0}|\n|part-00009-654322...| 478|         1|      {2}|      {2}|      {0}|\n|part-00012-f3a708...| 478|         1|      {3}|      {3}|      {0}|\n|part-00015-5ca541...| 478|         1|      {4}|      {4}|      {0}|\n+--------------------+----+----------+---------+---------+---------+\n</code></pre>"},{"location":"demo/data-skipping/#execute-query-with-data-skipping","title":"Execute Query with Data Skipping","text":"<pre><code>val q = sql(s\"SELECT * FROM $tableName WHERE id IN (2, 3)\")\nq.show\n</code></pre> <p>You should see the following logs and the output.</p> <pre><code>22/05/29 22:58:02 INFO PrepareDeltaScan: DELTA: Filtering files for query\n22/05/29 22:58:02 INFO PrepareDeltaScan: DELTA: Done\n+---+\n| id|\n+---+\n|  3|\n|  2|\n+---+\n</code></pre>"},{"location":"demo/data-skipping/#web-ui","title":"web UI","text":"<p>Open the web UI to review the query and the associated job with the name <code>Filtering files for query</code>.</p>"},{"location":"demo/dataChange/","title":"Demo: dataChange","text":"<p>This demo shows dataChange option in action.</p> <p>In combination with <code>Overwrite</code> mode, <code>dataChange</code> option can be used to transactionally rearrange data in a delta table.</p>"},{"location":"demo/dataChange/#start-spark-shell","title":"Start Spark Shell","text":"<pre><code>./bin/spark-shell \\\n  --packages io.delta:delta-core_2.12:2.2.0 \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n</code></pre>"},{"location":"demo/dataChange/#create-delta-table","title":"Create Delta Table","text":"<pre><code>val path = \"/tmp/delta/d01\"\n</code></pre> <p>Make sure that there is no delta table at the location. Remove it if exists and start over.</p> <pre><code>import org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, path)\nassert(deltaLog.tableExists == false)\n</code></pre> <p>Create the demo delta table (using SQL).</p> <pre><code>sql(s\"\"\"\n  CREATE TABLE delta.`$path`\n  USING delta\n  VALUES ((0, 'Jacek'), (1, 'Agata')) AS (id, name)\n  \"\"\")\n</code></pre>"},{"location":"demo/dataChange/#show-history-before","title":"Show History (Before)","text":"<pre><code>import io.delta.tables.DeltaTable\nval dt = DeltaTable.forPath(path)\nassert(dt.history.count == 1)\n</code></pre>"},{"location":"demo/dataChange/#repartition-table","title":"Repartition Table","text":"<p>The following <code>dataChange</code> example shows a batch query that repartitions a delta table (perhaps while other queries could be using the delta table).</p> <p>Let's check out the number of partitions.</p> <pre><code>spark.read.format(\"delta\").load(path).rdd.getNumPartitions\n</code></pre> <p>The key items to pay attention to are:</p> <ol> <li>The batch query is independent from any other running streaming or batch queries over the delta table</li> <li>The batch query reads from the same delta table it saves data to</li> <li>The save mode is overwrite</li> <li>dataChange option is disabled</li> </ol> <pre><code>spark\n  .read\n  .format(\"delta\")\n  .load(path)\n  .repartition(10)\n  .write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .option(\"dataChange\", false)\n  .save(path)\n</code></pre> <p>Let's check out the number of partitions after the repartition.</p> <pre><code>spark.read.format(\"delta\").load(path).rdd.getNumPartitions\n</code></pre>"},{"location":"demo/dataChange/#show-history-after","title":"Show History (After)","text":"<pre><code>assert(dt.history.count == 2)\n</code></pre> <pre><code>dt.history\n  .select('version, 'operation, 'operationParameters, 'operationMetrics)\n  .orderBy('version.asc)\n  .show(truncate = false)\n</code></pre> <pre><code>+-------+----------------------+------------------------------------------------------------------------------+-----------------------------------------------------------+\n|version|operation             |operationParameters                                                           |operationMetrics                                           |\n+-------+----------------------+------------------------------------------------------------------------------+-----------------------------------------------------------+\n|0      |CREATE TABLE AS SELECT|{isManaged -&gt; false, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}|{numFiles -&gt; 1, numOutputBytes -&gt; 1273, numOutputRows -&gt; 1}|\n|1      |WRITE                 |{mode -&gt; Overwrite, partitionBy -&gt; []}                                        |{numFiles -&gt; 2, numOutputBytes -&gt; 1992, numOutputRows -&gt; 1}|\n+-------+----------------------+------------------------------------------------------------------------------+-----------------------------------------------------------+\n</code></pre>"},{"location":"demo/generated-columns/","title":"Demo: Generated Columns","text":"<p>This demo shows Generated Columns in action.</p>"},{"location":"demo/generated-columns/#create-delta-table-with-generated-column","title":"Create Delta Table with Generated Column","text":"<p>This step uses DeltaColumnBuilder API to define a generated column using DeltaColumnBuilder.generatedAlwaysAs.</p> <pre><code>import io.delta.tables.DeltaTable\nval tableName = \"delta_gencols\"\nsql(s\"DROP TABLE IF EXISTS $tableName\")\n</code></pre>"},{"location":"demo/generated-columns/#primitive-type","title":"Primitive Type","text":"<pre><code>import org.apache.spark.sql.types.DataTypes\n\nDeltaTable.create\n.addColumn(\"id\", DataTypes.LongType, nullable = false)\n.addColumn(\nDeltaTable.columnBuilder(\"value\")\n.dataType(DataTypes.BooleanType)\n.generatedAlwaysAs(\"true\")\n.build)\n.tableName(tableName)\n.execute\n</code></pre>"},{"location":"demo/generated-columns/#complex-type","title":"Complex Type","text":"<p>With a complex type (e.g., <code>StructType</code>), you have to define fields with <code>nullable</code> disabled. Otherwise, you run into a very mysterious exception (that you don't want to spend you time on).</p> <pre><code>import org.apache.spark.sql.types._\n\nval dataType = StructType(\nStructField(\"long\", LongType, nullable = false) ::\nStructField(\"str\", StringType, nullable = false) :: Nil)\n\nval generationExpr = \"struct(id AS long, 'hello' AS str)\"\n\nval generatedColumn = DeltaTable.columnBuilder(\"metadata\")\n.dataType(dataType)\n.generatedAlwaysAs(generationExpr)\n.build\n\nDeltaTable.createOrReplace\n.addColumn(\"id\", LongType, nullable = false)\n.addColumn(generatedColumn)\n.tableName(tableName)\n.execute\n</code></pre>"},{"location":"demo/generated-columns/#review-metadata","title":"Review Metadata","text":"Scala <pre><code>import org.apache.spark.sql.delta.DeltaLog\nimport org.apache.spark.sql.catalyst.TableIdentifier\nval deltaLog = DeltaLog.forTable(spark, TableIdentifier(tableName))\n</code></pre> <pre><code>println(deltaLog.snapshot.metadata.dataSchema(\"value\").metadata.json)\n</code></pre> <pre><code>{\"delta.generationExpression\":\"true\"}\n</code></pre>"},{"location":"demo/generated-columns/#write-to-delta-table","title":"Write to Delta Table","text":"ScalaSQL <pre><code>spark.range(5).writeTo(tableName).append()\n</code></pre> <p>The following SQL query fails with an <code>AnalysisException</code> due to this issue.</p> <pre><code>--- FIXME: Fails with org.apache.spark.sql.\nsql(\"\"\"\nINSERT INTO delta_gencols (id)\nSELECT * FROM RANGE(5)\n\"\"\")\n</code></pre>"},{"location":"demo/generated-columns/#show-table","title":"Show Table","text":"ScalaSQL <pre><code>spark.table(tableName).orderBy('id).show\n</code></pre> <pre><code>SELECT * FROM delta_gencols\nORDER BY id\n</code></pre> <pre><code>+---+-----+\n| id|value|\n+---+-----+\n|  0| true|\n|  1| true|\n|  2| true|\n|  3| true|\n|  4| true|\n+---+-----+\n</code></pre>"},{"location":"demo/generated-columns/#invariantviolationexception","title":"InvariantViolationException","text":"<p>It is possible to give the value of the generated column, but it has to pass a <code>CHECK</code> constraint.</p> <p>The following one-row query will break the constraint since the value is not <code>true</code>.</p> Scala <pre><code>Seq(5L).toDF(\"id\")\n.withColumn(\"value\", lit(false))\n.writeTo(tableName)\n.append()\n</code></pre> <pre><code>org.apache.spark.sql.delta.schema.InvariantViolationException: CHECK constraint Generated Column (value &lt;=&gt; true) violated by row with values:\n - value : false\n  at org.apache.spark.sql.delta.schema.InvariantViolationException$.apply(InvariantViolationException.scala:72)\n  at org.apache.spark.sql.delta.schema.InvariantViolationException$.apply(InvariantViolationException.scala:82)\n  at org.apache.spark.sql.delta.schema.InvariantViolationException.apply(InvariantViolationException.scala)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n  at org.apache.spark.sql.delta.constraints.DeltaInvariantCheckerExec.$anonfun$doExecute$3(DeltaInvariantCheckerExec.scala:87)\n</code></pre>"},{"location":"demo/merge-operation/","title":"Demo: Merge Operation","text":"<p>This demo shows DeltaTable.merge operation (and the underlying MergeIntoCommand) in action.</p> <p>Tip</p> <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.commands.MergeIntoCommand</code> logger as described in Logging.</p>"},{"location":"demo/merge-operation/#target-table","title":"Target Table","text":""},{"location":"demo/merge-operation/#create-table","title":"Create Table","text":"ScalaSQL <pre><code>val path = \"/tmp/delta/demo\"\nimport io.delta.tables.DeltaTable\nval target = DeltaTable.create.addColumn(\"id\", \"long\").location(path).execute\n</code></pre> <pre><code>assert(target.isInstanceOf[io.delta.tables.DeltaTable])\nassert(target.history.count == 1, \"There must be version 0 only\")\n</code></pre> <pre><code>DROP TABLE IF EXISTS merge_demo;\n\nCREATE TABLE merge_demo (id LONG)\nUSING delta;\n</code></pre> <p>Please note that the above commands leave us with an empty Delta table. Let's fix it.</p> ScalaSQL <pre><code>import org.apache.spark.sql.SaveMode\nspark.range(5).write.format(\"delta\").mode(SaveMode.Append).save(path)\n</code></pre> <pre><code>assert(target.history.count == 2)\n</code></pre> <pre><code>INSERT INTO merge_demo\nSELECT * FROM range(5);\n</code></pre>"},{"location":"demo/merge-operation/#source-table","title":"Source Table","text":"ScalaSQL <pre><code>case class Person(id: Long, name: String)\nval source = Seq(Person(0, \"Zero\"), Person(1, \"One\")).toDF\n</code></pre> <pre><code>DROP TABLE IF EXISTS merge_demo_source;\n\nCREATE TABLE merge_demo_source (id LONG, name STRING)\nUSING delta;\n</code></pre> <p>Note the difference in the schema of the <code>target</code> and <code>source</code> datasets.</p> ScalaSQL <pre><code>target.toDF.printSchema\n</code></pre> <pre><code>DESC merge_demo;\n</code></pre> <pre><code>|-- id: long (nullable = true)\n</code></pre> ScalaSQL <pre><code>source.printSchema\n</code></pre> <pre><code>DESC merge_demo_source;\n</code></pre> <pre><code>root\n |-- id: long (nullable = false)\n |-- name: string (nullable = true)\n</code></pre>"},{"location":"demo/merge-operation/#merge-matched-delete-with-schema-evolution","title":"MERGE MATCHED DELETE with Schema Evolution","text":"<p>Not only are we about to update the matching rows, but also update the schema (schema evolution).</p> ScalaSQL <pre><code>val mergeBuilder = target.as(\"to\")\n.merge(\nsource = source.as(\"from\"),\ncondition = $\"to.id\" === $\"from.id\")\n</code></pre> <pre><code>assert(mergeBuilder.isInstanceOf[io.delta.tables.DeltaMergeBuilder])\n</code></pre> <pre><code>scala&gt; mergeBuilder.execute\norg.apache.spark.sql.AnalysisException: There must be at least one WHEN clause in a MERGE statement\n  at org.apache.spark.sql.catalyst.plans.logical.DeltaMergeInto$.apply(deltaMerge.scala:253)\n  at io.delta.tables.DeltaMergeBuilder.mergePlan(DeltaMergeBuilder.scala:268)\n  at io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:215)\n  at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:87)\n  at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:73)\n  at io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:120)\n  at io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:204)\n  ... 47 elided\n</code></pre> <pre><code>val mergeMatchedBuilder = mergeBuilder.whenMatched()\nassert(mergeMatchedBuilder.isInstanceOf[io.delta.tables.DeltaMergeMatchedActionBuilder])\n\nval mergeBuilderDeleteMatched = mergeMatchedBuilder.delete()\nassert(mergeBuilderDeleteMatched.isInstanceOf[io.delta.tables.DeltaMergeBuilder])\n\nmergeBuilderDeleteMatched.execute()\n</code></pre> <pre><code>MERGE INTO merge_demo to\nUSING merge_demo_source from\nON to.id = from.id;\n</code></pre> <pre><code>Error in query:\nThere must be at least one WHEN clause in a MERGE statement(line 1, pos 0)\n</code></pre> <pre><code>MERGE INTO merge_demo to\nUSING merge_demo_source from\nON to.id = from.id\nWHEN MATCHED THEN DELETE;\n</code></pre> <pre><code>assert(target.history.count == 3)\n</code></pre>"},{"location":"demo/merge-operation/#update-all-columns-except-one","title":"Update All Columns Except One","text":"<p>This demo shows how to update all columns except one on a match.</p> <pre><code>val targetDF = target\n.toDF\n.withColumn(\"num\", lit(1))\n.withColumn(\"updated\", lit(false))\n</code></pre> <pre><code>targetDF.sort('id.asc).show\n</code></pre> <pre><code>+---+---+-------+\n| id|num|updated|\n+---+---+-------+\n|  2|  1|  false|\n|  3|  1|  false|\n|  4|  1|  false|\n+---+---+-------+\n</code></pre> <p>Write the modified data out to the delta table (that will create a new version with the schema changed).</p> <pre><code>targetDF\n.write\n.format(\"delta\")\n.mode(\"overwrite\")\n.option(\"overwriteSchema\", true)\n.save(path)\n</code></pre> <p>Reload the delta table (with the new column changes).</p> <pre><code>val target = DeltaTable.forPath(path)\nval targetDF = target.toDF\n</code></pre> <pre><code>val sourceDF = Seq(0, 1, 2).toDF(\"num\")\n</code></pre> <p>Create an update map (with the columns of the target delta table and the new values).</p> <pre><code>val updates = Map(\n\"updated\" -&gt; lit(true))\n</code></pre> <pre><code>target.as(\"to\")\n.merge(\nsource = sourceDF.as(\"from\"),\ncondition = $\"to.id\" === $\"from.num\")\n.whenMatched.update(updates)\n.execute()\n</code></pre> <p>Reload the delta table (with the merge changes).</p> <pre><code>val target = DeltaTable.forPath(path)\ntarget.toDF.sort('id.asc).show\n</code></pre> <pre><code>+---+---+-------+\n| id|num|updated|\n+---+---+-------+\n|  2|  1|   true|\n|  3|  1|  false|\n|  4|  1|  false|\n+---+---+-------+\n</code></pre> <pre><code>assert(target.history.count == 5)\n</code></pre>"},{"location":"demo/merge-operation/#merge-not-matched-insert","title":"MERGE NOT MATCHED INSERT","text":"<p>Use <code>spark-sql</code> to execute the following query.</p> SQL <pre><code>MERGE INTO merge_demo to\nUSING merge_demo_source from\nON to.id = from.id\nWHEN NOT MATCHED THEN INSERT *;\n</code></pre>"},{"location":"demo/merge-operation/#streaming-cdf-read-and-merge","title":"Streaming CDF Read and MERGE","text":"<p>Note</p> <p>Until I figure out how to run two concurrent Spark applications using the same metastore I'm going to use <code>spark-sql</code> and <code>spark-shell</code> interchangeably.</p> <p>Use <code>spark-sql</code> to enable Change Data Feed on a delta table.</p> <pre><code>ALTER TABLE merge_demo\nSET TBLPROPERTIES (delta.enableChangeDataFeed = true);\n</code></pre> <pre><code>SHOW TBLPROPERTIES merge_demo;\n</code></pre> <p>Exit <code>spark-sql</code> and open <code>spark-shell</code>.</p> <p>Run a streaming CDF scan over the delta table.</p> <pre><code>spark\n.readStream\n.format(\"delta\")\n.option(\"readChangeFeed\", true)\n.table(\"merge_demo\")\n.writeStream\n.format(\"console\")\n.start\n</code></pre> <p>Execute <code>MERGE</code> command and observe the output of the streaming query.</p> <pre><code>sql(\"\"\"\n  MERGE INTO merge_demo to\n  USING merge_demo_source from\n  ON to.id = from.id\n  WHEN NOT MATCHED THEN INSERT *;\n\"\"\").show(truncate = false)\n</code></pre> <pre><code>-------------------------------------------\nBatch: 1\n-------------------------------------------\n+---+------------+---------------+-----------------+\n| id|_change_type|_commit_version|_commit_timestamp|\n+---+------------+---------------+-----------------+\n+---+------------+---------------+-----------------+\n</code></pre> <p>Why is the output empty?</p> <p>Why is the batch even generated since there is no data?</p>"},{"location":"demo/optimize/","title":"Demo: Optimize","text":"<p>This demo shows OPTIMIZE command in action.</p>"},{"location":"demo/optimize/#create-delta-table","title":"Create Delta Table","text":"Scala <pre><code>sql(\"DROP TABLE IF EXISTS nums\")\nspark.range(10e4.toLong)\n.repartitionByRange(3, $\"id\" % 10)\n.writeTo(\"nums\")\n.using(\"delta\")\n.create\n</code></pre> <p>Let's review the on-disk table representation.</p> Shell <pre><code>tree -s spark-warehouse/nums\n</code></pre> <pre><code>[        288]  spark-warehouse/nums\n\u251c\u2500\u2500 [        128]  _delta_log\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 [       1624]  00000000000000000000.json\n\u251c\u2500\u2500 [     161036]  part-00000-9d2b6a10-d00e-4cdb-aa24-84ff7346bf08-c000.snappy.parquet\n\u251c\u2500\u2500 [     121265]  part-00001-cb9456b6-037d-469f-b0d1-c384064beadd-c000.snappy.parquet\n\u2514\u2500\u2500 [     121037]  part-00002-4aa15e4b-9db4-4a4a-9f97-dd4f4396792b-c000.snappy.parquet\n\n2 directories, 4 files\n</code></pre>"},{"location":"demo/optimize/#optimize-table","title":"Optimize Table","text":"Scala <pre><code>val optimizeMetrics = sql(\"OPTIMIZE nums\")\n</code></pre> <pre><code>scala&gt; optimizeMetrics.printSchema\nroot\n |-- path: string (nullable = true)\n |-- metrics: struct (nullable = true)\n |    |-- numFilesAdded: long (nullable = false)\n |    |-- numFilesRemoved: long (nullable = false)\n |    |-- filesAdded: struct (nullable = true)\n |    |    |-- min: long (nullable = true)\n |    |    |-- max: long (nullable = true)\n |    |    |-- avg: double (nullable = false)\n |    |    |-- totalFiles: long (nullable = false)\n |    |    |-- totalSize: long (nullable = false)\n |    |-- filesRemoved: struct (nullable = true)\n |    |    |-- min: long (nullable = true)\n |    |    |-- max: long (nullable = true)\n |    |    |-- avg: double (nullable = false)\n |    |    |-- totalFiles: long (nullable = false)\n |    |    |-- totalSize: long (nullable = false)\n |    |-- partitionsOptimized: long (nullable = false)\n |    |-- zOrderStats: struct (nullable = true)\n |    |    |-- strategyName: string (nullable = true)\n |    |    |-- inputCubeFiles: struct (nullable = true)\n |    |    |    |-- num: long (nullable = false)\n |    |    |    |-- size: long (nullable = false)\n |    |    |-- inputOtherFiles: struct (nullable = true)\n |    |    |    |-- num: long (nullable = false)\n |    |    |    |-- size: long (nullable = false)\n |    |    |-- inputNumCubes: long (nullable = false)\n |    |    |-- mergedFiles: struct (nullable = true)\n |    |    |    |-- num: long (nullable = false)\n |    |    |    |-- size: long (nullable = false)\n |    |    |-- numOutputCubes: long (nullable = false)\n |    |    |-- mergedNumCubes: long (nullable = true)\n |    |-- numBatches: long (nullable = false)\n |    |-- totalConsideredFiles: long (nullable = false)\n |    |-- totalFilesSkipped: long (nullable = false)\n |    |-- preserveInsertionOrder: boolean (nullable = false)\n |    |-- numFilesSkippedToReduceWriteAmplification: long (nullable = false)\n |    |-- numBytesSkippedToReduceWriteAmplification: long (nullable = false)\n |    |-- startTimeMs: long (nullable = false)\n |    |-- endTimeMs: long (nullable = false)\n |    |-- totalClusterParallelism: long (nullable = false)\n |    |-- totalScheduledTasks: long (nullable = false)\n |    |-- autoCompactParallelismStats: struct (nullable = true)\n |    |    |-- maxClusterActiveParallelism: long (nullable = true)\n |    |    |-- minClusterActiveParallelism: long (nullable = true)\n |    |    |-- maxSessionActiveParallelism: long (nullable = true)\n |    |    |-- minSessionActiveParallelism: long (nullable = true)\n</code></pre> Scala <pre><code>optimizeMetrics.show(truncate = false)\n</code></pre> <pre><code>+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------+\n|path             |metrics                                                                                                                                           |\n+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------+\n|file:/tmp/numbers|{1, 3, {402620, 402620, 402620.0, 1, 402620}, {121037, 161036, 134446.0, 3, 403338}, 1, null, 1, 3, 0, false, 0, 0, 1678104379055, 0, 16, 0, null}|\n+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre> <p>Let's review the on-disk table representation (after <code>OPTIMIZE</code>).</p> Shell <pre><code>tree -s spark-warehouse/nums\n</code></pre> <pre><code>[        352]  spark-warehouse/nums\n\u251c\u2500\u2500 [        192]  _delta_log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [       1624]  00000000000000000000.json\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 [       1431]  00000000000000000001.json\n\u251c\u2500\u2500 [     402620]  part-00000-86af4440-5374-4c18-ae6f-07b0d3680e27-c000.snappy.parquet\n\u251c\u2500\u2500 [     161036]  part-00000-9d2b6a10-d00e-4cdb-aa24-84ff7346bf08-c000.snappy.parquet\n\u251c\u2500\u2500 [     121265]  part-00001-cb9456b6-037d-469f-b0d1-c384064beadd-c000.snappy.parquet\n\u2514\u2500\u2500 [     121037]  part-00002-4aa15e4b-9db4-4a4a-9f97-dd4f4396792b-c000.snappy.parquet\n\n2 directories, 6 files\n</code></pre> <p>Note one extra file (<code>part-00000-86af4440-5374-4c18-ae6f-07b0d3680e27-c000.snappy.parquet</code>) in the file listing.</p> Use DeltaLog to Review Log Files <p>You can review the transaction log (i.e., the<code>00000000000000000001.json</code> commit file in particular) or use DeltaLog.</p> Scala <pre><code>import org.apache.spark.sql.delta.DeltaLog\nimport org.apache.spark.sql.catalyst.TableIdentifier\nval log = DeltaLog.forTable(spark, tableName = TableIdentifier(\"nums\"))\nlog.getSnapshotAt(1).allFiles.select('path).show(truncate = false)\n</code></pre> <pre><code>+-------------------------------------------------------------------+\n|path                                                               |\n+-------------------------------------------------------------------+\n|part-00000-86af4440-5374-4c18-ae6f-07b0d3680e27-c000.snappy.parquet|\n+-------------------------------------------------------------------+\n</code></pre>"},{"location":"demo/optimize/#review-history","title":"Review History","text":"<p>OPTIMIZE is transactional and creates a new version.</p> Scala <pre><code>val history = sql(\"desc history nums\")\n.select(\n\"version\",\n\"operation\",\n\"operationParameters\",\n\"readVersion\",\n\"isolationLevel\",\n\"isBlindAppend\",\n\"operationMetrics\")\nhistory.show(truncate = false)\n</code></pre> <pre><code>+-------+----------------------+-----------------------------------------------------------------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|version|operation             |operationParameters                                                          |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                 |\n+-------+----------------------+-----------------------------------------------------------------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|1      |OPTIMIZE              |{predicate -&gt; [], zOrderBy -&gt; []}                                            |0          |SnapshotIsolation|false        |{numRemovedFiles -&gt; 3, numRemovedBytes -&gt; 403338, p25FileSize -&gt; 402620, minFileSize -&gt; 402620, numAddedFiles -&gt; 1, maxFileSize -&gt; 402620, p75FileSize -&gt; 402620, p50FileSize -&gt; 402620, numAddedBytes -&gt; 402620}|\n|0      |CREATE TABLE AS SELECT|{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}|null       |Serializable     |true         |{numFiles -&gt; 3, numOutputRows -&gt; 100000, numOutputBytes -&gt; 403338}                                                                                                                                               |\n+-------+----------------------+-----------------------------------------------------------------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"demo/replaceWhere/","title":"Demo: replaceWhere","text":"<p>This demo shows replaceWhere predicate option.</p> <p>In combination with <code>Overwrite</code> mode, a <code>replaceWhere</code> option can be used to transactionally replace data that matches a predicate.</p>"},{"location":"demo/replaceWhere/#create-delta-table","title":"Create Delta Table","text":"<pre><code>val table = \"d1\"\n</code></pre> <pre><code>sql(s\"\"\"\n  CREATE TABLE $table (`id` LONG, p STRING)\n  USING delta\n  PARTITIONED BY (p)\n  COMMENT 'Delta table'\n\"\"\")\n</code></pre> <pre><code>spark.catalog.listTables.show\n</code></pre> <pre><code>+----+--------+-----------+---------+-----------+\n|name|database|description|tableType|isTemporary|\n+----+--------+-----------+---------+-----------+\n|  d1| default|Delta table|  MANAGED|      false|\n+----+--------+-----------+---------+-----------+\n</code></pre> <pre><code>import org.apache.spark.sql.delta.DeltaLog\nval dt = DeltaLog.forTable(spark, table)\nval m = dt.snapshot.metadata\nval partitionSchema = m.partitionSchema\n</code></pre>"},{"location":"demo/replaceWhere/#write-data","title":"Write Data","text":"<pre><code>Seq((0L, \"a\"),\n(1L, \"a\"))\n.toDF(\"id\", \"p\")\n.write\n.format(\"delta\")\n.option(\"replaceWhere\", \"p = 'a'\")\n.mode(\"overwrite\")\n.saveAsTable(table)\n</code></pre>"},{"location":"demo/rolling-back-table-changes-restore-command/","title":"Demo: Rolling Back Table Changes (Restore Command)","text":"<p>This demo shows RESTORE command in action (using the SQL variant).</p>"},{"location":"demo/rolling-back-table-changes-restore-command/#logging","title":"Logging","text":"<p>Enable logging for RestoreTableCommand.</p>"},{"location":"demo/rolling-back-table-changes-restore-command/#start-spark-shell","title":"Start Spark Shell","text":"<p>Start Spark Shell with Delta Lake.</p>"},{"location":"demo/rolling-back-table-changes-restore-command/#create-delta-table","title":"Create Delta Table","text":"<p>Let's create a delta table using a mixture of Scala and SQL.</p> Scala <pre><code>sql(\"\"\"\nDROP TABLE IF EXISTS delta_demo\n\"\"\")\n</code></pre> CREATE DATASOURCE TABLE <p>Learn more in CREATE DATASOURCE TABLE.</p> ScalaSQL <pre><code>spark.range(1).writeTo(\"delta_demo\").using(\"delta\").create\n</code></pre> <pre><code>CREATE TABLE delta_demo\nUSING delta\nCOMMENT 'Demo delta table'\nAS VALUES 0 t(id)\n</code></pre> <pre><code>spark.table(\"delta_demo\").show\n</code></pre> <pre><code>+---+\n| id|\n+---+\n|  0|\n+---+\n</code></pre> <p>This is the first <code>0</code>th version of the delta table with just a single row.</p> Scala <pre><code>sql(\"\"\"\nDESC HISTORY delta_demo;\n\"\"\")\n.select('version, 'operation, 'operationParameters)\n.show(truncate = false)\n</code></pre> <pre><code>+-------+----------------------+-----------------------------------------------------------------------------+\n|version|operation             |operationParameters                                                          |\n+-------+----------------------+-----------------------------------------------------------------------------+\n|0      |CREATE TABLE AS SELECT|{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}|\n+-------+----------------------+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"demo/rolling-back-table-changes-restore-command/#create-multiple-table-versions","title":"Create Multiple Table Versions","text":"<p>Let's create multiple versions of the delta table by inserting some new rows.</p>"},{"location":"demo/rolling-back-table-changes-restore-command/#insert-into","title":"INSERT INTO","text":"INSERT INTO <p>Learn more in INSERT INTO.</p> <pre><code>sql(\"\"\"\nINSERT INTO delta_demo VALUES 1\n\"\"\")\n</code></pre> <p>That gives us another version.</p>"},{"location":"demo/rolling-back-table-changes-restore-command/#merge-into","title":"MERGE INTO","text":"<p>Let's use Delta Lake's own MERGE INTO command.</p> <pre><code>sql(\"\"\"\nMERGE INTO delta_demo target\nUSING (VALUES 2 source(id))\nON target.id = source.id\nWHEN NOT MATCHED THEN INSERT *\n\"\"\")\n</code></pre>"},{"location":"demo/rolling-back-table-changes-restore-command/#desc-history","title":"DESC HISTORY","text":"Scala <pre><code>sql(\"\"\"\nDESC HISTORY delta_demo;\n\"\"\")\n.select('version, 'operation, 'operationParameters)\n.show(truncate = false)\n</code></pre> <pre><code>+-------+----------------------+----------------------------------------------------------------------------------------------------------------------------------+\n|version|operation             |operationParameters                                                                                                               |\n+-------+----------------------+----------------------------------------------------------------------------------------------------------------------------------+\n|2      |MERGE                 |{predicate -&gt; (target.id = CAST(source.id AS BIGINT)), matchedPredicates -&gt; [], notMatchedPredicates -&gt; [{\"actionType\":\"insert\"}]}|\n|1      |WRITE                 |{mode -&gt; Append, partitionBy -&gt; []}                                                                                               |\n|0      |CREATE TABLE AS SELECT|{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}                                                     |\n+-------+----------------------+----------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"demo/rolling-back-table-changes-restore-command/#roll-back-with-restore","title":"Roll Back with RESTORE","text":"<p>The most recent version is <code>2</code> with the following rows:</p> Scala <pre><code>spark.table(\"delta_demo\").orderBy('id).show\n</code></pre> <pre><code>+---+\n| id|\n+---+\n|  0|\n|  1|\n|  2|\n+---+\n</code></pre> <p>Let's revert some changes to the delta table using RESTORE TABLE command.</p> <p>Let's restore the initial (<code>0</code>th) version and review the history of this delta table.</p> Scala <pre><code>sql(\"\"\"\nRESTORE delta_demo TO VERSION AS OF 0\n\"\"\").show\n</code></pre> <p>You should see the following INFO messages in the logs:</p> <pre><code>RestoreTableCommand: DELTA: RestoreTableCommand: compute missing files validation  (table path file:/Users/jacek/dev/apps/spark-3.2.2-bin-hadoop3.2/spark-warehouse/delta_demo)\nRestoreTableCommand: DELTA: Done\nRestoreTableCommand: DELTA: RestoreTableCommand: compute metrics  (table path file:/Users/jacek/dev/apps/spark-3.2.2-bin-hadoop3.2/spark-warehouse/delta_demo)\nRestoreTableCommand: DELTA: Done\nRestoreTableCommand: DELTA: RestoreTableCommand: compute add actions  (table path file:/Users/jacek/dev/apps/spark-3.2.2-bin-hadoop3.2/spark-warehouse/delta_demo)\nRestoreTableCommand: DELTA: Done\nRestoreTableCommand: DELTA: RestoreTableCommand: compute remove actions  (table path file:/Users/jacek/dev/apps/spark-3.2.2-bin-hadoop3.2/spark-warehouse/delta_demo)\nRestoreTableCommand: DELTA: Done\nRestoreTableCommand: Committed delta #3 to file:/Users/jacek/dev/apps/spark-3.2.2-bin-hadoop3.2/spark-warehouse/delta_demo/_delta_log. Wrote 4 actions.\n</code></pre> <p><code>RestoreTableCommand</code> should also give you some statistics.</p> <pre><code>+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n|table_size_after_restore|num_of_files_after_restore|num_removed_files|num_restored_files|removed_files_size|restored_files_size|\n+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n|                     774|                         2|                2|                 0|               956|                  0|\n+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n</code></pre> <p>Let's query the rows.</p> Scala <pre><code>spark.table(\"delta_demo\").show\n</code></pre> <pre><code>+---+\n| id|\n+---+\n|  0|\n+---+\n</code></pre> <p>That looks OK. That's the row of the <code>0</code>th version. Use the following query to prove it.</p> ScalaDelta API <pre><code>spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(\"delta_demo\").show\n</code></pre> <pre><code>import org.apache.spark.sql.catalyst.TableIdentifier\nval tid = TableIdentifier(\"delta_demo\")\n\nimport org.apache.spark.sql.delta.DeltaTableIdentifier\nval did = DeltaTableIdentifier(table = Some(tid))\n\nval log = did.getDeltaLog(spark)\nval snapshotAt = log.getSnapshotAt(0)\n\nval relation = log.createRelation(snapshotToUseOpt = Some(snapshotAt))\nval df = spark.baseRelationToDataFrame(relation)\ndf.show\n</code></pre> <pre><code>+---+\n| id|\n+---+\n|  0|\n+---+\n</code></pre> <p>Let's review the history.</p> Scala <pre><code>sql(\"\"\"\nDESC HISTORY delta_demo;\n\"\"\")\n.select('version, 'operation, 'operationParameters)\n.show(truncate = false)\n</code></pre> <pre><code>+-------+----------------------+----------------------------------------------------------------------------------------------------------------------------------+\n|version|operation             |operationParameters                                                                                                               |\n+-------+----------------------+----------------------------------------------------------------------------------------------------------------------------------+\n|3      |RESTORE               |{version -&gt; 0, timestamp -&gt; null}                                                                                                 |\n|2      |MERGE                 |{predicate -&gt; (target.id = CAST(source.id AS BIGINT)), matchedPredicates -&gt; [], notMatchedPredicates -&gt; [{\"actionType\":\"insert\"}]}|\n|1      |WRITE                 |{mode -&gt; Append, partitionBy -&gt; []}                                                                                               |\n|0      |CREATE TABLE AS SELECT|{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}                                                     |\n+-------+----------------------+----------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"demo/rolling-back-table-changes-restore-command/#web-ui","title":"web UI","text":"<p>Open the web UI to review all the Spark jobs submitted.</p>"},{"location":"demo/rolling-back-table-changes-restore-command/#restore-and-alter-table-set-tblproperties","title":"RESTORE and ALTER TABLE SET TBLPROPERTIES","text":"<p>What happens when we enable Change Data Feed on a delta table and restore the table to the version before the change?</p>"},{"location":"demo/rolling-back-table-changes-restore-command/#enable-change-data-feed","title":"Enable Change Data Feed","text":"<pre><code>sql(\"\"\"\nALTER TABLE delta_demo\nSET TBLPROPERTIES (delta.enableChangeDataFeed = true);\n\"\"\")\n</code></pre> <pre><code>sql(\"\"\"\nSHOW TBLPROPERTIES delta_demo;\n\"\"\").show(truncate = false)\n</code></pre> <pre><code>+--------------------------+-------+\n|key                       |value  |\n+--------------------------+-------+\n|Type                      |MANAGED|\n|delta.enableChangeDataFeed|true   |\n|delta.minReaderVersion    |1      |\n|delta.minWriterVersion    |4      |\n+--------------------------+-------+\n</code></pre>"},{"location":"demo/rolling-back-table-changes-restore-command/#history","title":"History","text":"<p>Let's review the history.</p> Scala <pre><code>sql(\"\"\"\nDESC HISTORY delta_demo;\n\"\"\")\n.select('version, 'operation)\n.show(truncate = false)\n</code></pre> <pre><code>+-------+----------------------+\n|version|operation             |\n+-------+----------------------+\n|4      |SET TBLPROPERTIES     |\n|3      |RESTORE               |\n|2      |MERGE                 |\n|1      |WRITE                 |\n|0      |CREATE TABLE AS SELECT|\n+-------+----------------------+\n</code></pre>"},{"location":"demo/rolling-back-table-changes-restore-command/#start-streaming-query","title":"Start Streaming Query","text":"<pre><code>spark\n.readStream\n.format(\"delta\")\n.option(\"readChangeFeed\", true)\n.table(\"delta_demo\")\n.writeStream\n.format(\"console\")\n.start\n</code></pre> <p>You should see the following output.</p> <pre><code>-------------------------------------------\nBatch: 0\n-------------------------------------------\n+---+------------+---------------+--------------------+\n| id|_change_type|_commit_version|   _commit_timestamp|\n+---+------------+---------------+--------------------+\n|  0|      insert|              4|2022-08-05 12:47:...|\n+---+------------+---------------+--------------------+\n</code></pre> <p>FIXME Why is the output printed out?</p> <p>CDF was just enabled so no write was CDF-aware. How is this led to the output?</p>"},{"location":"demo/rolling-back-table-changes-restore-command/#restore-to-pre-cdf-version","title":"Restore to pre-CDF Version","text":"<p>Let's restore to the previous version and see what happens with the streaming query.</p> Scala <pre><code>sql(\"\"\"\nRESTORE delta_demo TO VERSION AS OF 3\n\"\"\").show\n</code></pre> <pre><code>INFO RestoreTableCommand: DELTA: RestoreTableCommand: compute missing files validation  (table path file:/Users/jacek/dev/apps/spark-3.2.2-bin-hadoop3.2/spark-warehouse/delta_demo)\nINFO RestoreTableCommand: DELTA: Done\nINFO RestoreTableCommand: DELTA: RestoreTableCommand: compute metrics  (table path file:/Users/jacek/dev/apps/spark-3.2.2-bin-hadoop3.2/spark-warehouse/delta_demo)\nINFO RestoreTableCommand: DELTA: Done\nINFO RestoreTableCommand: DELTA: RestoreTableCommand: compute add actions  (table path file:/Users/jacek/dev/apps/spark-3.2.2-bin-hadoop3.2/spark-warehouse/delta_demo)\nINFO RestoreTableCommand: DELTA: Done\nINFO RestoreTableCommand: DELTA: RestoreTableCommand: compute remove actions  (table path file:/Users/jacek/dev/apps/spark-3.2.2-bin-hadoop3.2/spark-warehouse/delta_demo)\nINFO RestoreTableCommand: DELTA: Done\nINFO RestoreTableCommand: Committed delta #5 to file:/Users/jacek/dev/apps/spark-3.2.2-bin-hadoop3.2/spark-warehouse/delta_demo/_delta_log. Wrote 2 actions.\n</code></pre> <pre><code>+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n|table_size_after_restore|num_of_files_after_restore|num_removed_files|num_restored_files|removed_files_size|restored_files_size|\n+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n|                     774|                         2|                0|                 0|                 0|                  0|\n+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n</code></pre> <p>What's interesting is that the streaming query has produced no data (as if nothing has really happened).</p> <p>Let's review the history.</p> Scala <pre><code>sql(\"\"\"\nDESC HISTORY delta_demo;\n\"\"\")\n.select('version, 'operation)\n.show(truncate = false)\n</code></pre> <pre><code>+-------+----------------------+\n|version|operation             |\n+-------+----------------------+\n|5      |RESTORE               |\n|4      |SET TBLPROPERTIES     |\n|3      |RESTORE               |\n|2      |MERGE                 |\n|1      |WRITE                 |\n|0      |CREATE TABLE AS SELECT|\n+-------+----------------------+\n</code></pre> <p>Let's review the table properties (and <code>delta.enableChangeDataFeed</code> in particular).</p> <pre><code>sql(\"\"\"\nSHOW TBLPROPERTIES delta_demo;\n\"\"\").show(truncate = false)\n</code></pre> <pre><code>+----------------------+-------+\n|key                   |value  |\n+----------------------+-------+\n|Type                  |MANAGED|\n|delta.minReaderVersion|1      |\n|delta.minWriterVersion|4      |\n+----------------------+-------+\n</code></pre>"},{"location":"demo/schema-evolution/","title":"Demo: Schema Evolution","text":"<pre><code>/*\nspark-shell \\\n  --packages io.delta:delta-core_2.12:2.2.0 \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.databricks.delta.snapshotPartitions=1\n*/\n\ncase class PersonV1(id: Long, name: String)\nimport org.apache.spark.sql.Encoders\nval schemaV1 = Encoders.product[PersonV1].schema\nscala&gt; schemaV1.printTreeString\nroot\n |-- id: long (nullable = false)\n |-- name: string (nullable = true)\n\nval dataPath = \"/tmp/delta/people\"\n\n// Write data\nSeq(PersonV1(0, \"Zero\"), PersonV1(1, \"One\"))\n  .toDF\n  .write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .option(\"overwriteSchema\", \"true\")\n  .save(dataPath)\n\n// Committed delta #0 to file:/tmp/delta/people/_delta_log\n\nimport org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, dataPath)\nassert(deltaLog.snapshot.version == 0)\n\nscala&gt; deltaLog.snapshot.dataSchema.printTreeString\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n\nimport io.delta.tables.DeltaTable\nval dt = DeltaTable.forPath(dataPath)\nscala&gt; dt.toDF.show\n+---+----+\n| id|name|\n+---+----+\n|  0|Zero|\n|  1| One|\n+---+----+\n\nval main = dt.as(\"main\")\n\ncase class PersonV2(id: Long, name: String, newField: Boolean)\nval schemaV2 = Encoders.product[PersonV2].schema\nscala&gt; schemaV2.printTreeString\nroot\n |-- id: long (nullable = false)\n |-- name: string (nullable = true)\n |-- newField: boolean (nullable = false)\n\nval updates = Seq(\n  PersonV2(0, \"ZERO\", newField = true),\n  PersonV2(2, \"TWO\", newField = false)).toDF\n\n// Merge two datasets and create a new version\n// Schema evolution in play\nmain.merge(\n    source = updates.as(\"updates\"),\n    condition = $\"main.id\" === $\"updates.id\")\n  .whenMatched.updateAll\n  .execute\n\nval latestPeople = spark\n  .read\n  .format(\"delta\")\n  .load(dataPath)\nscala&gt; latestPeople.show\n+---+----+\n| id|name|\n+---+----+\n|  0|ZERO|\n|  1| One|\n+---+----+\n</code></pre>"},{"location":"demo/stream-processing-of-delta-table/","title":"Demo: Stream Processing of Delta Table","text":"<pre><code>/*\nspark-shell \\\n  --packages io.delta:delta-core_2.12:2.2.0 \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.databricks.delta.snapshotPartitions=1\n*/\nassert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\")\n\nimport org.apache.spark.sql.SparkSession\nassert(spark.isInstanceOf[SparkSession])\n\nval deltaTableDir = \"/tmp/delta/users\"\nval checkpointLocation = \"/tmp/checkpointLocation\"\n\n// Initialize the delta table\n// - No data\n// - Schema only\ncase class Person(id: Long, name: String, city: String)\nspark.emptyDataset[Person].write.format(\"delta\").save(deltaTableDir)\n\nimport org.apache.spark.sql.streaming.Trigger\nimport scala.concurrent.duration._\nval sq = spark\n  .readStream\n  .format(\"delta\")\n  .option(\"maxFilesPerTrigger\", 1) // Maximum number of files to scan per micro-batch\n  .load(deltaTableDir)\n  .writeStream\n  .format(\"console\")\n  .option(\"truncate\", false)\n  .option(\"checkpointLocation\", checkpointLocation)\n  .trigger(Trigger.ProcessingTime(10.seconds)) // Useful for debugging\n  .start\n\n// The streaming query over delta table\n// should display the 0th version as Batch 0\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n+---+----+----+\n|id |name|city|\n+---+----+----+\n+---+----+----+\n\n// Let's write to the delta table\nval users = Seq(\n    Person(0, \"Jacek\", \"Warsaw\"),\n    Person(1, \"Agata\", \"Warsaw\"),\n    Person(2, \"Jacek\", \"Paris\"),\n    Person(3, \"Domas\", \"Vilnius\")).toDF\n\n// More partitions are more file added\n// And per maxFilesPerTrigger as 1 file addition per micro-batch\n// You should see more micro-batches\nscala&gt; println(users.rdd.getNumPartitions)\n4\n\n// Change the default SaveMode.ErrorIfExists to more meaningful save mode\nimport org.apache.spark.sql.SaveMode\nusers\n  .write\n  .format(\"delta\")\n  .mode(SaveMode.Append) // Appending rows\n  .save(deltaTableDir)\n\n// Immediately after the above write finishes\n// New batches should be printed out to the console\n// Per the number of partitions of users dataset\n// And per maxFilesPerTrigger as 1 file addition\n// You should see as many micro-batches as files\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+---+-----+------+\n|id |name |city  |\n+---+-----+------+\n|0  |Jacek|Warsaw|\n+---+-----+------+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+---+-----+------+\n|id |name |city  |\n+---+-----+------+\n|1  |Agata|Warsaw|\n+---+-----+------+\n\n-------------------------------------------\nBatch: 3\n-------------------------------------------\n+---+-----+-----+\n|id |name |city |\n+---+-----+-----+\n|2  |Jacek|Paris|\n+---+-----+-----+\n\n-------------------------------------------\nBatch: 4\n-------------------------------------------\n+---+-----+-------+\n|id |name |city   |\n+---+-----+-------+\n|3  |Domas|Vilnius|\n+---+-----+-------+\n</code></pre>"},{"location":"demo/time-travel/","title":"Demo: Time Travel","text":"<p>This demo shows Time Travel in action.</p> <pre><code>./bin/spark-shell \\\n  --packages io.delta:delta-core_2.12:2.2.0 \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n</code></pre> <pre><code>import io.delta.implicits._\n\nSeq((\"r1\", 500), (\"r2\", 600)).toDF(\"id\", \"value\").write.delta(\"/tmp/delta/demo\")\n\nsql(\"DESCRIBE HISTORY delta.`/tmp/delta/demo`\").select('version, 'operation).show(truncate = false)\n\n// +-------+---------+\n// |version|operation|\n// +-------+---------+\n// |0      |WRITE    |\n// +-------+---------+\n\nsql(\"UPDATE delta.`/tmp/delta/demo` SET value = '700' WHERE id = 'r1'\")\n\nsql(\"DESCRIBE HISTORY delta.`/tmp/delta/demo`\").select('version, 'operation).show(truncate = false)\n\n// +-------+---------+\n// |version|operation|\n// +-------+---------+\n// |1      |UPDATE   |\n// |0      |WRITE    |\n// +-------+---------+\n\nspark.read.delta(\"/tmp/delta/demo\").show\n\n// +---+-----+\n// | id|value|\n// +---+-----+\n// | r1|  700|\n// | r2|  600|\n// +---+-----+\n\nspark.read.option(\"versionAsOf\", 0).delta(\"/tmp/delta/demo\").show\n\n// +---+-----+\n// | id|value|\n// +---+-----+\n// | r1|  500|\n// | r2|  600|\n// +---+-----+\n</code></pre> <p>The above query over a previous version is also possible using version pattern.</p> <pre><code>spark.read.delta(\"/tmp/delta/demo@v1\").show\n</code></pre>"},{"location":"demo/time-travel/#cataloged-delta-table","title":"Cataloged Delta Table","text":"<p>For a delta table registered in a catalog, you could use the following query:</p> <pre><code>spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(tableName).show\n</code></pre>"},{"location":"demo/user-metadata-for-labelling-commits/","title":"Demo: User Metadata for Labelling Commits","text":"<p>The demo shows how to differentiate commits of a write batch query using userMetadata option.</p> <p>Tip</p> <p>A fine example could be for distinguishing between two or more separate streaming write queries.</p>"},{"location":"demo/user-metadata-for-labelling-commits/#creating-delta-table","title":"Creating Delta Table","text":"<pre><code>val tableName = \"/tmp/delta-demo-userMetadata\"\n</code></pre> <pre><code>spark.range(5)\n  .write\n  .format(\"delta\")\n  .save(tableName)\n</code></pre>"},{"location":"demo/user-metadata-for-labelling-commits/#describing-history","title":"Describing History","text":"<pre><code>import io.delta.tables.DeltaTable\nval d = DeltaTable.forPath(tableName)\n</code></pre> <p>We are interested in a subset of the available history metadata.</p> <pre><code>d.history\n.select('version, 'operation, 'operationParameters, 'userMetadata)\n.show(truncate = false)\n</code></pre> <pre><code>+-------+---------+------------------------------------------+------------+\n|version|operation|operationParameters                       |userMetadata|\n+-------+---------+------------------------------------------+------------+\n|0      |WRITE    |[mode -&gt; ErrorIfExists, partitionBy -&gt; []]|null        |\n+-------+---------+------------------------------------------+------------+\n</code></pre>"},{"location":"demo/user-metadata-for-labelling-commits/#appending-data","title":"Appending Data","text":"<p>In this step, you're going to append new data to the existing Delta table.</p> <p>You're going to use userMetadata option for a custom user-defined historical marker (e.g. to know when this extra append happended in the life of the Delta table).</p> <pre><code>val userMetadata = \"two more rows for demo\"\n</code></pre> <p>Since you're appending new rows, it is required to use <code>Append</code> mode.</p> <pre><code>import org.apache.spark.sql.SaveMode.Append\n</code></pre> <p>The whole append write is as follows:</p> <pre><code>spark.range(start = 5, end = 7)\n.write\n.format(\"delta\")\n.option(\"userMetadata\", userMetadata)\n.mode(Append)\n.save(tableName)\n</code></pre> <p>That write query creates another version of the Delta table.</p>"},{"location":"demo/user-metadata-for-labelling-commits/#listing-versions-with-usermetadata","title":"Listing Versions with userMetadata","text":"<p>For the sake of the demo, you are going to show the versions of the Delta table with <code>userMetadata</code> defined.</p> <pre><code>d.history\n.select('version, 'operation, 'operationParameters, 'userMetadata)\n.where('userMetadata.isNotNull)\n.show(truncate = false)\n</code></pre> <pre><code>+-------+---------+-----------------------------------+----------------------+\n|version|operation|operationParameters                |userMetadata          |\n+-------+---------+-----------------------------------+----------------------+\n|1      |WRITE    |[mode -&gt; Append, partitionBy -&gt; []]|two more rows for demo|\n+-------+---------+-----------------------------------+----------------------+\n</code></pre>"},{"location":"demo/vacuum/","title":"Demo: Vacuum","text":"<p>This demo shows vacuum command in action.</p>"},{"location":"demo/vacuum/#start-spark-shell","title":"Start Spark Shell","text":"<pre><code>./bin/spark-shell \\\n  --packages io.delta:delta-core_2.12:2.2.0 \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n</code></pre>"},{"location":"demo/vacuum/#create-delta-table","title":"Create Delta Table","text":"<pre><code>val path = \"/tmp/delta/t1\"\n</code></pre> <p>Make sure that there is no delta table at the location. Remove it if exists and start over.</p> <pre><code>import org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, path)\nassert(deltaLog.tableExists == false)\n</code></pre> <p>Create a demo delta table (using Scala API). Write some data to the delta table, effectively creating the first version.</p> <pre><code>spark.range(4)\n.withColumn(\"p\", 'id % 2)\n.write\n.format(\"delta\")\n.partitionBy(\"p\")\n.save(path)\n</code></pre> <p>Display the available versions of the delta table.</p> <pre><code>import io.delta.tables.DeltaTable\nval dt = DeltaTable.forPath(path)\n</code></pre> <pre><code>val history = dt.history.select('version, 'operation, 'operationMetrics)\nhistory.show(truncate = false)\n</code></pre> <pre><code>+-------+---------+-----------------------------------------------------------+\n|version|operation|operationMetrics                                           |\n+-------+---------+-----------------------------------------------------------+\n|0      |WRITE    |[numFiles -&gt; 4, numOutputBytes -&gt; 1852, numOutputRows -&gt; 4]|\n+-------+---------+-----------------------------------------------------------+\n</code></pre>"},{"location":"demo/vacuum/#delete-all","title":"Delete All","text":"<p>Delete all data in the delta table, effectively creating the second version.</p> <pre><code>import io.delta.tables.DeltaTable\nDeltaTable.forPath(path).delete\n</code></pre> <p>Display the available versions of the delta table.</p> <pre><code>val history = dt.history.select('version, 'operation, 'operationMetrics)\nhistory.show(truncate = false)\n</code></pre> <pre><code>+-------+---------+-----------------------------------------------------------+\n|version|operation|operationMetrics                                           |\n+-------+---------+-----------------------------------------------------------+\n|1      |DELETE   |[numRemovedFiles -&gt; 4]                                     |\n|0      |WRITE    |[numFiles -&gt; 4, numOutputBytes -&gt; 1852, numOutputRows -&gt; 4]|\n+-------+---------+-----------------------------------------------------------+\n</code></pre>"},{"location":"demo/vacuum/#vacuum-dry-run-illegalargumentexception","title":"Vacuum DRY RUN (IllegalArgumentException)","text":"<p>Let's vacuum the delta table (in <code>DRY RUN</code> mode).</p> <pre><code>sql(s\"VACUUM delta.`$path` RETAIN 0 HOURS DRY RUN\")\n</code></pre> <pre><code>java.lang.IllegalArgumentException: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n</code></pre> <p>Attempting to vacuum the delta table (even with <code>DRY RUN</code>) gives an <code>IllegalArgumentException</code> because of the default values of the following:</p> <ul> <li>spark.databricks.delta.retentionDurationCheck.enabled configuration property</li> <li>deletedFileRetentionDuration table property</li> </ul>"},{"location":"demo/vacuum/#vacuum-dry-run","title":"Vacuum DRY RUN","text":""},{"location":"demo/vacuum/#retentiondurationcheckenabled-configuration-property","title":"retentionDurationCheck.enabled Configuration Property","text":"<p>Turn the spark.databricks.delta.retentionDurationCheck.enabled configuration property off and give the <code>VACUUM</code> command a go again.</p> <pre><code>spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", false)\n</code></pre> <pre><code>val q = sql(s\"VACUUM delta.`$path` RETAIN 0 HOURS DRY RUN\")\n</code></pre> <p>You should see the following message in the console:</p> <pre><code>Found 4 files and directories in a total of 3 directories that are safe to delete.\n</code></pre> <p>The result <code>DataFrame</code> is the paths that are safe to delete which are all of the data files in the delta table.</p> <pre><code>q.show(truncate = false)\n</code></pre> <pre><code>+------------------------------------------------------------------------------------------+\n|path                                                                                      |\n+------------------------------------------------------------------------------------------+\n|file:/tmp/delta/t1/p=0/part-00011-40983cc5-18bf-4d91-8b7b-eb805b8c862d.c000.snappy.parquet|\n|file:/tmp/delta/t1/p=1/part-00015-9576ff56-28f7-410e-8ea3-e43352a5083c.c000.snappy.parquet|\n|file:/tmp/delta/t1/p=0/part-00003-8e300857-ad6a-4889-b944-d31624a8024f.c000.snappy.parquet|\n|file:/tmp/delta/t1/p=1/part-00007-4fba265a-0884-44c3-84ed-4e9aee524e3a.c000.snappy.parquet|\n+------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"demo/vacuum/#deletedfileretentionduration-table-property","title":"deletedFileRetentionDuration Table Property","text":"<p>Let's <code>DESCRIBE DETAIL</code> to review the current table properties (incl. deletedFileRetentionDuration).</p> <pre><code>val tid = s\"delta.`$path`\"\n</code></pre> <pre><code>val detail = sql(s\"DESCRIBE DETAIL $tid\").select('format, 'location, 'properties)\ndetail.show(truncate = false)\n</code></pre> <pre><code>+------+------------------+----------+\n|format|location          |properties|\n+------+------------------+----------+\n|delta |file:/tmp/delta/t1|[]        |\n+------+------------------+----------+\n</code></pre> <p>Prefix the <code>deletedFileRetentionDuration</code> table property with <code>delta.</code> for <code>ALTER TABLE</code> to accept it as a Delta property.</p> <pre><code>sql(s\"ALTER TABLE $tid SET TBLPROPERTIES (delta.deletedFileRetentionDuration = '0 hours')\")\n</code></pre> <pre><code>val detail = sql(s\"DESCRIBE DETAIL $tid\").select('format, 'location, 'properties)\ndetail.show(truncate = false)\n</code></pre> <pre><code>+------+------------------+-----------------------------------------------+\n|format|location          |properties                                     |\n+------+------------------+-----------------------------------------------+\n|delta |file:/tmp/delta/t1|[delta.deletedFileRetentionDuration -&gt; 0 hours]|\n+------+------------------+-----------------------------------------------+\n</code></pre> <p>Display the available versions of the delta table and note that <code>ALTER TABLE</code> gives a new version. This time you include <code>operationParameters</code> column (not <code>operationMetrics</code> as less important).</p> <pre><code>val history = dt.history.select('version, 'operation, 'operationParameters)\nhistory.show(truncate = false)\n</code></pre> <pre><code>+-------+-----------------+----------------------------------------------------------------+\n|version|operation        |operationParameters                                             |\n+-------+-----------------+----------------------------------------------------------------+\n|2      |SET TBLPROPERTIES|[properties -&gt; {\"delta.deletedFileRetentionDuration\":\"0 hours\"}]|\n|1      |DELETE           |[predicate -&gt; []]                                               |\n|0      |WRITE            |[mode -&gt; ErrorIfExists, partitionBy -&gt; [\"p\"]]                   |\n+-------+-----------------+----------------------------------------------------------------+\n</code></pre> <p>You can access the table properties (table configuration) using DeltaLog Scala API.</p> <pre><code>import org.apache.spark.sql.delta.DeltaLog\nval log = DeltaLog.forTable(spark, path)\nlog.snapshot.metadata.configuration\n</code></pre> <p>Let's revert the latest change to <code>spark.databricks.delta.retentionDurationCheck.enabled</code> and turn it on back.</p> <pre><code>spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", true)\n</code></pre> <pre><code>val q = sql(s\"VACUUM delta.`$path` RETAIN 0 HOURS DRY RUN\")\n</code></pre> <p>You should see the following message in the console:</p> <pre><code>Found 4 files and directories in a total of 3 directories that are safe to delete.\n</code></pre> <p>The result <code>DataFrame</code> is the paths that are safe to delete which are all of the data files in the delta table.</p> <pre><code>q.show(truncate = false)\n</code></pre> <pre><code>+------------------------------------------------------------------------------------------+\n|path                                                                                      |\n+------------------------------------------------------------------------------------------+\n|file:/tmp/delta/t1/p=0/part-00011-40983cc5-18bf-4d91-8b7b-eb805b8c862d.c000.snappy.parquet|\n|file:/tmp/delta/t1/p=1/part-00015-9576ff56-28f7-410e-8ea3-e43352a5083c.c000.snappy.parquet|\n|file:/tmp/delta/t1/p=0/part-00003-8e300857-ad6a-4889-b944-d31624a8024f.c000.snappy.parquet|\n|file:/tmp/delta/t1/p=1/part-00007-4fba265a-0884-44c3-84ed-4e9aee524e3a.c000.snappy.parquet|\n+------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"demo/vacuum/#tree-delta-table-directory","title":"Tree Delta Table Directory","text":"<p>In a terminal (outside <code>spark-shell</code>) run <code>tree</code> or a similar command to review what the directory of the delta table looks like.</p> <pre><code>tree /tmp/delta/t1\n</code></pre> <pre><code>/tmp/delta/t1\n\u251c\u2500\u2500 _delta_log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 00000000000000000000.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 00000000000000000001.json\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 00000000000000000002.json\n\u251c\u2500\u2500 p=0\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 part-00003-8e300857-ad6a-4889-b944-d31624a8024f.c000.snappy.parquet\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 part-00011-40983cc5-18bf-4d91-8b7b-eb805b8c862d.c000.snappy.parquet\n\u2514\u2500\u2500 p=1\n    \u251c\u2500\u2500 part-00007-4fba265a-0884-44c3-84ed-4e9aee524e3a.c000.snappy.parquet\n    \u2514\u2500\u2500 part-00015-9576ff56-28f7-410e-8ea3-e43352a5083c.c000.snappy.parquet\n\n3 directories, 7 files\n</code></pre>"},{"location":"demo/vacuum/#retain-0-hours","title":"Retain 0 Hours","text":"<p>Let's clean up (vacuum) the delta table entirely, effectively deleting all the data files physically from disk.</p> <p>Back in <code>spark-shell</code>, run <code>VACUUM</code> SQL command. Note that you're going to use it with no <code>DRY RUN</code>.</p> <pre><code>sql(s\"VACUUM delta.`$path` RETAIN 0 HOURS\").show(truncate = false)\n</code></pre> <pre><code>Deleted 4 files and directories in a total of 3 directories.\n+------------------+\n|path              |\n+------------------+\n|file:/tmp/delta/t1|\n+------------------+\n</code></pre> <p>In a terminal (outside <code>spark-shell</code>), run <code>tree</code> or a similar command to review what the directory of the delta table looks like.</p> <pre><code>tree /tmp/delta/t1\n</code></pre> <pre><code>/tmp/delta/t1\n\u251c\u2500\u2500 _delta_log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 00000000000000000000.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 00000000000000000001.json\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 00000000000000000002.json\n\u251c\u2500\u2500 p=0\n\u2514\u2500\u2500 p=1\n\n3 directories, 3 files\n</code></pre> <p>Switch to <code>spark-shell</code> and display the available versions of the delta table. There should really be no change compared to the last time you executed it.</p> <pre><code>val history = dt.history.select('version, 'operation, 'operationParameters)\nhistory.show(truncate = false)\n</code></pre> <pre><code>+-------+-----------------+----------------------------------------------------------------+\n|version|operation        |operationParameters                                             |\n+-------+-----------------+----------------------------------------------------------------+\n|2      |SET TBLPROPERTIES|[properties -&gt; {\"delta.deletedFileRetentionDuration\":\"0 hours\"}]|\n|1      |DELETE           |[predicate -&gt; []]                                               |\n|0      |WRITE            |[mode -&gt; ErrorIfExists, partitionBy -&gt; [\"p\"]]                   |\n+-------+-----------------+----------------------------------------------------------------+\n</code></pre>"},{"location":"exceptions/","title":"Delta Exceptions","text":"<p>Among the new features of Delta Lake 1.0.0 are public DeltaConcurrentModificationExceptions for conflicts between concurrent operations. This allows you to catch such exceptions and retry write operations.</p>"},{"location":"exceptions/ConcurrentAppendException/","title":"ConcurrentAppendException","text":"<p><code>ConcurrentAppendException</code> is a DeltaConcurrentModificationException.</p>"},{"location":"exceptions/ConcurrentAppendException/#creating-instance","title":"Creating Instance","text":"<p><code>ConcurrentAppendException</code> takes the following to be created:</p> <ul> <li> Error Message <p><code>ConcurrentAppendException</code> is created\u00a0when:</p> <ul> <li><code>DeltaErrors</code> utility is used to concurrentAppendException</li> </ul>"},{"location":"exceptions/ConcurrentDeleteDeleteException/","title":"ConcurrentDeleteDeleteException","text":"<p><code>ConcurrentDeleteDeleteException</code> is a DeltaConcurrentModificationException.</p>"},{"location":"exceptions/ConcurrentDeleteDeleteException/#creating-instance","title":"Creating Instance","text":"<p><code>ConcurrentDeleteDeleteException</code> takes the following to be created:</p> <ul> <li> Error Message <p><code>ConcurrentDeleteDeleteException</code> is created\u00a0when:</p> <ul> <li><code>DeltaErrors</code> utility is used to concurrentDeleteDeleteException</li> </ul>"},{"location":"exceptions/ConcurrentDeleteReadException/","title":"ConcurrentDeleteReadException","text":"<p><code>ConcurrentDeleteReadException</code> is a DeltaConcurrentModificationException.</p>"},{"location":"exceptions/ConcurrentDeleteReadException/#creating-instance","title":"Creating Instance","text":"<p><code>ConcurrentDeleteReadException</code> takes the following to be created:</p> <ul> <li> Error Message <p><code>ConcurrentDeleteReadException</code> is created\u00a0when:</p> <ul> <li><code>DeltaErrors</code> utility is used to concurrentDeleteReadException</li> </ul>"},{"location":"exceptions/ConcurrentTransactionException/","title":"ConcurrentTransactionException","text":"<p><code>ConcurrentTransactionException</code> is a DeltaConcurrentModificationException.</p>"},{"location":"exceptions/ConcurrentTransactionException/#creating-instance","title":"Creating Instance","text":"<p><code>ConcurrentTransactionException</code> takes the following to be created:</p> <ul> <li> Error Message <p><code>ConcurrentTransactionException</code> is created\u00a0when:</p> <ul> <li><code>DeltaErrors</code> utility is used to concurrentTransactionException</li> </ul>"},{"location":"exceptions/ConcurrentWriteException/","title":"ConcurrentWriteException","text":"<p><code>ConcurrentWriteException</code> is a DeltaConcurrentModificationException.</p>"},{"location":"exceptions/ConcurrentWriteException/#creating-instance","title":"Creating Instance","text":"<p><code>ConcurrentWriteException</code> takes the following to be created:</p> <ul> <li> Error Message <p><code>ConcurrentWriteException</code> is created\u00a0when:</p> <ul> <li><code>DeltaErrors</code> utility is used to concurrentWriteException</li> </ul>"},{"location":"exceptions/DeltaConcurrentModificationException/","title":"DeltaConcurrentModificationException","text":"<p><code>DeltaConcurrentModificationException</code>\u00a0is an extension of the <code>ConcurrentModificationException</code> (Java) abstraction for commit conflict exceptions.</p> <p>Note</p> <p>There are two <code>DeltaConcurrentModificationException</code> abstractions in two different packages:</p> <ul> <li><code>io.delta.exceptions</code></li> <li><code>org.apache.spark.sql.delta</code> (obsolete since 1.0.0)</li> </ul>"},{"location":"exceptions/DeltaConcurrentModificationException/#implementations","title":"Implementations","text":"<ul> <li>ConcurrentAppendException</li> <li>ConcurrentDeleteDeleteException</li> <li>ConcurrentDeleteReadException</li> <li>ConcurrentTransactionException</li> <li>ConcurrentWriteException</li> <li>MetadataChangedException</li> <li>ProtocolChangedException</li> </ul>"},{"location":"exceptions/DeltaConcurrentModificationException/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaConcurrentModificationException</code> takes the following to be created:</p> <ul> <li> Error Message Abstract Class <p><code>DeltaConcurrentModificationException</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete DeltaConcurrentModificationExceptions.</p>"},{"location":"exceptions/MetadataChangedException/","title":"MetadataChangedException","text":"<p><code>MetadataChangedException</code> is a DeltaConcurrentModificationException.</p>"},{"location":"exceptions/MetadataChangedException/#creating-instance","title":"Creating Instance","text":"<p><code>MetadataChangedException</code> takes the following to be created:</p> <ul> <li> Error Message <p><code>MetadataChangedException</code> is created\u00a0when:</p> <ul> <li><code>DeltaErrors</code> utility is used to metadataChangedException</li> </ul>"},{"location":"exceptions/ProtocolChangedException/","title":"ProtocolChangedException","text":"<p><code>ProtocolChangedException</code> is a DeltaConcurrentModificationException.</p>"},{"location":"exceptions/ProtocolChangedException/#creating-instance","title":"Creating Instance","text":"<p><code>ProtocolChangedException</code> takes the following to be created:</p> <ul> <li> Error Message <p><code>ProtocolChangedException</code> is created\u00a0when:</p> <ul> <li><code>DeltaErrors</code> utility is used to protocolChangedException</li> </ul>"},{"location":"features/","title":"Features","text":"<p>The following is a list of the features of Delta Lake that make it so amazing (even awesomesauce \ud83d\ude0f):</p> <ul> <li>Change Data Feed</li> <li>CHECK Constraints</li> <li>Column Invariants</li> <li>Column Mapping</li> <li>Commands</li> <li>Data Skipping</li> <li>Delta SQL</li> <li>Developer API</li> <li>Generated Columns</li> <li>Spark SQL integration with support for batch and streaming queries</li> <li>Table Constraints</li> <li>Time Travel</li> </ul>"},{"location":"generated-columns/","title":"Generated Columns","text":"<p>Generated Columns are generation expressions that can produce column values at write time (unless provided by a query).</p> <p>Generated Columns can be defined using the following high-level operator:</p> <ul> <li>DeltaColumnBuilder.generatedAlwaysAs</li> </ul> <p>SQL Not Supported</p> <p>SQL support is not available yet and tracked as #1100.</p> <p>Generated Columns are stored in the table metadata.</p> <p>A column is a generated column only if the Minimum Writer Version is at least 4 and the column metadata contains generation expressions.</p> <p>Generated Columns is a new feature in Delta Lake 1.0.0.</p>"},{"location":"generated-columns/#generation-expression","title":"Generation Expression","text":"<p>Generation Expression is a SQL expression to generate values.</p> <p>Generation Expression is attached to a column using delta.generationExpression metadata key.</p>"},{"location":"generated-columns/#protocol","title":"Protocol <p>Generated columns require the Minimum Writer Version (of a delta table) to be at least 4.</p>","text":""},{"location":"generated-columns/#deltagenerationexpression","title":"delta.generationExpression <p>Generated Columns feature uses the <code>delta.generationExpression</code> key in the <code>Metadata</code> of a <code>StructField</code> (Spark SQL) to mark a column as generated.</p>","text":""},{"location":"generated-columns/#demo","title":"Demo <p>Generated Columns</p>","text":""},{"location":"generated-columns/GeneratedColumn/","title":"GeneratedColumn Utility","text":"<p><code>GeneratedColumn</code> is a utility for Generated Columns.</p> <pre><code>import org.apache.spark.sql.delta.GeneratedColumn\n</code></pre>"},{"location":"generated-columns/GeneratedColumn/#isgeneratedcolumn","title":"isGeneratedColumn <pre><code>isGeneratedColumn(\n  protocol: Protocol,\n  field: StructField): Boolean\nisGeneratedColumn(\n  field: StructField): Boolean\n</code></pre> <p><code>isGeneratedColumn</code> returns <code>true</code> when the following all hold:</p> <ol> <li>satisfyGeneratedColumnProtocol</li> <li>The metadata of the given <code>StructField</code> (Spark SQL) contains (a binding for) the delta.generationExpression key.</li> </ol> <p><code>isGeneratedColumn</code> is used when:</p> <ul> <li><code>ColumnWithDefaultExprUtils</code> utility is used to removeDefaultExpressions and columnHasDefaultExpr</li> <li><code>GeneratedColumn</code> utility is used to hasGeneratedColumns, getGeneratedColumns, enforcesGeneratedColumns and validateGeneratedColumns</li> </ul>","text":""},{"location":"generated-columns/GeneratedColumn/#getgeneratedcolumns","title":"getGeneratedColumns <pre><code>getGeneratedColumns(\n  snapshot: Snapshot): Seq[StructField]\n</code></pre> <p><code>getGeneratedColumns</code> satisfyGeneratedColumnProtocol (with the protocol of the given Snapshot) and returns generated columns (based on the schema of the Metadata of the given Snapshot).</p> <p><code>getGeneratedColumns</code>\u00a0is used when:</p> <ul> <li>PreprocessTableUpdate logical resolution rule is executed (and toCommand)</li> </ul>","text":""},{"location":"generated-columns/GeneratedColumn/#enforcesgeneratedcolumns","title":"enforcesGeneratedColumns <pre><code>enforcesGeneratedColumns(\n  protocol: Protocol,\n  metadata: Metadata): Boolean\n</code></pre> <p><code>enforcesGeneratedColumns</code> is <code>true</code> when the following all hold:</p> <ul> <li>satisfyGeneratedColumnProtocol with the given Protocol</li> <li>There is at least one generated column in the table schema (of the given Metadata)</li> </ul>  <p><code>enforcesGeneratedColumns</code>\u00a0is used when:</p> <ul> <li><code>TransactionalWrite</code> is requested to write data out (and normalizeData)</li> </ul>","text":""},{"location":"generated-columns/GeneratedColumn/#satisfygeneratedcolumnprotocol","title":"satisfyGeneratedColumnProtocol <pre><code>satisfyGeneratedColumnProtocol(\n  protocol: Protocol): Boolean\n</code></pre> <p><code>satisfyGeneratedColumnProtocol</code> is <code>true</code> when the minWriterVersion of the given Protocol is at least <code>4</code>.</p> <p><code>satisfyGeneratedColumnProtocol</code>\u00a0is used when:</p> <ul> <li><code>ColumnWithDefaultExprUtils</code> utility is used to satisfyProtocol</li> <li><code>GeneratedColumn</code> utility is used to isGeneratedColumn, getGeneratedColumns, enforcesGeneratedColumns and generatePartitionFilters</li> <li>AlterTableChangeColumnDeltaCommand is executed</li> <li><code>ImplicitMetadataOperation</code> is requested to mergeSchema</li> </ul>","text":""},{"location":"generated-columns/GeneratedColumn/#addgeneratedcolumnsorreturnconstraints","title":"addGeneratedColumnsOrReturnConstraints <pre><code>addGeneratedColumnsOrReturnConstraints(\n  deltaLog: DeltaLog,\n  queryExecution: QueryExecution,\n  schema: StructType,\n  df: DataFrame): (DataFrame, Seq[Constraint])\n</code></pre> <p><code>addGeneratedColumnsOrReturnConstraints</code> returns a <code>DataFrame</code> with generated columns (missing in the schema) and constraints for generated columns (existing in the schema).</p> <p><code>addGeneratedColumnsOrReturnConstraints</code> finds generated columns (among the top-level columns in the given schema (StructType)).</p> <p>For every generated column, <code>addGeneratedColumnsOrReturnConstraints</code> creates a Check constraint with the following:</p> <ul> <li><code>Generated Column</code> name</li> <li><code>EqualNullSafe</code> expression that compares the generated column expression with the value provided by the user</li> </ul> <p>In the end, <code>addGeneratedColumnsOrReturnConstraints</code> uses <code>select</code> operator on the given <code>DataFrame</code>.</p> <p><code>addGeneratedColumnsOrReturnConstraints</code>\u00a0is used when:</p> <ul> <li><code>TransactionalWrite</code> is requested to write data out (and normalizeData)</li> </ul>","text":""},{"location":"generated-columns/GeneratedColumn/#hasgeneratedcolumns","title":"hasGeneratedColumns <pre><code>hasGeneratedColumns(\n  schema: StructType): Boolean\n</code></pre> <p><code>hasGeneratedColumns</code> returns <code>true</code> if any of the top-level columns in the given <code>StructType</code> (Spark SQL) is a generated column.</p> <p><code>hasGeneratedColumns</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to verify a new metadata</li> <li><code>Protocol</code> is requested for the required minimum protocol</li> <li><code>SchemaUtils</code> utility is used to findDependentGeneratedColumns</li> </ul>","text":""},{"location":"generated-columns/GeneratedColumn/#validategeneratedcolumns","title":"validateGeneratedColumns <pre><code>validateGeneratedColumns(\n  spark: SparkSession,\n  schema: StructType): Unit\n</code></pre> <p><code>validateGeneratedColumns</code>...FIXME</p> <p><code>validateGeneratedColumns</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to verify a new metadata</li> </ul>","text":""},{"location":"limit-pushdown/","title":"LIMIT Pushdown","text":"<p>As part of Data Skipping, Delta Lake 2.2 supports LIMIT pushdown optimization to scan only the files up to the limit.</p> <p>Delta Lake uses file-level row counts to find the necessary number of data files for scanning.</p>"},{"location":"sql/","title":"Delta SQL","text":"<p>Delta Lake registers custom SQL statements (using DeltaSparkSessionExtension that injects DeltaSqlParser with DeltaSqlAstBuilder).</p> <p>The SQL statements support table identifiers of the format <code>delta.`path`</code> (with backticks), e.g. <code>delta.`/tmp/delta/t1`</code> while <code>path</code> is between single quotes, e.g. <code>'/tmp/delta/t1'</code>.</p> <p>The SQL statements can also refer to tables that are registered in a catalog (metastore).</p>"},{"location":"sql/#alter-table-add-constraint","title":"ALTER TABLE ADD CONSTRAINT <pre><code>ALTER TABLE table\nADD CONSTRAINT name\nCHECK (expr+)\n</code></pre> <p>Creates an AlterTableAddConstraint</p>","text":""},{"location":"sql/#alter-table-drop-constraint","title":"ALTER TABLE DROP CONSTRAINT <pre><code>ALTER TABLE table\nDROP CONSTRAINT (IF EXISTS)? name\n</code></pre> <p>Creates a AlterTableDropConstraint</p>","text":""},{"location":"sql/#convert-to-delta","title":"CONVERT TO DELTA <pre><code>CONVERT TO DELTA table\n  (PARTITIONED BY ( colTypeList ))?\n</code></pre> <p>Executes ConvertToDeltaCommand</p>","text":""},{"location":"sql/#describe-detail","title":"DESCRIBE DETAIL <pre><code>(DESC | DESCRIBE) DETAIL (path | table)\n</code></pre> <p>Executes DescribeDeltaDetailCommand</p>","text":""},{"location":"sql/#describe-history","title":"DESCRIBE HISTORY <pre><code>(DESC | DESCRIBE) HISTORY (path | table)\n  (LIMIT limit)?\n</code></pre> <p>Executes DescribeDeltaHistoryCommand</p>","text":""},{"location":"sql/#generate","title":"GENERATE <pre><code>GENERATE modeName FOR TABLE table\n</code></pre> <p>Executes DeltaGenerateCommand</p>","text":""},{"location":"sql/#optimize","title":"OPTIMIZE <pre><code>OPTIMIZE (path | table)\n  (WHERE partitionPredicate)?\n  (zorderSpec)?\n\nzorderSpec\n    : ZORDER BY '(' interleave (, interleave)* ')'\n    | ZORDER BY interleave (, interleave)*\n    ;\n</code></pre> <p>Executes OptimizeTableCommand on a delta table (identified by a directory <code>path</code> or a <code>table</code> name)</p> <p>Parsed by DeltaSqlAstBuilder that creates an OptimizeTableCommand</p>","text":""},{"location":"sql/#restore","title":"RESTORE <pre><code>RESTORE TABLE? table\nTO? temporalClause\n\ntemporalClause\n    : FOR? (SYSTEM_VERSION | VERSION) AS OF version\n    | FOR? (SYSTEM_TIME | TIMESTAMP) AS OF timestamp\n    ;\n</code></pre> <p>Creates a RestoreTableStatement</p>","text":""},{"location":"sql/#vacuum","title":"VACUUM <pre><code>VACUUM (path | table)\n  (RETAIN number HOURS)? (DRY RUN)?\n</code></pre> <p>Executes VacuumTableCommand</p>","text":""},{"location":"sql/DeltaSqlAstBuilder/","title":"DeltaSqlAstBuilder","text":"<p><code>DeltaSqlAstBuilder</code> is a command builder for the Delta SQL statements (described in DeltaSqlBase.g4 ANTLR grammar).</p> <p><code>DeltaSqlParser</code> is used by DeltaSqlParser.</p> SQL Statement Logical Command ALTER TABLE ADD CONSTRAINT AlterTableAddConstraint ALTER TABLE DROP CONSTRAINT AlterTableDropConstraint CONVERT TO DELTA ConvertToDeltaCommand DESCRIBE DETAIL DescribeDeltaDetailCommand DESCRIBE HISTORY DescribeDeltaHistoryCommand GENERATE DeltaGenerateCommand OPTIMIZE OptimizeTableCommand RESTORE RestoreTableStatement VACUUM VacuumTableCommand"},{"location":"sql/DeltaSqlAstBuilder/#maybetimetravelchild","title":"maybeTimeTravelChild <pre><code>maybeTimeTravelChild(\n  ctx: TemporalClauseContext,\n  child: LogicalPlan): LogicalPlan\n</code></pre> <p><code>maybeTimeTravelChild</code> creates a TimeTravel (with <code>sql</code> ID).</p> <p><code>maybeTimeTravelChild</code> is used when:</p> <ul> <li><code>DeltaSqlAstBuilder</code> is requested to parse RESTORE command</li> </ul>","text":""},{"location":"sql/DeltaSqlParser/","title":"DeltaSqlParser","text":"<p><code>DeltaSqlParser</code> is a SQL parser (Spark SQL) for Delta SQL.</p> <p><code>DeltaSqlParser</code> is registered in a Spark SQL application using DeltaSparkSessionExtension.</p>"},{"location":"sql/DeltaSqlParser/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaSqlParser</code> takes the following to be created:</p> <ul> <li> Delegate <code>ParserInterface</code> (Spark SQL) (to fall back to for unsupported SQLs) <p><code>DeltaSqlParser</code> is created when:</p> <ul> <li><code>DeltaSparkSessionExtension</code> is requested to register Delta SQL support</li> </ul>"},{"location":"sql/DeltaSqlParser/#deltasqlastbuilder","title":"DeltaSqlAstBuilder <p><code>DeltaSqlParser</code> uses DeltaSqlAstBuilder to convert SQL statements to their runtime representation (as a <code>LogicalPlan</code>).</p> <p>In case a SQL statement could not be parsed to a <code>LogicalPlan</code>, <code>DeltaSqlAstBuilder</code> requests the delegate ParserInterface to handle it.</p>","text":""},{"location":"storage/","title":"Storage","text":"<p>Delta Lake can now automatically load the correct LogStore needed for common storage systems hosting the Delta table being read or written to.</p> <p>LogStoreProvider uses DelegatingLogStore unless spark.delta.logStore.class configuration property is defined.</p> <p>The scheme of the Delta table path is used to dynamically load the necessary LogStore implementation. This also allows the same application to simultaneously read and write to Delta tables on different cloud storage systems.</p> <p>DelegatingLogStore allows for custom LogStores per URI scheme before using the default LogStores.</p> Schemes Default LogStore <code>s3</code>, <code>s3a</code>, <code>s3n</code> S3SingleDriverLogStore <code>abfs</code>, <code>abfss</code>, <code>adl</code>, <code>wasb</code>, <code>wasbs</code> <code>AzureLogStore</code> <p><code>DelegatingLogStore</code> uses HDFSLogStore as the default LogStore.</p>"},{"location":"storage/DelegatingLogStore/","title":"DelegatingLogStore","text":"<p><code>DelegatingLogStore</code> is the default LogStore.</p>"},{"location":"storage/DelegatingLogStore/#creating-instance","title":"Creating Instance","text":"<p><code>DelegatingLogStore</code> takes the following to be created:</p> <ul> <li> <code>Configuration</code> (Apache Hadoop) <p><code>DelegatingLogStore</code> is created\u00a0when:</p> <ul> <li><code>LogStore</code> utility is used to createLogStoreWithClassName</li> </ul>"},{"location":"storage/DelegatingLogStore/#default-logstore","title":"Default LogStore <pre><code>defaultLogStore: LogStore\n</code></pre> <p><code>DelegatingLogStore</code> creates a LogStore (lazily) that is used when requested to schemeBasedLogStore.</p>  Lazy Value <p><code>defaultLogStore</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p>","text":""},{"location":"storage/DelegatingLogStore/#logstore-by-scheme-lookup-table","title":"LogStore by Scheme Lookup Table <pre><code>schemeToLogStoreMap: Map[String, LogStore]\n</code></pre> <p><code>DelegatingLogStore</code> uses an internal registry of LogStores by scheme for looking them up once created.</p>","text":""},{"location":"storage/DelegatingLogStore/#looking-up-logstore-delegate-by-path","title":"Looking Up LogStore Delegate by Path <pre><code>getDelegate(\n  path: Path): LogStore\n</code></pre> <p><code>getDelegate</code> is a mere alias of schemeBasedLogStore.</p>","text":""},{"location":"storage/DelegatingLogStore/#schemebasedlogstore","title":"schemeBasedLogStore <pre><code>schemeBasedLogStore(\n  path: Path): LogStore\n</code></pre> <p><code>schemeBasedLogStore</code> takes the scheme component (of the URI) of the given path.</p> <p>If undefined, <code>schemeBasedLogStore</code> gives the defaultLogStore.</p> <p>For a scheme defined, <code>schemeBasedLogStore</code> looks it up in the schemeToLogStoreMap registry and returns it when found.</p> <p>Otherwise, <code>schemeBasedLogStore</code> creates a LogStore based on the following (in the order):</p> <ol> <li>Scheme-specific configuration key to look up the class name of the <code>LogStore</code> in the <code>SparkConf</code></li> <li>Default LogStore class name for the scheme</li> <li>Uses the defaultLogStore</li> </ol> <p><code>schemeBasedLogStore</code> registers the <code>LogStore</code> in the schemeToLogStoreMap registry for future lookups.</p> <p><code>schemeBasedLogStore</code> prints out the following INFO message to the logs:</p> <pre><code>LogStore [className] is used for scheme [scheme]\n</code></pre>","text":""},{"location":"storage/DelegatingLogStore/#default-logstore-class-name-for-scheme","title":"Default LogStore (Class Name) for Scheme <pre><code>getDefaultLogStoreClassName(\n  scheme: String): Option[String]\n</code></pre> <p><code>getDefaultLogStoreClassName</code> returns the class name of the LogStore for a given <code>scheme</code> or <code>None</code> (undefined).</p>    Schemes Class Name     <code>s3</code>, <code>s3a</code>, <code>s3n</code> S3SingleDriverLogStore   <code>abfs</code>, <code>abfss</code>, <code>adl</code>, <code>wasb</code>, <code>wasbs</code> <code>AzureLogStore</code>","text":""},{"location":"storage/DelegatingLogStore/#creating-logstore","title":"Creating LogStore <pre><code>createLogStore(\n  className: String): LogStore\n</code></pre> <p><code>createLogStore</code> creates a LogStore for the given class name.</p> <p><code>createLogStore</code>\u00a0is used when:</p> <ul> <li><code>DelegatingLogStore</code> is requested to schemeBasedLogStore and defaultLogStore</li> </ul>","text":""},{"location":"storage/HDFSLogStore/","title":"HDFSLogStore","text":"<p><code>HDFSLogStore</code> is a HadoopFileSystemLogStore.</p> <p><code>HDFSLogStore</code> is the default LogStore of DelegatingLogStore.</p>"},{"location":"storage/HDFSLogStore/#creating-instance","title":"Creating Instance","text":"<p><code>HDFSLogStore</code> takes the following to be created:</p> <ul> <li> <code>SparkConf</code> (Apache Spark) <li> Hadoop Configuration <p><code>HDFSLogStore</code> is created\u00a0when:</p> <ul> <li><code>DelegatingLogStore</code> is requested for the default LogStore</li> </ul>"},{"location":"storage/HadoopFileSystemLogStore/","title":"HadoopFileSystemLogStore","text":"<p><code>HadoopFileSystemLogStore</code>\u00a0is an extension of the LogStore abstraction for Hadoop DFS-based log stores.</p>"},{"location":"storage/HadoopFileSystemLogStore/#implementations","title":"Implementations","text":"<ul> <li>AzureLogStore</li> <li>HDFSLogStore</li> <li>LocalLogStore</li> <li>S3SingleDriverLogStore</li> </ul>"},{"location":"storage/HadoopFileSystemLogStore/#creating-instance","title":"Creating Instance","text":"<p><code>HadoopFileSystemLogStore</code> takes the following to be created:</p> <ul> <li> <code>SparkConf</code> (Apache Spark) <li> Hadoop Configuration Abstract Class <p><code>HadoopFileSystemLogStore</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete HadoopFileSystemLogStores.</p>"},{"location":"storage/LogStore/","title":"LogStore","text":"<p><code>LogStore</code> is an abstraction of transaction log stores (to read and write physical log files and checkpoints).</p> <p><code>LogStore</code> is created using LogStoreProvider based on spark.delta.logStore.class configuration property.</p>"},{"location":"storage/LogStore/#contract","title":"Contract","text":""},{"location":"storage/LogStore/#invalidatecache","title":"invalidateCache <pre><code>invalidateCache(): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>DelegatingLogStore</code> is requested to invalidateCache</li> </ul>","text":""},{"location":"storage/LogStore/#ispartialwritevisible","title":"isPartialWriteVisible <pre><code>isPartialWriteVisible(\n  path: Path): Boolean // (1)!\nisPartialWriteVisible(\n  path: Path,\n  hadoopConf: Configuration): Boolean\n</code></pre> <ol> <li>deprecated</li> </ol> <p>Whether a partial write is visible when writing to <code>path</code></p> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>Checkpoints</code> is requested to writeCheckpoint</li> <li><code>OptimisticTransactionImpl</code> is requested to isCommitLockEnabled</li> <li><code>DelegatingLogStore</code> is requested to isPartialWriteVisible</li> </ul>","text":""},{"location":"storage/LogStore/#listfrom","title":"listFrom <pre><code>listFrom(\n  path: Path): Iterator[FileStatus]\nlistFrom(\n  path: String): Iterator[FileStatus]  \n</code></pre> <p>Used when:</p> <ul> <li><code>Checkpoints</code> is requested to findLastCompleteCheckpoint</li> <li><code>DeltaHistoryManager</code> is requested to getEarliestDeltaFile, getEarliestReproducibleCommit and getCommits</li> <li><code>DeltaLog</code> is requested to getChanges</li> <li><code>MetadataCleanup</code> is requested to listExpiredDeltaLogs</li> <li><code>SnapshotManagement</code> is requested to listFrom</li> <li><code>DelegatingLogStore</code> is requested to listFrom</li> <li><code>DeltaFileOperations</code> utility is used to listUsingLogStore</li> </ul>","text":""},{"location":"storage/LogStore/#read","title":"read <pre><code>read(\n  path: Path): Seq[String]\nread(\n  path: String): Seq[String]\n</code></pre> <p>Used when:</p> <ul> <li><code>Checkpoints</code> is requested to loadMetadataFromFile</li> <li><code>ReadChecksum</code> is requested to readChecksum</li> <li><code>DeltaLog</code> is requested to getChanges</li> <li><code>OptimisticTransactionImpl</code> is requested to checkForConflicts</li> <li><code>DelegatingLogStore</code> is requested to read</li> <li><code>LogStore</code> is requested to readAsIterator</li> </ul>","text":""},{"location":"storage/LogStore/#write","title":"write <pre><code>write(\n  path: Path,\n  actions: Iterator[String],\n  overwrite: Boolean = false): Unit\nwrite(\n  path: String,\n  actions: Iterator[String]): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>Checkpoints</code> is requested to checkpoint</li> <li><code>OptimisticTransactionImpl</code> is requested to doCommit</li> <li><code>DeltaCommand</code> is requested to commitLarge</li> <li><code>GenerateSymlinkManifestImpl</code> is requested to writeManifestFiles</li> <li><code>DelegatingLogStore</code> is requested to write</li> </ul>","text":""},{"location":"storage/LogStore/#implementations","title":"Implementations","text":"<ul> <li>DelegatingLogStore</li> <li>HadoopFileSystemLogStore</li> <li>LogStoreAdaptor</li> </ul>"},{"location":"storage/LogStore/#creating-logstore","title":"Creating LogStore <pre><code>apply(\n  sc: SparkContext): LogStore\napply(\n  sparkConf: SparkConf,\n  hadoopConf: Configuration): LogStore\n</code></pre> <p><code>apply</code> creates a LogStore.</p> <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>GenerateSymlinkManifestImpl</code> is requested to writeManifestFiles and writeSingleManifestFile</li> <li><code>DeltaHistoryManager</code> is requested to getHistory and getActiveCommitAtTime</li> <li><code>DeltaFileOperations</code> is requested to recursiveListDirs, localListDirs, and localListFrom</li> </ul>","text":""},{"location":"storage/LogStore/#createlogstorewithclassname","title":"createLogStoreWithClassName <pre><code>createLogStoreWithClassName(\n  className: String,\n  sparkConf: SparkConf,\n  hadoopConf: Configuration): LogStore\n</code></pre> <p><code>createLogStoreWithClassName</code> branches off based on the given <code>className</code>.</p> <p><code>createLogStoreWithClassName</code> creates a DelegatingLogStore when the <code>className</code> is the fully-qualified class name of <code>DelegatingLogStore</code>.</p> <p>Otherwise, <code>createLogStoreWithClassName</code> loads the class and braches off based on the class type.</p> <p>For io.delta.storage.LogStore, <code>createLogStoreWithClassName</code> creates an instance thereof (with the given <code>Configuration</code>) and wraps it up in a LogStoreAdaptor.</p> <p>For all other cases, <code>createLogStoreWithClassName</code> creates an instance thereof (with the given <code>SparkConf</code> and <code>Configuration</code>).</p> <p><code>createLogStoreWithClassName</code>\u00a0is used when:</p> <ul> <li><code>DelegatingLogStore</code> is requested to createLogStore</li> <li><code>LogStoreProvider</code> is requested to createLogStore</li> </ul>","text":""},{"location":"storage/LogStore/#logstoreschemeconfkey","title":"logStoreSchemeConfKey <pre><code>logStoreSchemeConfKey(\n  scheme: String): String\n</code></pre> <p><code>logStoreSchemeConfKey</code> simply returns the following text for the given <code>scheme</code>:</p> <pre><code>spark.delta.logStore.[scheme].impl\n</code></pre> <p><code>logStoreSchemeConfKey</code>\u00a0is used when:</p> <ul> <li><code>DelegatingLogStore</code> is requested to schemeBasedLogStore</li> </ul>","text":""},{"location":"storage/LogStoreAdaptor/","title":"LogStoreAdaptor","text":"<p><code>LogStoreAdaptor</code> is a LogStore.</p>"},{"location":"storage/LogStoreAdaptor/#creating-instance","title":"Creating Instance","text":"<p><code>LogStoreAdaptor</code> takes the following to be created:</p> <ul> <li> io.delta.storage.LogStore <p><code>LogStoreAdaptor</code> is created\u00a0when:</p> <ul> <li><code>LogStore</code> utility is used to createLogStoreWithClassName</li> </ul>"},{"location":"storage/LogStoreProvider/","title":"LogStoreProvider","text":"<p><code>LogStoreProvider</code> is an abstraction of providers of LogStores.</p>"},{"location":"storage/LogStoreProvider/#sparkdeltalogstoreclass","title":"spark.delta.logStore.class <p><code>LogStoreProvider</code> uses the spark.delta.logStore.class configuration property for the LogStore to create (for a DeltaLog, a DeltaHistoryManager, and DeltaFileOperations).</p>","text":""},{"location":"storage/LogStoreProvider/#creating-logstore","title":"Creating LogStore <pre><code>createLogStore(\n  spark: SparkSession): LogStore\ncreateLogStore(\n  sparkConf: SparkConf,\n  hadoopConf: Configuration): LogStore\n</code></pre> <p><code>createLogStore</code> creates a LogStore based on spark.delta.logStore.class configuration property (if defined) or defaults to DelegatingLogStore.</p> <p><code>createLogStore</code> is used when:</p> <ul> <li><code>DeltaLog</code> is requested for the LogStore</li> <li>LogStore.apply utility is used</li> </ul>","text":""},{"location":"storage/S3SingleDriverLogStore/","title":"S3SingleDriverLogStore","text":"<p><code>S3SingleDriverLogStore</code> is a HadoopFileSystemLogStore.</p>"},{"location":"storage/S3SingleDriverLogStore/#creating-instance","title":"Creating Instance","text":"<p><code>S3SingleDriverLogStore</code> takes the following to be created:</p> <ul> <li> <code>SparkConf</code> (Apache Spark) <li> Hadoop Configuration"},{"location":"time-travel/","title":"Time Travel","text":"<p>Delta Lake supports time travelling which is loading a Delta table at a given version or timestamp (defined by path, versionAsOf or timestampAsOf options).</p> <p>Delta Lake allows <code>path</code> option to include time travel patterns (<code>@v123</code> and <code>@yyyyMMddHHmmssSSS</code>) unless the internal spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled configuration property is turned off.</p> <p>Time Travel cannot be specified for catalog delta tables.</p> <p>Time travel is described using DeltaTimeTravelSpec.</p> <p>Demo: Time Travel</p>"},{"location":"time-travel/DeltaTimeTravelSpec/","title":"DeltaTimeTravelSpec","text":""},{"location":"time-travel/DeltaTimeTravelSpec/#creating-instance","title":"Creating Instance","text":"<p><code>DeltaTimeTravelSpec</code> takes the following to be created:</p> <ul> <li>Timestamp</li> <li>Version</li> <li>Creation Source ID</li> </ul> <p><code>DeltaTimeTravelSpec</code> asserts that either version or timestamp is provided (and throws an <code>AssertionError</code>).</p> <p><code>DeltaTimeTravelSpec</code> is created\u00a0when:</p> <ul> <li>DeltaAnalysis logical resolution rule is resolving RestoreTableStatement unary logical operator</li> <li><code>DeltaTimeTravelSpec</code> utility is used to resolve a path</li> <li><code>DeltaDataSource</code> utility is used to getTimeTravelVersion</li> <li><code>DeltaSource</code> is requested to for the getStartingVersion</li> </ul>"},{"location":"time-travel/DeltaTimeTravelSpec/#version","title":"Version <pre><code>version: Option[Long]\n</code></pre> <p><code>DeltaTimeTravelSpec</code> can be given a version when created:</p> <ul> <li>DeltaAnalysis logical resolution rule is executed (to resolve a RestoreTableStatement)</li> <li><code>DeltaTimeTravelSpec</code> utility is used to resolve path</li> <li><code>DeltaDataSource</code> utility is used to getTimeTravelVersion</li> </ul> <p><code>version</code> is mutually exclusive with the timestamp (so only one can be specified).</p>","text":""},{"location":"time-travel/DeltaTimeTravelSpec/#creation-source-id","title":"Creation Source ID <p><code>DeltaTimeTravelSpec</code> is given a Creation Source ID when created.</p> <p>The Creation Source ID indicates the API used to time travel:</p> <ul> <li>Creation Source ID of the TimeTravel of RestoreTableStatement unary logical operator (when DeltaAnalysis logical resolution rule is executed)</li> <li><code>atSyntax.path</code> for DeltaTimeTravelSpec</li> <li><code>dfReader</code> for DeltaDataSource</li> <li><code>deltaSource</code> for DeltaSource</li> </ul>","text":""},{"location":"time-travel/DeltaTimeTravelSpec/#time-travel-patterns","title":"Time Travel Patterns <p><code>DeltaTimeTravelSpec</code> defines regular expressions for timestamp- and version-based time travel identifiers:</p> <ul> <li>Version URI: <code>(path)@[vV](some numbers)</code></li> <li>Timestamp URI: <code>(path)@(yyyyMMddHHmmssSSS)</code></li> </ul>","text":""},{"location":"time-travel/DeltaTimeTravelSpec/#resolving-path","title":"Resolving Path <pre><code>resolvePath(\n  conf: SQLConf,\n  identifier: String): (DeltaTimeTravelSpec, String)\n</code></pre> <p><code>resolvePath</code>...FIXME</p> <p><code>resolvePath</code>\u00a0is used when <code>DeltaTableUtils</code> utility is used to extractIfPathContainsTimeTravel.</p>","text":""},{"location":"time-travel/DeltaTimeTravelSpec/#gettimestamp","title":"getTimestamp <pre><code>getTimestamp(\n  timeZone: String): Timestamp\n</code></pre> <p><code>getTimestamp</code>...FIXME</p> <p><code>getTimestamp</code>\u00a0is used when <code>DeltaTableUtils</code> utility is used to resolveTimeTravelVersion.</p>","text":""},{"location":"time-travel/DeltaTimeTravelSpec/#isapplicable","title":"isApplicable <pre><code>isApplicable(\n  conf: SQLConf,\n  identifier: String): Boolean\n</code></pre> <p><code>isApplicable</code> is <code>true</code> when all of the following hold:</p> <ul> <li>spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled is <code>true</code></li> <li>identifierContainsTimeTravel is <code>true</code></li> </ul> <p><code>isApplicable</code>\u00a0is used when <code>DeltaTableUtils</code> utility is used to extractIfPathContainsTimeTravel.</p>","text":""},{"location":"time-travel/DeltaTimeTravelSpec/#identifiercontainstimetravel","title":"identifierContainsTimeTravel <pre><code>identifierContainsTimeTravel(\n  identifier: String): Boolean\n</code></pre> <p><code>identifierContainsTimeTravel</code> is <code>true</code> when the given <code>identifier</code> is either timestamp or version time travel pattern.</p>","text":""},{"location":"developer-api/","title":"Developer API","text":""},{"location":"developer-api/#developerapi","title":"DeveloperApi","text":"<ul> <li>DeltaColumnBuilder</li> <li>DeltaTable</li> <li>DeltaTableBuilder</li> </ul>"}]}