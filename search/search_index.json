{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"<p>Welcome to The Internals of Delta Lake online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, an IT freelancer specializing in Apache Spark, Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB, mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring the internals of Delta Lake as much as I have.</p>  <p>Flannery O'Connor</p> <p>I write to discover what I know.</p>   \"The Internals Of\" series <p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p>  <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a deep dive into Delta Lake \ud83d\udd25</p>  <p>Last update: 2022-04-27</p>","title":"The Internals of Delta Lake 1.2.0"},{"location":"Action/","text":"<p><code>Action</code> is an abstraction of operations that change (the state of) a delta table.</p>","title":"Action"},{"location":"Action/#contract","text":"","title":"Contract"},{"location":"Action/#serializing-to-json","text":"","title":"Serializing to JSON <pre><code>json: String\n</code></pre> <p>Serializes (converts) the (wrapped) action to JSON format</p> <p><code>json</code> uses Jackson library (with jackson-module-scala) as the JSON processor.</p> <p>Used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to doCommit</li> <li><code>DeltaCommand</code> is requested to commitLarge</li> </ul>"},{"location":"Action/#wrapping-up-as-singleaction","text":"","title":"Wrapping Up as SingleAction <pre><code>wrap: SingleAction\n</code></pre> <p>Wraps the action into a SingleAction for serialization</p> <p>Used when:</p> <ul> <li><code>Snapshot</code> is requested to stateReconstruction</li> <li><code>Action</code> is requested to serialize to JSON format</li> </ul>"},{"location":"Action/#implementations","text":"<ul> <li>CommitInfo</li> <li>FileAction</li> <li>Metadata</li> <li>Protocol</li> <li>SetTransaction</li> </ul>  Sealed Trait <p><code>Action</code> is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).</p>","title":"Implementations"},{"location":"Action/#log-schema","text":"","title":"Log Schema <pre><code>logSchema: StructType\n</code></pre> <p><code>logSchema</code> is the schema (Spark SQL) of SingleActions for <code>Snapshot</code> to convert a DeltaLogFileIndex to a LogicalRelation and emptyActions.</p> <pre><code>import org.apache.spark.sql.delta.actions.Action.logSchema\nlogSchema.printTreeString\n</code></pre> <pre><code>root\n |-- txn: struct (nullable = true)\n |    |-- appId: string (nullable = true)\n |    |-- version: long (nullable = false)\n |    |-- lastUpdated: long (nullable = true)\n |-- add: struct (nullable = true)\n |    |-- path: string (nullable = true)\n |    |-- partitionValues: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |    |-- size: long (nullable = false)\n |    |-- modificationTime: long (nullable = false)\n |    |-- dataChange: boolean (nullable = false)\n |    |-- stats: string (nullable = true)\n |    |-- tags: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |-- remove: struct (nullable = true)\n |    |-- path: string (nullable = true)\n |    |-- deletionTimestamp: long (nullable = true)\n |    |-- dataChange: boolean (nullable = false)\n |    |-- extendedFileMetadata: boolean (nullable = false)\n |    |-- partitionValues: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |    |-- size: long (nullable = false)\n |    |-- tags: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |-- metaData: struct (nullable = true)\n |    |-- id: string (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- description: string (nullable = true)\n |    |-- format: struct (nullable = true)\n |    |    |-- provider: string (nullable = true)\n |    |    |-- options: map (nullable = true)\n |    |    |    |-- key: string\n |    |    |    |-- value: string (valueContainsNull = true)\n |    |-- schemaString: string (nullable = true)\n |    |-- partitionColumns: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- configuration: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |    |-- createdTime: long (nullable = true)\n |-- protocol: struct (nullable = true)\n |    |-- minReaderVersion: integer (nullable = false)\n |    |-- minWriterVersion: integer (nullable = false)\n |-- cdc: struct (nullable = true)\n |    |-- path: string (nullable = true)\n |    |-- partitionValues: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |    |-- size: long (nullable = false)\n |    |-- tags: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |-- commitInfo: struct (nullable = true)\n |    |-- version: long (nullable = true)\n |    |-- timestamp: timestamp (nullable = true)\n |    |-- userId: string (nullable = true)\n |    |-- userName: string (nullable = true)\n |    |-- operation: string (nullable = true)\n |    |-- operationParameters: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |    |-- job: struct (nullable = true)\n |    |    |-- jobId: string (nullable = true)\n |    |    |-- jobName: string (nullable = true)\n |    |    |-- runId: string (nullable = true)\n |    |    |-- jobOwnerId: string (nullable = true)\n |    |    |-- triggerType: string (nullable = true)\n |    |-- notebook: struct (nullable = true)\n |    |    |-- notebookId: string (nullable = true)\n |    |-- clusterId: string (nullable = true)\n |    |-- readVersion: long (nullable = true)\n |    |-- isolationLevel: string (nullable = true)\n |    |-- isBlindAppend: boolean (nullable = true)\n |    |-- operationMetrics: map (nullable = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |    |-- userMetadata: string (nullable = true)\n</code></pre>"},{"location":"Action/#deserializing-action-from-json","text":"","title":"Deserializing Action (from JSON) <pre><code>fromJson(\n  json: String): Action\n</code></pre> <p><code>fromJson</code> utility...FIXME</p> <p><code>fromJson</code>\u00a0is used when:</p> <ul> <li><code>DeltaHistoryManager</code> is requested for CommitInfo of the given delta file</li> <li><code>DeltaLog</code> is requested for the changes of the given delta version and later</li> <li><code>OptimisticTransactionImpl</code> is requested to checkForConflicts</li> <li><code>DeltaCommand</code> is requested to commitLarge</li> </ul>"},{"location":"ActiveOptimisticTransactionRule/","text":"<p><code>ActiveOptimisticTransactionRule</code> is a logical optimization rule (Spark SQL).</p>","title":"ActiveOptimisticTransactionRule Logical Optimization Rule"},{"location":"ActiveOptimisticTransactionRule/#creating-instance","text":"<p><code>ActiveOptimisticTransactionRule</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL)  <p><code>ActiveOptimisticTransactionRule</code> is created\u00a0when:</p> <ul> <li><code>DeltaSparkSessionExtension</code> is requested to inject extensions</li> </ul>","title":"Creating Instance"},{"location":"ActiveOptimisticTransactionRule/#executing-rule","text":"","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>\u00a0is part of the <code>Rule</code> (Spark SQL) abstraction.</p> <p><code>apply</code>...FIXME</p>"},{"location":"AddCDCFile/","text":"<p><code>AddCDCFile</code> is a FileAction.</p>","title":"AddCDCFile"},{"location":"AddCDCFile/#creating-instance","text":"<p><code>AddCDCFile</code> takes the following to be created:</p> <ul> <li> Path <li> Partition values (<code>Map[String, String]</code>) <li> Size (in bytes) <li> Tags (default: <code>null</code>)  <p><code>AddCDCFile</code> does not seem to be created\u00a0ever.</p>","title":"Creating Instance"},{"location":"AddCDCFile/#datachange","text":"","title":"dataChange <pre><code>dataChange: Boolean\n</code></pre> <p><code>dataChange</code>\u00a0is part of the FileAction abstraction.</p> <p><code>dataChange</code>\u00a0is always turned off (<code>false</code>).</p>"},{"location":"AddCDCFile/#converting-to-singleaction","text":"","title":"Converting to SingleAction <pre><code>wrap: SingleAction\n</code></pre> <p><code>wrap</code>\u00a0is part of the Action abstraction.</p> <p><code>wrap</code> creates a new SingleAction with the <code>cdc</code> field set to this <code>AddCDCFile</code>.</p>"},{"location":"AddFile/","text":"<p><code>AddFile</code> is a FileAction that represents an action of adding a file to a delta table.</p>","title":"AddFile"},{"location":"AddFile/#creating-instance","text":"<p><code>AddFile</code> takes the following to be created:</p> <ul> <li> Path <li> Partition values (<code>Map[String, String]</code>) <li> Size (in bytes) <li> Modification time <li> <code>dataChange</code> flag <li> Stats (default: <code>null</code>) <li> Tags (<code>Map[String, String]</code>) (default: <code>null</code>)  <p><code>AddFile</code> is created\u00a0when:</p> <ul> <li> <p>ConvertToDeltaCommand is executed (for every data file to import)</p> </li> <li> <p><code>DelayedCommitProtocol</code> is requested to commit a task (after successful write) (for optimistic transactional writers)</p> </li> </ul>","title":"Creating Instance"},{"location":"AddFile/#converting-to-singleaction","text":"","title":"Converting to SingleAction <pre><code>wrap: SingleAction\n</code></pre> <p><code>wrap</code>\u00a0is part of the Action abstraction.</p> <p><code>wrap</code> creates a new SingleAction with the <code>add</code> field set to this <code>AddFile</code>.</p>"},{"location":"AddFile/#converting-to-removefile-with-defaults","text":"","title":"Converting to RemoveFile with Defaults <pre><code>remove: RemoveFile\n</code></pre> <p><code>remove</code> creates a RemoveFile for the path (with the current time and <code>dataChange</code> flag enabled).</p> <p><code>remove</code> is used when:</p> <ul> <li>MergeIntoCommand is executed</li> <li><code>WriteIntoDelta</code> is requested to write (with <code>Overwrite</code> mode)</li> <li><code>DeltaSink</code> is requested to add a streaming micro-batch (with <code>Complete</code> output mode)</li> </ul>"},{"location":"AddFile/#converting-to-removefile","text":"","title":"Converting to RemoveFile <pre><code>removeWithTimestamp(\n  timestamp: Long = System.currentTimeMillis(),\n  dataChange: Boolean = true): RemoveFile\n</code></pre> <p><code>remove</code> creates a new RemoveFile action for the path with the given <code>timestamp</code> and <code>dataChange</code> flag.</p> <p><code>removeWithTimestamp</code> is used when:</p> <ul> <li><code>AddFile</code> is requested to create a RemoveFile action with the defaults</li> <li>CreateDeltaTableCommand, DeleteCommand and UpdateCommand commands are executed</li> <li><code>DeltaCommand</code> is requested to removeFilesFromPaths</li> </ul>"},{"location":"AdmissionLimits/","text":"<p><code>AdmissionLimits</code> is used by DeltaSource to control how much data should be processed by a single micro-batch.</p>","title":"AdmissionLimits"},{"location":"AdmissionLimits/#creating-instance","text":"<p><code>AdmissionLimits</code> takes the following to be created:</p> <ul> <li> Maximum Number of Files (based on maxFilesPerTrigger option) <li> Maximum Bytes (based on maxBytesPerTrigger option)  <p><code>AdmissionLimits</code> is created\u00a0when:</p> <ul> <li><code>DeltaSource</code> is requested to getChangesWithRateLimit, getStartingOffset, getDefaultReadLimit</li> </ul>","title":"Creating Instance"},{"location":"AdmissionLimits/#converting-readlimit-to-admissionlimits","text":"","title":"Converting ReadLimit to AdmissionLimits <pre><code>apply(\n  limit: ReadLimit): Option[AdmissionLimits]\n</code></pre> <p><code>apply</code> creates an <code>AdmissionLimits</code> for the given <code>ReadLimit</code> (Spark Structured Streaming).</p>    ReadLimit AdmissionLimits     <code>ReadAllAvailable</code> <code>None</code>   <code>ReadMaxFiles</code> Maximum Number of Files   <code>ReadMaxBytes</code> Maximum Bytes   <code>CompositeLimit</code> Maximum Number of Files and Maximum Bytes    <p><code>apply</code> throws an <code>UnsupportedOperationException</code> for unknown <code>ReadLimit</code>s:</p> <pre><code>Unknown ReadLimit: [limit]\n</code></pre> <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>DeltaSource</code> is requested for the latest available offset</li> </ul>"},{"location":"AdmissionLimits/#admitting-addfile","text":"","title":"Admitting AddFile <pre><code>admit(\n  add: Option[AddFile]): Boolean\n</code></pre> <p><code>admit</code>...FIXME</p> <p><code>admit</code>\u00a0is used when:</p> <ul> <li><code>DeltaSource</code> is requested to getChangesWithRateLimit</li> </ul>"},{"location":"CachedDS/","text":"<p><code>CachedDS</code> is used when StateCache is requested to cacheDS.</p> <p> When created, <code>CachedDS</code> immediately initializes the <code>cachedDs</code> internal registry that requests the Dataset to generate a <code>RDD[InternalRow]</code> and associates the RDD with the given name: <ul> <li>Delta Table State for Snapshot</li> <li>Delta Source Snapshot for DeltaSourceSnapshot</li> </ul> <p>The RDD is marked to be persisted using <code>StorageLevel.MEMORY_AND_DISK_SER</code> storage level.</p>  <p>Note</p> <p><code>CachedDS</code> is an internal class of <code>StateCache</code> and has access to its internals.</p>","title":"CachedDS \u2014 Cached Delta State"},{"location":"CachedDS/#creating-instance","text":"<p><code>CachedDS</code> takes the following to be created:</p> <ul> <li> <code>Dataset[A]</code> <li> Name  <p><code>CachedDS</code> is created when <code>StateCache</code> is requested to cacheDS.</p>","title":"Creating Instance"},{"location":"CachedDS/#getds-method","text":"","title":"getDS Method <pre><code>getDS: Dataset[A]\n</code></pre> <p><code>getDS</code>...FIXME</p> <p><code>getDS</code> is used when:</p> <ul> <li><code>Snapshot</code> is requested to state</li> <li><code>DeltaSourceSnapshot</code> is requested to initialFiles</li> </ul>"},{"location":"Checkpoints/","text":"<p><code>Checkpoints</code> is an abstraction of DeltaLogs that can checkpoint the current state of a delta table.</p> <p> <code>Checkpoints</code> requires to be used with DeltaLog (or subtypes) only.","title":"Checkpoints"},{"location":"Checkpoints/#contract","text":"","title":"Contract"},{"location":"Checkpoints/#datapath","text":"","title":"dataPath <pre><code>dataPath: Path\n</code></pre> <p>Hadoop Path to the data directory of the delta table</p>"},{"location":"Checkpoints/#dologcleanup","text":"","title":"doLogCleanup <pre><code>doLogCleanup(): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>Checkpoints</code> is requested to checkpoint</li> </ul>"},{"location":"Checkpoints/#logpath","text":"","title":"logPath <pre><code>logPath: Path\n</code></pre> <p>Hadoop Path to the log directory of the delta table</p>"},{"location":"Checkpoints/#metadata","text":"","title":"Metadata <pre><code>metadata: Metadata\n</code></pre> <p>Metadata of the delta table</p>"},{"location":"Checkpoints/#snapshot","text":"","title":"snapshot <pre><code>snapshot: Snapshot\n</code></pre> <p>Snapshot of the delta table</p>"},{"location":"Checkpoints/#store","text":"","title":"store <pre><code>store: LogStore\n</code></pre> <p>LogStore</p>"},{"location":"Checkpoints/#implementations","text":"<ul> <li>DeltaLog</li> </ul>","title":"Implementations"},{"location":"Checkpoints/#_last_checkpoint-metadata-file","text":"","title":"_last_checkpoint Metadata File <p><code>Checkpoints</code> uses _last_checkpoint metadata file (under the log path) for the following:</p> <ul> <li> <p>Writing checkpoint metadata out</p> </li> <li> <p>Loading checkpoint metadata in</p> </li> </ul>"},{"location":"Checkpoints/#checkpointing","text":"","title":"Checkpointing <pre><code>checkpoint(): Unit\ncheckpoint(\n  snapshotToCheckpoint: Snapshot): CheckpointMetaData\n</code></pre> <p><code>checkpoint</code> writes a checkpoint of the current state of the delta table (Snapshot). That produces a checkpoint metadata with the version, the number of actions and possibly parts (for multi-part checkpoints).</p> <p><code>checkpoint</code> requests the LogStore to overwrite the _last_checkpoint file with the JSON-encoded checkpoint metadata.</p> <p>In the end, <code>checkpoint</code> cleans up the expired logs (if enabled).</p> <p><code>checkpoint</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to postCommit (based on checkpoint interval table property)</li> <li>ConvertToDelta command is executed (that in the end requests <code>DeltaCommand</code> to updateAndCheckpoint)</li> </ul>"},{"location":"Checkpoints/#writing-out-state-checkpoint","text":"","title":"Writing Out State Checkpoint <pre><code>writeCheckpoint(\n  spark: SparkSession,\n  deltaLog: DeltaLog,\n  snapshot: Snapshot): CheckpointMetaData\n</code></pre> <p><code>writeCheckpoint</code>...FIXME</p>"},{"location":"Checkpoints/#loading-latest-checkpoint-metadata","text":"","title":"Loading Latest Checkpoint Metadata <pre><code>lastCheckpoint: Option[CheckpointMetaData]\n</code></pre> <p><code>lastCheckpoint</code> loadMetadataFromFile (allowing for 3 retries).</p> <p><code>lastCheckpoint</code> is used when:</p> <ul> <li><code>SnapshotManagement</code> is requested to load the latest snapshot</li> <li><code>MetadataCleanup</code> is requested to listExpiredDeltaLogs</li> </ul>"},{"location":"Checkpoints/#loadmetadatafromfile","text":"","title":"loadMetadataFromFile <pre><code>loadMetadataFromFile(\n  tries: Int): Option[CheckpointMetaData]\n</code></pre> <p><code>loadMetadataFromFile</code> loads the _last_checkpoint file (in JSON format) and converts it to <code>CheckpointMetaData</code> (with a version, size and parts).</p> <p><code>loadMetadataFromFile</code> uses the LogStore to read the _last_checkpoint file.</p> <p>In case the _last_checkpoint file is corrupted, <code>loadMetadataFromFile</code>...FIXME</p>"},{"location":"CommitInfo/","text":"<p><code>CommitInfo</code> is an Action defined by the following properties:</p> <ul> <li> Version (optional) <li> Timestamp <li> User ID (optional) <li> User Name (optional) <li> Operation <li> Operation Parameters <li> JobInfo (optional) <li> NotebookInfo (optional) <li> Cluster ID (optional) <li> Read Version (optional) <li> Isolation Level (optional) <li>isBlindAppend flag (optional)</li> <li> Operation Metrics (optional) <li> User metadata (optional) <li> Tags <li>engineInfo</li> <li> Transaction ID  <p><code>CommitInfo</code> is created (using apply and empty utilities) when:</p> <ul> <li><code>DeltaHistoryManager</code> is requested for version and commit history (for DeltaTable.history operator and DESCRIBE HISTORY SQL command)</li> <li><code>OptimisticTransactionImpl</code> is requested to commit (with spark.databricks.delta.commitInfo.enabled configuration property enabled)</li> <li><code>DeltaCommand</code> is requested to commitLarge (for ConvertToDeltaCommand command and <code>FileAlreadyExistsException</code> was thrown)</li> </ul> <p><code>CommitInfo</code> is used as a part of OptimisticTransactionImpl and <code>CommitStats</code>.</p>","title":"CommitInfo"},{"location":"CommitInfo/#engineinfo","text":"","title":"engineInfo <p><code>CommitInfo</code> can be given extra <code>engineInfo</code> identifier (when created) for the engine that made the commit.</p> <p>This <code>engineInfo</code> is by default getEngineInfo.</p>"},{"location":"CommitInfo/#getengineinfo","text":"","title":"getEngineInfo <pre><code>getEngineInfo: Option[String]\n</code></pre> <p><code>getEngineInfo</code> is the following text:</p> <pre><code>Apache-Spark/[SPARK_VERSION] Delta-Lake/[VERSION]\n</code></pre>"},{"location":"CommitInfo/#blind-append","text":"","title":"Blind Append <p><code>CommitInfo</code> is given <code>isBlindAppend</code> flag (when created) to indicate whether a commit has blindly appended data without caring about existing files.</p> <p><code>isBlindAppend</code> flag is used while checking for logical conflicts with concurrent updates (at commit).</p> <p><code>isBlindAppend</code> flag is always <code>false</code> when <code>DeltaCommand</code> is requested to commitLarge.</p>"},{"location":"CommitInfo/#deltahistorymanager","text":"","title":"DeltaHistoryManager <p><code>CommitInfo</code> can be looked up using DeltaHistoryManager for the following:</p> <ul> <li>DESCRIBE HISTORY SQL command</li> <li>DeltaTable.history operation</li> </ul>"},{"location":"CommitInfo/#sparkdatabricksdeltacommitinfoenabled","text":"","title":"spark.databricks.delta.commitInfo.enabled <p><code>CommitInfo</code> is added (logged) to a delta log only with spark.databricks.delta.commitInfo.enabled configuration property enabled.</p>"},{"location":"CommitInfo/#creating-empty-commitinfo","text":"","title":"Creating Empty CommitInfo <pre><code>empty(\n  version: Option[Long] = None): CommitInfo\n</code></pre> <p><code>empty</code>...FIXME</p> <p><code>empty</code>\u00a0is used when:</p> <ul> <li><code>DeltaHistoryManager</code> is requested to getCommitInfo</li> </ul>"},{"location":"CommitInfo/#creating-commitinfo","text":"","title":"Creating CommitInfo <pre><code>apply(\n  time: Long,\n  operation: String,\n  operationParameters: Map[String, String],\n  commandContext: Map[String, String],\n  readVersion: Option[Long],\n  isolationLevel: Option[String],\n  isBlindAppend: Option[Boolean],\n  operationMetrics: Option[Map[String, String]],\n  userMetadata: Option[String],\n  tags: Option[Map[String, String]],\n  txnId: Option[String]): CommitInfo\n</code></pre> <p><code>apply</code> creates a <code>CommitInfo</code> (for the given arguments and based on the given <code>commandContext</code> for the user ID, user name, job, notebook, cluster).</p> <p><code>commandContext</code> argument is always empty, but could be customized using ConvertToDeltaCommandBase.</p> <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to commit (with spark.databricks.delta.commitInfo.enabled configuration property enabled)</li> <li><code>DeltaCommand</code> is requested to commitLarge</li> </ul>"},{"location":"DelayedCommitProtocol/","text":"<p><code>DelayedCommitProtocol</code> is a <code>FileCommitProtocol</code> (Apache Spark) to write out data to a directory and return the files added.</p> <p><code>DelayedCommitProtocol</code> is used to model a distributed write that is orchestrated by the Spark driver with the write itself happening on executors.</p>  <p>Note</p> <p><code>FileCommitProtocol</code> allows to track a write job (with a write task per partition) and inform the driver when all the write tasks finished successfully (and were committed) to consider the write job completed. <code>TaskCommitMessage</code> (Spark Core) allows to \"transfer\" the files added (written out) on the executors to the driver for the optimistic transactional writer.</p>  <p><code>DelayedCommitProtocol</code> is a <code>Serializable</code>.</p>","title":"DelayedCommitProtocol"},{"location":"DelayedCommitProtocol/#creating-instance","text":"<p><code>DelayedCommitProtocol</code> takes the following to be created:</p> <ul> <li> Job ID <li> Path (to write files to) <li> (optional) Length of Random Prefix  <p><code>DelayedCommitProtocol</code> is created\u00a0when:</p> <ul> <li><code>TransactionalWrite</code> is requested for a committer (to write a structured query to the directory)</li> </ul>","title":"Creating Instance"},{"location":"DelayedCommitProtocol/#addedfiles","text":"","title":"addedFiles <pre><code>addedFiles: ArrayBuffer[(Map[String, String], String)]\n</code></pre> <p><code>DelayedCommitProtocol</code> uses <code>addedFiles</code> internal registry to track the files added by a Spark write task.</p> <p><code>addedFiles</code> is used on the executors only.</p> <p><code>addedFiles</code> is initialized (as an empty collection) when setting up a task.</p> <p><code>addedFiles</code> is used when:</p> <ul> <li><code>DelayedCommitProtocol</code> is requested to commit a task (on an executor and create a <code>TaskCommitMessage</code> with the files added while a task was writing out a partition of a streaming query)</li> </ul>"},{"location":"DelayedCommitProtocol/#addedstatuses","text":"","title":"addedStatuses <pre><code>addedStatuses: ArrayBuffer[AddFile]\n</code></pre> <p><code>DelayedCommitProtocol</code> uses <code>addedStatuses</code> internal registry to track the files that were added by write tasks (on executors) once all they finish successfully and the write job is committed (on a driver).</p> <p><code>addedStatuses</code> is used on the driver only.</p> <p><code>addedStatuses</code> is used when:</p> <ul> <li><code>DelayedCommitProtocol</code> is requested to commit a job (on a driver)</li> <li><code>TransactionalWrite</code> is requested to write out a structured query</li> </ul>"},{"location":"DelayedCommitProtocol/#setting-up-job","text":"","title":"Setting Up Job <pre><code>setupJob(\n  jobContext: JobContext): Unit\n</code></pre> <p><code>setupJob</code>\u00a0is part of the <code>FileCommitProtocol</code> (Apache Spark) abstraction.</p> <p><code>setupJob</code> is a noop.</p>"},{"location":"DelayedCommitProtocol/#committing-job","text":"","title":"Committing Job <pre><code>commitJob(\n  jobContext: JobContext,\n  taskCommits: Seq[TaskCommitMessage]): Unit\n</code></pre> <p><code>commitJob</code>\u00a0is part of the <code>FileCommitProtocol</code> (Apache Spark) abstraction.</p> <p><code>commitJob</code> adds the AddFiles (from the given <code>taskCommits</code> from every commitTask) to the addedStatuses internal registry.</p>"},{"location":"DelayedCommitProtocol/#aborting-job","text":"","title":"Aborting Job <pre><code>abortJob(\n  jobContext: JobContext): Unit\n</code></pre> <p><code>abortJob</code>\u00a0is part of the <code>FileCommitProtocol</code> (Apache Spark) abstraction.</p> <p><code>abortJob</code> is a noop.</p>"},{"location":"DelayedCommitProtocol/#setting-up-task","text":"","title":"Setting Up Task <pre><code>setupTask(\n  taskContext: TaskAttemptContext): Unit\n</code></pre> <p><code>setupTask</code>\u00a0is part of the <code>FileCommitProtocol</code> (Apache Spark) abstraction.</p> <p><code>setupTask</code> initializes the addedFiles internal registry to be empty.</p>"},{"location":"DelayedCommitProtocol/#new-temp-file-relative-path","text":"","title":"New Temp File (Relative Path) <pre><code>newTaskTempFile(\n  taskContext: TaskAttemptContext,\n  dir: Option[String],\n  ext: String): String\n</code></pre> <p><code>newTaskTempFile</code>\u00a0is part of the <code>FileCommitProtocol</code> (Apache Spark) abstraction.</p> <p><code>newTaskTempFile</code> creates a file name for the given <code>TaskAttemptContext</code> and <code>ext</code>.</p> <p><code>newTaskTempFile</code> tries to parsePartitions with the given <code>dir</code> or falls back to an empty <code>partitionValues</code>.</p>  <p>Note</p> <p>The given <code>dir</code> defines a partition directory if the streaming query (and hence the write) is partitioned.</p>  <p><code>newTaskTempFile</code> builds a path (based on the given <code>randomPrefixLength</code> and the <code>dir</code>, or uses the file name directly).</p>  <p>Fixme</p> <p>When are the optional <code>dir</code> and the randomPrefixLength defined?</p>  <p><code>newTaskTempFile</code> adds the partition values and the relative path to the addedFiles internal registry.</p> <p>In the end, <code>newTaskTempFile</code> returns the absolute path of the (relative) path in the directory.</p>"},{"location":"DelayedCommitProtocol/#file-name","text":"","title":"File Name <pre><code>getFileName(\n  taskContext: TaskAttemptContext,\n  ext: String,\n  partitionValues: Map[String, String]): String\n</code></pre> <p><code>getFileName</code> takes the task ID from the given <code>TaskAttemptContext</code> (Apache Spark) (for the <code>split</code> part below).</p> <p><code>getFileName</code> generates a random UUID (for the <code>uuid</code> part below).</p> <p>In the end, <code>getFileName</code> returns a file name of the format:</p> <pre><code>part-[split]-[uuid][ext]\n</code></pre>"},{"location":"DelayedCommitProtocol/#new-temp-file-absolute-path","text":"","title":"New Temp File (Absolute Path) <pre><code>newTaskTempFileAbsPath(\n  taskContext: TaskAttemptContext,\n  absoluteDir: String,\n  ext: String): String\n</code></pre> <p><code>newTaskTempFileAbsPath</code>\u00a0is part of the <code>FileCommitProtocol</code> (Apache Spark) abstraction.</p> <p><code>newTaskTempFileAbsPath</code> throws an <code>UnsupportedOperationException</code>:</p> <pre><code>[this] does not support adding files with an absolute path\n</code></pre>"},{"location":"DelayedCommitProtocol/#committing-task","text":"","title":"Committing Task <pre><code>commitTask(\n  taskContext: TaskAttemptContext): TaskCommitMessage\n</code></pre> <p><code>commitTask</code>\u00a0is part of the <code>FileCommitProtocol</code> (Apache Spark) abstraction.</p> <p><code>commitTask</code> creates a <code>TaskCommitMessage</code> with an AddFile for every file added if there are any. Otherwise, <code>commitTask</code> creates an empty <code>TaskCommitMessage</code>.</p>  <p>Note</p> <p>A file is added (to the addedFiles internal registry) when <code>DelayedCommitProtocol</code> is requested for a new file (path).</p>"},{"location":"DelayedCommitProtocol/#aborting-task","text":"","title":"Aborting Task <pre><code>abortTask(\n  taskContext: TaskAttemptContext): Unit\n</code></pre> <p><code>abortTask</code>\u00a0is part of the <code>FileCommitProtocol</code> (Apache Spark) abstraction.</p> <p><code>abortTask</code> is a noop.</p>"},{"location":"DelayedCommitProtocol/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.files.DelayedCommitProtocol</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.delta.files.DelayedCommitProtocol=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"DeltaAnalysis/","text":"<p><code>DeltaAnalysis</code> is a logical resolution rule (Spark SQL).</p>","title":"DeltaAnalysis Logical Resolution Rule"},{"location":"DeltaAnalysis/#creating-instance","text":"<p><code>DeltaAnalysis</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> <li> <code>SQLConf</code>  <p><code>DeltaAnalysis</code> is created\u00a0when:</p> <ul> <li><code>DeltaSparkSessionExtension</code> is requested to inject Delta extensions</li> </ul>","title":"Creating Instance"},{"location":"DeltaAnalysis/#executing-rule","text":"<pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> is part of the <code>Rule</code> (Spark SQL) abstraction.</p> <p><code>apply</code> resolves logical operators.</p>","title":"Executing Rule"},{"location":"DeltaAnalysis/#altertableaddconstraintstatement","text":"","title":"AlterTableAddConstraintStatement <p><code>apply</code> creates an <code>AlterTable</code> (Spark SQL) logical command with an AddConstraint table change.</p>"},{"location":"DeltaAnalysis/#altertabledropconstraintstatement","text":"","title":"AlterTableDropConstraintStatement <p><code>apply</code> creates an <code>AlterTable</code> (Spark SQL) logical command with an DropConstraint table change.</p>"},{"location":"DeltaAnalysis/#appenddelta","text":"","title":"AppendDelta"},{"location":"DeltaAnalysis/#datasourcev2relation","text":"","title":"DataSourceV2Relation"},{"location":"DeltaAnalysis/#deletefromtable","text":"","title":"DeleteFromTable"},{"location":"DeltaAnalysis/#deltatable","text":"","title":"DeltaTable"},{"location":"DeltaAnalysis/#mergeintotable","text":"","title":"MergeIntoTable <pre><code>MergeIntoTable(target, source, condition, matched, notMatched)\n</code></pre> <p><code>apply</code> resolves <code>MergeIntoTable</code> (Spark SQL) logical command into a DeltaMergeInto.</p> <p><code>apply</code> creates the following for the <code>matched</code> actions:</p> <ul> <li>DeltaMergeIntoDeleteClauses for <code>DeleteAction</code>s</li> <li>DeltaMergeIntoUpdateClauses for <code>UpdateAction</code>s</li> </ul> <p><code>apply</code> throws an <code>AnalysisException</code> for <code>InsertAction</code>s:</p> <pre><code>Insert clauses cannot be part of the WHEN MATCHED clause in MERGE INTO.\n</code></pre> <p><code>apply</code> creates the following for the <code>notMatched</code> actions:</p> <ul> <li>DeltaMergeIntoInsertClauses for <code>InsertAction</code>s</li> </ul> <p><code>apply</code> throws an <code>AnalysisException</code> for the other actions:</p> <pre><code>[name] clauses cannot be part of the WHEN NOT MATCHED clause in MERGE INTO.\n</code></pre> <p>In the end, <code>apply</code> creates a DeltaMergeInto logical command (with the matched and not-matched actions).</p>"},{"location":"DeltaAnalysis/#overwritedelta","text":"","title":"OverwriteDelta"},{"location":"DeltaAnalysis/#updatetable","text":"","title":"UpdateTable"},{"location":"DeltaCatalog/","text":"<p><code>DeltaCatalog</code> is a <code>DelegatingCatalogExtension</code> (Spark SQL) and a <code>StagingTableCatalog</code> (Spark SQL).</p> <p><code>DeltaCatalog</code> is registered using <code>spark.sql.catalog.spark_catalog</code> (Spark SQL) configuration property.</p>","title":"DeltaCatalog"},{"location":"DeltaCatalog/#altering-table","text":"","title":"Altering Table <pre><code>alterTable(\n  ident: Identifier,\n  changes: TableChange*): Table\n</code></pre> <p><code>alterTable</code> is part of the <code>TableCatalog</code> (Spark SQL) abstraction.</p> <p><code>alterTable</code> loads the table and continues for DeltaTableV2. Otherwise, <code>alterTable</code> delegates to the parent <code>TableCatalog</code>.</p> <p><code>alterTable</code>...FIXME</p>"},{"location":"DeltaCatalog/#creating-table","text":"","title":"Creating Table <pre><code>createTable(\n  ident: Identifier,\n  schema: StructType,\n  partitions: Array[Transform],\n  properties: util.Map[String, String]): Table\n</code></pre> <p><code>createTable</code> is part of the <code>TableCatalog</code> (Spark SQL) abstraction.</p> <p><code>createTable</code>...FIXME</p>"},{"location":"DeltaCatalog/#loading-table","text":"","title":"Loading Table <pre><code>loadTable(\n  ident: Identifier): Table\n</code></pre> <p><code>loadTable</code> is part of the <code>TableCatalog</code> (Spark SQL) abstraction.</p> <p><code>loadTable</code> loads a table by the given identifier from a catalog.</p> <p>If found and the table is a delta table (Spark SQL's V1Table with <code>delta</code> provider), <code>loadTable</code> creates a DeltaTableV2.</p>"},{"location":"DeltaCatalog/#creating-delta-table","text":"","title":"Creating Delta Table <pre><code>createDeltaTable(\n  ident: Identifier,\n  schema: StructType,\n  partitions: Array[Transform],\n  properties: util.Map[String, String],\n  sourceQuery: Option[LogicalPlan],\n  operation: TableCreationModes.CreationMode): Table\n</code></pre> <p><code>createDeltaTable</code>...FIXME</p> <p><code>createDeltaTable</code> is used when:</p> <ul> <li><code>DeltaCatalog</code> is requested to createTable</li> <li><code>StagedDeltaTableV2</code> is requested to commitStagedChanges</li> </ul>"},{"location":"DeltaColumnBuilder/","text":"<p><code>DeltaColumnBuilder</code> is a builder interface to create columns programmatically.</p> <p><code>DeltaColumnBuilder</code> is created using DeltaTable.columnBuilder utility.</p> <p>In the end, <code>DeltaColumnBuilder</code> is supposed to be built.</p>","title":"DeltaColumnBuilder"},{"location":"DeltaColumnBuilder/#iodeltatables-package","text":"<p><code>DeltaColumnBuilder</code> belongs to <code>io.delta.tables</code> package.</p> <pre><code>import io.delta.tables.DeltaColumnBuilder\n</code></pre>","title":"io.delta.tables Package"},{"location":"DeltaColumnBuilder/#creating-instance","text":"<p><code>DeltaColumnBuilder</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> Column Name","title":"Creating Instance"},{"location":"DeltaColumnBuilder/#operators","text":"","title":"Operators"},{"location":"DeltaColumnBuilder/#build","text":"","title":"build <pre><code>build(): StructField\n</code></pre> <p>Creates a <code>StructField</code> (Spark SQL)</p>"},{"location":"DeltaColumnBuilder/#comment","text":"","title":"comment <pre><code>comment(\n  comment: String): DeltaColumnBuilder\n</code></pre>"},{"location":"DeltaColumnBuilder/#datatype","text":"","title":"dataType <pre><code>dataType(\n  dataType: DataType): DeltaColumnBuilder\ndataType(\n  dataType: String): DeltaColumnBuilder\n</code></pre>"},{"location":"DeltaColumnBuilder/#generatedalwaysas","text":"","title":"generatedAlwaysAs <pre><code>generatedAlwaysAs(\n  expr: String): DeltaColumnBuilder\n</code></pre> <p>Registers the Generation Expression of this field</p>"},{"location":"DeltaColumnBuilder/#nullable","text":"","title":"nullable <pre><code>nullable(\n  nullable: Boolean): DeltaColumnBuilder\n</code></pre>"},{"location":"DeltaColumnBuilder/#generation-expression","text":"","title":"Generation Expression <pre><code>generationExpr: Option[String] = None\n</code></pre> <p><code>DeltaColumnBuilder</code> uses <code>generationExpr</code> internal registry for the generatedAlwaysAs expression.</p> <p>When requested to build a StructField, <code>DeltaColumnBuilder</code> registers it under delta.generationExpression key in the metadata (of the field).</p>"},{"location":"DeltaConfig/","text":"<p><code>DeltaConfig</code> (of type <code>T</code>) represents a named configuration property of a delta table with values (of type <code>T</code>).</p>","title":"DeltaConfig"},{"location":"DeltaConfig/#creating-instance","text":"<p><code>DeltaConfig</code> takes the following to be created:</p> <ul> <li> Configuration Key <li> Default Value <li> Conversion function (from text representation of the <code>DeltaConfig</code> to the <code>T</code> type, i.e. <code>String =&gt; T</code>) <li> Validation function (that guards from incorrect values, i.e. <code>T =&gt; Boolean</code>) <li> Help message <li> (optional) Minimum version of protocol supported (default: undefined)  <p><code>DeltaConfig</code> is created\u00a0when:</p> <ul> <li><code>DeltaConfigs</code> utility is used to build a DeltaConfig</li> </ul>","title":"Creating Instance"},{"location":"DeltaConfig/#reading-configuration-property-from-metadata","text":"","title":"Reading Configuration Property From Metadata <pre><code>fromMetaData(\n  metadata: Metadata): T\n</code></pre> <p><code>fromMetaData</code> looks up the key in the configuration of the given Metadata. If not found, <code>fromMetaData</code> gives the default value.</p> <p>In the end, <code>fromMetaData</code> converts the text representation to the proper type using fromString conversion function.</p> <p><code>fromMetaData</code> is used when:</p> <ul> <li><code>Checkpoints</code> utility is used to buildCheckpoint</li> <li><code>DeltaErrors</code> utility is used to logFileNotFoundException</li> <li><code>DeltaLog</code> is requested for checkpointInterval and deletedFileRetentionDuration table properties, and to assert a table is not read-only</li> <li><code>MetadataCleanup</code> is requested for the enableExpiredLogCleanup and the deltaRetentionMillis</li> <li><code>OptimisticTransactionImpl</code> is requested to commit</li> <li><code>Snapshot</code> is requested for the numIndexedCols</li> </ul>"},{"location":"DeltaConfig/#demo","text":"","title":"Demo <pre><code>import org.apache.spark.sql.delta.{DeltaConfig, DeltaConfigs}\n</code></pre> <pre><code>scala&gt; :type DeltaConfigs.TOMBSTONE_RETENTION\norg.apache.spark.sql.delta.DeltaConfig[org.apache.spark.unsafe.types.CalendarInterval]\n</code></pre> <pre><code>import org.apache.spark.sql.delta.DeltaLog\nval path = \"/tmp/delta/t1\"\nval t1 = DeltaLog.forTable(spark, path)\n</code></pre> <pre><code>val metadata = t1.snapshot.metadata\nval retention = DeltaConfigs.TOMBSTONE_RETENTION.fromMetaData(metadata)\n</code></pre> <pre><code>scala&gt; :type retention\norg.apache.spark.unsafe.types.CalendarInterval\n</code></pre>"},{"location":"DeltaConfigs/","text":"<p><code>DeltaConfigs</code> holds the table properties that can be set on a delta table.</p>","title":"DeltaConfigs (DeltaConfigsBase)"},{"location":"DeltaConfigs/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"DeltaConfigs/#appendonly","text":"","title":"appendOnly <p>Whether a delta table is append-only (<code>true</code>) or not (<code>false</code>). When enabled, a table allows appends only and no updates or deletes.</p> <p>Default: <code>false</code></p> <p>Used when:</p> <ul> <li><code>DeltaLog</code> is requested to assertRemovable (that in turn uses <code>DeltaErrors</code> utility to modifyAppendOnlyTableException)</li> <li><code>Protocol</code> utility is used to requiredMinimumProtocol</li> </ul>"},{"location":"DeltaConfigs/#autooptimize","text":"","title":"autoOptimize <p>Whether this delta table will automagically optimize the layout of files during writes.</p> <p>Default: <code>false</code></p>"},{"location":"DeltaConfigs/#checkpointinterval","text":"","title":"checkpointInterval <p>How often to checkpoint the state of a delta table (at the end of transaction commit)</p> <p>Default: <code>10</code></p>"},{"location":"DeltaConfigs/#checkpointretentionduration","text":"","title":"checkpointRetentionDuration <p>How long to keep checkpoint files around before deleting them</p> <p>Default: <code>interval 2 days</code></p> <p>The most recent checkpoint is never deleted. It is acceptable to keep checkpoint files beyond this duration until the next calendar day.</p>"},{"location":"DeltaConfigs/#checkpointwritestatsasjson","text":"","title":"checkpoint.writeStatsAsJson <p>Controls whether to write file statistics in the checkpoint in JSON format as the <code>stats</code> column.</p> <p>Default: <code>true</code></p>"},{"location":"DeltaConfigs/#checkpointwritestatsasstruct","text":"","title":"checkpoint.writeStatsAsStruct <p>Controls whether to write file statistics in the checkpoint in the struct format in the <code>stats_parsed</code> column and partition values as a struct as <code>partitionValues_parsed</code></p> <p>Default: <code>undefined</code> (<code>Option[Boolean]</code>)</p>"},{"location":"DeltaConfigs/#compatibilitysymlinkformatmanifestenabled","text":"","title":"compatibility.symlinkFormatManifest.enabled <p>Whether to register the GenerateSymlinkManifest post-commit hook while committing a transaction or not</p> <p>Default: <code>false</code></p>"},{"location":"DeltaConfigs/#dataskippingnumindexedcols","text":"","title":"dataSkippingNumIndexedCols <p>The number of columns to collect stats on for data skipping. <code>-1</code> means collecting stats for all columns.</p> <p>Default: <code>32</code></p>"},{"location":"DeltaConfigs/#deletedfileretentionduration","text":"","title":"deletedFileRetentionDuration <p>How long to keep logically deleted data files around before deleting them physically (to prevent failures in stale readers after compactions or partition overwrites)</p> <p>Default: <code>interval 1 week</code></p>"},{"location":"DeltaConfigs/#enableexpiredlogcleanup","text":"","title":"enableExpiredLogCleanup <p>Whether to clean up expired log files and checkpoints</p> <p>Default: <code>true</code></p>"},{"location":"DeltaConfigs/#enablefullretentionrollback","text":"","title":"enableFullRetentionRollback <p>Controls whether or not a delta table can be rolled back to any point within logRetentionDuration. When disabled, the table can be rolled back checkpointRetentionDuration only.</p> <p>Default: <code>true</code></p>"},{"location":"DeltaConfigs/#logretentionduration","text":"","title":"logRetentionDuration <p>How long to keep obsolete logs around before deleting them. Delta can keep logs beyond the duration until the next calendar day to avoid constantly creating checkpoints.</p> <p>Default: <code>interval 30 days</code> (<code>CalendarInterval</code>)</p>"},{"location":"DeltaConfigs/#minreaderversion","text":"","title":"minReaderVersion <p>The protocol reader version</p> <p>Default: <code>1</code></p> <p>This property is not stored as a table property in the <code>Metadata</code> action. It is stored as its own action. Having it modelled as a table property makes it easier to upgrade, and view the version.</p>"},{"location":"DeltaConfigs/#minwriterversion","text":"","title":"minWriterVersion <p>The protocol reader version</p> <p>Default: <code>3</code></p> <p>This property is not stored as a table property in the <code>Metadata</code> action. It is stored as its own action. Having it modelled as a table property makes it easier to upgrade, and view the version.</p>"},{"location":"DeltaConfigs/#randomizefileprefixes","text":"","title":"randomizeFilePrefixes <p>Whether to use a random prefix in a file path instead of partition information (may be required for very high volume S3 calls to better be partitioned across S3 servers)</p> <p>Default: <code>false</code></p>"},{"location":"DeltaConfigs/#randomprefixlength","text":"","title":"randomPrefixLength <p>The length of the random prefix in a file path for randomizeFilePrefixes</p> <p>Default: <code>2</code></p>"},{"location":"DeltaConfigs/#sampleretentionduration","text":"","title":"sampleRetentionDuration <p>How long to keep delta sample files around before deleting them</p> <p>Default: <code>interval 7 days</code></p>"},{"location":"DeltaConfigs/#building-configuration","text":"","title":"Building Configuration <pre><code>buildConfig[T](\n  key: String,\n  defaultValue: String,\n  fromString: String =&gt; T,\n  validationFunction: T =&gt; Boolean,\n  helpMessage: String,\n  minimumProtocolVersion: Option[Protocol] = None): DeltaConfig[T]\n</code></pre> <p><code>buildConfig</code> creates a DeltaConfig for the given <code>key</code> (with delta prefix added) and adds it to the entries internal registry.</p> <p><code>buildConfig</code> is used to define all of the configuration properties in a type-safe way and (as a side effect) register them with the system-wide entries internal registry.</p>"},{"location":"DeltaConfigs/#system-wide-configuration-entries-registry","text":"","title":"System-Wide Configuration Entries Registry <pre><code>entries: HashMap[String, DeltaConfig[_]]\n</code></pre> <p><code>DeltaConfigs</code> utility (a Scala object) uses <code>entries</code> internal registry of DeltaConfigs by their key.</p> <p>New entries are added in buildConfig.</p> <p><code>entries</code> is used when:</p> <ul> <li>validateConfigurations</li> <li>mergeGlobalConfigs</li> <li>normalizeConfigKey and normalizeConfigKeys</li> </ul>"},{"location":"DeltaConfigs/#mergeglobalconfigs-utility","text":"","title":"mergeGlobalConfigs Utility <pre><code>mergeGlobalConfigs(\n  sqlConfs: SQLConf,\n  tableConf: Map[String, String],\n  protocol: Protocol): Map[String, String]\n</code></pre> <p><code>mergeGlobalConfigs</code> finds all spark.databricks.delta.properties.defaults-prefixed configuration properties among the entries.</p> <p><code>mergeGlobalConfigs</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to withGlobalConfigDefaults</li> <li><code>InitialSnapshot</code> is created</li> </ul>"},{"location":"DeltaConfigs/#validateconfigurations-utility","text":"","title":"validateConfigurations Utility <pre><code>validateConfigurations(\n  configurations: Map[String, String]): Map[String, String]\n</code></pre> <p><code>validateConfigurations</code>...FIXME</p> <p><code>validateConfigurations</code> is used when:</p> <ul> <li><code>DeltaCatalog</code> is requested to verifyTableAndSolidify and alterTable</li> </ul>"},{"location":"DeltaConfigs/#normalizeconfigkeys-utility","text":"","title":"normalizeConfigKeys Utility <pre><code>normalizeConfigKeys(\n  propKeys: Seq[String]): Seq[String]\n</code></pre> <p><code>normalizeConfigKeys</code>...FIXME</p> <p><code>normalizeConfigKeys</code> is used when:</p> <ul> <li>AlterTableUnsetPropertiesDeltaCommand is executed</li> </ul>"},{"location":"DeltaConfigs/#sparkdatabricksdeltapropertiesdefaults-prefix","text":"","title":"spark.databricks.delta.properties.defaults Prefix <p>DeltaConfigs uses spark.databricks.delta.properties.defaults prefix for global configuration properties.</p>"},{"location":"DeltaDataSource/","text":"<p><code>DeltaDataSource</code> is a DataSourceRegister and is the entry point to all the features provided by <code>delta</code> data source that supports batch and streaming queries.</p>","title":"DeltaDataSource"},{"location":"DeltaDataSource/#datasourceregister-and-delta-alias","text":"","title":"DataSourceRegister and delta Alias <p><code>DeltaDataSource</code> is a <code>DataSourceRegister</code> (Spark SQL) and registers delta alias.</p> <p><code>DeltaDataSource</code> is registered using META-INF/services/org.apache.spark.sql.sources.DataSourceRegister:</p> <pre><code>org.apache.spark.sql.delta.sources.DeltaDataSource\n</code></pre>"},{"location":"DeltaDataSource/#relationprovider","text":"","title":"RelationProvider <p><code>DeltaDataSource</code> is a <code>RelationProvider</code> (Spark SQL).</p>"},{"location":"DeltaDataSource/#creating-relation","text":"","title":"Creating Relation <pre><code>createRelation(\n  sqlContext: SQLContext,\n  parameters: Map[String, String]): BaseRelation\n</code></pre> <p><code>createRelation</code> verifies the given parameters.</p> <p><code>createRelation</code> extracts time travel specification from the given parameters.</p> <p>With spark.databricks.delta.loadFileSystemConfigsFromDataFrameOptions enabled, <code>createRelation</code> uses the given <code>parameters</code> as <code>options</code>.</p> <p>In the end, <code>createRelation</code> creates a DeltaTableV2 (with the required <code>path</code> option and the optional time travel specification) and requests it for an insertable HadoopFsRelation.</p>  <p><code>createRelation</code> makes sure that there is <code>path</code> parameter defined (in the given <code>parameters</code>) or throws an <code>IllegalArgumentException</code>:</p> <pre><code>'path' is not specified\n</code></pre>  <p><code>createRelation</code> is part of the <code>RelationProvider</code> (Spark SQL) abstraction.</p>"},{"location":"DeltaDataSource/#creatablerelationprovider","text":"","title":"CreatableRelationProvider <p><code>DeltaDataSource</code> is a <code>CreatableRelationProvider</code> (Spark SQL).</p>"},{"location":"DeltaDataSource/#creating-relation_1","text":"","title":"Creating Relation <pre><code>createRelation(\n  sqlContext: SQLContext,\n  mode: SaveMode,\n  parameters: Map[String, String],\n  data: DataFrame): BaseRelation\n</code></pre> <p><code>createRelation</code> creates a DeltaLog for the required <code>path</code> parameter (from the given <code>parameters</code>) and the given <code>parameters</code> itself.</p> <p><code>createSource</code> creates a DeltaOptions (with the given <code>parameters</code> and the current <code>SQLConf</code>).</p> <p><code>createRelation</code> creates and executes a WriteIntoDelta command for the given <code>data</code>.</p> <p>In the end, <code>createRelation</code> requests the <code>DeltaLog</code> for a HadoopFsRelation.</p>  <p><code>createRelation</code> makes sure that there is <code>path</code> parameter defined (in the given <code>parameters</code>) or throws an <code>IllegalArgumentException</code>:</p> <pre><code>'path' is not specified\n</code></pre>  <p><code>createRelation</code> is part of the <code>CreatableRelationProvider</code> (Spark SQL) abstraction.</p>"},{"location":"DeltaDataSource/#streamsourceprovider","text":"","title":"StreamSourceProvider <p><code>DeltaDataSource</code> is a <code>StreamSourceProvider</code> (Spark Structured Streaming).</p>"},{"location":"DeltaDataSource/#creating-deltasource","text":"","title":"Creating DeltaSource <pre><code>createSource(\n  sqlContext: SQLContext,\n  metadataPath: String,\n  schema: Option[StructType],\n  providerName: String,\n  parameters: Map[String, String]): Source\n</code></pre> <p><code>createSource</code> creates a DeltaLog for the required <code>path</code> parameter (from the given <code>parameters</code>).</p> <p><code>createSource</code> creates a DeltaOptions (with the given <code>parameters</code> and the current <code>SQLConf</code>).</p> <p>In the end, <code>createSource</code> creates a DeltaSource (with the <code>DeltaLog</code> and the <code>DeltaOptions</code>).</p>  <p><code>createSource</code> makes sure that there is <code>path</code> parameter defined (in the given <code>parameters</code>) or throws an <code>IllegalArgumentException</code>:</p> <pre><code>'path' is not specified\n</code></pre>  <p><code>createSource</code> makes sure that there is no <code>schema</code> specified or throws an <code>AnalysisException</code>:</p> <pre><code>Delta does not support specifying the schema at read time.\n</code></pre>  <p><code>createSource</code> makes sure that there is schema available (in the Snapshot) of the <code>DeltaLog</code> or throws an <code>AnalysisException</code>:</p> <pre><code>Table schema is not set.  Write data into it or use CREATE TABLE to set the schema.\n</code></pre>  <p><code>createSource</code> is part of the <code>StreamSourceProvider</code> (Spark Structured Streaming) abstraction.</p>"},{"location":"DeltaDataSource/#streaming-schema","text":"","title":"Streaming Schema <pre><code>sourceSchema(\n  sqlContext: SQLContext,\n  schema: Option[StructType],\n  providerName: String,\n  parameters: Map[String, String]): (String, StructType)\n</code></pre> <p><code>sourceSchema</code> creates a DeltaLog for the required <code>path</code> parameter (from the given <code>parameters</code>).</p> <p><code>sourceSchema</code> takes the schema (of the Snapshot) of the <code>DeltaLog</code> and removes generation expressions (if defined).</p> <p>In the end, <code>sourceSchema</code> returns the delta name with the schema (of the Delta table without the generation expressions).</p>  <p><code>createSource</code> makes sure that there is no <code>schema</code> specified or throws an <code>AnalysisException</code>:</p> <pre><code>Delta does not support specifying the schema at read time.\n</code></pre>  <p><code>createSource</code> makes sure that there is <code>path</code> parameter defined (in the given <code>parameters</code>) or throws an <code>IllegalArgumentException</code>:</p> <pre><code>'path' is not specified\n</code></pre>  <p><code>createSource</code> makes sure that there is no time travel specified using the following:</p> <ul> <li>path parameter</li> <li>options (in the given <code>parameters</code>)</li> </ul> <p>If either is set, <code>createSource</code> throws an <code>AnalysisException</code>:</p> <pre><code>Cannot time travel views, subqueries or streams.\n</code></pre>  <p><code>sourceSchema</code> is part of the <code>StreamSourceProvider</code> (Spark Structured Streaming) abstraction.</p>"},{"location":"DeltaDataSource/#streamsinkprovider","text":"","title":"StreamSinkProvider <p><code>DeltaDataSource</code> is a <code>StreamSinkProvider</code> (Spark Structured Streaming).</p> <p><code>DeltaDataSource</code> supports <code>Append</code> and <code>Complete</code> output modes only.</p>  <p>Tip</p> <p>Consult the demo Using Delta Lake (as Streaming Sink) in Streaming Queries.</p>"},{"location":"DeltaDataSource/#creating-streaming-sink","text":"","title":"Creating Streaming Sink <pre><code>createSink(\n  sqlContext: SQLContext,\n  parameters: Map[String, String],\n  partitionColumns: Seq[String],\n  outputMode: OutputMode): Sink\n</code></pre> <p><code>createSink</code> creates a DeltaOptions (with the given <code>parameters</code> and the current <code>SQLConf</code>).</p> <p>In the end, <code>createSink</code> creates a DeltaSink (with the required <code>path</code> parameter, the given <code>partitionColumns</code> and the <code>DeltaOptions</code>).</p>  <p><code>createSink</code> makes sure that there is <code>path</code> parameter defined (in the given <code>parameters</code>) or throws an <code>IllegalArgumentException</code>:</p> <pre><code>'path' is not specified\n</code></pre>  <p><code>createSink</code> makes sure that the given <code>outputMode</code> is either <code>Append</code> or <code>Complete</code>, or throws an <code>IllegalArgumentException</code>:</p> <pre><code>Data source [dataSource] does not support [outputMode] output mode\n</code></pre>  <p><code>createSink</code> is part of the <code>StreamSinkProvider</code> (Spark Structured Streaming) abstraction.</p>"},{"location":"DeltaDataSource/#tableprovider","text":"","title":"TableProvider <p><code>DeltaDataSource</code> is a<code>TableProvider</code> (Spark SQL).</p> <p><code>DeltaDataSource</code> allows registering Delta tables in a <code>HiveMetaStore</code>. Delta creates a transaction log at the table root directory, and the Hive MetaStore contains no information but the table format and the location of the table. All table properties, schema and partitioning information live in the transaction log to avoid a split brain situation.</p> <p>The feature was added in SC-34233.</p>"},{"location":"DeltaDataSource/#loading-delta-table","text":"","title":"Loading Delta Table <pre><code>getTable(\n  schema: StructType,\n  partitioning: Array[Transform],\n  properties: Map[String, String]): Table\n</code></pre> <p><code>getTable</code> is part of the <code>TableProvider</code> (Spark SQL) abstraction.</p>  <p><code>getTable</code> creates a DeltaTableV2 (with the path from the given <code>properties</code>).</p>  <p><code>getTable</code> throws an <code>IllegalArgumentException</code> when <code>path</code> option is not specified:</p> <pre><code>'path' is not specified\n</code></pre>"},{"location":"DeltaDataSource/#utilities","text":"","title":"Utilities"},{"location":"DeltaDataSource/#gettimetravelversion","text":"","title":"getTimeTravelVersion <pre><code>getTimeTravelVersion(\n  parameters: Map[String, String]): Option[DeltaTimeTravelSpec]\n</code></pre> <p><code>getTimeTravelVersion</code>...FIXME</p> <p><code>getTimeTravelVersion</code> is used when <code>DeltaDataSource</code> is requested to create a relation (as a RelationProvider).</p>"},{"location":"DeltaDataSource/#parsepathidentifier","text":"","title":"parsePathIdentifier <pre><code>parsePathIdentifier(\n  spark: SparkSession,\n  userPath: String): (Path, Seq[(String, String)], Option[DeltaTimeTravelSpec])\n</code></pre> <p><code>parsePathIdentifier</code>...FIXME</p> <p><code>parsePathIdentifier</code> is used when <code>DeltaTableV2</code> is requested for metadata (for a non-catalog table).</p>"},{"location":"DeltaErrors/","text":"","title":"DeltaErrors Utility"},{"location":"DeltaErrors/#concurrentappendexception","text":"","title":"concurrentAppendException <pre><code>concurrentAppendException(\n  conflictingCommit: Option[CommitInfo],\n  partition: String,\n  customRetryMsg: Option[String] = None): ConcurrentAppendException\n</code></pre> <p><code>concurrentAppendException</code> creates a ConcurrentAppendException with the following message:</p> <pre><code>Files were added to [partition] by a concurrent update. [customRetryMsg] | Please try the operation again.\nConflicting commit: [commitinfo]\nRefer to [docs]/concurrency-control.html for more details.\n</code></pre> <p><code>concurrentAppendException</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to check logical conflicts with concurrent updates</li> </ul>"},{"location":"DeltaErrors/#concurrentdeletedeleteexception","text":"","title":"concurrentDeleteDeleteException <pre><code>concurrentDeleteDeleteException(\n  conflictingCommit: Option[CommitInfo],\n  file: String): ConcurrentDeleteDeleteException\n</code></pre> <p><code>concurrentDeleteDeleteException</code> creates a ConcurrentDeleteDeleteException with the following message:</p> <pre><code>This transaction attempted to delete one or more files that were deleted (for example [file]) by a concurrent update. Please try the operation again.\nConflicting commit: [commitinfo]\nRefer to [docs]/concurrency-control.html for more details.\n</code></pre> <p><code>concurrentDeleteDeleteException</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to check for logical conflicts with concurrent updates</li> </ul>"},{"location":"DeltaErrors/#concurrentdeletereadexception","text":"","title":"concurrentDeleteReadException <pre><code>concurrentDeleteReadException(\n  conflictingCommit: Option[CommitInfo],\n  file: String): ConcurrentDeleteReadException\n</code></pre> <p><code>concurrentDeleteReadException</code> creates a ConcurrentDeleteReadException with the following message:</p> <pre><code>This transaction attempted to read one or more files that were deleted (for example [file]) by a concurrent update. Please try the operation again.\nConflicting commit: [commitinfo]\nRefer to [docs]/concurrency-control.html for more details.\n</code></pre> <p><code>concurrentDeleteReadException</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to check for logical conflicts with concurrent updates</li> </ul>"},{"location":"DeltaErrors/#concurrenttransactionexception","text":"","title":"concurrentTransactionException <pre><code>concurrentTransactionException(\n  conflictingCommit: Option[CommitInfo]): ConcurrentTransactionException\n</code></pre> <p><code>concurrentTransactionException</code> creates a ConcurrentTransactionException with the following message:</p> <pre><code>This error occurs when multiple streaming queries are using the same checkpoint to write into this table.\nDid you run multiple instances of the same streaming query at the same time?\nConflicting commit: [commitinfo]\nRefer to [docs]/concurrency-control.html for more details.\n</code></pre> <p><code>concurrentTransactionException</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to check for logical conflicts with concurrent updates</li> </ul>"},{"location":"DeltaErrors/#concurrentwriteexception","text":"","title":"concurrentWriteException <pre><code>concurrentWriteException(\n  conflictingCommit: Option[CommitInfo]): ConcurrentWriteException\n</code></pre> <p><code>concurrentWriteException</code> creates a ConcurrentWriteException with the following message:</p> <pre><code>A concurrent transaction has written new data since the current transaction read the table. Please try the operation again.\nConflicting commit: [commitinfo]\nRefer to [docs]/concurrency-control.html for more details.\n</code></pre> <p><code>concurrentWriteException</code>\u00a0is used when:</p> <ul> <li>Convert to Delta command is executed (and <code>DeltaCommand</code> is requested to commitLarge)</li> </ul>"},{"location":"DeltaErrors/#metadatachangedexception","text":"","title":"metadataChangedException <pre><code>metadataChangedException(\n  conflictingCommit: Option[CommitInfo]): MetadataChangedException\n</code></pre> <p><code>metadataChangedException</code> creates a MetadataChangedException with the following message:</p> <pre><code>The metadata of the Delta table has been changed by a concurrent update. Please try the operation again.\nConflicting commit: [commitinfo]\nRefer to [docs]/concurrency-control.html for more details.\n</code></pre> <p><code>metadataChangedException</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to check for logical conflicts with concurrent updates</li> </ul>"},{"location":"DeltaErrors/#protocolchangedexception","text":"","title":"protocolChangedException <pre><code>protocolChangedException(\n  conflictingCommit: Option[CommitInfo]): ProtocolChangedException\n</code></pre> <p><code>protocolChangedException</code> creates a ProtocolChangedException with the following message:</p> <pre><code>The protocol version of the Delta table has been changed by a concurrent update.\n[additionalInfo]\nPlease try the operation again.\nConflicting commit: [commitinfo]\nRefer to [docs]/concurrency-control.html for more details.\n</code></pre> <p><code>protocolChangedException</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to check for logical conflicts with concurrent updates</li> </ul>"},{"location":"DeltaErrors/#modifyappendonlytableexception","text":"","title":"modifyAppendOnlyTableException <pre><code>modifyAppendOnlyTableException: Throwable\n</code></pre> <p><code>modifyAppendOnlyTableException</code> throws an <code>UnsupportedOperationException</code>:</p> <pre><code>This table is configured to only allow appends. If you would like to permit updates or deletes, use 'ALTER TABLE &lt;table_name&gt; SET TBLPROPERTIES (appendOnly=false)'.\n</code></pre> <p><code>modifyAppendOnlyTableException</code> is used when:</p> <ul> <li><code>DeltaLog</code> is requested to assertRemovable</li> </ul>"},{"location":"DeltaErrors/#notnullcolumnmissingexception","text":"","title":"notNullColumnMissingException <pre><code>notNullColumnMissingException(\n  constraint: Constraints.NotNull): Throwable\n</code></pre> <p><code>notNullColumnMissingException</code> creates a InvariantViolationException with the following error message:</p> <pre><code>Column [name], which has a NOT NULL constraint, is missing from the data being written into the table.\n</code></pre> <p><code>notNullColumnMissingException</code>\u00a0is used when:</p> <ul> <li><code>DeltaInvariantCheckerExec</code> utility is used to buildInvariantChecks</li> </ul>"},{"location":"DeltaErrors/#reporting-post-commit-hook-failure","text":"","title":"Reporting Post-Commit Hook Failure <pre><code>postCommitHookFailedException(\n  failedHook: PostCommitHook,\n  failedOnCommitVersion: Long,\n  extraErrorMessage: String,\n  error: Throwable): Throwable\n</code></pre> <p><code>postCommitHookFailedException</code> throws a <code>RuntimeException</code>:</p> <pre><code>Committing to the Delta table version [failedOnCommitVersion] succeeded but error while executing post-commit hook [failedHook]: [extraErrorMessage]\n</code></pre> <p><code>postCommitHookFailedException</code> is used when:</p> <ul> <li><code>GenerateSymlinkManifestImpl</code> is requested to handleError</li> </ul>"},{"location":"DeltaFileFormat/","text":"<p><code>DeltaFileFormat</code> is an abstraction of format metadata that specify the file format of a delta table.</p>","title":"DeltaFileFormat"},{"location":"DeltaFileFormat/#contract","text":"","title":"Contract"},{"location":"DeltaFileFormat/#fileformat","text":"","title":"FileFormat <pre><code>fileFormat: FileFormat\n</code></pre> <p><code>FileFormat</code> (Spark SQL) of a delta table</p> <p>Default: <code>ParquetFileFormat</code> (Spark SQL)</p> <p>Used when:</p> <ul> <li><code>DeltaLog</code> is requested for a relation (in batch queries) and DataFrame</li> <li><code>DeltaCommand</code> is requested for a relation</li> <li><code>TransactionalWrite</code> is requested to write data out</li> </ul>"},{"location":"DeltaFileFormat/#implementations","text":"<ul> <li>Snapshot</li> </ul>","title":"Implementations"},{"location":"DeltaFileOperations/","text":"","title":"DeltaFileOperations Utilities"},{"location":"DeltaFileOperations/#listusinglogstore","text":"","title":"listUsingLogStore <pre><code>listUsingLogStore(\n  logStore: LogStore,\n  subDirs: Iterator[String],\n  recurse: Boolean,\n  hiddenFileNameFilter: String =&gt; Boolean): Iterator[SerializableFileStatus]\n</code></pre> <p><code>listUsingLogStore</code>...FIXME</p> <p><code>listUsingLogStore</code>\u00a0is used when:</p> <ul> <li><code>DeltaFileOperations</code> utility is used to recurseDirectories, recursiveListDirs and localListDirs</li> </ul>"},{"location":"DeltaFileOperations/#locallistdirs","text":"","title":"localListDirs <pre><code>localListDirs(\n  spark: SparkSession,\n  dirs: Seq[String],\n  recursive: Boolean = true,\n  fileFilter: String =&gt; Boolean = defaultHiddenFileFilter): Seq[SerializableFileStatus]\n</code></pre> <p><code>localListDirs</code>...FIXME</p> <p><code>localListDirs</code> seems not used.</p>"},{"location":"DeltaFileOperations/#recursedirectories","text":"","title":"recurseDirectories <pre><code>recurseDirectories(\n  logStore: LogStore,\n  filesAndDirs: Iterator[SerializableFileStatus],\n  hiddenFileNameFilter: String =&gt; Boolean): Iterator[SerializableFileStatus]\n</code></pre> <p><code>recurseDirectories</code>...FIXME</p> <p><code>recurseDirectories</code>\u00a0is used when:</p> <ul> <li><code>DeltaFileOperations</code> utility is used to listUsingLogStore and recursiveListDirs</li> </ul>"},{"location":"DeltaFileOperations/#recursivelistdirs","text":"","title":"recursiveListDirs <pre><code>recursiveListDirs(\n  spark: SparkSession,\n  subDirs: Seq[String],\n  hadoopConf: Broadcast[SerializableConfiguration],\n  hiddenFileNameFilter: String =&gt; Boolean = defaultHiddenFileFilter,\n  fileListingParallelism: Option[Int] = None): Dataset[SerializableFileStatus]\n</code></pre> <p><code>recursiveListDirs</code>...FIXME</p> <p><code>recursiveListDirs</code>\u00a0is used when:</p> <ul> <li><code>ManualListingFileManifest</code> is requested to doList</li> <li><code>VacuumCommand</code> utility is used to gc</li> </ul>"},{"location":"DeltaFileOperations/#trydeletenonrecursive","text":"","title":"tryDeleteNonRecursive <pre><code>tryDeleteNonRecursive(\n  fs: FileSystem,\n  path: Path,\n  tries: Int = 3): Boolean\n</code></pre> <p><code>tryDeleteNonRecursive</code>...FIXME</p> <p><code>tryDeleteNonRecursive</code>\u00a0is used when:</p> <ul> <li><code>VacuumCommandImpl</code> is requested to delete</li> </ul>"},{"location":"DeltaHistoryManager/","text":"<p><code>DeltaHistoryManager</code> is used for version and commit history of a delta table.</p>","title":"DeltaHistoryManager"},{"location":"DeltaHistoryManager/#creating-instance","text":"<p><code>DeltaHistoryManager</code> takes the following to be created:</p> <ul> <li> DeltaLog <li> Maximum number of keys (default: <code>1000</code>)  <p><code>DeltaHistoryManager</code> is created\u00a0when:</p> <ul> <li><code>DeltaLog</code> is requested for one</li> <li><code>DeltaTableOperations</code> is requested to execute history command</li> </ul>","title":"Creating Instance"},{"location":"DeltaHistoryManager/#version-and-commit-history","text":"","title":"Version and Commit History <pre><code>getHistory(\n  start: Long,\n  end: Option[Long] = None): Seq[CommitInfo]\ngetHistory(\n  limitOpt: Option[Int]): Seq[CommitInfo]\n</code></pre> <p><code>getHistory</code>...FIXME</p> <p><code>getHistory</code>\u00a0is used when:</p> <ul> <li><code>DeltaTableOperations</code> is requested to executeHistory (for DeltaTable.history operator)</li> <li>DescribeDeltaHistoryCommand is executed (for DESCRIBE HISTORY SQL command)</li> </ul>"},{"location":"DeltaHistoryManager/#getcommitinfo-utility","text":"","title":"getCommitInfo Utility <pre><code>getCommitInfo(\n  logStore: LogStore,\n  basePath: Path,\n  version: Long): CommitInfo\n</code></pre> <p><code>getCommitInfo</code>...FIXME</p>"},{"location":"DeltaHistoryManager/#getactivecommitattime","text":"","title":"getActiveCommitAtTime <pre><code>getActiveCommitAtTime(\n  timestamp: Timestamp,\n  canReturnLastCommit: Boolean,\n  mustBeRecreatable: Boolean = true,\n  canReturnEarliestCommit: Boolean = false): Commit\n</code></pre> <p><code>getActiveCommitAtTime</code>...FIXME</p> <p><code>getActiveCommitAtTime</code>\u00a0is used when:</p> <ul> <li><code>DeltaTableUtils</code> utility is used to resolveTimeTravelVersion</li> <li><code>DeltaSource</code> is requested for getStartingVersion</li> </ul>"},{"location":"DeltaLog/","text":"<p><code>DeltaLog</code> is a transaction log (change log) of all the changes to (the state of) a Delta table.</p>","title":"DeltaLog"},{"location":"DeltaLog/#creating-instance","text":"<p><code>DeltaLog</code> takes the following to be created:</p> <ul> <li> Log directory (Hadoop Path) <li> Data directory (Hadoop Path) <li> Options (<code>Map[String, String]</code>) <li> <code>Clock</code>  <p><code>DeltaLog</code> is created (indirectly via DeltaLog.apply utility) when:</p> <ul> <li>DeltaLog.forTable utility is used</li> </ul>","title":"Creating Instance"},{"location":"DeltaLog/#_delta_log-metadata-directory","text":"","title":"_delta_log Metadata Directory <p><code>DeltaLog</code> uses _delta_log metadata directory for the transaction log of a Delta table.</p> <p>The <code>_delta_log</code> directory is in the given data path directory (when created using DeltaLog.forTable utility).</p> <p>The <code>_delta_log</code> directory is resolved (in the DeltaLog.apply utility) using the application-wide Hadoop Configuration.</p> <p>Once resolved and turned into a qualified path, the <code>_delta_log</code> directory is cached.</p>"},{"location":"DeltaLog/#deltalog-cache","text":"","title":"DeltaLog Cache <pre><code>deltaLogCache: Cache[(Path, Map[String, String]), DeltaLog]\n</code></pre> <p><code>DeltaLog</code> uses Guava's Cache as a cache of <code>DeltaLog</code>s by their HDFS-qualified _delta_log directories (with their<code>fs.</code>-prefixed file system options).</p> <p><code>deltaLogCache</code> is part of <code>DeltaLog</code> Scala object and so becomes an application-wide cache by design (an object in Scala is available as a single instance).</p>"},{"location":"DeltaLog/#caching-deltalog-instance","text":"<p>A new instance of <code>DeltaLog</code> is added when DeltaLog.apply utility is used and the instance is not available for a path (and file system options).</p>","title":"Caching DeltaLog Instance"},{"location":"DeltaLog/#cache-size","text":"<p>The size of the cache is controlled by <code>delta.log.cacheSize</code> system property.</p>","title":"Cache Size"},{"location":"DeltaLog/#deltalog-instance-expiration","text":"<p><code>DeltaLog</code>s expire and are automatically removed from the <code>deltaLogCache</code> after 60 minutes (non-configurable) of inactivity. Upon expiration, <code>deltaLogCache</code> requests the Snapshot of the <code>DeltaLog</code> to uncache.</p>","title":"DeltaLog Instance Expiration"},{"location":"DeltaLog/#cache-clearance","text":"<p><code>deltaLogCache</code> is invalidated:</p> <ul> <li> <p>For a delta table using DeltaLog.invalidateCache utility</p> </li> <li> <p>For all delta tables using DeltaLog.clearCache utility</p> </li> </ul>","title":"Cache Clearance"},{"location":"DeltaLog/#deltalogfortable","text":"","title":"DeltaLog.forTable <pre><code>// There are many forTable's\nforTable(...): DeltaLog\n</code></pre> <p><code>forTable</code> is an utility that creates a DeltaLog with _delta_log directory (in the given <code>dataPath</code> directory).</p>"},{"location":"DeltaLog/#demo-creating-deltalog","text":"","title":"Demo: Creating DeltaLog <pre><code>import org.apache.spark.sql.SparkSession\nassert(spark.isInstanceOf[SparkSession])\n\nval dataPath = \"/tmp/delta/t1\"\nimport org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, dataPath)\n\nimport org.apache.hadoop.fs.Path\nval expected = new Path(s\"file:$dataPath/_delta_log/_last_checkpoint\")\nassert(deltaLog.LAST_CHECKPOINT == expected)\n</code></pre>"},{"location":"DeltaLog/#tableexists","text":"","title":"tableExists <pre><code>tableExists: Boolean\n</code></pre> <p><code>tableExists</code> requests the current Snapshot for the version and checks out whether it is <code>0</code> or higher.</p> <p>is used when:</p> <ul> <li><code>DeltaTable</code> utility is used to isDeltaTable</li> <li>DeltaUnsupportedOperationsCheck logical check rule is executed</li> <li><code>DeltaTableV2</code> is requested to toBaseRelation</li> </ul>"},{"location":"DeltaLog/#accessing-current-version","text":"","title":"Accessing Current Version <p>A common idiom (if not the only way) to know the current version of the delta table is to request the <code>DeltaLog</code> for the current state (snapshot) and then for the version.</p> <pre><code>import org.apache.spark.sql.delta.DeltaLog\nassert(deltaLog.isInstanceOf[DeltaLog])\n\nval deltaVersion = deltaLog.snapshot.version\nscala&gt; println(deltaVersion)\n5\n</code></pre>"},{"location":"DeltaLog/#initialization","text":"","title":"Initialization <p>When created, <code>DeltaLog</code> does the following:</p> <ol> <li> <p>Creates the LogStore based on spark.delta.logStore.class configuration property</p> </li> <li> <p>Initializes the current snapshot</p> </li> <li> <p>Updates state of the delta table when there is no metadata checkpoint (e.g. the version of the state is <code>-1</code>)</p> </li> </ol> <p>In other words, the version of (the <code>DeltaLog</code> of) a delta table is at version <code>0</code> at the very minimum.</p> <pre><code>assert(deltaLog.snapshot.version &gt;= 0)\n</code></pre>"},{"location":"DeltaLog/#filterfilelist","text":"","title":"filterFileList <pre><code>filterFileList(\n  partitionSchema: StructType,\n  files: DataFrame,\n  partitionFilters: Seq[Expression],\n  partitionColumnPrefixes: Seq[String] = Nil): DataFrame\n</code></pre> <p><code>filterFileList</code>...FIXME</p> <p><code>filterFileList</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to checkAndRetry</li> <li><code>PartitionFiltering</code> is requested to filesForScan</li> <li><code>WriteIntoDelta</code> is requested to write</li> <li><code>SnapshotIterator</code> is requested to iterator</li> <li><code>TahoeBatchFileIndex</code> is requested to matchingFiles</li> <li><code>DeltaDataSource</code> utility is requested to verifyAndCreatePartitionFilters</li> </ul>"},{"location":"DeltaLog/#fileformats","text":"","title":"FileFormats <p><code>DeltaLog</code> defines two <code>FileFormat</code>s (Spark SQL):</p> <ul> <li> <p> <code>ParquetFileFormat</code> for indices of delta files  <li> <p> <code>JsonFileFormat</code> for indices of checkpoint files   <p>These <code>FileFormat</code>s are used to create DeltaLogFileIndexes for Snapshots that in turn used them for stateReconstruction.</p>"},{"location":"DeltaLog/#logstore","text":"","title":"LogStore <p><code>DeltaLog</code> uses a LogStore for...FIXME</p>"},{"location":"DeltaLog/#executing-single-threaded-operation-in-new-transaction","text":"","title":"Executing Single-Threaded Operation in New Transaction <pre><code>withNewTransaction[T](\n  thunk: OptimisticTransaction =&gt; T): T\n</code></pre> <p><code>withNewTransaction</code> starts a new transaction (that is active for the whole thread) and executes the given <code>thunk</code> block.</p> <p>In the end, <code>withNewTransaction</code> makes the transaction no longer active.</p> <p><code>withNewTransaction</code> is used when:</p> <ul> <li> <p>DeleteCommand, MergeIntoCommand, UpdateCommand, and WriteIntoDelta commands are executed</p> </li> <li> <p><code>DeltaSink</code> is requested to add a streaming micro-batch</p> </li> </ul>"},{"location":"DeltaLog/#starting-new-transaction","text":"","title":"Starting New Transaction <pre><code>startTransaction(): OptimisticTransaction\n</code></pre> <p><code>startTransaction</code> updates and creates a new OptimisticTransaction (for this <code>DeltaLog</code>).</p>  <p>Note</p> <p><code>startTransaction</code> is a \"subset\" of withNewTransaction.</p>  <p><code>startTransaction</code> is used when:</p> <ul> <li> <p><code>DeltaLog</code> is requested to upgradeProtocol</p> </li> <li> <p><code>AlterDeltaTableCommand</code> is requested to startTransaction</p> </li> <li> <p>ConvertToDeltaCommand and CreateDeltaTableCommand are executed</p> </li> </ul>"},{"location":"DeltaLog/#throwing-unsupportedoperationexception-for-append-only-tables","text":"","title":"Throwing UnsupportedOperationException for Append-Only Tables <pre><code>assertRemovable(): Unit\n</code></pre> <p><code>assertRemovable</code> throws an <code>UnsupportedOperationException</code> for the appendOnly table property (in the Metadata) enabled (<code>true</code>):</p> <pre><code>This table is configured to only allow appends. If you would like to permit updates or deletes, use 'ALTER TABLE &lt;table_name&gt; SET TBLPROPERTIES (appendOnly=false)'.\n</code></pre> <p><code>assertRemovable</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to prepareCommit</li> <li>DeleteCommand, UpdateCommand, WriteIntoDelta (with <code>Overwrite</code> mode) are executed</li> <li><code>DeltaSink</code> is requested to addBatch (with <code>Complete</code> output mode)</li> </ul>"},{"location":"DeltaLog/#metadata","text":"","title":"metadata <pre><code>metadata: Metadata\n</code></pre> <p><code>metadata</code> is part of the Checkpoints abstraction.</p> <p><code>metadata</code> requests the current Snapshot for the metadata or creates a new one (if the current Snapshot is not initialized).</p>"},{"location":"DeltaLog/#update","text":"","title":"update <pre><code>update(\n  stalenessAcceptable: Boolean = false): Snapshot\n</code></pre> <p><code>update</code> branches off based on a combination of flags: the given <code>stalenessAcceptable</code> and isSnapshotStale.</p> <p>For the <code>stalenessAcceptable</code> not acceptable (default) and the snapshot not stale, <code>update</code> simply acquires the deltaLogLock lock and updateInternal (with <code>isAsync</code> flag off).</p> <p>For all other cases, <code>update</code>...FIXME</p> <p><code>update</code> is used when:</p> <ul> <li> <p><code>DeltaHistoryManager</code> is requested to getHistory, getActiveCommitAtTime, and checkVersionExists</p> </li> <li> <p><code>DeltaLog</code> is created (with no checkpoint created), and requested to startTransaction and withNewTransaction</p> </li> <li> <p><code>OptimisticTransactionImpl</code> is requested to doCommit and checkAndRetry</p> </li> <li> <p><code>ConvertToDeltaCommand</code> is requested to run and streamWrite</p> </li> <li> <p><code>VacuumCommand</code> utility is used to gc</p> </li> <li> <p><code>TahoeLogFileIndex</code> is requested for the (historical or latest) snapshot</p> </li> <li> <p><code>DeltaDataSource</code> is requested for a relation</p> </li> </ul>"},{"location":"DeltaLog/#tryupdate","text":"","title":"tryUpdate <pre><code>tryUpdate(\n  isAsync: Boolean = false): Snapshot\n</code></pre> <p><code>tryUpdate</code>...FIXME</p>"},{"location":"DeltaLog/#snapshot","text":"","title":"Snapshot <pre><code>snapshot: Snapshot\n</code></pre> <p><code>snapshot</code> returns the current snapshot.</p> <p><code>snapshot</code> is used when:</p> <ul> <li> <p>OptimisticTransaction is created</p> </li> <li> <p><code>Checkpoints</code> is requested to checkpoint</p> </li> <li> <p><code>DeltaLog</code> is requested for the metadata, to upgradeProtocol, getSnapshotAt, createRelation</p> </li> <li> <p><code>OptimisticTransactionImpl</code> is requested to getNextAttemptVersion</p> </li> <li> <p>DeleteCommand, DeltaGenerateCommand, DescribeDeltaDetailCommand, UpdateCommand commands are executed</p> </li> <li> <p>GenerateSymlinkManifest is executed</p> </li> <li> <p><code>DeltaCommand</code> is requested to buildBaseRelation</p> </li> <li> <p><code>TahoeFileIndex</code> is requested for the table version, partitionSchema</p> </li> <li> <p><code>TahoeLogFileIndex</code> is requested for the table size</p> </li> <li> <p><code>DeltaDataSource</code> is requested for the schema of the streaming delta source</p> </li> <li> <p>DeltaSource is created and requested for the getStartingOffset, getBatch</p> </li> </ul>"},{"location":"DeltaLog/#current-state-snapshot","text":"","title":"Current State Snapshot <pre><code>currentSnapshot: Snapshot\n</code></pre> <p><code>currentSnapshot</code> is a Snapshot based on the metadata checkpoint if available or a new <code>Snapshot</code> instance (with version being <code>-1</code>).</p>  <p>Note</p> <p>For a new <code>Snapshot</code> instance (with version being <code>-1</code>) <code>DeltaLog</code> immediately updates the state.</p>  <p>Internally, <code>currentSnapshot</code>...FIXME</p> <p><code>currentSnapshot</code> is available using snapshot method.</p> <p><code>currentSnapshot</code> is used when:</p> <ul> <li><code>DeltaLog</code> is requested to updateInternal, update and tryUpdate</li> </ul>"},{"location":"DeltaLog/#creating-insertable-hadoopfsrelation-for-batch-queries","text":"","title":"Creating Insertable HadoopFsRelation For Batch Queries <pre><code>createRelation(\n  partitionFilters: Seq[Expression] = Nil,\n  snapshotToUseOpt: Option[Snapshot] = None,\n  isTimeTravelQuery: Boolean = false,\n  cdcOptions: CaseInsensitiveStringMap = CaseInsensitiveStringMap.empty): BaseRelation\n</code></pre> <p><code>createRelation</code>...FIXME</p> <p><code>createRelation</code> creates a TahoeLogFileIndex for the data path, the given <code>partitionFilters</code> and a version (if defined).</p> <p><code>createRelation</code>...FIXME</p> <p>In the end, <code>createRelation</code> creates a <code>HadoopFsRelation</code> for the <code>TahoeLogFileIndex</code> and...FIXME. The <code>HadoopFsRelation</code> is also an InsertableRelation.</p> <p><code>createRelation</code> is used when:</p> <ul> <li><code>DeltaTableV2</code> is requested to toBaseRelation</li> <li><code>WriteIntoDeltaBuilder</code> is requested to buildForV1Write</li> <li><code>DeltaDataSource</code> is requested for a writable relation</li> </ul>"},{"location":"DeltaLog/#insert","text":"","title":"insert <pre><code>insert(\n  data: DataFrame,\n  overwrite: Boolean): Unit\n</code></pre> <p><code>insert</code>...FIXME</p> <p><code>insert</code> is part of the <code>InsertableRelation</code> (Spark SQL) abstraction.</p>"},{"location":"DeltaLog/#retrieving-state-of-delta-table-at-given-version","text":"","title":"Retrieving State Of Delta Table At Given Version <pre><code>getSnapshotAt(\n  version: Long,\n  commitTimestamp: Option[Long] = None,\n  lastCheckpointHint: Option[CheckpointInstance] = None): Snapshot\n</code></pre> <p><code>getSnapshotAt</code>...FIXME</p> <p><code>getSnapshotAt</code> is used when:</p> <ul> <li> <p><code>DeltaLog</code> is requested for a relation, and to updateInternal</p> </li> <li> <p><code>DeltaSource</code> is requested for the snapshot of a delta table at a given version</p> </li> <li> <p><code>TahoeLogFileIndex</code> is requested for historicalSnapshotOpt</p> </li> </ul>"},{"location":"DeltaLog/#checkpoint-interval","text":"","title":"Checkpoint Interval <pre><code>checkpointInterval: Int\n</code></pre> <p><code>checkpointInterval</code> is the current value of checkpointInterval table property (from the Metadata).</p> <p><code>checkpointInterval</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to postCommit</li> </ul>"},{"location":"DeltaLog/#changes-actions-of-delta-version-and-later","text":"","title":"Changes (Actions) Of Delta Version And Later <pre><code>getChanges(\n  startVersion: Long): Iterator[(Long, Seq[Action])]\n</code></pre> <p><code>getChanges</code> gives all actions (changes) per delta log file for the given <code>startVersion</code> of a delta table and later.</p> <pre><code>val dataPath = \"/tmp/delta/users\"\nimport org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, dataPath)\nassert(deltaLog.isInstanceOf[DeltaLog])\nval changesPerVersion = deltaLog.getChanges(startVersion = 0)\n</code></pre> <p>Internally, <code>getChanges</code> requests the LogStore for files that are lexicographically greater or equal to the delta log file for the given <code>startVersion</code> (in the logPath) and leaves only delta log files (e.g. files with numbers only as file name and <code>.json</code> file extension).</p> <p>For every delta file, <code>getChanges</code> requests the LogStore to read the JSON content (every line is an action), and then deserializes it to an action.</p> <p><code>getChanges</code> is used when:</p> <ul> <li><code>DeltaSource</code> is requested for the indexed file additions (FileAdd actions)</li> </ul>"},{"location":"DeltaLog/#creating-dataframe-for-given-addfiles","text":"","title":"Creating DataFrame For Given AddFiles <pre><code>createDataFrame(\n  snapshot: Snapshot,\n  addFiles: Seq[AddFile],\n  isStreaming: Boolean = false,\n  actionTypeOpt: Option[String] = None): DataFrame\n</code></pre> <p><code>createDataFrame</code> uses the action type based on the optional action type (if defined) or uses the following based on the <code>isStreaming</code> flag:</p> <ul> <li>streaming when <code>isStreaming</code> flag is enabled (<code>true</code>)</li> <li>batch when <code>isStreaming</code> flag is disabled (<code>false</code>)</li> </ul>  <p>Note</p> <p><code>actionTypeOpt</code> seems not to be defined ever.</p>  <p><code>createDataFrame</code> creates a new TahoeBatchFileIndex (for the action type, and the given AddFiles and Snapshot).</p> <p><code>createDataFrame</code> creates a <code>HadoopFsRelation</code> (Spark SQL) with the <code>TahoeBatchFileIndex</code> and the other properties based on the given <code>Snapshot</code> (and the associated Metadata).</p> <p>In the end, <code>createDataFrame</code> creates a <code>DataFrame</code> with a logical query plan with a <code>LogicalRelation</code> (Spark SQL) over the <code>HadoopFsRelation</code>.</p> <p><code>createDataFrame</code> is used when:</p> <ul> <li>MergeIntoCommand is executed</li> <li><code>DeltaSource</code> is requested for a DataFrame for data between start and end offsets</li> </ul>"},{"location":"DeltaLog/#minfileretentiontimestamp","text":"","title":"minFileRetentionTimestamp <pre><code>minFileRetentionTimestamp: Long\n</code></pre> <p><code>minFileRetentionTimestamp</code> is the timestamp that is tombstoneRetentionMillis before the current time (per the given Clock).</p> <p><code>minFileRetentionTimestamp</code> is used when:</p> <ul> <li> <p><code>DeltaLog</code> is requested for the currentSnapshot, to updateInternal, and to getSnapshotAt</p> </li> <li> <p><code>VacuumCommand</code> is requested for garbage collecting of a delta table</p> </li> </ul>"},{"location":"DeltaLog/#tombstoneretentionmillis","text":"","title":"tombstoneRetentionMillis <pre><code>tombstoneRetentionMillis: Long\n</code></pre> <p><code>tombstoneRetentionMillis</code> gives the value of deletedFileRetentionDuration table property (from the Metadata).</p> <p><code>tombstoneRetentionMillis</code> is used when:</p> <ul> <li> <p><code>DeltaLog</code> is requested for minFileRetentionTimestamp</p> </li> <li> <p><code>VacuumCommand</code> is requested for garbage collecting of a delta table</p> </li> </ul>"},{"location":"DeltaLog/#updateinternal","text":"","title":"updateInternal <pre><code>updateInternal(\n  isAsync: Boolean): Snapshot\n</code></pre> <p><code>updateInternal</code>...FIXME</p> <p><code>updateInternal</code> is used when:</p> <ul> <li><code>DeltaLog</code> is requested to update (directly or via tryUpdate)</li> </ul>"},{"location":"DeltaLog/#invalidating-cached-deltalog-instance-by-path","text":"","title":"Invalidating Cached DeltaLog Instance By Path <pre><code>invalidateCache(\n  spark: SparkSession,\n  dataPath: Path): Unit\n</code></pre> <p><code>invalidateCache</code>...FIXME</p> <p><code>invalidateCache</code> is a public API and does not seem to be used at all.</p>"},{"location":"DeltaLog/#protocolread","text":"","title":"protocolRead <pre><code>protocolRead(\n  protocol: Protocol): Unit\n</code></pre> <p><code>protocolRead</code>...FIXME</p> <p><code>protocolRead</code> is used when:</p> <ul> <li> <p><code>OptimisticTransactionImpl</code> is requested to validate and retry a commit</p> </li> <li> <p>Snapshot is created</p> </li> <li> <p><code>DeltaSource</code> is requested to verifyStreamHygieneAndFilterAddFiles</p> </li> </ul>"},{"location":"DeltaLog/#upgradeprotocol","text":"","title":"upgradeProtocol <pre><code>upgradeProtocol(\n  newVersion: Protocol = Protocol()): Unit\n</code></pre> <p><code>upgradeProtocol</code>...FIXME</p> <p><code>upgradeProtocol</code> is used when:</p> <ul> <li><code>DeltaTable</code> is requested to upgradeTableProtocol</li> </ul>"},{"location":"DeltaLog/#logstoreprovider","text":"","title":"LogStoreProvider <p><code>DeltaLog</code> is a LogStoreProvider.</p>"},{"location":"DeltaLog/#looking-up-cached-or-creating-new-deltalog-instance","text":"","title":"Looking Up Cached Or Creating New DeltaLog Instance <pre><code>apply(\n  spark: SparkSession,\n  rawPath: Path,\n  clock: Clock = new SystemClock): DeltaLog // (1)!\napply(\n  spark: SparkSession,\n  rawPath: Path,\n  options: Map[String, String],\n  clock: Clock): DeltaLog\n</code></pre> <ol> <li>Uses empty <code>options</code></li> </ol>  <p>Note</p> <p><code>rawPath</code> is a Hadoop Path to the _delta_log directory at the root of the data of a delta table.</p>  <p><code>apply</code> creates a Hadoop <code>Configuration</code> (perhaps with <code>fs.</code>-prefixed options when spark.databricks.delta.loadFileSystemConfigsFromDataFrameOptions configuration property is enabled).</p> <p><code>apply</code> resolves the raw path to be HDFS-qualified (using the given Hadoop <code>Path</code> to get a Hadoop <code>FileSystem</code>).</p> <p>In the end, <code>apply</code> looks up a <code>DeltaLog</code> for the HDFS-qualified path (with the file system options) in the deltaLogCache or creates (and caches) a new DeltaLog.</p>"},{"location":"DeltaLog/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.DeltaLog</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.delta.DeltaLog=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"DeltaLogFileIndex/","text":"<p><code>DeltaLogFileIndex</code> is a <code>FileIndex</code> (Spark SQL) for Snapshot (for the commit and checkpoint files).</p>","title":"DeltaLogFileIndex"},{"location":"DeltaLogFileIndex/#creating-instance","text":"<p><code>DeltaLogFileIndex</code> takes the following to be created:</p> <ul> <li>FileFormat</li> <li> Files (as Hadoop FileStatuses)  <p>While being created, <code>DeltaLogFileIndex</code> prints out the following INFO message to the logs:</p> <pre><code>Created [this]\n</code></pre> <p><code>DeltaLogFileIndex</code> is created (indirectly using apply utility) when <code>Snapshot</code> is requested for <code>DeltaLogFileIndex</code> for commit or checkpoint files.</p>","title":"Creating Instance"},{"location":"DeltaLogFileIndex/#fileformat","text":"","title":"FileFormat <p><code>DeltaLogFileIndex</code> is given a <code>FileFormat</code> (Spark SQL) when created:</p> <ul> <li><code>JsonFileFormat</code> (Spark SQL) for commit files</li> <li><code>ParquetFileFormat</code> (Spark SQL) for checkpoint files</li> </ul>"},{"location":"DeltaLogFileIndex/#text-representation","text":"","title":"Text Representation <pre><code>toString: String\n</code></pre> <p><code>toString</code> returns the following (using the given FileFormat, the number of files and their estimated size):</p> <pre><code>DeltaLogFileIndex([format], numFilesInSegment: [files], totalFileSize: [sizeInBytes])\n</code></pre>"},{"location":"DeltaLogFileIndex/#creating-deltalogfileindex","text":"","title":"Creating DeltaLogFileIndex <pre><code>apply(\n  format: FileFormat,\n  files: Seq[FileStatus]): Option[DeltaLogFileIndex]\n</code></pre> <p><code>apply</code> creates a new <code>DeltaLogFileIndex</code> (for a non-empty collection of files).</p> <p><code>apply</code> is used when <code>Snapshot</code> is requested for <code>DeltaLogFileIndex</code> for commit or checkpoint files.</p>"},{"location":"DeltaLogFileIndex/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.DeltaLogFileIndex</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.delta.DeltaLogFileIndex=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"DeltaLogging/","text":"<p><code>DeltaLogging</code> is a DeltaProgressReporter.</p>","title":"DeltaLogging"},{"location":"DeltaOptionParser/","text":"<p><code>DeltaOptionParser</code> is an abstraction of options for reading from and writing to delta tables.</p>","title":"DeltaOptionParser"},{"location":"DeltaOptionParser/#contract","text":"","title":"Contract"},{"location":"DeltaOptionParser/#sqlconf","text":"","title":"SQLConf <pre><code>sqlConf: SQLConf\n</code></pre> <p>Used when:</p> <ul> <li><code>DeltaWriteOptionsImpl</code> is requested for canMergeSchema</li> </ul>"},{"location":"DeltaOptionParser/#options","text":"","title":"Options <pre><code>options: CaseInsensitiveMap[String]\n</code></pre>"},{"location":"DeltaOptionParser/#implementations","text":"<ul> <li>DeltaReadOptions</li> <li>DeltaWriteOptions</li> <li>DeltaWriteOptionsImpl</li> </ul>","title":"Implementations"},{"location":"DeltaOptions/","text":"<p><code>DeltaOptions</code> is a DeltaWriteOptions and DeltaReadOptions.</p> <p><code>DeltaOptions</code> is a type-safe abstraction of write and read options.</p> <p><code>DeltaOptions</code> is used to create WriteIntoDelta command, DeltaSink, and DeltaSource.</p> <p><code>DeltaOptions</code> is a <code>Serializable</code> (Java) (so it can be used in Spark tasks).</p>","title":"DeltaOptions"},{"location":"DeltaOptions/#creating-instance","text":"<p><code>DeltaOptions</code> takes the following to be created:</p> <ul> <li> Case-Insensitive Options <li> <code>SQLConf</code> (Spark SQL)  <p>When created, <code>DeltaOptions</code> verifies the input options.</p> <p><code>DeltaOptions</code> is created\u00a0when:</p> <ul> <li><code>DeltaLog</code> is requested for a relation (for DeltaDataSource as a CreatableRelationProvider and a RelationProvider)</li> <li><code>DeltaCatalog</code> is requested to createDeltaTable</li> <li><code>WriteIntoDeltaBuilder</code> is requested to buildForV1Write</li> <li>CreateDeltaTableCommand is executed</li> <li><code>DeltaDataSource</code> is requested for a streaming source (to create a DeltaSource for Structured Streaming), a streaming sink (to create a DeltaSink for Structured Streaming), and for an insertable HadoopFsRelation</li> </ul>","title":"Creating Instance"},{"location":"DeltaOptions/#verifying-options","text":"","title":"Verifying Options <pre><code>verifyOptions(\n  options: CaseInsensitiveMap[String]): Unit\n</code></pre> <p><code>verifyOptions</code> finds invalid options among the input <code>options</code>.</p>  <p>Note</p> <p>In the open-source version <code>verifyOptions</code> does nothing. The underlying objects (<code>recordDeltaEvent</code> and the others) are no-ops.</p>  <p><code>verifyOptions</code> is used when:</p> <ul> <li><code>DeltaOptions</code> is created</li> <li><code>DeltaDataSource</code> is requested for a relation (for loading data in batch queries)</li> </ul>"},{"location":"DeltaProgressReporter/","text":"<p><code>DeltaProgressReporter</code> is an abstraction of progress reporters (loggers).</p>","title":"DeltaProgressReporter"},{"location":"DeltaProgressReporter/#implementations","text":"<ul> <li>DeltaLogging</li> </ul>","title":"Implementations"},{"location":"DeltaProgressReporter/#withstatuscode","text":"","title":"withStatusCode <pre><code>withStatusCode[T](\n  statusCode: String,\n  defaultMessage: String,\n  data: Map[String, Any] = Map.empty)(body: =&gt; T): T\n</code></pre> <p><code>withStatusCode</code> prints out the following INFO message to the logs:</p> <pre><code>[statusCode]: [defaultMessage]\n</code></pre> <p><code>withStatusCode</code> withJobDescription with the given <code>defaultMessage</code> and <code>body</code>.</p> <p><code>withStatusCode</code> prints out the following INFO message to the logs:</p> <pre><code>[statusCode]: Done\n</code></pre> <p><code>withStatusCode</code>\u00a0is used when:</p> <ul> <li><code>PartitionFiltering</code> is requested for the files to scan</li> <li><code>Snapshot</code> is requested for the state</li> <li>DeleteCommand, MergeIntoCommand, UpdateCommand are executed</li> <li><code>GenerateSymlinkManifest</code> is requested to recordManifestGeneration</li> </ul>"},{"location":"DeltaProgressReporter/#withjobdescription","text":"","title":"withJobDescription <pre><code>withJobDescription[U](\n  jobDesc: String)(body: =&gt; U): U\n</code></pre> <p><code>withJobDescription</code>...FIXME</p>"},{"location":"DeltaProgressReporter/#logging","text":"","title":"Logging <p>Since <code>DeltaProgressReporter</code> is an abstraction, logging is configured using the logger of the implementations.</p>"},{"location":"DeltaReadOptions/","text":"<p><code>DeltaReadOptions</code> is...FIXME</p>","title":"DeltaReadOptions"},{"location":"DeltaSQLConf/","text":"<p><code>DeltaSQLConf</code> contains spark.databricks.delta-prefixed configuration properties to configure behaviour of Delta Lake.</p>","title":"DeltaSQLConf \u2014 spark.databricks.delta Configuration Properties"},{"location":"DeltaSQLConf/#alterlocationbypassschemacheck","text":"","title":"alterLocation.bypassSchemaCheck <p>spark.databricks.delta.alterLocation.bypassSchemaCheck enables Alter Table Set Location on Delta to go through even if the Delta table in the new location has a different schema from the original Delta table</p> <p>Default: <code>false</code></p>"},{"location":"DeltaSQLConf/#checklatestschemaonread","text":"","title":"checkLatestSchemaOnRead <p>spark.databricks.delta.checkLatestSchemaOnRead enables a check that ensures that users won't read corrupt data if the source schema changes in an incompatible way.</p> <p>Default: <code>true</code></p> <p>Delta always tries to give users the latest version of table data without having to call <code>REFRESH TABLE</code> or redefine their DataFrames when used in the context of streaming. There is a possibility that the schema of the latest version of the table may be incompatible with the schema at the time of DataFrame creation.</p>"},{"location":"DeltaSQLConf/#checkpointpartsize","text":"","title":"checkpoint.partSize <p>spark.databricks.delta.checkpoint.partSize (internal) is the limit at which we will start parallelizing the checkpoint. We will attempt to write maximum of this many actions per checkpoint.</p> <p>Default: <code>5000000</code></p>"},{"location":"DeltaSQLConf/#commitinfoenabled","text":"","title":"commitInfo.enabled <p>spark.databricks.delta.commitInfo.enabled controls whether to log commit information into a Delta log.</p> <p>Default: <code>true</code></p>"},{"location":"DeltaSQLConf/#commitinfousermetadata","text":"","title":"commitInfo.userMetadata <p>spark.databricks.delta.commitInfo.userMetadata is an arbitrary user-defined metadata to include in CommitInfo (requires spark.databricks.delta.commitInfo.enabled).</p> <p>Default: (empty)</p>"},{"location":"DeltaSQLConf/#commitvalidationenabled","text":"","title":"commitValidation.enabled <p>spark.databricks.delta.commitValidation.enabled (internal) controls whether to perform validation checks before commit or not</p> <p>Default: <code>true</code></p>"},{"location":"DeltaSQLConf/#convertmetadatacheckenabled","text":"","title":"convert.metadataCheck.enabled <p>spark.databricks.delta.convert.metadataCheck.enabled enables validation during convert to delta, if there is a difference between the catalog table's properties and the Delta table's configuration, we should error.</p> <p>If disabled, merge the two configurations with the same semantics as update and merge</p> <p>Default: <code>true</code></p>"},{"location":"DeltaSQLConf/#dummyfilemanagernumoffiles","text":"","title":"dummyFileManager.numOfFiles <p>spark.databricks.delta.dummyFileManager.numOfFiles (internal) controls how many dummy files to write in DummyFileManager</p> <p>Default: <code>3</code></p>"},{"location":"DeltaSQLConf/#dummyfilemanagerprefix","text":"","title":"dummyFileManager.prefix <p>spark.databricks.delta.dummyFileManager.prefix (internal) is the file prefix to use in DummyFileManager</p> <p>Default: <code>.s3-optimization-</code></p>"},{"location":"DeltaSQLConf/#historymaxkeysperlist","text":"","title":"history.maxKeysPerList <p>spark.databricks.delta.history.maxKeysPerList (internal) controls how many commits to list when performing a parallel search.</p> <p>The default is the maximum keys returned by S3 per list call. Azure can return 5000, therefore we choose 1000.</p> <p>Default: <code>1000</code></p>"},{"location":"DeltaSQLConf/#historymetricsenabled","text":"","title":"history.metricsEnabled <p>spark.databricks.delta.history.metricsEnabled enables metrics reporting in <code>DESCRIBE HISTORY</code> (CommitInfo will record the operation metrics when a <code>OptimisticTransactionImpl</code> is committed and the spark.databricks.delta.commitInfo.enabled configuration property is enabled).</p> <p>Requires spark.databricks.delta.commitInfo.enabled configuration property to be enabled</p> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to getOperationMetrics</li> <li><code>ConvertToDeltaCommand</code> is requested to streamWrite</li> <li><code>SQLMetricsReporting</code> is requested to registerSQLMetrics</li> <li><code>TransactionalWrite</code> is requested to writeFiles</li> </ul>  <p>Github Commit</p> <p>The feature was added as part of [SC-24567][DELTA] Add additional metrics to Describe Delta History commit.</p>"},{"location":"DeltaSQLConf/#importbatchsizeschemainference","text":"","title":"import.batchSize.schemaInference <p>spark.databricks.delta.import.batchSize.schemaInference (internal) is the number of files per batch for schema inference during import.</p> <p>Default: <code>1000000</code></p>"},{"location":"DeltaSQLConf/#importbatchsizestatscollection","text":"","title":"import.batchSize.statsCollection <p>spark.databricks.delta.import.batchSize.statsCollection (internal) is the number of files per batch for stats collection during import.</p> <p>Default: <code>50000</code></p>"},{"location":"DeltaSQLConf/#loadfilesystemconfigsfromdataframeoptions","text":"","title":"loadFileSystemConfigsFromDataFrameOptions <p>spark.databricks.delta.loadFileSystemConfigsFromDataFrameOptions (internal) controls whether to load file systems configs provided in <code>DataFrameReader</code> or <code>DataFrameWriter</code> options when calling <code>DataFrameReader.load/DataFrameWriter.save</code> using a Delta table path.</p> <p>Not supported for <code>DataFrameReader.table</code> and <code>DataFrameWriter.saveAsTable</code></p> <p>Default: <code>true</code></p>"},{"location":"DeltaSQLConf/#maxsnapshotlineagelength","text":"","title":"maxSnapshotLineageLength <p>spark.databricks.delta.maxSnapshotLineageLength (internal) is the maximum lineage length of a Snapshot before Delta forces to build a Snapshot from scratch</p> <p>Default: <code>50</code></p>"},{"location":"DeltaSQLConf/#mergemaxinsertcount","text":"","title":"merge.maxInsertCount <p>spark.databricks.delta.merge.maxInsertCount (internal) is the maximum row count of inserts in each MERGE execution</p> <p>Default: <code>10000L</code></p>"},{"location":"DeltaSQLConf/#mergeoptimizeinsertonlymergeenabled","text":"","title":"merge.optimizeInsertOnlyMerge.enabled <p>spark.databricks.delta.merge.optimizeInsertOnlyMerge.enabled (internal) controls merge without any matched clause (i.e., insert-only merge) will be optimized by avoiding rewriting old files and just inserting new files</p> <p>Default: <code>true</code></p>"},{"location":"DeltaSQLConf/#mergeoptimizematchedonlymergeenabled","text":"","title":"merge.optimizeMatchedOnlyMerge.enabled <p>spark.databricks.delta.merge.optimizeMatchedOnlyMerge.enabled (internal) controls merge without 'when not matched' clause will be optimized to use a right outer join instead of a full outer join</p> <p>Default: <code>true</code></p>"},{"location":"DeltaSQLConf/#mergerepartitionbeforewriteenabled","text":"","title":"merge.repartitionBeforeWrite.enabled <p>spark.databricks.delta.merge.repartitionBeforeWrite.enabled (internal) controls merge will repartition the output by the table's partition columns before writing the files</p> <p>Default: <code>false</code></p>"},{"location":"DeltaSQLConf/#optimizemaxfilesize","text":"","title":"optimize.maxFileSize <p>spark.databricks.delta.optimize.maxFileSize (internal) Target file size produced by OPTIMIZE command.</p> <p>Default: <code>1024 * 1024 * 1024</code></p> <p>Used when:</p> <ul> <li><code>OptimizeExecutor</code> is requested to optimize</li> </ul>"},{"location":"DeltaSQLConf/#optimizemaxthreads","text":"","title":"optimize.maxThreads <p>spark.databricks.delta.optimize.maxThreads (internal) Maximum number of parallel jobs allowed in OPTIMIZE command. Increasing the maximum parallel jobs allows <code>OPTIMIZE</code> command to run faster, but increases the job management on the Spark driver.</p> <p>Default: <code>15</code></p> <p>Used when:</p> <ul> <li><code>OptimizeExecutor</code> is requested to optimize</li> </ul>"},{"location":"DeltaSQLConf/#optimizeminfilesize","text":"","title":"optimize.minFileSize <p>spark.databricks.delta.optimize.minFileSize (internal) Files which are smaller than this threshold (in bytes) will be grouped together and rewritten as larger files by the OPTIMIZE command.</p> <p>Default: <code>1024 * 1024 * 1024</code></p> <p>Used when:</p> <ul> <li><code>OptimizeExecutor</code> is requested to optimize</li> </ul>"},{"location":"DeltaSQLConf/#partitioncolumnvalidityenabled","text":"","title":"partitionColumnValidity.enabled <p>spark.databricks.delta.partitionColumnValidity.enabled (internal) enables validation of the partition column names (just like the data columns)</p> <p>Default: <code>true</code></p>"},{"location":"DeltaSQLConf/#propertiesdefaultsminreaderversion","text":"","title":"properties.defaults.minReaderVersion <p>spark.databricks.delta.properties.defaults.minReaderVersion is the default reader protocol version to create new tables with, unless a feature that requires a higher version for correctness is enabled.</p> <p>Default: <code>1</code></p> <p>Available values: <code>1</code></p> <p>Used when:</p> <ul> <li><code>Protocol</code> utility is used to create a Protocol</li> </ul>"},{"location":"DeltaSQLConf/#propertiesdefaultsminwriterversion","text":"","title":"properties.defaults.minWriterVersion <p>spark.databricks.delta.properties.defaults.minWriterVersion is the default writer protocol version to create new tables with, unless a feature that requires a higher version for correctness is enabled.</p> <p>Default: <code>2</code></p> <p>Available values: <code>1</code>, <code>2</code>, <code>3</code></p> <p>Used when:</p> <ul> <li><code>Protocol</code> utility is used to create a Protocol</li> </ul>"},{"location":"DeltaSQLConf/#retentiondurationcheckenabled","text":"","title":"retentionDurationCheck.enabled <p>spark.databricks.delta.retentionDurationCheck.enabled adds a check preventing users from running vacuum with a very short retention period, which may end up corrupting a Delta log.</p> <p>Default: <code>true</code></p>"},{"location":"DeltaSQLConf/#samplingenabled","text":"","title":"sampling.enabled <p>spark.databricks.delta.sampling.enabled (internal) enables sample-based estimation</p> <p>Default: <code>false</code></p>"},{"location":"DeltaSQLConf/#schemaautomergeenabled","text":"","title":"schema.autoMerge.enabled <p>spark.databricks.delta.schema.autoMerge.enabled enables schema merging on appends and overwrites.</p> <p>Default: <code>false</code></p> <p>Equivalent DataFrame option: mergeSchema</p>"},{"location":"DeltaSQLConf/#snapshotisolationenabled","text":"","title":"snapshotIsolation.enabled <p>spark.databricks.delta.snapshotIsolation.enabled (internal) controls whether queries on Delta tables are guaranteed to have snapshot isolation</p> <p>Default: <code>true</code></p>"},{"location":"DeltaSQLConf/#snapshotpartitions","text":"","title":"snapshotPartitions <p>spark.databricks.delta.snapshotPartitions (internal) is the number of partitions to use for state reconstruction (when building a snapshot of a Delta table).</p> <p>Default: <code>50</code></p>"},{"location":"DeltaSQLConf/#stalenesslimit","text":"","title":"stalenessLimit <p>spark.databricks.delta.stalenessLimit (in millis) allows you to query the last loaded state of the Delta table without blocking on a table update. You can use this configuration to reduce the latency on queries when up-to-date results are not a requirement. Table updates will be scheduled on a separate scheduler pool in a FIFO queue, and will share cluster resources fairly with your query. If a table hasn't updated past this time limit, we will block on a synchronous state update before running the query.</p> <p>Default: <code>0</code> (no tables can be stale)</p>"},{"location":"DeltaSQLConf/#statecorruptionisfatal","text":"","title":"state.corruptionIsFatal <p>spark.databricks.delta.state.corruptionIsFatal (internal) throws a fatal error when the recreated Delta State doesn't match committed checksum file</p> <p>Default: <code>true</code></p>"},{"location":"DeltaSQLConf/#statereconstructionvalidationenabled","text":"","title":"stateReconstructionValidation.enabled <p>spark.databricks.delta.stateReconstructionValidation.enabled (internal) controls whether to perform validation checks on the reconstructed state</p> <p>Default: <code>true</code></p>"},{"location":"DeltaSQLConf/#statscollect","text":"","title":"stats.collect <p>spark.databricks.delta.stats.collect (internal) enables statistics to be collected while writing files into a Delta table</p> <p>Default: <code>true</code></p>  <p>Note</p> <p>The property seems unused.</p>"},{"location":"DeltaSQLConf/#statslimitpushdownenabled","text":"","title":"stats.limitPushdown.enabled <p>spark.databricks.delta.stats.limitPushdown.enabled (internal) enables using the limit clause and file statistics to prune files before they are collected to the driver</p> <p>Default: <code>true</code></p>"},{"location":"DeltaSQLConf/#statslocalcachemaxnumfiles","text":"","title":"stats.localCache.maxNumFiles <p>spark.databricks.delta.stats.localCache.maxNumFiles (internal) is the maximum number of files for a table to be considered a delta small table. Some metadata operations (such as using data skipping) are optimized for small tables using driver local caching and local execution.</p> <p>Default: <code>2000</code></p>"},{"location":"DeltaSQLConf/#statsskipping","text":"","title":"stats.skipping <p>spark.databricks.delta.stats.skipping (internal) enables statistics used for skipping</p> <p>Default: <code>true</code></p>"},{"location":"DeltaSQLConf/#timetravelresolveonidentifierenabled","text":"","title":"timeTravel.resolveOnIdentifier.enabled <p>spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled (internal) controls whether to resolve patterns as <code>@v123</code> and <code>@yyyyMMddHHmmssSSS</code> in path identifiers as time travel nodes.</p> <p>Default: <code>true</code></p>"},{"location":"DeltaSQLConf/#vacuumparalleldeleteenabled","text":"","title":"vacuum.parallelDelete.enabled <p>spark.databricks.delta.vacuum.parallelDelete.enabled enables parallelizing the deletion of files during vacuum command.</p> <p>Default: <code>false</code></p> <p>Enabling may result hitting rate limits on some storage backends. When enabled, parallelization is controlled by the default number of shuffle partitions.</p>"},{"location":"DeltaSink/","text":"<p><code>DeltaSink</code> is the <code>Sink</code> (Spark Structured Streaming) of the delta data source for streaming queries.</p>","title":"DeltaSink"},{"location":"DeltaSink/#creating-instance","text":"<p><code>DeltaSink</code> takes the following to be created:</p> <ul> <li> <code>SQLContext</code> <li> Hadoop Path of the delta table (to write data to as configured by the path option) <li> Names of the partition columns <li> <code>OutputMode</code> (Spark Structured Streaming) <li> DeltaOptions  <p><code>DeltaSink</code> is created\u00a0when:</p> <ul> <li><code>DeltaDataSource</code> is requested for a streaming sink</li> </ul>","title":"Creating Instance"},{"location":"DeltaSink/#deltalog","text":"","title":"DeltaLog <pre><code>deltaLog: DeltaLog\n</code></pre> <p><code>deltaLog</code> is a DeltaLog that is created for the delta table when <code>DeltaSink</code> is created.</p> <p><code>deltaLog</code> is used when:</p> <ul> <li>DeltaSink is requested to add a streaming micro-batch</li> </ul>"},{"location":"DeltaSink/#adding-streaming-micro-batch","text":"","title":"Adding Streaming Micro-Batch <pre><code>addBatch(\n  batchId: Long,\n  data: DataFrame): Unit\n</code></pre> <p><code>addBatch</code> requests the DeltaLog to start a new transaction.</p> <p><code>addBatch</code> registers the following performance metrics.</p>    Name web UI     <code>numAddedFiles</code> number of files added.   <code>numRemovedFiles</code> number of files removed.    <p><code>addBatch</code> makes sure that <code>sql.streaming.queryId</code> local property is defined (attached to the query's current thread).</p> <p>If the batch reads the same delta table as this sink is going to write to, <code>addBatch</code> requests the <code>OptimisticTransaction</code> to readWholeTable.</p> <p><code>addBatch</code> updates the metadata.</p> <p><code>addBatch</code> determines the deleted files based on the OutputMode. For <code>Complete</code> output mode, <code>addBatch</code>...FIXME</p> <p><code>addBatch</code> requests the <code>OptimisticTransaction</code> to write data out.</p> <p><code>addBatch</code> updates the <code>numRemovedFiles</code> and <code>numAddedFiles</code> performance metrics, and requests the <code>OptimisticTransaction</code> to register the SQLMetrics.</p> <p>In the end, <code>addBatch</code> requests the <code>OptimisticTransaction</code> to commit (with a new SetTransaction, AddFiles and RemoveFiles, and StreamingUpdate operation).</p>  <p><code>addBatch</code> is part of the <code>Sink</code> (Spark Structured Streaming) abstraction.</p>"},{"location":"DeltaSink/#text-representation","text":"","title":"Text Representation <pre><code>toString(): String\n</code></pre> <p><code>DeltaSink</code> uses the following text representation (with the path):</p> <pre><code>DeltaSink[path]\n</code></pre>"},{"location":"DeltaSink/#implicitmetadataoperation","text":"","title":"ImplicitMetadataOperation <p><code>DeltaSink</code> is an ImplicitMetadataOperation.</p>"},{"location":"DeltaSource/","text":"<p><code>DeltaSource</code> is the <code>Source</code> (Spark Structured Streaming) of the delta data source for streaming queries.</p>","title":"DeltaSource"},{"location":"DeltaSource/#creating-instance","text":"<p><code>DeltaSource</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> DeltaLog <li> DeltaOptions <li> Filters (default: empty)  <p><code>DeltaSource</code> is created\u00a0when:</p> <ul> <li><code>DeltaDataSource</code> is requested for a streaming source</li> </ul>","title":"Creating Instance"},{"location":"DeltaSource/#demo","text":"<pre><code>val q = spark\n  .readStream               // Creating a streaming query\n  .format(\"delta\")          // Using delta data source\n  .load(\"/tmp/delta/users\") // Over data in a delta table\n  .writeStream\n  .format(\"memory\")\n  .option(\"queryName\", \"demo\")\n  .start\nimport org.apache.spark.sql.execution.streaming.{MicroBatchExecution, StreamingQueryWrapper}\nval plan = q.asInstanceOf[StreamingQueryWrapper]\n  .streamingQuery\n  .asInstanceOf[MicroBatchExecution]\n  .logicalPlan\nimport org.apache.spark.sql.execution.streaming.StreamingExecutionRelation\nval relation = plan.collect { case r: StreamingExecutionRelation =&gt; r }.head\n\nimport org.apache.spark.sql.delta.sources.DeltaSource\nassert(relation.source.asInstanceOf[DeltaSource])\n\nscala&gt; println(relation.source)\nDeltaSource[file:/tmp/delta/users]\n</code></pre>","title":"Demo"},{"location":"DeltaSource/#latest-available-offset","text":"","title":"Latest Available Offset <pre><code>latestOffset(\n  startOffset: streaming.Offset,\n  limit: ReadLimit): streaming.Offset\n</code></pre> <p><code>latestOffset</code>\u00a0is part of the <code>SupportsAdmissionControl</code> (Spark Structured Streaming) abstraction.</p> <p><code>latestOffset</code> determines the latest offset (currentOffset) based on whether the previousOffset internal registry is initialized or not.</p> <p><code>latestOffset</code> prints out the following DEBUG message to the logs (using the previousOffset internal registry).</p> <pre><code>previousOffset -&gt; currentOffset: [previousOffset] -&gt; [currentOffset]\n</code></pre> <p>In the end, <code>latestOffset</code> returns the previousOffset if defined or <code>null</code>.</p>"},{"location":"DeltaSource/#no-previousoffset","text":"","title":"No previousOffset <p>For no previousOffset, <code>getOffset</code> retrieves the starting offset (with a new AdmissionLimits for the given <code>ReadLimit</code>).</p>"},{"location":"DeltaSource/#previousoffset-available","text":"","title":"previousOffset Available <p>When the previousOffset is defined (which is when the <code>DeltaSource</code> is requested for another micro-batch), <code>latestOffset</code> gets the changes as an indexed AddFiles (with the previousOffset and a new AdmissionLimits for the given <code>ReadLimit</code>).</p> <p><code>latestOffset</code> takes the last AddFile if available.</p> <p>With no <code>AddFile</code>, <code>latestOffset</code> returns the previousOffset.</p> <p>With the previousOffset and the last indexed AddFile both available, <code>latestOffset</code> creates a new DeltaSourceOffset for the version, index, and <code>isLast</code> flag from the last indexed AddFile.</p>  <p>Note</p> <p><code>isStartingVersion</code> local value is enabled (<code>true</code>) when the following holds:</p> <ul> <li> <p>Version of the last indexed AddFile is equal to the reservoirVersion of the previous ending offset</p> </li> <li> <p>isStartingVersion flag of the previous ending offset is enabled (<code>true</code>)</p> </li> </ul>"},{"location":"DeltaSource/#getstartingoffset","text":"","title":"getStartingOffset <pre><code>getStartingOffset(\n  limits: Option[AdmissionLimits]): Option[Offset]\n</code></pre> <p><code>getStartingOffset</code>...FIXME (review me)</p> <p><code>getStartingOffset</code> requests the DeltaLog for the version of the delta table (by requesting for the current state (snapshot) and then for the version).</p> <p><code>getStartingOffset</code> takes the last file from the files added (with rate limit) for the version of the delta table, <code>-1L</code> as the <code>fromIndex</code>, and the <code>isStartingVersion</code> flag enabled (<code>true</code>).</p> <p><code>getStartingOffset</code> returns a new DeltaSourceOffset for the tableId, the version and the index of the last file added, and whether the last file added is the last file of its version.</p> <p><code>getStartingOffset</code> returns <code>None</code> (offset not available) when either happens:</p> <ul> <li> <p>the version of the delta table is negative (below <code>0</code>)</p> </li> <li> <p>no files were added in the version</p> </li> </ul> <p><code>getStartingOffset</code> throws an <code>AssertionError</code> when the version of the last file added is smaller than the delta table's version:</p> <pre><code>assertion failed: getChangesWithRateLimit returns an invalid version: [v] (expected: &gt;= [version])\n</code></pre>"},{"location":"DeltaSource/#getchangeswithratelimit","text":"","title":"getChangesWithRateLimit <pre><code>getChangesWithRateLimit(\n  fromVersion: Long,\n  fromIndex: Long,\n  isStartingVersion: Boolean): Iterator[IndexedFile]\n</code></pre> <p><code>getChangesWithRateLimit</code> gets the changes (as indexed AddFiles) for the given <code>fromVersion</code>, <code>fromIndex</code>, and <code>isStartingVersion</code> flag.</p>"},{"location":"DeltaSource/#getoffset","text":"","title":"getOffset <pre><code>getOffset: Option[Offset]\n</code></pre> <p><code>getOffset</code> is part of the <code>Source</code> (Spark Structured Streaming) abstraction.</p> <p><code>getOffset</code> has been replaced by the newer latestOffset and so throws an <code>UnsupportedOperationException</code> when called:</p> <pre><code>latestOffset(Offset, ReadLimit) should be called instead of this method\n</code></pre>"},{"location":"DeltaSource/#requesting-micro-batch-dataframe","text":"","title":"Requesting Micro-Batch DataFrame <pre><code>getBatch(\n  start: Option[Offset],\n  end: Offset): DataFrame\n</code></pre> <p><code>getBatch</code> is part of the <code>Source</code> (Spark Structured Streaming) abstraction.</p> <p><code>getBatch</code> creates an DeltaSourceOffset for the tableId (aka reservoirId) and the given <code>end</code> offset.</p> <p><code>getBatch</code> gets the changes...FIXME</p> <p><code>getBatch</code> prints out the following DEBUG message to the logs:</p> <pre><code>start: [start] end: [end] [addFiles]\n</code></pre> <p>In the end, <code>getBatch</code> requests the DeltaLog to createDataFrame (for the current snapshot of the DeltaLog, <code>addFiles</code> and <code>isStreaming</code> flag on).</p>"},{"location":"DeltaSource/#snapshot-management","text":"","title":"Snapshot Management <p><code>DeltaSource</code> uses internal registries for the DeltaSourceSnapshot and the version to avoid requesting the DeltaLog for getSnapshotAt.</p>"},{"location":"DeltaSource/#snapshot","text":"","title":"Snapshot <p><code>DeltaSource</code> uses <code>initialState</code> internal registry for the DeltaSourceSnapshot of the state of the delta table at the initialStateVersion.</p> <p><code>DeltaSourceSnapshot</code> is used for AddFiles of the delta table at a given version.</p> <p>Initially uninitialized (<code>null</code>).</p> <p><code>DeltaSourceSnapshot</code> is created (initialized) when uninitialized or the version requested is different from the current one.</p> <p><code>DeltaSourceSnapshot</code> is closed and dereferenced (<code>null</code>) when <code>DeltaSource</code> is requested to cleanUpSnapshotResources (due to version change, another micro-batch or stop).</p>"},{"location":"DeltaSource/#version","text":"","title":"Version <p><code>DeltaSource</code> uses <code>initialStateVersion</code> internal registry to keep track of the version of DeltaSourceSnapshot (when requested for AddFiles of the delta table at a given version).</p> <p>Changes (alongside the initialState) to the version requested when <code>DeltaSource</code> is requested for the snapshot at a given version (only when the versions are different)</p> <p>Used when:</p> <ul> <li><code>DeltaSource</code> is requested for AddFiles of the delta table at a given version and to cleanUpSnapshotResources (and unpersist the current snapshot)</li> </ul>"},{"location":"DeltaSource/#stopping","text":"","title":"Stopping <pre><code>stop(): Unit\n</code></pre> <p><code>stop</code> is part of the <code>Source</code> (Spark Structured Streaming) abstraction.</p> <p><code>stop</code> simply cleanUpSnapshotResources.</p>"},{"location":"DeltaSource/#previous-offset","text":"","title":"Previous Offset <p>Ending DeltaSourceOffset of the latest micro-batch</p> <p>Starts uninitialized (<code>null</code>).</p> <p>Used when <code>DeltaSource</code> is requested for the latest available offset.</p>"},{"location":"DeltaSource/#addfiles-of-delta-table-at-given-version","text":"","title":"AddFiles of Delta Table at Given Version <pre><code>getSnapshotAt(\n  version: Long): Iterator[IndexedFile]\n</code></pre> <p><code>getSnapshotAt</code> requests the DeltaSourceSnapshot for the data files (as indexed AddFiles).</p> <p>In case the DeltaSourceSnapshot hasn't been initialized yet (<code>null</code>) or the requested version is different from the initialStateVersion, <code>getSnapshotAt</code> does the following:</p> <ol> <li> <p>cleanUpSnapshotResources</p> </li> <li> <p>Requests the DeltaLog for the state (snapshot) of the delta table at the version</p> </li> <li> <p>Creates a new DeltaSourceSnapshot for the state (snapshot) as the current DeltaSourceSnapshot</p> </li> <li> <p>Changes the initialStateVersion internal registry to the requested version</p> </li> </ol> <p><code>getSnapshotAt</code> is used when:</p> <ul> <li><code>DeltaSource</code> is requested to getChanges (with <code>isStartingVersion</code> flag enabled)</li> </ul>"},{"location":"DeltaSource/#getchanges","text":"","title":"getChanges <pre><code>getChanges(\n  fromVersion: Long,\n  fromIndex: Long,\n  isStartingVersion: Boolean): Iterator[IndexedFile]\n</code></pre> <p><code>getChanges</code> branches based on <code>isStartingVersion</code> flag (enabled or not):</p> <ul> <li> <p>For <code>isStartingVersion</code> flag enabled (<code>true</code>), <code>getChanges</code> gets the state (snapshot) for the given <code>fromVersion</code> followed by (filtered out) indexed AddFiles for the next version after the given <code>fromVersion</code></p> </li> <li> <p>For <code>isStartingVersion</code> flag disabled (<code>false</code>), <code>getChanges</code> simply gives (filtered out) indexed AddFiles for the given <code>fromVersion</code></p> </li> </ul>  <p>Note</p> <p><code>isStartingVersion</code> flag simply adds the state (snapshot) before (filtered out) indexed AddFiles when enabled (<code>true</code>).</p> <p><code>isStartingVersion</code> flag is enabled when <code>DeltaSource</code> is requested for the following:</p> <ul> <li> <p>Micro-batch with data between start and end offsets and the start offset is not given or is for the starting version</p> </li> <li> <p>Latest available offset with no end offset of the latest micro-batch or the end offset of the latest micro-batch for the starting version</p> </li> </ul>  <p>In the end, <code>getChanges</code> filters out (excludes) indexed AddFiles that are not with the version later than the given <code>fromVersion</code> or the index greater than the given <code>fromIndex</code>.</p> <p><code>getChanges</code> is used when:</p> <ul> <li><code>DeltaSource</code> is requested for the latest available offset (when requested for the files added (with rate limit)) and getBatch</li> </ul>"},{"location":"DeltaSource/#filterandindexdeltalogs","text":"","title":"filterAndIndexDeltaLogs <pre><code>filterAndIndexDeltaLogs(\n  startVersion: Long): Iterator[IndexedFile]\n</code></pre> <p><code>filterAndIndexDeltaLogs</code>...FIXME</p>"},{"location":"DeltaSource/#verifystreamhygieneandfilteraddfiles","text":"","title":"verifyStreamHygieneAndFilterAddFiles <pre><code>verifyStreamHygieneAndFilterAddFiles(\n  actions: Seq[Action],\n  version: Long): Seq[Action]\n</code></pre> <p><code>verifyStreamHygieneAndFilterAddFiles</code>...FIXME</p>"},{"location":"DeltaSource/#cleanupsnapshotresources","text":"","title":"cleanUpSnapshotResources <pre><code>cleanUpSnapshotResources(): Unit\n</code></pre> <p><code>cleanUpSnapshotResources</code> does the following when the initial DeltaSourceSnapshot internal registry is not empty:</p> <ul> <li>Requests the DeltaSourceSnapshot to close (with the <code>unpersistSnapshot</code> flag based on whether the initialStateVersion is earlier than the snapshot version)</li> <li>Dereferences (nullifies) the DeltaSourceSnapshot</li> </ul> <p>Otherwise, <code>cleanUpSnapshotResources</code> does nothing.</p> <p><code>cleanUpSnapshotResources</code> is used when:</p> <ul> <li><code>DeltaSource</code> is requested to getSnapshotAt, getBatch and stop</li> </ul>"},{"location":"DeltaSource/#readlimit","text":"","title":"ReadLimit <pre><code>getDefaultReadLimit: ReadLimit\n</code></pre> <p><code>getDefaultReadLimit</code>\u00a0is part of the <code>SupportsAdmissionControl</code> (Spark Structured Streaming) abstraction.</p> <p><code>getDefaultReadLimit</code> creates a AdmissionLimits and requests it for a corresponding ReadLimit.</p>"},{"location":"DeltaSource/#retrieving-last-element-from-iterator","text":"","title":"Retrieving Last Element From Iterator <pre><code>iteratorLast[T](\n  iter: Iterator[T]): Option[T]\n</code></pre> <p><code>iteratorLast</code> simply returns the last element of the given <code>Iterator</code> (Scala) or <code>None</code>.</p> <p><code>iteratorLast</code> is used when:</p> <ul> <li><code>DeltaSource</code> is requested to getStartingOffset and getOffset</li> </ul>"},{"location":"DeltaSource/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.sources.DeltaSource</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.delta.sources.DeltaSource=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"DeltaSourceOffset/","text":"<p><code>DeltaSourceOffset</code> is an <code>Offset</code> (Spark Structured Streaming) for DeltaSource.</p>","title":"DeltaSourceOffset \u2014 Streaming Offset"},{"location":"DeltaSourceOffset/#creating-instance","text":"<p><code>DeltaSourceOffset</code> takes the following to be created:</p> <ul> <li> Source Version <li> Reservoir ID (aka Table ID) <li> Reservoir Version <li> Index <li> <code>isStartingVersion</code> flag  <p><code>DeltaSourceOffset</code> is created (using apply utility)\u00a0when:</p> <ul> <li><code>DeltaSource</code> is requested for the starting and latest offsets</li> </ul>","title":"Creating Instance"},{"location":"DeltaSourceOffset/#creating-deltasourceoffset","text":"","title":"Creating DeltaSourceOffset <pre><code>apply(\n  reservoirId: String,\n  offset: Offset): DeltaSourceOffset\napply(\n  reservoirId: String,\n  reservoirVersion: Long,\n  index: Long,\n  isStartingVersion: Boolean): DeltaSourceOffset\n</code></pre> <p><code>apply</code> creates a DeltaSourceOffset (for the version and the given arguments) or converts a <code>SerializedOffset</code> to a <code>DeltaSourceOffset</code>.</p> <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>DeltaSource</code> is requested for the starting and latest offsets</li> </ul>"},{"location":"DeltaSourceOffset/#validatesourceversion","text":"","title":"validateSourceVersion <pre><code>validateSourceVersion(\n  json: String): Unit\n</code></pre> <p><code>validateSourceVersion</code>...FIXME</p>"},{"location":"DeltaSourceOffset/#source-version","text":"","title":"Source Version <p><code>DeltaSourceOffset</code> uses <code>1</code> for the version (and does not allow changing it).</p> <p>The version is used when:</p> <ul> <li>DeltaSourceOffset.apply and validateSourceVersion utilities are used</li> </ul>"},{"location":"DeltaSourceSnapshot/","text":"<p><code>DeltaSourceSnapshot</code> is a SnapshotIterator and a StateCache for DeltaSource.</p>","title":"DeltaSourceSnapshot"},{"location":"DeltaSourceSnapshot/#creating-instance","text":"<p><code>DeltaSourceSnapshot</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> Snapshot <li> Filter Expressions (Spark SQL)  <p><code>DeltaSourceSnapshot</code> is created\u00a0when:</p> <ul> <li><code>DeltaSource</code> is requested for the snapshot of a delta table at a given version</li> </ul>","title":"Creating Instance"},{"location":"DeltaSourceSnapshot/#initial-files-indexed-addfiles","text":"","title":"Initial Files (Indexed AddFiles) <pre><code>initialFiles: Dataset[IndexedFile]\n</code></pre>"},{"location":"DeltaSourceSnapshot/#dataset-of-indexed-addfiles","text":"","title":"Dataset of Indexed AddFiles <p><code>initialFiles</code> requests the Snapshot for all AddFiles (in the snapshot) (<code>Dataset[AddFile]</code>).</p> <p><code>initialFiles</code> sorts the AddFile dataset (<code>Dataset[AddFile]</code>) by modificationTime and path in ascending order.</p> <p><code>initialFiles</code> indexes the <code>AddFiles</code> (using <code>RDD.zipWithIndex</code> operator) that gives a <code>RDD[(AddFile, Long)]</code>.</p> <p><code>initialFiles</code> converts the <code>RDD</code> to a <code>DataFrame</code> of two columns: <code>add</code> and <code>index</code>.</p> <p><code>initialFiles</code> adds the two new columns:</p> <ul> <li>version</li> <li><code>isLast</code> as <code>false</code> literal</li> </ul> <p><code>initialFiles</code> converts (projects) <code>DataFrame</code> to <code>Dataset[IndexedFile]</code>.</p>"},{"location":"DeltaSourceSnapshot/#creating-cachedds","text":"","title":"Creating CachedDS <p><code>initialFiles</code> creates a CachedDS with the following name (with the version and path of the Snapshot):</p> <pre><code>Delta Source Snapshot #[version] - [path]\n</code></pre>"},{"location":"DeltaSourceSnapshot/#cached-dataset-of-indexed-addfiles","text":"","title":"Cached Dataset of Indexed AddFiles <p>In the end, <code>initialFiles</code> requests the CachedDS to getDS.</p>"},{"location":"DeltaSourceSnapshot/#usage","text":"","title":"Usage <p><code>initialFiles</code> is used when:</p> <ul> <li><code>SnapshotIterator</code> is requested for the AddFiles</li> </ul>"},{"location":"DeltaSourceSnapshot/#closing","text":"","title":"Closing <pre><code>close(\n  unpersistSnapshot: Boolean): Unit\n</code></pre> <p><code>close</code>\u00a0is part of the SnapshotIterator abstraction.</p> <p><code>close</code> requests the Snapshot to uncache when the given <code>unpersistSnapshot</code> flag is enabled.</p>"},{"location":"DeltaSourceUtils/","text":"","title":"DeltaSourceUtils Utility"},{"location":"DeltaSourceUtils/#deltagenerationexpression","text":"","title":"delta.generationExpression <p><code>DeltaSourceUtils</code> defines <code>delta.generationExpression</code> metadata key for the generation expression of a generated column of a delta table.</p> <p>Used when:</p> <ul> <li>GeneratedColumn utility is used</li> <li>others (less important)</li> </ul>"},{"location":"DeltaSparkSessionExtension/","text":"<p>DeltaSparkSessionExtension is used to register (inject) the following extensions to a <code>SparkSession</code>:</p> <ul> <li>Delta SQL support (using DeltaSqlParser)</li> <li>DeltaAnalysis logical resolution rule</li> <li>DeltaUnsupportedOperationsCheck</li> <li>PreprocessTableUpdate logical resolution rule</li> <li>PreprocessTableMerge logical resolution rule</li> <li>PreprocessTableDelete logical resolution rule</li> <li>ActiveOptimisticTransactionRule logical optimization rule</li> </ul> <p><code>DeltaSparkSessionExtension</code> is registered using spark.sql.extensions configuration property (while creating a SparkSession in a Spark application).</p>","title":"DeltaSparkSessionExtension"},{"location":"DeltaTable/","text":"<p><code>DeltaTable</code> is the management interface of delta tables.</p>","title":"DeltaTable"},{"location":"DeltaTable/#iodeltatables-package","text":"<p><code>DeltaTable</code> belongs to <code>io.delta.tables</code> package.</p> <pre><code>import io.delta.tables.DeltaTable\n</code></pre>","title":"io.delta.tables Package"},{"location":"DeltaTable/#creating-instance","text":"<p><code>DeltaTable</code> takes the following to be created:</p> <ul> <li> Table Data (<code>Dataset[Row]</code>) <li> DeltaTableV2  <p><code>DeltaTable</code> is created using DeltaTable.forPath and DeltaTable.forName utilities (and indirectly using create, createIfNotExists, createOrReplace and replace).</p>","title":"Creating Instance"},{"location":"DeltaTable/#deltalog","text":"","title":"DeltaLog <pre><code>deltaLog: DeltaLog\n</code></pre> <p>DeltaLog of the DeltaTableV2.</p>"},{"location":"DeltaTable/#utilities-static-methods","text":"","title":"Utilities (Static Methods)"},{"location":"DeltaTable/#columnbuilder","text":"","title":"columnBuilder <pre><code>columnBuilder(\n  colName: String): DeltaColumnBuilder\ncolumnBuilder(\n  spark: SparkSession,\n  colName: String): DeltaColumnBuilder\n</code></pre> <p>Creates a DeltaColumnBuilder</p>"},{"location":"DeltaTable/#converttodelta","text":"","title":"convertToDelta <pre><code>convertToDelta(\n  spark: SparkSession,\n  identifier: String): DeltaTable\nconvertToDelta(\n  spark: SparkSession,\n  identifier: String,\n  partitionSchema: String): DeltaTable\nconvertToDelta(\n  spark: SparkSession,\n  identifier: String,\n  partitionSchema: StructType): DeltaTable\n</code></pre> <p><code>convertToDelta</code> converts the parquet table to delta format</p>  <p>Note</p> <p>Refer to Demo: Converting Parquet Dataset Into Delta Format for a demo of <code>DeltaTable.convertToDelta</code>.</p>"},{"location":"DeltaTable/#create","text":"","title":"create <pre><code>create(): DeltaTableBuilder\ncreate(\n  spark: SparkSession): DeltaTableBuilder\n</code></pre> <p>Creates a DeltaTableBuilder</p>"},{"location":"DeltaTable/#createifnotexists","text":"","title":"createIfNotExists <pre><code>createIfNotExists(): DeltaTableBuilder\ncreateIfNotExists(\n  spark: SparkSession): DeltaTableBuilder\n</code></pre> <p>Creates a DeltaTableBuilder (with <code>CreateTableOptions</code> and <code>ifNotExists</code> flag enabled)</p>"},{"location":"DeltaTable/#createorreplace","text":"","title":"createOrReplace <pre><code>createOrReplace(): DeltaTableBuilder\ncreateOrReplace(\n  spark: SparkSession): DeltaTableBuilder\n</code></pre> <p>Creates a DeltaTableBuilder (with <code>ReplaceTableOptions</code> and <code>orCreate</code> flag enabled)</p>"},{"location":"DeltaTable/#forname","text":"","title":"forName <pre><code>forName(\n  sparkSession: SparkSession,\n  tableName: String): DeltaTable\nforName(\n  tableOrViewName: String): DeltaTable\n</code></pre> <p><code>forName</code> uses <code>ParserInterface</code> (of the given <code>SparkSession</code>) to parse the given table name.</p> <p><code>forName</code> checks whether the given table name is of a Delta table and, if so, creates a DeltaTable with the following:</p> <ul> <li><code>Dataset</code> that represents loading data from the specified table name (using <code>SparkSession.table</code> operator)</li> <li>DeltaTableV2</li> </ul> <p><code>forName</code> throws an <code>AnalysisException</code> when the given table name is for non-Delta table:</p> <pre><code>[deltaTableIdentifier] is not a Delta table.\n</code></pre>"},{"location":"DeltaTable/#forpath","text":"","title":"forPath <pre><code>forPath(\n  sparkSession: SparkSession,\n  path: String): DeltaTable\nforPath(\n  path: String): DeltaTable\n</code></pre> <p><code>forPath</code> checks whether the given table name is of a Delta table and, if so, creates a DeltaTable with the following:</p> <ul> <li><code>Dataset</code> that represents loading data from the specified <code>path</code> using delta data source</li> <li>DeltaTableV2</li> </ul> <p><code>forPath</code> throws an <code>AnalysisException</code> when the given <code>path</code> does not belong to a delta table:</p> <pre><code>[deltaTableIdentifier] is not a Delta table.\n</code></pre>"},{"location":"DeltaTable/#isdeltatable","text":"","title":"isDeltaTable <pre><code>isDeltaTable(\n  sparkSession: SparkSession,\n  identifier: String): Boolean\nisDeltaTable(\n  identifier: String): Boolean\n</code></pre> <p><code>isDeltaTable</code>...FIXME</p>"},{"location":"DeltaTable/#replace","text":"","title":"replace <pre><code>replace(): DeltaTableBuilder\nreplace(\n  spark: SparkSession): DeltaTableBuilder\n</code></pre> <p>Creates a DeltaTableBuilder (with <code>ReplaceTableOptions</code> and <code>orCreate</code> flag disabled)</p>"},{"location":"DeltaTable/#operators","text":"","title":"Operators"},{"location":"DeltaTable/#alias","text":"","title":"alias <pre><code>alias(\n  alias: String): DeltaTable\n</code></pre> <p>Applies an alias to the <code>DeltaTable</code> (equivalent to as)</p>"},{"location":"DeltaTable/#as","text":"","title":"as <pre><code>as(\n  alias: String): DeltaTable\n</code></pre> <p>Applies an alias to the <code>DeltaTable</code></p>"},{"location":"DeltaTable/#delete","text":"","title":"delete <pre><code>delete(): Unit\ndelete(\n  condition: Column): Unit\ndelete(\n  condition: String): Unit\n</code></pre> <p>Executes DeleteFromTable command</p>"},{"location":"DeltaTable/#generate","text":"","title":"generate <pre><code>generate(\n  mode: String): Unit\n</code></pre> <p>Executes the DeltaGenerateCommand</p>"},{"location":"DeltaTable/#history","text":"","title":"history <pre><code>history(): DataFrame\nhistory(\n  limit: Int): DataFrame\n</code></pre> <p>Requests the DeltaHistoryManager for history.</p>"},{"location":"DeltaTable/#merge","text":"","title":"merge <pre><code>merge(\n  source: DataFrame,\n  condition: Column): DeltaMergeBuilder\nmerge(\n  source: DataFrame,\n  condition: String): DeltaMergeBuilder\n</code></pre> <p>Creates a DeltaMergeBuilder</p>"},{"location":"DeltaTable/#todf","text":"","title":"toDF <pre><code>toDF: Dataset[Row]\n</code></pre> <p>Returns the DataFrame representation of the DeltaTable</p>"},{"location":"DeltaTable/#update","text":"","title":"update <pre><code>update(\n  condition: Column,\n  set: Map[String, Column]): Unit\nupdate(\n  set: Map[String, Column]): Unit\n</code></pre> <p>Executes UpdateTable command</p>"},{"location":"DeltaTable/#updateexpr","text":"","title":"updateExpr <pre><code>updateExpr(\n  set: Map[String, String]): Unit\nupdateExpr(\n  condition: String,\n  set: Map[String, String]): Unit\n</code></pre> <p>Executes UpdateTable command</p>"},{"location":"DeltaTable/#upgradetableprotocol","text":"","title":"upgradeTableProtocol <pre><code>upgradeTableProtocol(\n  readerVersion: Int,\n  writerVersion: Int): Unit\n</code></pre> <p>Updates the protocol version of the table to leverage new features.</p> <p>Upgrading the reader version will prevent all clients that have an older version of Delta Lake from accessing this table.</p> <p>Upgrading the writer version will prevent older versions of Delta Lake to write to this table.</p> <p>The reader or writer version cannot be downgraded.</p> <p>Internally, <code>upgradeTableProtocol</code> creates a new Protocol (with the given versions) and requests the DeltaLog to upgradeProtocol.</p>  [SC-44271][DELTA] Introduce default protocol version for Delta tables <p><code>upgradeTableProtocol</code> was introduced in [SC-44271][DELTA] Introduce default protocol version for Delta tables commit.</p>"},{"location":"DeltaTable/#vacuum","text":"","title":"vacuum <pre><code>vacuum(): DataFrame\nvacuum(\n  retentionHours: Double): DataFrame\n</code></pre> <p>Deletes files and directories (recursively) in the DeltaTable that are not needed by the table (and maintains older versions up to the given retention threshold).</p>"},{"location":"DeltaTableBuilder/","text":"<p><code>DeltaTableBuilder</code> is a builder interface to create DeltaTables programmatically.</p> <p><code>DeltaTableBuilder</code> is created using the following DeltaTable utilities:</p> <ul> <li>DeltaTable.create</li> <li>DeltaTable.createIfNotExists</li> <li>DeltaTable.replace</li> <li>DeltaTable.createOrReplace</li> </ul> <p>In the end, <code>DeltaTableBuilder</code> is supposed to be executed to take action.</p>","title":"DeltaTableBuilder"},{"location":"DeltaTableBuilder/#iodeltatables-package","text":"<p><code>DeltaTableBuilder</code> belongs to <code>io.delta.tables</code> package.</p> <pre><code>import io.delta.tables.DeltaTableBuilder\n</code></pre>","title":"io.delta.tables Package"},{"location":"DeltaTableBuilder/#creating-instance","text":"<p><code>DeltaTableBuilder</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> <code>DeltaTableBuilderOptions</code>","title":"Creating Instance"},{"location":"DeltaTableBuilder/#operators","text":"","title":"Operators"},{"location":"DeltaTableBuilder/#addcolumn","text":"","title":"addColumn <pre><code>addColumn(\n  colName: String,\n  dataType: DataType): DeltaTableBuilder\naddColumn(\n  colName: String,\n  dataType: DataType,\n  nullable: Boolean): DeltaTableBuilder\naddColumn(\n  colName: String,\n  dataType: String): DeltaTableBuilder\naddColumn(\n  colName: String,\n  dataType: String,\n  nullable: Boolean): DeltaTableBuilder\naddColumn(\n  col: StructField): DeltaTableBuilder\n</code></pre> <p>Adds a column (that could be defined using DeltaColumnBuilder)</p>"},{"location":"DeltaTableBuilder/#addcolumns","text":"","title":"addColumns <pre><code>addColumns(\n  cols: StructType): DeltaTableBuilder\n</code></pre> <p>Adds columns based on the given <code>StructType</code> (Spark SQL)</p>"},{"location":"DeltaTableBuilder/#comment","text":"","title":"comment <pre><code>comment(\n  comment: String): DeltaTableBuilder\n</code></pre>"},{"location":"DeltaTableBuilder/#execute","text":"","title":"execute <pre><code>execute(): DeltaTable\n</code></pre> <p>Creates a DeltaTable</p>"},{"location":"DeltaTableBuilder/#location","text":"","title":"location <pre><code>location(\n  location: String): DeltaTableBuilder\n</code></pre>"},{"location":"DeltaTableBuilder/#partitionedby","text":"","title":"partitionedBy <pre><code>partitionedBy(\n  colNames: String*): DeltaTableBuilder\n</code></pre>"},{"location":"DeltaTableBuilder/#property","text":"","title":"property <pre><code>property(\n  key: String,\n  value: String): DeltaTableBuilder\n</code></pre>"},{"location":"DeltaTableBuilder/#tablename","text":"","title":"tableName <pre><code>tableName(\n  identifier: String): DeltaTableBuilder\n</code></pre>"},{"location":"DeltaTableIdentifier/","text":"<p><code>DeltaTableIdentifier</code> is an identifier of a delta table by TableIdentifier or directory depending whether it is a catalog table or not (and living non-cataloged).</p>","title":"DeltaTableIdentifier"},{"location":"DeltaTableIdentifier/#creating-instance","text":"<p><code>DeltaTableIdentifier</code> takes the following to be created:</p> <ul> <li> Directory (default: undefined) <li> <code>TableIdentifier</code> (default: undefined)","title":"Creating Instance"},{"location":"DeltaTableIdentifier/#creating-deltatableidentifier","text":"","title":"Creating DeltaTableIdentifier <pre><code>apply(\n  spark: SparkSession,\n  identifier: TableIdentifier): Option[DeltaTableIdentifier]\n</code></pre> <p><code>apply</code> creates a new DeltaTableIdentifier for the given <code>TableIdentifier</code> if the specified table identifier represents a Delta table or <code>None</code>.</p> <p><code>apply</code> is used when:</p> <ul> <li>VacuumTableCommand, DeltaGenerateCommand, DescribeDeltaDetailCommand and DescribeDeltaHistoryCommand are executed</li> </ul>"},{"location":"DeltaTableIdentifier/#creating-deltalog","text":"","title":"Creating DeltaLog <pre><code>getDeltaLog(\n  spark: SparkSession): DeltaLog\n</code></pre> <p><code>getDeltaLog</code> creates a DeltaLog (for the location).</p>  <p>Note</p> <p><code>getDeltaLog</code> does not seem to be used.</p>"},{"location":"DeltaTableIdentifier/#location-path","text":"","title":"Location Path <pre><code>getPath(\n  spark: SparkSession): Path\n</code></pre> <p><code>getPath</code> creates a Hadoop Path for the path if defined or requests <code>SessionCatalog</code> (Spark SQL) for the table metadata and uses the <code>locationUri</code>.</p>"},{"location":"DeltaTableOperations/","text":"<p><code>DeltaTableOperations</code> is an abstraction of management services for executing delete, generate, history, update, and vacuum operations (commands).</p> <p> <code>DeltaTableOperations</code> is assumed to be associated with a DeltaTable.    Method Command DeltaTable Operator      executeDelete DeltaDelete DeltaTable.delete    executeGenerate DeltaGenerateCommand DeltaTable.generate    executeHistory DeltaHistoryManager.getHistory DeltaTable.history    executeUpdate <code>UpdateTable</code> (Spark SQL) DeltaTable.update and DeltaTable.updateExpr    executeVacuum VacuumCommand.gc DeltaTable.vacuum","title":"DeltaTableOperations \u2014 Delta DML Operations"},{"location":"DeltaTableOperations/#implementations","text":"<ul> <li>DeltaTable</li> </ul>","title":"Implementations"},{"location":"DeltaTableUtils/","text":"","title":"DeltaTableUtils Utility"},{"location":"DeltaTableUtils/#extractifpathcontainstimetravel","text":"","title":"extractIfPathContainsTimeTravel <pre><code>extractIfPathContainsTimeTravel(\n  session: SparkSession,\n  path: String): (String, Option[DeltaTimeTravelSpec])\n</code></pre> <p><code>extractIfPathContainsTimeTravel</code> uses the internal spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled configuration property to find time travel patterns in the given <code>path</code>.</p> <p><code>extractIfPathContainsTimeTravel</code>...FIXME</p> <p><code>extractIfPathContainsTimeTravel</code>\u00a0is used when:</p> <ul> <li><code>DeltaDataSource</code> is requested to sourceSchema and parsePathIdentifier</li> </ul>"},{"location":"DeltaTableUtils/#finddeltatableroot","text":"","title":"findDeltaTableRoot <pre><code>findDeltaTableRoot(\n  spark: SparkSession,\n  path: Path,\n  options: Map[String, String] = Map.empty): Option[Path]\n</code></pre> <p><code>findDeltaTableRoot</code> traverses the Hadoop DFS-compliant path upwards (to the root directory of the file system) until <code>_delta_log</code> or <code>_samples</code> directories are found, or the root directory is reached.</p> <p>For <code>_delta_log</code> or <code>_samples</code> directories, <code>findDeltaTableRoot</code> returns the parent directory (of <code>_delta_log</code> directory).</p> <p><code>findDeltaTableRoot</code>\u00a0is used when:</p> <ul> <li>DeltaTable.isDeltaTable utility is used</li> <li>VacuumTableCommand is executed</li> <li><code>DeltaTableUtils</code> utility is used to isDeltaTable</li> <li><code>DeltaDataSource</code> utility is used to parsePathIdentifier</li> </ul>"},{"location":"DeltaTableUtils/#ispredicatepartitioncolumnsonly","text":"","title":"isPredicatePartitionColumnsOnly <pre><code>isPredicatePartitionColumnsOnly(\n  condition: Expression,\n  partitionColumns: Seq[String],\n  spark: SparkSession): Boolean\n</code></pre> <p><code>isPredicatePartitionColumnsOnly</code> holds <code>true</code> when all of the references of the <code>condition</code> expression are among the <code>partitionColumns</code>.</p> <p><code>isPredicatePartitionColumnsOnly</code>\u00a0is used when:</p> <ul> <li><code>DeltaTableUtils</code> is used to isPredicateMetadataOnly</li> <li><code>OptimisticTransactionImpl</code> is requested for the filterFiles</li> <li><code>DeltaSourceSnapshot</code> is requested for the partition and data filters</li> </ul>"},{"location":"DeltaTableUtils/#isdeltatable","text":"","title":"isDeltaTable <pre><code>isDeltaTable(\n  table: CatalogTable): Boolean\nisDeltaTable(\n  spark: SparkSession,\n  path: Path): Boolean\nisDeltaTable(\n  spark: SparkSession,\n  tableName: TableIdentifier): Boolean\n</code></pre> <p><code>isDeltaTable</code>...FIXME</p> <p><code>isDeltaTable</code>\u00a0is used when:</p> <ul> <li><code>DeltaCatalog</code> is requested to loadTable</li> <li>DeltaTable.forName, DeltaTable.forPath and DeltaTable.isDeltaTable utilities are used</li> <li><code>DeltaTableIdentifier</code> utility is used to create a DeltaTableIdentifier from a TableIdentifier</li> <li><code>DeltaUnsupportedOperationsCheck</code> is requested to fail</li> </ul>"},{"location":"DeltaTableUtils/#resolvetimetravelversion","text":"","title":"resolveTimeTravelVersion <pre><code>resolveTimeTravelVersion(\n  conf: SQLConf,\n  deltaLog: DeltaLog,\n  tt: DeltaTimeTravelSpec): (Long, String)\n</code></pre> <p><code>resolveTimeTravelVersion</code>...FIXME</p> <p><code>resolveTimeTravelVersion</code> is used when:</p> <ul> <li><code>DeltaLog</code> is requested to create a relation (per partition filters and time travel)</li> <li><code>DeltaTableV2</code> is requested for a Snapshot</li> </ul>"},{"location":"DeltaTableUtils/#splitmetadataanddatapredicates","text":"","title":"splitMetadataAndDataPredicates <pre><code>splitMetadataAndDataPredicates(\n  condition: Expression,\n  partitionColumns: Seq[String],\n  spark: SparkSession): (Seq[Expression], Seq[Expression])\n</code></pre> <p><code>splitMetadataAndDataPredicates</code> splits conjunctive (and) predicates in the given <code>condition</code> expression and partitions them into two collections based on the isPredicateMetadataOnly predicate (with the given <code>partitionColumns</code>).</p> <p><code>splitMetadataAndDataPredicates</code>\u00a0is used when:</p> <ul> <li><code>PartitionFiltering</code> is requested for filesForScan</li> <li>DeleteCommand is executed (with a delete condition)</li> <li>UpdateCommand is executed</li> </ul>"},{"location":"DeltaTableUtils/#ispredicatemetadataonly","text":"","title":"isPredicateMetadataOnly <pre><code>isPredicateMetadataOnly(\n  condition: Expression,\n  partitionColumns: Seq[String],\n  spark: SparkSession): Boolean\n</code></pre> <p><code>isPredicateMetadataOnly</code> holds <code>true</code> when the following hold about the given <code>condition</code>:</p> <ol> <li>Is partition column only (given the <code>partitionColumns</code>)</li> <li>Does not contain a subquery</li> </ol>"},{"location":"DeltaTableV2/","text":"<p><code>DeltaTableV2</code> is a logical representation of a writable Delta table.</p>","title":"DeltaTableV2"},{"location":"DeltaTableV2/#creating-instance","text":"<p><code>DeltaTableV2</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> Path (Hadoop HDFS) <li> Optional Catalog Metadata (Spark SQL) <li> Optional Table ID <li>Optional DeltaTimeTravelSpec</li> <li>Options</li> <li> CDC Options (empty in Delta Lake OSS)  <p><code>DeltaTableV2</code> is created when:</p> <ul> <li><code>DeltaTable</code> utility is used to forPath and forName</li> <li><code>DeltaCatalog</code> is requested to load a table</li> <li><code>DeltaDataSource</code> is requested to load a table or create a table relation</li> </ul>","title":"Creating Instance"},{"location":"DeltaTableV2/#options","text":"","title":"Options <p><code>DeltaTableV2</code> can be given options (as a <code>Map[String, String]</code>). Options are empty by default.</p> <p>The options are defined when <code>DeltaDataSource</code> is requested for a relation with spark.databricks.delta.loadFileSystemConfigsFromDataFrameOptions configuration property enabled.</p> <p>The options are used for the following:</p> <ul> <li>Looking up <code>path</code> or <code>paths</code> options</li> <li>Creating the DeltaLog</li> </ul>"},{"location":"DeltaTableV2/#deltalog","text":"","title":"DeltaLog <p><code>DeltaTableV2</code> creates a DeltaLog for the rootPath and the given options.</p>"},{"location":"DeltaTableV2/#table","text":"","title":"Table <p><code>DeltaTableV2</code> is a <code>Table</code> (Spark SQL).</p>"},{"location":"DeltaTableV2/#supportswrite","text":"","title":"SupportsWrite <p><code>DeltaTableV2</code> is a <code>SupportsWrite</code> (Spark SQL).</p>"},{"location":"DeltaTableV2/#v2tablewithv1fallback","text":"","title":"V2TableWithV1Fallback <p><code>DeltaTableV2</code> is a <code>V2TableWithV1Fallback</code> (Spark SQL).</p>"},{"location":"DeltaTableV2/#deltatimetravelspec","text":"","title":"DeltaTimeTravelSpec <p><code>DeltaTableV2</code> may be given a DeltaTimeTravelSpec when created.</p> <p><code>DeltaTimeTravelSpec</code> is assumed not to be defined by default (<code>None</code>).</p> <p><code>DeltaTableV2</code> is given a <code>DeltaTimeTravelSpec</code> when:</p> <ul> <li><code>DeltaDataSource</code> is requested for a BaseRelation</li> </ul> <p><code>DeltaTimeTravelSpec</code> is used for timeTravelSpec.</p>"},{"location":"DeltaTableV2/#properties","text":"","title":"Properties <pre><code>properties(): Map[String, String]\n</code></pre> <p><code>properties</code>\u00a0is part of the <code>Table</code> (Spark SQL) abstraction.</p> <p><code>properties</code> requests the Snapshot for the table properties and adds the following:</p>    Name Value     <code>provider</code> <code>delta</code>   <code>location</code> path   <code>comment</code> description (of the Metadata) if available   <code>Type</code> table type of the CatalogTable if available"},{"location":"DeltaTableV2/#table-capabilities","text":"","title":"Table Capabilities <pre><code>capabilities(): Set[TableCapability]\n</code></pre> <p><code>capabilities</code>\u00a0is part of the <code>Table</code> (Spark SQL) abstraction.</p> <p><code>capabilities</code> is the following:</p> <ul> <li><code>ACCEPT_ANY_SCHEMA</code> (Spark SQL)</li> <li><code>BATCH_READ</code> (Spark SQL)</li> <li><code>V1_BATCH_WRITE</code> (Spark SQL)</li> <li><code>OVERWRITE_BY_FILTER</code> (Spark SQL)</li> <li><code>TRUNCATE</code> (Spark SQL)</li> </ul>"},{"location":"DeltaTableV2/#creating-writebuilder","text":"","title":"Creating WriteBuilder <pre><code>newWriteBuilder(\n  info: LogicalWriteInfo): WriteBuilder\n</code></pre> <p><code>newWriteBuilder</code>\u00a0is part of the <code>SupportsWrite</code> (Spark SQL) abstraction.</p> <p><code>newWriteBuilder</code> creates a WriteIntoDeltaBuilder (for the DeltaLog and the options from the <code>LogicalWriteInfo</code>).</p>"},{"location":"DeltaTableV2/#snapshot","text":"","title":"Snapshot <pre><code>snapshot: Snapshot\n</code></pre> <p><code>DeltaTableV2</code> has a Snapshot. In other words, <code>DeltaTableV2</code> represents a Delta table at a specific version.</p>  <p>Scala lazy value</p> <p><code>snapshot</code> is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged.</p>  <p><code>DeltaTableV2</code> uses the DeltaLog to load it at a given version (based on the optional timeTravelSpec) or update to the latest version.</p> <p><code>snapshot</code> is used when <code>DeltaTableV2</code> is requested for the schema, partitioning and properties.</p>"},{"location":"DeltaTableV2/#deltatimetravelspec_1","text":"","title":"DeltaTimeTravelSpec <pre><code>timeTravelSpec: Option[DeltaTimeTravelSpec]\n</code></pre> <p><code>DeltaTableV2</code> may have a DeltaTimeTravelSpec specified that is either given or extracted from the path (for timeTravelByPath).</p> <p><code>timeTravelSpec</code> throws an <code>AnalysisException</code> when timeTravelOpt and timeTravelByPath are both defined:</p> <pre><code>Cannot specify time travel in multiple formats.\n</code></pre> <p><code>timeTravelSpec</code> is used when <code>DeltaTableV2</code> is requested for a Snapshot and BaseRelation.</p>"},{"location":"DeltaTableV2/#deltatimetravelspec-by-path","text":"","title":"DeltaTimeTravelSpec by Path <pre><code>timeTravelByPath: Option[DeltaTimeTravelSpec]\n</code></pre>  <p>Scala lazy value</p> <p><code>timeTravelByPath</code> is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged.</p>  <p><code>timeTravelByPath</code> is undefined when CatalogTable is defined.</p> <p>With no CatalogTable defined, <code>DeltaTableV2</code> parses the given Path for the <code>timeTravelByPath</code> (that resolvePath under the covers).</p>"},{"location":"DeltaTableV2/#converting-to-insertable-hadoopfsrelation","text":"","title":"Converting to Insertable HadoopFsRelation <pre><code>toBaseRelation: BaseRelation\n</code></pre> <p><code>toBaseRelation</code> verifyAndCreatePartitionFilters for the Path, the current Snapshot and partitionFilters.</p> <p>In the end, <code>toBaseRelation</code> requests the DeltaLog for an insertable HadoopFsRelation.</p> <p><code>toBaseRelation</code> is used when:</p> <ul> <li><code>DeltaDataSource</code> is requested to createRelation</li> <li><code>DeltaRelation</code> utility is used to <code>fromV2Relation</code></li> </ul>"},{"location":"DeltaTimeTravelSpec/","text":"","title":"DeltaTimeTravelSpec"},{"location":"DeltaTimeTravelSpec/#time-travel-patterns","text":"","title":"Time Travel Patterns <p><code>DeltaTimeTravelSpec</code> defines regular expressions for timestamp- and version-based time travel identifiers:</p> <ul> <li>Version URI: <code>(path)@[vV](some numbers)</code></li> <li>Timestamp URI: <code>(path)@(yyyyMMddHHmmssSSS)</code></li> </ul>"},{"location":"DeltaTimeTravelSpec/#creating-instance","text":"","title":"Creating Instance <p><code>DeltaTimeTravelSpec</code> takes the following to be created:</p> <ul> <li> Timestamp <li> Version <li> <code>creationSource</code> identifier  <p><code>DeltaTimeTravelSpec</code> asserts that either version or timestamp is provided (and throws an <code>AssertionError</code>).</p> <p><code>DeltaTimeTravelSpec</code> is created\u00a0when:</p> <ul> <li><code>DeltaTimeTravelSpec</code> utility is used to resolve a path</li> <li><code>DeltaDataSource</code> utility is used to getTimeTravelVersion</li> </ul>"},{"location":"DeltaTimeTravelSpec/#resolving-path","text":"","title":"Resolving Path <pre><code>resolvePath(\n  conf: SQLConf,\n  identifier: String): (DeltaTimeTravelSpec, String)\n</code></pre> <p><code>resolvePath</code>...FIXME</p> <p><code>resolvePath</code>\u00a0is used when <code>DeltaTableUtils</code> utility is used to extractIfPathContainsTimeTravel.</p>"},{"location":"DeltaTimeTravelSpec/#gettimestamp","text":"","title":"getTimestamp <pre><code>getTimestamp(\n  timeZone: String): Timestamp\n</code></pre> <p><code>getTimestamp</code>...FIXME</p> <p><code>getTimestamp</code>\u00a0is used when <code>DeltaTableUtils</code> utility is used to resolveTimeTravelVersion.</p>"},{"location":"DeltaTimeTravelSpec/#isapplicable","text":"","title":"isApplicable <pre><code>isApplicable(\n  conf: SQLConf,\n  identifier: String): Boolean\n</code></pre> <p><code>isApplicable</code> is <code>true</code> when all of the following hold:</p> <ul> <li>spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled is <code>true</code></li> <li>identifierContainsTimeTravel is <code>true</code></li> </ul> <p><code>isApplicable</code>\u00a0is used when <code>DeltaTableUtils</code> utility is used to extractIfPathContainsTimeTravel.</p>"},{"location":"DeltaTimeTravelSpec/#identifiercontainstimetravel","text":"","title":"identifierContainsTimeTravel <pre><code>identifierContainsTimeTravel(\n  identifier: String): Boolean\n</code></pre> <p><code>identifierContainsTimeTravel</code> is <code>true</code> when the given <code>identifier</code> is either timestamp or version time travel pattern.</p>"},{"location":"DeltaUnsupportedOperationsCheck/","text":"<p>DeltaUnsupportedOperationsCheck is a logical check rule that adds helpful error messages when Delta is being used with unsupported Hive operations or if an unsupported operation is executed.</p>","title":"DeltaUnsupportedOperationsCheck"},{"location":"DeltaWriteOptions/","text":"<p><code>DeltaWriteOptions</code> is a type-safe abstraction of the write-related DeltaOptions.</p> <p><code>DeltaWriteOptions</code> is DeltaWriteOptionsImpl and DeltaOptionParser.</p>","title":"DeltaWriteOptions"},{"location":"DeltaWriteOptions/#replacewhere","text":"","title":"replaceWhere <pre><code>replaceWhere: Option[String]\n</code></pre> <p><code>replaceWhere</code> is the value of replaceWhere option.</p> <p><code>replaceWhere</code> is used when:</p> <ul> <li><code>WriteIntoDelta</code> command is created and executed</li> <li><code>CreateDeltaTableCommand</code> command is requested for a Delta Operation (for history purposes)</li> </ul>"},{"location":"DeltaWriteOptions/#usermetadata","text":"","title":"userMetadata <pre><code>userMetadata: Option[String]\n</code></pre> <p><code>userMetadata</code> is the value of userMetadata option.</p>"},{"location":"DeltaWriteOptions/#optimizewrite","text":"","title":"optimizeWrite <pre><code>optimizeWrite: Option[Boolean]\n</code></pre> <p><code>optimizeWrite</code> is the value of optimizeWrite option.</p>"},{"location":"DeltaWriteOptionsImpl/","text":"<p><code>DeltaWriteOptionsImpl</code> is a DeltaOptionParser.</p>","title":"DeltaWriteOptionsImpl"},{"location":"DeltaWriteOptionsImpl/#canmergeschema","text":"","title":"canMergeSchema <pre><code>canMergeSchema: Boolean\n</code></pre> <p><code>canMergeSchema</code> is the value of mergeSchema option (if defined) or spark.databricks.delta.schema.autoMerge.enabled configuration property.</p> <p><code>canMergeSchema</code> is used when:</p> <ul> <li><code>WriteIntoDelta</code> is created</li> <li><code>DeltaSink</code> is created</li> </ul>"},{"location":"DeltaWriteOptionsImpl/#canoverwriteschema","text":"","title":"canOverwriteSchema <pre><code>canOverwriteSchema: Boolean\n</code></pre> <p><code>canOverwriteSchema</code> is the value of overwriteSchema option.</p> <p><code>canOverwriteSchema</code> is used when:</p> <ul> <li><code>CreateDeltaTableCommand</code> is requested to replaceMetadataIfNecessary</li> <li><code>WriteIntoDelta</code> is created</li> <li><code>DeltaSink</code> is created</li> </ul>"},{"location":"DeltaWriteOptionsImpl/#rearrangeonly","text":"","title":"rearrangeOnly <pre><code>rearrangeOnly: Boolean\n</code></pre> <p><code>rearrangeOnly</code> is the value of dataChange option.</p> <p><code>rearrangeOnly</code> is used when:</p> <ul> <li><code>WriteIntoDelta</code> is requested to write</li> </ul>"},{"location":"FileAction/","text":"<p><code>FileAction</code>\u00a0is an extension of the Action abstraction for actions that can add or remove files.</p>","title":"FileAction"},{"location":"FileAction/#contract","text":"","title":"Contract"},{"location":"FileAction/#path","text":"","title":"Path <pre><code>path: String\n</code></pre>"},{"location":"FileAction/#datachange","text":"","title":"dataChange <pre><code>dataChange: Boolean\n</code></pre> <p>Controls the transaction isolation level for committing a transaction</p>    Isolation Level Description     SnapshotIsolation No data changes (<code>dataChange</code> is <code>false</code> for all <code>FileAction</code>s to be committed)   Serializable      <p>There can be no RemoveFiles with <code>dataChange</code> enabled for appendOnly unmodifiable tables (or an UnsupportedOperationException is thrown).</p>    dataChange Value When     <code>false</code> <code>InMemoryLogReplay</code> is requested to replay a version   <code>true</code> ConvertToDeltaCommand is executed (and requested to create an AddFile with the flag turned on)   Opposite of dataChange option <code>WriteIntoDelta</code> is requested to write (with dataChange option turned off for rearrange-only writes)    <p><code>dataChange</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to commit (and determines the isolation level), prepareCommit, attempt a commit (for <code>bytesNew</code> statistics)</li> <li><code>DeltaSource</code> is requested to getChanges (and verifyStreamHygieneAndFilterAddFiles)</li> </ul>"},{"location":"FileAction/#implementations","text":"<ul> <li>AddCDCFile</li> <li>AddFile</li> <li>RemoveFile</li> </ul>  Sealed Trait <p><code>FileAction</code> is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).</p>","title":"Implementations"},{"location":"FileNames/","text":"<p>= FileNames Utility</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| checkpointPrefix a| [[checkpointPrefix]] Creates a Hadoop <code>Path</code> for a file name with a given <code>version</code>:</p> <pre><code>[version][%020d].checkpoint\n</code></pre> <p>E.g. <code>00000000000000000005.checkpoint</code></p> <p>| isCheckpointFile a| [[isCheckpointFile]]</p> <p>| isDeltaFile a| [[isDeltaFile]]</p> <p>|===</p> <p>== [[deltaFile]] Creating Hadoop Path To Delta File -- <code>deltaFile</code> Utility</p>","title":"FileNames"},{"location":"FileNames/#source-scala","text":"<p>deltaFile(   path: Path,   version: Long): Path</p>  <p><code>deltaFile</code> creates a Hadoop <code>Path</code> to a file of the format <code>[version][%020d].json</code> in the <code>path</code> directory, e.g. <code>00000000000000000001.json</code>.</p> <p>NOTE: <code>deltaFile</code> is used when...FIXME</p>","title":"[source, scala]"},{"location":"GenerateSymlinkManifest/","text":"<p><code>GenerateSymlinkManifest</code> is a post-commit hook to generate incremental and full Hive-style manifests for delta tables.</p> <p><code>GenerateSymlinkManifest</code> is registered when <code>OptimisticTransactionImpl</code> is requested to commit (with delta.compatibility.symlinkFormatManifest.enabled table property enabled).</p>","title":"GenerateSymlinkManifest (GenerateSymlinkManifestImpl)"},{"location":"GenerateSymlinkManifest/#executing-post-commit-hook","text":"","title":"Executing Post-Commit Hook <pre><code>run(\n  spark: SparkSession,\n  txn: OptimisticTransactionImpl,\n  committedActions: Seq[Action]): Unit\n</code></pre> <p><code>run</code> is part of the PostCommitHook abstraction.</p> <p><code>run</code> generates an incremental manifest for the committed actions (the deltaLog and snapshot are from the <code>OptimisticTransactionImpl</code>).</p>"},{"location":"GenerateSymlinkManifest/#generateincrementalmanifest","text":"","title":"generateIncrementalManifest <pre><code>generateIncrementalManifest(\n  spark: SparkSession,\n  deltaLog: DeltaLog,\n  txnReadSnapshot: Snapshot,\n  actions: Seq[Action]): Unit\n</code></pre> <p><code>generateIncrementalManifest</code>...FIXME</p>"},{"location":"GenerateSymlinkManifest/#generatefullmanifest","text":"","title":"generateFullManifest <pre><code>generateFullManifest(\n  spark: SparkSession,\n  deltaLog: DeltaLog): Unit\n</code></pre> <p><code>generateFullManifest</code>...FIXME</p> <p><code>generateFullManifest</code> is used when:</p> <ul> <li><code>GenerateSymlinkManifestImpl</code> is requested to generateIncrementalManifest</li> <li>DeltaGenerateCommand is executed (with <code>symlink_format_manifest</code> mode)</li> </ul>"},{"location":"ImplicitMetadataOperation/","text":"<p><code>ImplicitMetadataOperation</code> is an abstraction of operations that can update metadata of a delta table (while writing out a new data).</p> <p><code>ImplicitMetadataOperation</code> operations can update schema by merging and overwriting schema.</p>","title":"ImplicitMetadataOperation"},{"location":"ImplicitMetadataOperation/#contract","text":"","title":"Contract"},{"location":"ImplicitMetadataOperation/#canmergeschema","text":"","title":"canMergeSchema <pre><code>canMergeSchema: Boolean\n</code></pre> <p>Used when:</p> <ul> <li><code>MergeIntoCommand</code> command is executed</li> <li><code>ImplicitMetadataOperation</code> is requested to updateMetadata</li> </ul>"},{"location":"ImplicitMetadataOperation/#canoverwriteschema","text":"","title":"canOverwriteSchema <pre><code>canOverwriteSchema: Boolean\n</code></pre> <p>Used when:</p> <ul> <li><code>ImplicitMetadataOperation</code> is requested to updateMetadata</li> </ul>"},{"location":"ImplicitMetadataOperation/#implementations","text":"<ul> <li>DeltaSink</li> <li>MergeIntoCommand</li> <li>WriteIntoDelta</li> </ul>","title":"Implementations"},{"location":"ImplicitMetadataOperation/#updating-metadata","text":"","title":"Updating Metadata <pre><code>updateMetadata( // (1)\n  txn: OptimisticTransaction,\n  data: Dataset[_],\n  partitionColumns: Seq[String],\n  configuration: Map[String, String],\n  isOverwriteMode: Boolean,\n  rearrangeOnly: Boolean = false): Unit\nupdateMetadata(\n  spark: SparkSession,\n  txn: OptimisticTransaction,\n  schema: StructType,\n  partitionColumns: Seq[String],\n  configuration: Map[String, String],\n  isOverwriteMode: Boolean,\n  rearrangeOnly: Boolean): Unit\n</code></pre> <ol> <li>Uses the <code>SparkSession</code> and the schema of the given <code>Dataset</code> and assumes the <code>rearrangeOnly</code> to be off</li> </ol> <p><code>updateMetadata</code>...FIXME</p> <p><code>updateMetadata</code> is used when:</p> <ul> <li>MergeIntoCommand and WriteIntoDelta commands are executed</li> <li><code>DeltaSink</code> is requested to add a streaming micro-batch</li> </ul>"},{"location":"ImplicitMetadataOperation/#normalizing-partition-columns","text":"","title":"Normalizing Partition Columns <pre><code>normalizePartitionColumns(\n  spark: SparkSession,\n  partitionCols: Seq[String],\n  schema: StructType): Seq[String]\n</code></pre> <p><code>normalizePartitionColumns</code>...FIXME</p>"},{"location":"InMemoryLogReplay/","text":"<p><code>InMemoryLogReplay</code> is used at the very last phase of state reconstruction (of a cached delta state).</p> <p><code>InMemoryLogReplay</code> handles a single partition of the state reconstruction dataset (based on the spark.databricks.delta.snapshotPartitions configuration property).</p>","title":"InMemoryLogReplay"},{"location":"InMemoryLogReplay/#creating-instance","text":"<p><code>InMemoryLogReplay</code> takes the following to be created:</p> <ul> <li> <code>minFileRetentionTimestamp</code> (Snapshot.minFileRetentionTimestamp)  <p><code>InMemoryLogReplay</code> is created\u00a0when:</p> <ul> <li><code>Snapshot</code> is requested for state reconstruction</li> </ul>","title":"Creating Instance"},{"location":"InMemoryLogReplay/#lifecycle","text":"<p>The lifecycle of <code>InMemoryLogReplay</code> is as follows:</p> <ol> <li> <p>Created (with Snapshot.minFileRetentionTimestamp)</p> </li> <li> <p>Append all SingleActions of a partition (based on the spark.databricks.delta.snapshotPartitions configuration property)</p> </li> <li> <p>Checkpoint</p> </li> </ol>","title":"Lifecycle"},{"location":"InMemoryLogReplay/#replaying-version","text":"","title":"Replaying Version <pre><code>append(\n  version: Long,\n  actions: Iterator[Action]): Unit\n</code></pre> <p><code>append</code> sets the currentVersion to the given <code>version</code>.</p> <p><code>append</code> adds the given actions to their respective registries.</p>    Action Registry     SetTransaction transactions by appId   Metadata currentMetaData   Protocol currentProtocolVersion   AddFile 1. activeFiles by path and with dataChange flag disabled     2. Removes the path from tombstones (so there's only one FileAction for a path)   RemoveFile 1. Removes the path from activeFiles (so there's only one FileAction for a path)     2. tombstones by path and with dataChange flag disabled   CommitInfo Ignored   AddCDCFile Ignored    <p><code>append</code> throws an <code>AssertionError</code> when the currentVersion is <code>-1</code> or one before the given <code>version</code>:</p> <pre><code>Attempted to replay version [version], but state is at [currentVersion]\n</code></pre>"},{"location":"InMemoryLogReplay/#current-state-of-delta-table","text":"","title":"Current State of Delta Table <pre><code>checkpoint: Iterator[Action]\n</code></pre> <p><code>checkpoint</code> returns an <code>Iterator</code> (Scala) of Actions in the following order:</p> <ul> <li>currentProtocolVersion if defined (non-<code>null</code>)</li> <li>currentMetaData if defined (non-<code>null</code>)</li> <li>SetTransactions</li> <li>AddFiles and RemoveFiles sorted by path (lexicographically)</li> </ul>"},{"location":"InMemoryLogReplay/#gettombstones","text":"","title":"getTombstones <pre><code>getTombstones: Iterable[FileAction]\n</code></pre> <p><code>getTombstones</code> uses the tombstones internal registry for RemoveFiles with deletionTimestamp after (greater than) the minFileRetentionTimestamp.</p>"},{"location":"IsolationLevel/","text":"<p><code>IsolationLevel</code> is an abstraction of consistency guarantees to be provided when <code>OptimisticTransaction</code> is committed.</p>","title":"IsolationLevel"},{"location":"IsolationLevel/#implementations","text":"<p>Sealed Trait</p> <p><code>IsolationLevel</code> is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).</p>","title":"Implementations"},{"location":"IsolationLevel/#serializable","text":"","title":"Serializable <p><code>Serializable</code> is the most strict consistency guarantee.</p> <p><code>Serializable</code> is the isolation level for data-changing commits.</p> <p>For <code>Serializable</code> commits, <code>OptimisticTransactionImpl</code> adds extra <code>addedFilesToCheckForConflicts</code> (<code>changedData</code> or <code>blindAppend</code> AddFiles) when checkForConflicts.</p> <p><code>Serializable</code> is a valid table isolation level.</p> <p>For operations that do not modify data in a table, there is no difference between Serializable and SnapshotIsolation.</p> <p><code>Serializable</code> is used for ConvertToDeltaCommand command.</p>"},{"location":"IsolationLevel/#snapshotisolation","text":"","title":"SnapshotIsolation <p><code>SnapshotIsolation</code> is the least strict consistency guarantee.</p> <p><code>SnapshotIsolation</code> is the isolation level for commits with no data changed.</p> <p>For <code>SnapshotIsolation</code> commits, <code>OptimisticTransactionImpl</code> adds no extra <code>addedFilesToCheckForConflicts</code> when checkForConflicts.</p> <p>For operations that do not modify data in a table, there is no difference between Serializable and SnapshotIsolation.</p>"},{"location":"IsolationLevel/#writeserializable","text":"","title":"WriteSerializable <p>The default <code>IsolationLevel</code></p> <p>For <code>WriteSerializable</code> commits, <code>OptimisticTransactionImpl</code> adds extra <code>addedFilesToCheckForConflicts</code> (<code>changedData</code> AddFiles) when checkForConflicts. Blind appends don't (seem to) conflict with <code>WriteSerializable</code> commits.</p> <p><code>WriteSerializable</code> a valid table isolation level.</p>"},{"location":"IsolationLevel/#consistency-guarantee-strictness-ordering","text":"","title":"Consistency Guarantee Strictness Ordering <p>The following are all the isolation levels in descending order of guarantees provided:</p> <ol> <li>Serializable (the most strict level)</li> <li>WriteSerializable</li> <li>SnapshotIsolation (the least strict one)</li> </ol>"},{"location":"IsolationLevel/#valid-table-isolation-levels","text":"","title":"Valid Table Isolation Levels  <p>Not Used in OSS Delta Lake</p> <p>This feature is not used.</p>  <p>The following are the valid isolation levels that can be specified as the table isolation level:</p> <ul> <li>Serializable</li> <li>WriteSerializable</li> </ul>"},{"location":"LogSegment/","text":"<p><code>LogSegment</code> are the delta and checkpoint files that all together are a given version of a delta table.</p>","title":"LogSegment"},{"location":"LogSegment/#creating-instance","text":"<p><code>LogSegment</code> takes the following to be created:</p> <ul> <li> Log Path (Apache Hadoop) <li> Version <li> Delta <code>FileStatus</code>es (Apache Hadoop) <li> Checkpoint <code>FileStatus</code>es (Apache Hadoop) <li> Checkpoint Version <li> Timestamp of the Last Commit  <p><code>LogSegment</code> is created\u00a0when:</p> <ul> <li><code>SnapshotManagement</code> is requested for the LogSegment at a given version</li> </ul>","title":"Creating Instance"},{"location":"LogStore/","text":"<p><code>LogStore</code> is an abstraction of transaction log stores (to read and write Delta log files).</p> <p><code>LogStore</code> is created for LogStoreAdaptor.</p>","title":"LogStore (io.delta.storage)"},{"location":"LogStore/#iodeltastorage","text":"<p><code>LogStore</code> is part of <code>io.delta.storage</code> package meant for Delta Lake developers.</p>  <p>Note</p> <p>There is another internal LogStore in <code>org.apache.spark.sql.delta.storage</code> package.</p>","title":"io.delta.storage"},{"location":"LogStore/#contract","text":"","title":"Contract"},{"location":"LogStore/#ispartialwritevisible","text":"","title":"isPartialWriteVisible <pre><code>Boolean isPartialWriteVisible(\n  Path path,\n  Configuration hadoopConf)\n</code></pre> <p>Used when:</p> <ul> <li><code>LogStoreAdaptor</code> is requested to isPartialWriteVisible</li> </ul>"},{"location":"LogStore/#listfrom","text":"","title":"listFrom <pre><code>Iterator&lt;FileStatus&gt; listFrom(\n  Path path,\n  Configuration hadoopConf) throws FileNotFoundException\n</code></pre> <p>Used when:</p> <ul> <li><code>LogStoreAdaptor</code> is requested to listFrom</li> </ul>"},{"location":"LogStore/#read","text":"","title":"read <pre><code>CloseableIterator&lt;String&gt; read(\n  Path path,\n  Configuration hadoopConf)\n</code></pre> <p>Used when:</p> <ul> <li><code>LogStoreAdaptor</code> is requested to read and readAsIterator</li> </ul>"},{"location":"LogStore/#resolvepathonphysicalstorage","text":"","title":"resolvePathOnPhysicalStorage <pre><code>Path resolvePathOnPhysicalStorage(\n  Path path,\n  Configuration hadoopConf)\n</code></pre> <p>Used when:</p> <ul> <li><code>LogStoreAdaptor</code> is requested to resolvePathOnPhysicalStorage</li> </ul>"},{"location":"LogStore/#write","text":"","title":"write <pre><code>void write(\n  Path path,\n  Iterator&lt;String&gt; actions,\n  Boolean overwrite,\n  Configuration hadoopConf) throws FileAlreadyExistsException\n</code></pre> <p>Used when:</p> <ul> <li><code>LogStoreAdaptor</code> is requested to write</li> </ul>"},{"location":"LogStore/#creating-instance","text":"<p><code>LogStore</code> takes the following to be created:</p> <ul> <li> <code>Configuration</code> (Apache Hadoop)   Abstract Class <p><code>LogStore</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete LogStores.</p>","title":"Creating Instance"},{"location":"Metadata/","text":"<p><code>Metadata</code> is an Action to update the metadata of a delta table (indirectly via the Snapshot).</p> <p>Use DescribeDeltaDetailCommand to review the metadata of a delta table.</p>","title":"Metadata"},{"location":"Metadata/#creating-instance","text":"<p><code>Metadata</code> takes the following to be created:</p> <ul> <li>Id (default: random UUID)</li> <li> Name (default: <code>null</code>) <li> Description (default: <code>null</code>) <li> Format (default: empty) <li> Schema (default: <code>null</code>) <li> Partition Columns (default: <code>Nil</code>) <li> Table Configuration (default: <code>Map.empty</code>) <li> Created Time (default: current time)  <p><code>Metadata</code> is created when:</p> <ul> <li><code>DeltaLog</code> is requested for the metadata (but that should be rare)</li> <li><code>InitialSnapshot</code> is created</li> <li>ConvertToDeltaCommand is executed</li> <li><code>ImplicitMetadataOperation</code> is requested to updateMetadata</li> </ul>","title":"Creating Instance"},{"location":"Metadata/#updating-metadata","text":"<p><code>Metadata</code> can be updated in a transaction once only (and only when created for an uninitialized table, when readVersion is <code>-1</code>).</p> <pre><code>txn.metadata\n</code></pre>","title":"Updating Metadata"},{"location":"Metadata/#demo","text":"<pre><code>val path = \"/tmp/delta/users\"\n\nimport org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, path)\n\nimport org.apache.spark.sql.delta.actions.Metadata\nassert(deltaLog.snapshot.metadata.isInstanceOf[Metadata])\n\ndeltaLog.snapshot.metadata.id\n</code></pre>","title":"Demo"},{"location":"Metadata/#table-id","text":"","title":"Table ID <p><code>Metadata</code> uses a Table ID (aka reservoirId) to uniquely identify a delta table and is never going to change through the history of the table.</p> <p>Table ID is given <code>Metadata</code> is created or defaults to a random UUID (Java).</p>  <p>Note</p> <p>When I asked the question tableId and reservoirId - Why two different names for metadata ID? on delta-users mailing list, Tathagata Das wrote:</p>  <p>Any reference to \"reservoir\" is just legacy code. In the early days of this project, the project was called \"Tahoe\" and each table is called a \"reservoir\" (Tahoe is one of the 2nd deepest lake in US, and is a very large reservoir of water ;) ). So you may still find those two terms all around the codebase.</p> <p>In some cases, like DeltaSourceOffset, the term <code>reservoirId</code> is in the json that is written to the streaming checkpoint directory. So we cannot change that for backward compatibility.</p>"},{"location":"MetadataCleanup/","text":"<p><code>MetadataCleanup</code> is an abstraction of metadata cleaners that can clean up expired checkpoints and delta logs of a delta table.</p> <p> <code>MetadataCleanup</code> requires to be used with DeltaLog (or subtypes) only.","title":"MetadataCleanup"},{"location":"MetadataCleanup/#implementations","text":"<ul> <li>DeltaLog</li> </ul>","title":"Implementations"},{"location":"MetadataCleanup/#table-properties","text":"","title":"Table Properties"},{"location":"MetadataCleanup/#enableexpiredlogcleanup","text":"","title":"enableExpiredLogCleanup <p><code>MetadataCleanup</code> uses enableExpiredLogCleanup table configuration to enable log cleanup.</p>"},{"location":"MetadataCleanup/#logretentionduration","text":"","title":"logRetentionDuration <p><code>MetadataCleanup</code> uses logRetentionDuration table configuration for how long to keep around obsolete logs.</p>"},{"location":"MetadataCleanup/#cleaning-up-expired-logs","text":"","title":"Cleaning Up Expired Logs <pre><code>doLogCleanup(): Unit\n</code></pre> <p><code>doLogCleanup</code>\u00a0is part of the Checkpoints abstraction.</p> <p><code>doLogCleanup</code> cleanUpExpiredLogs when enabled.</p>"},{"location":"MetadataCleanup/#cleanupexpiredlogs","text":"","title":"cleanUpExpiredLogs <pre><code>cleanUpExpiredLogs(): Unit\n</code></pre> <p><code>cleanUpExpiredLogs</code> calculates a <code>fileCutOffTime</code> based on the current time and the logRetentionDuration table property.</p> <p><code>cleanUpExpiredLogs</code> prints out the following INFO message to the logs:</p> <pre><code>Starting the deletion of log files older than [date]\n</code></pre> <p><code>cleanUpExpiredLogs</code> finds the expired delta logs (based on the <code>fileCutOffTime</code>) and deletes the files (using Hadoop's FileSystem.delete non-recursively). <code>cleanUpExpiredLogs</code> counts the files deleted (and uses it in the summary INFO message).</p> <p>In the end, <code>cleanUpExpiredLogs</code> prints out the following INFO message to the logs:</p> <pre><code>Deleted [numDeleted] log files older than [date]\n</code></pre>"},{"location":"MetadataCleanup/#finding-expired-log-files","text":"","title":"Finding Expired Log Files <pre><code>listExpiredDeltaLogs(\n  fileCutOffTime: Long): Iterator[FileStatus]\n</code></pre> <p><code>listExpiredDeltaLogs</code> loads the most recent checkpoint if available.</p> <p>If the last checkpoint is not available, <code>listExpiredDeltaLogs</code> returns an empty iterator.</p> <p><code>listExpiredDeltaLogs</code> requests the LogStore for the paths (in the same directory) that are (lexicographically) greater or equal to the <code>0</code>th checkpoint file (per checkpointPrefix format) of the checkpoint and delta files in the log directory.</p> <p>In the end, <code>listExpiredDeltaLogs</code> creates a <code>BufferingLogDeletionIterator</code> that...FIXME</p>"},{"location":"MetadataCleanup/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for the Implementations logger to see what happens inside.</p>"},{"location":"Operation/","text":"<p><code>Operation</code> is an abstraction of operations that can be executed on a Delta table.</p> <p>Operation is described by a name and parameters (that are simply used to create a CommitInfo for <code>OptimisticTransactionImpl</code> when committed and, as a way to bypass a transaction, ConvertToDeltaCommand).</p> <p>Operation may have performance metrics.</p>","title":"Operation"},{"location":"Operation/#contract","text":"","title":"Contract"},{"location":"Operation/#parameters","text":"","title":"Parameters <pre><code>parameters: Map[String, Any]\n</code></pre> <p>Parameters of the operation (to create a CommitInfo with the JSON-encoded values)</p> <p>Used when:</p> <ul> <li><code>Operation</code> is requested for the parameters with the values in JSON format</li> </ul>"},{"location":"Operation/#implementations","text":"<p>Sealed Abstract Class</p> <p><code>Operation</code> is a Scala sealed abstract class which means that all of the implementations are in the same compilation unit (a single file).</p>","title":"Implementations"},{"location":"Operation/#addcolumns","text":"","title":"AddColumns <p>Name: <code>ADD COLUMNS</code></p> <p>Parameters:</p> <ul> <li><code>columns</code></li> </ul> <p>Used when:</p> <ul> <li>AlterTableAddColumnsDeltaCommand is executed (and committed to a Delta table)</li> </ul>"},{"location":"Operation/#addconstraint","text":"","title":"AddConstraint"},{"location":"Operation/#changecolumn","text":"","title":"ChangeColumn"},{"location":"Operation/#convert","text":"","title":"Convert"},{"location":"Operation/#createtable","text":"","title":"CreateTable"},{"location":"Operation/#delete","text":"","title":"Delete"},{"location":"Operation/#dropconstraint","text":"","title":"DropConstraint"},{"location":"Operation/#manualupdate","text":"","title":"ManualUpdate"},{"location":"Operation/#merge","text":"","title":"Merge <p>Name: <code>MERGE</code></p> <p>Parameters:</p> <ul> <li>predicate</li> <li>matchedPredicates</li> <li>notMatchedPredicates</li> </ul> <p>changesData: <code>true</code></p> <p>Used when:</p> <ul> <li>MergeIntoCommand is executed (and committed to a Delta table)</li> </ul>"},{"location":"Operation/#replacecolumns","text":"","title":"ReplaceColumns"},{"location":"Operation/#replacetable","text":"","title":"ReplaceTable"},{"location":"Operation/#settableproperties","text":"","title":"SetTableProperties <p>Name: <code>SET TBLPROPERTIES</code></p> <p>Parameters:</p> <ul> <li>properties</li> </ul> <p>Used when:</p> <ul> <li>AlterTableSetPropertiesDeltaCommand is executed</li> </ul>"},{"location":"Operation/#streamingupdate","text":"","title":"StreamingUpdate <p>Name: <code>STREAMING UPDATE</code></p> <p>Parameters:</p> <ul> <li>outputMode</li> <li>queryId</li> <li>epochId</li> </ul> <p>Used when:</p> <ul> <li><code>DeltaSink</code> is requested to addBatch</li> </ul>"},{"location":"Operation/#truncate","text":"","title":"Truncate"},{"location":"Operation/#unsettableproperties","text":"","title":"UnsetTableProperties"},{"location":"Operation/#update","text":"","title":"Update"},{"location":"Operation/#updatecolumnmetadata","text":"","title":"UpdateColumnMetadata"},{"location":"Operation/#updateschema","text":"","title":"UpdateSchema"},{"location":"Operation/#upgradeprotocol","text":"","title":"UpgradeProtocol"},{"location":"Operation/#write","text":"","title":"Write"},{"location":"Operation/#creating-instance","text":"<p><code>Operation</code> takes the following to be created:</p> <ul> <li> Name of this operation   Abstract Class <p><code>Operation</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete Operations.</p>","title":"Creating Instance"},{"location":"Operation/#serializing-parameter-values-to-json-format","text":"","title":"Serializing Parameter Values (to JSON Format) <pre><code>jsonEncodedValues: Map[String, String]\n</code></pre> <p><code>jsonEncodedValues</code> converts the values of the parameters to JSON format.</p> <p><code>jsonEncodedValues</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to commit</li> <li><code>ConvertToDeltaCommand</code> command is requested to streamWrite</li> </ul>"},{"location":"Operation/#operation-metrics","text":"","title":"Operation Metrics <pre><code>operationMetrics: Set[String]\n</code></pre> <p><code>operationMetrics</code> is empty by default (and is expected to be overriden by concrete operations).</p> <p><code>operationMetrics</code> is used when:</p> <ul> <li><code>Operation</code> is requested to transformMetrics</li> </ul>"},{"location":"Operation/#transforming-performance-metrics","text":"","title":"Transforming Performance Metrics <pre><code>transformMetrics(\n  metrics: Map[String, SQLMetric]): Map[String, String]\n</code></pre> <p><code>transformMetrics</code> returns a collection of <code>SQLMetric</code>s (Spark SQL) and their values (as text) that are defined as the operation metrics.</p> <p><code>transformMetrics</code> is used when:</p> <ul> <li><code>SQLMetricsReporting</code> is requested for operation metrics</li> </ul>"},{"location":"Operation/#user-metadata","text":"","title":"User Metadata <pre><code>userMetadata: Option[String]\n</code></pre> <p><code>userMetadata</code> is undefined (<code>None</code>) by default (and is expected to be overriden by concrete operations).</p> <p><code>userMetadata</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested for the user metadata</li> </ul>"},{"location":"Operation/#changesdata-flag","text":"","title":"changesData Flag <pre><code>changesData: Boolean\n</code></pre> <p><code>changesData</code> is disabled (<code>false</code>) by default (and is expected to be overriden by concrete operations).</p>  <p>Note</p> <p><code>changesData</code> seems not used.</p>"},{"location":"OptimisticTransaction/","text":"<p><code>OptimisticTransaction</code> is an OptimisticTransactionImpl (which seems more of a class name change than anything more important).</p> <p><code>OptimisticTransaction</code> is created for changes to a delta table at a given version.</p> <p>When <code>OptimisticTransaction</code> (as a OptimisticTransactionImpl) is about to be committed (that does doCommit internally), the LogStore (of the delta table) is requested to write actions to a delta file (e.g. <code>_delta_log/00000000000000000001.json</code> for the attempt version <code>1</code>). Unless a <code>FileAlreadyExistsException</code> is thrown a commit is considered successful or retried.</p> <p><code>OptimisticTransaction</code> can be associated with a thread as an active transaction.</p>","title":"OptimisticTransaction"},{"location":"OptimisticTransaction/#demo","text":"<pre><code>import org.apache.spark.sql.delta.DeltaLog\nval dir = \"/tmp/delta/users\"\nval log = DeltaLog.forTable(spark, dir)\n\nval txn = log.startTransaction()\n\n// ...changes to a delta table...\nval addFile = AddFile(\"foo\", Map.empty, 1L, System.currentTimeMillis(), dataChange = true)\nval removeFile = addFile.remove\nval actions = addFile :: removeFile :: Nil\n\ntxn.commit(actions, op)\n</code></pre> <p>Alternatively, you could do the following instead.</p> <pre><code>deltaLog.withNewTransaction { txn =&gt;\n  // ...transactional changes to a delta table\n}\n</code></pre>","title":"Demo"},{"location":"OptimisticTransaction/#creating-instance","text":"<p><code>OptimisticTransaction</code> takes the following to be created:</p> <ul> <li> DeltaLog <li> Snapshot <li> <code>Clock</code>   <p>Note</p> <p>The DeltaLog and Snapshot are part of the OptimisticTransactionImpl abstraction (which in turn inherits them as a TransactionalWrite and simply changes to <code>val</code> from <code>def</code>).</p>  <p><code>OptimisticTransaction</code> is created\u00a0when <code>DeltaLog</code> is used for the following:</p> <ul> <li>Starting a new transaction</li> <li>Executing a single-threaded operation (in a new transaction)</li> </ul>","title":"Creating Instance"},{"location":"OptimisticTransaction/#active-thread-local-optimistictransaction","text":"","title":"Active Thread-Local OptimisticTransaction <pre><code>active: ThreadLocal[OptimisticTransaction]\n</code></pre> <p><code>active</code> is a Java ThreadLocal with the <code>OptimisticTransaction</code> of the current thread.</p>  <p>ThreadLocal</p> <p><code>ThreadLocal</code> provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its get or set method) has its own, independently initialized copy of the variable.</p> <p><code>ThreadLocal</code> instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID).</p>  <p><code>active</code> is assigned to the current thread using setActive utility and cleared in clearActive.</p> <p><code>active</code> is available using getActive utility.</p> <p>There can only be one active <code>OptimisticTransaction</code> (or an <code>IllegalStateException</code> is thrown).</p>"},{"location":"OptimisticTransaction/#setactive","text":"","title":"setActive <pre><code>setActive(\n  txn: OptimisticTransaction): Unit\n</code></pre> <p><code>setActive</code> associates the given <code>OptimisticTransaction</code> as active with the current thread.</p> <p><code>setActive</code> throws an <code>IllegalStateException</code> if there is an active OptimisticTransaction already associated:</p> <pre><code>Cannot set a new txn as active when one is already active\n</code></pre> <p><code>setActive</code> is used when:</p> <ul> <li><code>DeltaLog</code> is requested to execute an operation in a new transaction</li> </ul>"},{"location":"OptimisticTransaction/#clearactive","text":"","title":"clearActive <pre><code>clearActive(): Unit\n</code></pre> <p><code>clearActive</code> clears the active transaction (so no transaction is associated with the current thread).</p> <p><code>clearActive</code> is used when:</p> <ul> <li><code>DeltaLog</code> is requested to execute an operation in a new transaction</li> </ul>"},{"location":"OptimisticTransaction/#getactive","text":"","title":"getActive <pre><code>getActive(): Option[OptimisticTransaction]\n</code></pre> <p>getActive returns the active transaction (if available).</p> <p>getActive seems unused.</p>"},{"location":"OptimisticTransaction/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.OptimisticTransaction</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.delta.OptimisticTransaction=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"OptimisticTransactionImpl/","text":"<p><code>OptimisticTransactionImpl</code> is an extension of the TransactionalWrite abstraction for optimistic transactions that can modify a delta table (at a given version) and can be committed eventually.</p> <p>In other words, <code>OptimisticTransactionImpl</code> is a set of actions as part of an Operation that changes the state of a delta table transactionally.</p>","title":"OptimisticTransactionImpl"},{"location":"OptimisticTransactionImpl/#contract","text":"","title":"Contract"},{"location":"OptimisticTransactionImpl/#clock","text":"","title":"Clock <pre><code>clock: Clock\n</code></pre>"},{"location":"OptimisticTransactionImpl/#deltalog","text":"","title":"DeltaLog <pre><code>deltaLog: DeltaLog\n</code></pre> <p>DeltaLog (of a delta table) that this transaction is changing</p> <p><code>deltaLog</code> is part of the TransactionalWrite abstraction and seems to change it to <code>val</code> (from <code>def</code>).</p>"},{"location":"OptimisticTransactionImpl/#snapshot","text":"","title":"Snapshot <pre><code>snapshot: Snapshot\n</code></pre> <p>Snapshot (of the delta table) that this transaction is changing</p> <p><code>snapshot</code> is part of the TransactionalWrite contract and seems to change it to <code>val</code> (from <code>def</code>).</p>"},{"location":"OptimisticTransactionImpl/#implementations","text":"<ul> <li>OptimisticTransaction</li> </ul>","title":"Implementations"},{"location":"OptimisticTransactionImpl/#table-version-at-reading-time","text":"","title":"Table Version at Reading Time <pre><code>readVersion: Long\n</code></pre> <p><code>readVersion</code> requests the Snapshot for the version.</p> <p><code>readVersion</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to updateMetadata and commit</li> <li>AlterDeltaTableCommand, ConvertToDeltaCommand, CreateDeltaTableCommand commands are executed</li> <li><code>DeltaCommand</code> is requested to commitLarge</li> <li><code>WriteIntoDelta</code> is requested to write</li> <li><code>ImplicitMetadataOperation</code> is requested to updateMetadata</li> </ul>"},{"location":"OptimisticTransactionImpl/#transactional-commit","text":"","title":"Transactional Commit <pre><code>commit(\n  actions: Seq[Action],\n  op: DeltaOperations.Operation): Long\n</code></pre> <p><code>commit</code> attempts to commit the transaction (with the Actions and the Operation) and gives the commit version.</p>"},{"location":"OptimisticTransactionImpl/#usage","text":"","title":"Usage <p><code>commit</code>\u00a0is used when:</p> <ul> <li><code>DeltaLog</code> is requested to upgrade the protocol</li> <li>ALTER delta table commands (AlterTableSetPropertiesDeltaCommand, AlterTableUnsetPropertiesDeltaCommand, AlterTableAddColumnsDeltaCommand, AlterTableChangeColumnDeltaCommand, AlterTableReplaceColumnsDeltaCommand, AlterTableAddConstraintDeltaCommand, AlterTableDropConstraintDeltaCommand) are executed</li> <li>ConvertToDeltaCommand command is executed</li> <li>CreateDeltaTableCommand command is executed</li> <li>DeleteCommand command is executed</li> <li>MergeIntoCommand command is executed</li> <li>UpdateCommand command is executed</li> <li>WriteIntoDelta command is executed</li> <li><code>DeltaSink</code> is requested to addBatch</li> </ul>"},{"location":"OptimisticTransactionImpl/#preparing-commit","text":"","title":"Preparing Commit <p><code>commit</code> firstly prepares a commit (that gives the final actions to commit that may be different from the given actions).</p>"},{"location":"OptimisticTransactionImpl/#isolation-level","text":"","title":"Isolation Level <p><code>commit</code> determines the isolation level based on FileActions (in the given actions) and their dataChange flag.</p> <p>With all actions with dataChange flag disabled (<code>false</code>), <code>commit</code> assumes no data changed and chooses SnapshotIsolation else Serializable.</p>"},{"location":"OptimisticTransactionImpl/#blind-append","text":"","title":"Blind Append <p><code>commit</code> is considered blind append when the following all hold:</p> <ol> <li>There are only AddFiles among FileActions in the actions (onlyAddFiles)</li> <li>It does not depend on files, i.e. the readPredicates and readFiles are empty (dependsOnFiles)</li> </ol>"},{"location":"OptimisticTransactionImpl/#commitinfo","text":"","title":"CommitInfo <p><code>commit</code>...FIXME</p>"},{"location":"OptimisticTransactionImpl/#registering-post-commit-hook","text":"","title":"Registering Post-Commit Hook <p><code>commit</code> registers the GenerateSymlinkManifest post-commit hook when there is a FileAction among the actions and the compatibility.symlinkFormatManifest.enabled table property is enabled.</p>"},{"location":"OptimisticTransactionImpl/#commit-version","text":"","title":"Commit Version <p><code>commit</code> doCommit with the next version, the actions, attempt number <code>0</code>, and the select isolation level.</p> <p><code>commit</code> prints out the following INFO message to the logs:</p> <pre><code>Committed delta #[commitVersion] to [logPath]\n</code></pre>"},{"location":"OptimisticTransactionImpl/#performing-post-commit-operations","text":"","title":"Performing Post-Commit Operations <p><code>commit</code> postCommit (with the version committed and the actions).</p>"},{"location":"OptimisticTransactionImpl/#executing-post-commit-hooks","text":"","title":"Executing Post-Commit Hooks <p>In the end, commit runs post-commit hooks and returns the version of the successful commit.</p>"},{"location":"OptimisticTransactionImpl/#docommitretryiteratively","text":"","title":"doCommitRetryIteratively <pre><code>doCommitRetryIteratively(\n  attemptVersion: Long,\n  actions: Seq[Action],\n  isolationLevel: IsolationLevel): Long\n</code></pre> <p><code>doCommitRetryIteratively</code>...FIXME</p>"},{"location":"OptimisticTransactionImpl/#checking-logical-conflicts-with-concurrent-updates","text":"","title":"Checking Logical Conflicts with Concurrent Updates <pre><code>checkForConflicts(\n  checkVersion: Long,\n  actions: Seq[Action],\n  attemptNumber: Int,\n  commitIsolationLevel: IsolationLevel): Long\n</code></pre> <p><code>checkForConflicts</code> checks for logical conflicts (of the given <code>actions</code>) with concurrent updates (actions of the commits since the transaction has started).</p> <p><code>checkForConflicts</code> gives the next possible commit version unless the following happened between the time of read (<code>checkVersion</code>) and the time of this commit attempt:</p> <ol> <li>Client is up to date with the table protocol for reading and writing (and hence allowed to access the table)</li> <li>Protocol version has changed</li> <li>Metadata has changed</li> <li>AddFiles have been added that the txn should have read based on the given IsolationLevel (Concurrent Append)</li> <li>AddFiles that the txn read have been deleted (Concurrent Delete)</li> <li>Files have been deleted by the txn and since the time of read (Concurrent Delete)</li> <li>Idempotent transactions have conflicted (Multiple Streaming Queries with the same checkpoint location)</li> </ol> <p><code>checkForConflicts</code> takes the next possible commit version.</p> <p>For every commit since the time of read (<code>checkVersion</code>) and this commit attempt, <code>checkForConflicts</code> does the following:</p> <ul> <li> <p>FIXME</p> </li> <li> <p>Prints out the following INFO message to the logs:</p> <pre><code>Completed checking for conflicts Version: [version] Attempt: [attemptNumber] Time: [totalCheckAndRetryTime] ms\n</code></pre> </li> </ul> <p>In the end, <code>checkForConflicts</code> prints out the following INFO message to the logs:</p> <pre><code>No logical conflicts with deltas [[checkVersion], [nextAttemptVersion]), retrying.\n</code></pre>"},{"location":"OptimisticTransactionImpl/#getprettypartitionmessage","text":"","title":"getPrettyPartitionMessage <pre><code>getPrettyPartitionMessage(\n  partitionValues: Map[String, String]): String\n</code></pre> <p><code>getPrettyPartitionMessage</code>...FIXME</p>"},{"location":"OptimisticTransactionImpl/#postcommit","text":"","title":"postCommit <pre><code>postCommit(\n  commitVersion: Long,\n  commitActions: Seq[Action]): Unit\n</code></pre> <p><code>postCommit</code> turns the committed flag on.</p> <p><code>postCommit</code> requests the DeltaLog to checkpoint when the given <code>commitVersion</code> is not <code>0</code> (first commit) and the checkpoint interval has been reached (based on the given <code>commitVersion</code>).</p>  <p>Note</p> <p><code>commitActions</code> argument is not used.</p>  <p><code>postCommit</code> prints out the following WARN message to the logs in case of <code>IllegalStateException</code>:</p> <pre><code>Failed to checkpoint table state.\n</code></pre>"},{"location":"OptimisticTransactionImpl/#preparecommit","text":"","title":"prepareCommit <pre><code>prepareCommit(\n  actions: Seq[Action],\n  op: DeltaOperations.Operation): Seq[Action]\n</code></pre> <p><code>prepareCommit</code> adds the newMetadata action (if available) to the given actions.</p> <p><code>prepareCommit</code> verifyNewMetadata if there was one.</p> <p><code>prepareCommit</code>...FIXME</p> <p><code>prepareCommit</code> requests the DeltaLog to protocolWrite.</p> <p><code>prepareCommit</code>...FIXME</p>"},{"location":"OptimisticTransactionImpl/#multiple-metadata-changes-not-allowed","text":"","title":"Multiple Metadata Changes Not Allowed <p><code>prepareCommit</code> throws an <code>AssertionError</code> when there are multiple metadata changes in the transaction (by means of Metadata actions):</p> <pre><code>Cannot change the metadata more than once in a transaction.\n</code></pre>"},{"location":"OptimisticTransactionImpl/#committing-transaction-allowed-once-only","text":"","title":"Committing Transaction Allowed Once Only <p>prepareCommit throws an <code>AssertionError</code> when the committed internal flag is enabled:</p> <pre><code>Transaction already committed.\n</code></pre>"},{"location":"OptimisticTransactionImpl/#registering-post-commit-hook_1","text":"","title":"Registering Post-Commit Hook <pre><code>registerPostCommitHook(\n  hook: PostCommitHook): Unit\n</code></pre> <p><code>registerPostCommitHook</code> registers (adds) the given PostCommitHook to the postCommitHooks internal registry.</p>"},{"location":"OptimisticTransactionImpl/#runpostcommithooks","text":"","title":"runPostCommitHooks <pre><code>runPostCommitHooks(\n  version: Long,\n  committedActions: Seq[Action]): Unit\n</code></pre> <p><code>runPostCommitHooks</code> simply runs every post-commit hook registered (in the postCommitHooks internal registry).</p> <p><code>runPostCommitHooks</code> clears the active transaction (making all follow-up operations non-transactional).</p>  <p>Note</p> <p>Hooks may create new transactions.</p>"},{"location":"OptimisticTransactionImpl/#handling-non-fatal-exceptions","text":"","title":"Handling Non-Fatal Exceptions <p>For non-fatal exceptions, <code>runPostCommitHooks</code> prints out the following ERROR message to the logs, records the delta event, and requests the post-commit hook to handle the error.</p> <pre><code>Error when executing post-commit hook [name] for commit [version]\n</code></pre>"},{"location":"OptimisticTransactionImpl/#assertionerror","text":"","title":"AssertionError <p><code>runPostCommitHooks</code> throws an <code>AssertionError</code> when committed flag is disabled:</p> <pre><code>Can't call post commit hooks before committing\n</code></pre>"},{"location":"OptimisticTransactionImpl/#next-possible-commit-version","text":"","title":"Next Possible Commit Version <pre><code>getNextAttemptVersion(\n  previousAttemptVersion: Long): Long\n</code></pre> <p><code>getNextAttemptVersion</code> requests the DeltaLog to update (and give the latest state snapshot of the delta table).</p> <p>In the end, <code>getNextAttemptVersion</code> requests the <code>Snapshot</code> for the version and increments it.</p>  <p>Note</p> <p>The input <code>previousAttemptVersion</code> argument is not used.</p>"},{"location":"OptimisticTransactionImpl/#operation-metrics","text":"","title":"Operation Metrics <pre><code>getOperationMetrics(\n  op: Operation): Option[Map[String, String]]\n</code></pre> <p><code>getOperationMetrics</code> gives the metrics of the given Operation when the spark.databricks.delta.history.metricsEnabled configuration property is enabled. Otherwise, <code>getOperationMetrics</code> gives <code>None</code>.</p>"},{"location":"OptimisticTransactionImpl/#commitinfo_1","text":"","title":"CommitInfo <p><code>OptimisticTransactionImpl</code> creates a CommitInfo when requested to commit with spark.databricks.delta.commitInfo.enabled configuration enabled.</p> <p><code>OptimisticTransactionImpl</code> uses the <code>CommitInfo</code> to <code>recordDeltaEvent</code> (as a <code>CommitStats</code>).</p>"},{"location":"OptimisticTransactionImpl/#attempting-commit","text":"","title":"Attempting Commit <pre><code>doCommit(\n  attemptVersion: Long,\n  actions: Seq[Action],\n  attemptNumber: Int,\n  isolationLevel: IsolationLevel): Long\n</code></pre> <p><code>doCommit</code> returns the given <code>attemptVersion</code> as the commit version if successful or checkAndRetry.</p> <p><code>doCommit</code> is used when:</p> <ul> <li>OptimisticTransactionImpl is requested to commit (and checkAndRetry).</li> </ul>  <p>Internally, <code>doCommit</code> prints out the following DEBUG message to the logs:</p> <pre><code>Attempting to commit version [attemptVersion] with [n] actions with [isolationLevel] isolation level\n</code></pre>"},{"location":"OptimisticTransactionImpl/#writing-out","text":"","title":"Writing Out <p><code>doCommit</code> requests the DeltaLog for the LogStore to write out the given actions to a delta file in the log directory with the <code>attemptVersion</code> version, e.g.</p> <pre><code>00000000000000000001.json\n</code></pre> <p><code>doCommit</code> writes the actions out in JSON format.</p>  <p>Note</p> <p>LogStores must throw a <code>java.nio.file.FileAlreadyExistsException</code> exception if the delta file already exists. Any <code>FileAlreadyExistsExceptions</code> are caught by doCommit itself to checkAndRetry.</p>"},{"location":"OptimisticTransactionImpl/#post-commit-snapshot","text":"","title":"Post-Commit Snapshot <p><code>doCommit</code> requests the DeltaLog to update.</p>"},{"location":"OptimisticTransactionImpl/#illegalstateexception","text":"","title":"IllegalStateException <p><code>doCommit</code> throws an <code>IllegalStateException</code> when the version of the snapshot after update is smaller than the given <code>attemptVersion</code> version.</p> <pre><code>The committed version is [attemptVersion] but the current version is [version].\n</code></pre>"},{"location":"OptimisticTransactionImpl/#commitstats","text":"","title":"CommitStats <p><code>doCommit</code> records a new <code>CommitStats</code> and returns the given <code>attemptVersion</code> as the commit version.</p>"},{"location":"OptimisticTransactionImpl/#filealreadyexistsexceptions","text":"","title":"FileAlreadyExistsExceptions <p><code>doCommit</code> catches <code>FileAlreadyExistsExceptions</code> and checkAndRetry.</p>"},{"location":"OptimisticTransactionImpl/#retrying-commit","text":"","title":"Retrying Commit <pre><code>checkAndRetry(\n  checkVersion: Long,\n  actions: Seq[Action],\n  attemptNumber: Int): Long\n</code></pre> <p><code>checkAndRetry</code>...FIXME</p> <p><code>checkAndRetry</code> is used when OptimisticTransactionImpl is requested to commit (and attempts a commit that failed with an <code>FileAlreadyExistsException</code>).</p>"},{"location":"OptimisticTransactionImpl/#verifynewmetadata","text":"","title":"verifyNewMetadata <pre><code>verifyNewMetadata(\n  metadata: Metadata): Unit\n</code></pre> <p><code>verifyNewMetadata</code>...FIXME</p> <p><code>verifyNewMetadata</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to prepareCommit and updateMetadata</li> </ul>"},{"location":"OptimisticTransactionImpl/#withglobalconfigdefaults","text":"","title":"withGlobalConfigDefaults <pre><code>withGlobalConfigDefaults(\n  metadata: Metadata): Metadata\n</code></pre> <p><code>withGlobalConfigDefaults</code>...FIXME</p> <p><code>withGlobalConfigDefaults</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to updateMetadata and updateMetadataForNewTable</li> </ul>"},{"location":"OptimisticTransactionImpl/#looking-up-transaction-version-for-given-streaming-query-id","text":"","title":"Looking Up Transaction Version For Given (Streaming Query) ID <pre><code>txnVersion(\n  id: String): Long\n</code></pre> <p><code>txnVersion</code> simply registers (adds) the given ID in the readTxn internal registry.</p> <p>In the end, <code>txnVersion</code> requests the Snapshot for the transaction version for the given ID or <code>-1</code>.</p> <p><code>txnVersion</code> is used when:</p> <ul> <li><code>DeltaSink</code> is requested to add a streaming micro-batch</li> </ul>"},{"location":"OptimisticTransactionImpl/#user-defined-metadata","text":"","title":"User-Defined Metadata <pre><code>getUserMetadata(\n  op: Operation): Option[String]\n</code></pre> <p><code>getUserMetadata</code> returns the Operation.md#userMetadata[userMetadata] of the given Operation.md[] (if defined) or the value of DeltaSQLConf.md#DELTA_USER_METADATA[spark.databricks.delta.commitInfo.userMetadata] configuration property.</p> <p><code>getUserMetadata</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to commit (and spark.databricks.delta.commitInfo.enabled configuration property is enabled)</li> <li>ConvertToDeltaCommand is executed (and in turn requests <code>DeltaCommand</code> to commitLarge)</li> </ul>"},{"location":"OptimisticTransactionImpl/#internal-registries","text":"","title":"Internal Registries"},{"location":"OptimisticTransactionImpl/#post-commit-hooks","text":"","title":"Post-Commit Hooks <pre><code>postCommitHooks: ArrayBuffer[PostCommitHook]\n</code></pre> <p><code>OptimisticTransactionImpl</code> manages PostCommitHooks that will be executed right after a commit is successful.</p> <p>Post-commit hooks can be registered, but only the GenerateSymlinkManifest post-commit hook is supported.</p>"},{"location":"OptimisticTransactionImpl/#newmetadata","text":"","title":"newMetadata <pre><code>newMetadata: Option[Metadata]\n</code></pre> <p><code>OptimisticTransactionImpl</code> uses the <code>newMetadata</code> internal registry for a new Metadata that should be committed with this transaction.</p> <p><code>newMetadata</code> is initially undefined (<code>None</code>). It can be updated only once and before the transaction writes out any files.</p> <p><code>newMetadata</code> is used when prepareCommit and doCommit (for statistics).</p> <p><code>newMetadata</code> is available using metadata method.</p>"},{"location":"OptimisticTransactionImpl/#readfiles","text":"","title":"readFiles <pre><code>readFiles: HashSet[AddFile]\n</code></pre> <p><code>OptimisticTransactionImpl</code> uses <code>readFiles</code> registry to track AddFiles that have been seen (scanned) by this transaction (when requested to filterFiles).</p> <p>Used to determine isBlindAppend and checkForConflicts (and fail if the files have been deleted that the txn read).</p>"},{"location":"OptimisticTransactionImpl/#readpredicates","text":"","title":"readPredicates <pre><code>readPredicates: ArrayBuffer[Expression]\n</code></pre> <p><code>readPredicates</code> holds predicate expressions for partitions the transaction is modifying.</p> <p><code>readPredicates</code> is added a new predicate expression when filterFiles and readWholeTable.</p> <p><code>readPredicates</code> is used when checkAndRetry.</p>"},{"location":"OptimisticTransactionImpl/#internal-properties","text":"","title":"Internal Properties"},{"location":"OptimisticTransactionImpl/#committed","text":"","title":"committed <p>Controls whether the transaction has been committed or not (and prevents prepareCommit from being executed again)</p> <p>Default: <code>false</code></p> <p>Enabled in postCommit</p>"},{"location":"OptimisticTransactionImpl/#readtxn","text":"","title":"readTxn <p>Streaming query IDs that have been seen by this transaction</p> <p>A new queryId is added when <code>OptimisticTransactionImpl</code> is requested for txnVersion</p> <p>Used when <code>OptimisticTransactionImpl</code> is requested to checkAndRetry (to fail with a <code>ConcurrentTransactionException</code> for idempotent transactions that have conflicted)</p>"},{"location":"OptimisticTransactionImpl/#snapshotmetadata","text":"","title":"snapshotMetadata <p>Metadata of the Snapshot</p>"},{"location":"OptimisticTransactionImpl/#readwholetable","text":"","title":"readWholeTable <pre><code>readWholeTable(): Unit\n</code></pre> <p><code>readWholeTable</code> simply adds <code>True</code> literal to the readPredicates internal registry.</p> <p><code>readWholeTable</code> is used when:</p> <ul> <li><code>DeltaSink</code> is requested to add a streaming micro-batch (and the batch reads the same Delta table as this sink is going to write to)</li> </ul>"},{"location":"OptimisticTransactionImpl/#updatemetadatafornewtable","text":"","title":"updateMetadataForNewTable <pre><code>updateMetadataForNewTable(\n  metadata: Metadata): Unit\n</code></pre> <p><code>updateMetadataForNewTable</code>...FIXME</p> <p><code>updateMetadataForNewTable</code> is used when:</p> <ul> <li>ConvertToDeltaCommand and CreateDeltaTableCommand are executed</li> </ul>"},{"location":"OptimisticTransactionImpl/#metadata","text":"","title":"Metadata <pre><code>metadata: Metadata\n</code></pre> <p><code>metadata</code> is part of the TransactionalWrite abstraction.</p> <p><code>metadata</code> is either the newMetadata (if defined) or the snapshotMetadata.</p>"},{"location":"OptimisticTransactionImpl/#updating-metadata","text":"","title":"Updating Metadata <pre><code>updateMetadata(\n  metadata: Metadata): Unit\n</code></pre> <p><code>updateMetadata</code> updates the newMetadata internal property based on the readVersion:</p> <ul> <li> <p>For <code>-1</code>, <code>updateMetadata</code> updates the configuration of the given metadata with a new metadata based on the <code>SQLConf</code> (of the active <code>SparkSession</code>), the configuration of the given metadata and a new Protocol</p> </li> <li> <p>For other versions, <code>updateMetadata</code> leaves the given Metadata unchanged</p> </li> </ul> <p><code>updateMetadata</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to updateMetadataForNewTable</li> <li>AlterTableSetPropertiesDeltaCommand, AlterTableUnsetPropertiesDeltaCommand, AlterTableAddColumnsDeltaCommand, AlterTableChangeColumnDeltaCommand, AlterTableReplaceColumnsDeltaCommand are executed</li> <li> <p>ConvertToDeltaCommand is executed</p> </li> <li> <p><code>ImplicitMetadataOperation</code> is requested to updateMetadata</p> </li> </ul>"},{"location":"OptimisticTransactionImpl/#assertionerror_1","text":"","title":"AssertionError <p><code>updateMetadata</code> throws an <code>AssertionError</code> when the hasWritten flag is enabled:</p> <pre><code>Cannot update the metadata in a transaction that has already written data.\n</code></pre>"},{"location":"OptimisticTransactionImpl/#assertionerror_2","text":"","title":"AssertionError <p><code>updateMetadata</code> throws an <code>AssertionError</code> when the newMetadata is not empty:</p> <pre><code>Cannot change the metadata more than once in a transaction.\n</code></pre>"},{"location":"OptimisticTransactionImpl/#files-to-scan-matching-given-predicates","text":"","title":"Files To Scan Matching Given Predicates <pre><code>filterFiles(): Seq[AddFile] // (1)\nfilterFiles(\n  filters: Seq[Expression]): Seq[AddFile]\n</code></pre> <ol> <li>No filters = all files</li> </ol> <p><code>filterFiles</code> gives the files to scan for the given predicates (filter expressions).</p> <p>Internally, <code>filterFiles</code> requests the Snapshot for the filesForScan (for no projection attributes and the given filters).</p> <p><code>filterFiles</code> finds the partition predicates among the given filters (and the partition columns of the Metadata).</p> <p><code>filterFiles</code> registers (adds) the partition predicates (in the readPredicates internal registry) and the files to scan (in the readFiles internal registry).</p> <p><code>filterFiles</code> is used when:</p> <ul> <li>ActiveOptimisticTransactionRule is executed</li> <li><code>DeltaSink</code> is requested to add a streaming micro-batch (with <code>Complete</code> output mode)</li> <li>DeleteCommand, MergeIntoCommand and UpdateCommand, WriteIntoDelta are executed</li> <li>CreateDeltaTableCommand is executed</li> </ul>"},{"location":"PartitionFiltering/","text":"<p><code>PartitionFiltering</code> is an abstraction of snapshots with partition filtering for scan.</p>","title":"PartitionFiltering"},{"location":"PartitionFiltering/#implementations","text":"<p>Snapshot is the default and only known <code>PartitionFiltering</code> in Delta Lake.</p>","title":"Implementations"},{"location":"PartitionFiltering/#files-to-scan-matching-projection-attributes-and-predicates","text":"","title":"Files to Scan (Matching Projection Attributes and Predicates) <pre><code>filesForScan(\n  projection: Seq[Attribute],\n  filters: Seq[Expression],\n  keepStats: Boolean = false): DeltaScan\n</code></pre> <p><code>filesForScan</code>...FIXME</p> <p><code>filesForScan</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested for the files to scan matching given predicates</li> <li><code>TahoeLogFileIndex</code> is requested for the files matching predicates and the input files</li> </ul>"},{"location":"PinnedTahoeFileIndex/","text":"<p><code>PinnedTahoeFileIndex</code> is a TahoeFileIndex.</p>","title":"PinnedTahoeFileIndex"},{"location":"PinnedTahoeFileIndex/#creating-instance","text":"<p><code>PinnedTahoeFileIndex</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> DeltaLog <li> Hadoop Path <li> Snapshot  <p><code>PinnedTahoeFileIndex</code> is created\u00a0when:</p> <ul> <li>ActiveOptimisticTransactionRule logical optimization rule is executed</li> </ul>","title":"Creating Instance"},{"location":"PostCommitHook/","text":"<p><code>PostCommitHook</code> is an abstraction of post-commit hooks that can be executed (at the end of transaction commit).</p>","title":"PostCommitHook"},{"location":"PostCommitHook/#contract","text":"","title":"Contract"},{"location":"PostCommitHook/#name","text":"","title":"Name <pre><code>name: String\n</code></pre> <p>User-friendly name of the hook for error reporting</p>"},{"location":"PostCommitHook/#executing-post-commit-hook","text":"","title":"Executing Post-Commit Hook <pre><code>run(\n  spark: SparkSession,\n  txn: OptimisticTransactionImpl,\n  committedActions: Seq[Action]): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to runPostCommitHooks (at the end of transaction commit).</li> </ul>"},{"location":"PostCommitHook/#implementations","text":"<ul> <li>GenerateSymlinkManifestImpl</li> </ul>","title":"Implementations"},{"location":"PreprocessTableDelete/","text":"<p><code>PreprocessTableDelete</code> is a post-hoc logical resolution rule (<code>Rule[LogicalPlan]</code>) to resolve DeltaDelete commands in a logical query plan into DeleteCommands.</p> <p><code>PreprocessTableDelete</code> is installed (injected) into a <code>SparkSession</code> using DeltaSparkSessionExtension.</p>","title":"PreprocessTableDelete Logical Resolution Rule"},{"location":"PreprocessTableDelete/#creating-instance","text":"<p><code>PreprocessTableDelete</code> takes the following to be created:</p> <ul> <li> <code>SQLConf</code> (Spark SQL)  <p><code>PreprocessTableDelete</code> is created when:</p> <ul> <li>DeltaSparkSessionExtension is executed (and registers Delta SQL support)</li> </ul>","title":"Creating Instance"},{"location":"PreprocessTableDelete/#executing-rule","text":"","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>\u00a0is part of the <code>Rule</code> (Spark SQL) abstraction.</p> <p>apply resolves (replaces) DeltaDelete logical commands (in a logical query plan) into DeleteCommands.</p>"},{"location":"PreprocessTableDelete/#tocommand","text":"","title":"toCommand <pre><code>toCommand(\n  d: DeltaDelete): DeleteCommand\n</code></pre> <p><code>toCommand</code>...FIXME</p>"},{"location":"PreprocessTableMerge/","text":"<p><code>PreprocessTableMerge</code> is a post-hoc logical resolution rule (Spark SQL) to resolve DeltaMergeInto logical commands (in a logical query plan) into MergeIntoCommands.</p> <p><code>PreprocessTableMerge</code> is injected (installed) into a <code>SparkSession</code> using DeltaSparkSessionExtension.</p>","title":"PreprocessTableMerge Logical Resolution Rule"},{"location":"PreprocessTableMerge/#creating-instance","text":"<p><code>PreprocessTableMerge</code> takes the following to be created:</p> <ul> <li> <code>SQLConf</code> (Spark SQL)  <p><code>PreprocessTableMerge</code> is created\u00a0when:</p> <ul> <li><code>DeltaSparkSessionExtension</code> is requested to register Delta SQL support</li> <li><code>DeltaMergeBuilder</code> is requested to execute</li> </ul>","title":"Creating Instance"},{"location":"PreprocessTableMerge/#executing-rule","text":"","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>\u00a0is part of the <code>Rule</code> (Spark SQL) abstraction.</p> <p>In summary, <code>apply</code> resolves (replaces) DeltaMergeInto logical commands (in a logical query plan) into corresponding MergeIntoCommands.</p> <p>Internally, <code>apply</code>...FIXME</p>"},{"location":"PreprocessTableUpdate/","text":"<p><code>PreprocessTableUpdate</code> is a post-hoc logical resolution rule (<code>Rule[LogicalPlan]</code>) to resolve DeltaUpdateTable commands in a logical query plan into UpdateCommands.</p> <p><code>PreprocessTableUpdate</code> is installed (injected) into a <code>SparkSession</code> using DeltaSparkSessionExtension.</p>","title":"PreprocessTableUpdate Logical Resolution Rule"},{"location":"PreprocessTableUpdate/#creating-instance","text":"<p><code>PreprocessTableUpdate</code> takes the following to be created:</p> <ul> <li> <code>SQLConf</code> (Spark SQL)  <p><code>PreprocessTableUpdate</code> is created when:</p> <ul> <li>DeltaSparkSessionExtension is executed (and registers Delta SQL support)</li> </ul>","title":"Creating Instance"},{"location":"PreprocessTableUpdate/#executing-rule","text":"","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code>\u00a0is part of the <code>Rule</code> (Spark SQL) abstraction.</p> <p>apply resolves (replaces) DeltaUpdateTable logical commands (in a logical query plan) into UpdateCommands.</p>"},{"location":"PreprocessTableUpdate/#tocommand","text":"","title":"toCommand <pre><code>toCommand(\n  update: DeltaUpdateTable): UpdateCommand\n</code></pre> <p><code>toCommand</code>...FIXME</p>"},{"location":"Protocol/","text":"<p><code>Protocol</code> is an Action.</p>","title":"Protocol"},{"location":"Protocol/#creating-instance","text":"<p><code>Protocol</code> takes the following to be created:</p> <ul> <li> Minimum Reader Version Allowed (default: <code>1</code>) <li> Minimum Writer Version Allowed (default: <code>3</code>)  <p><code>Protocol</code> is created\u00a0when:</p> <ul> <li><code>DeltaTable</code> is requested to upgradeTableProtocol</li> <li>FIXME</li> </ul>","title":"Creating Instance"},{"location":"Protocol/#fornewtable-utility","text":"","title":"forNewTable Utility <pre><code>forNewTable(\n  spark: SparkSession,\n  metadata: Metadata): Protocol\n</code></pre> <p><code>forNewTable</code> creates a new Protocol for the given <code>SparkSession</code> and Metadata.</p> <p><code>forNewTable</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to updateMetadata and updateMetadataForNewTable</li> <li><code>InitialSnapshot</code> is requested to <code>computedState</code></li> </ul>"},{"location":"Protocol/#apply","text":"","title":"apply <pre><code>apply(\n  spark: SparkSession,\n  metadataOpt: Option[Metadata]): Protocol\n</code></pre> <p><code>apply</code>...FIXME</p>"},{"location":"Protocol/#checkprotocolrequirements-utility","text":"","title":"checkProtocolRequirements Utility <pre><code>checkProtocolRequirements(\n  spark: SparkSession,\n  metadata: Metadata,\n  current: Protocol): Option[Protocol]\n</code></pre> <p><code>checkProtocolRequirements</code>...FIXME</p> <p><code>checkProtocolRequirements</code>\u00a0is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to verifyNewMetadata</li> </ul>"},{"location":"Protocol/#minimum-protocol-required","text":"","title":"Minimum Protocol Required <pre><code>requiredMinimumProtocol(\n  spark: SparkSession,\n  metadata: Metadata): (Protocol, Seq[String])\n</code></pre> <p><code>requiredMinimumProtocol</code>...FIXME</p> <p><code>requiredMinimumProtocol</code>\u00a0is used when:</p> <ul> <li><code>Protocol</code> utility is used to create a Protocol and checkProtocolRequirements</li> </ul>"},{"location":"ReadChecksum/","text":"<p><code>ReadChecksum</code> is...FIXME</p>","title":"ReadChecksum"},{"location":"RemoveFile/","text":"<p><code>RemoveFile</code> is a FileAction that represents an action of removing (deleting) a file from a delta table.</p>","title":"RemoveFile"},{"location":"RemoveFile/#creating-instance","text":"<p><code>RemoveFile</code> takes the following to be created:</p> <ul> <li> Path <li> Deletion Timestamp (optional) <li> <code>dataChange</code> flag <li> <code>extendedFileMetadata</code> flag (default: <code>false</code>) <li> Partition values (default: <code>null</code>) <li> Size (in bytes) (default: <code>0</code>) <li> Tags (<code>Map[String, String]</code>) (default: <code>null</code>)  <p><code>RemoveFile</code> is created\u00a0when:</p> <ul> <li><code>AddFile</code> action is requested to removeWithTimestamp</li> </ul>","title":"Creating Instance"},{"location":"SQLMetricsReporting/","text":"<p><code>SQLMetricsReporting</code> is an extension for OptimisticTransactionImpl to track performance metrics of Operations for reporting.</p>","title":"SQLMetricsReporting"},{"location":"SQLMetricsReporting/#implementations","text":"<ul> <li>OptimisticTransactionImpl</li> </ul>","title":"Implementations"},{"location":"SQLMetricsReporting/#operationsqlmetrics-registry","text":"","title":"operationSQLMetrics Registry <pre><code>operationSQLMetrics: Map[String, SQLMetric]\n</code></pre> <p><code>SQLMetricsReporting</code> uses <code>operationSQLMetrics</code> internal registry for <code>SQLMetric</code>s (Spark SQL) by their names.</p> <p><code>SQLMetric</code>s are registered only when spark.databricks.delta.history.metricsEnabled configuration property is enabled.</p> <p><code>operationSQLMetrics</code> is used when <code>SQLMetricsReporting</code> is requested for the following:</p> <ul> <li>Operation Metrics</li> <li>getMetric</li> </ul>"},{"location":"SQLMetricsReporting/#registering-sqlmetrics","text":"","title":"Registering SQLMetrics <pre><code>registerSQLMetrics(\n  spark: SparkSession,\n  metrics: Map[String, SQLMetric]): Unit\n</code></pre> <p><code>registerSQLMetrics</code> adds (registers) the given metrics to the operationSQLMetrics internal registry only when spark.databricks.delta.history.metricsEnabled configuration property is enabled.</p> <p><code>registerSQLMetrics</code> is used when:</p> <ul> <li>DeleteCommand, MergeIntoCommand, UpdateCommand commands are executed</li> <li><code>TransactionalWrite</code> is requested to writeFiles</li> <li><code>DeltaSink</code> is requested to addBatch</li> </ul>"},{"location":"SQLMetricsReporting/#operation-metrics","text":"","title":"Operation Metrics <pre><code>getMetricsForOperation(\n  operation: Operation): Map[String, String]\n</code></pre> <p><code>getMetricsForOperation</code> requests the given Operation to transform the operation metrics.</p> <p><code>getMetricsForOperation</code> is used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested for the operation metrics</li> </ul>"},{"location":"SQLMetricsReporting/#looking-up-operation-metric","text":"","title":"Looking Up Operation Metric <pre><code>getMetric(\n  name: String): Option[SQLMetric]\n</code></pre> <p><code>getMetric</code> uses the operationSQLMetrics registry to look up the <code>SQLMetric</code> by name.</p> <p><code>getMetric</code> is used when:</p> <ul> <li>UpdateCommand is executed</li> </ul>"},{"location":"SchemaUtils/","text":"","title":"SchemaUtils Utility"},{"location":"SchemaUtils/#mergeschemas","text":"","title":"mergeSchemas <pre><code>mergeSchemas(\n  tableSchema: StructType,\n  dataSchema: StructType,\n  allowImplicitConversions: Boolean = false): StructType\n</code></pre> <p><code>mergeSchemas</code>...FIXME</p> <p><code>mergeSchemas</code>\u00a0is used when:</p> <ul> <li>PreprocessTableMerge logical resolution rule is executed</li> <li>ConvertToDeltaCommand is executed</li> <li><code>ImplicitMetadataOperation</code> is requested to update metadata</li> </ul>"},{"location":"SetTransaction/","text":"<p><code>SetTransaction</code> is an Action defined by the following properties:</p> <ul> <li> Application ID (i.e. streaming query ID) <li> Version (i.e. micro-batch ID) <li> Last Updated (optional) (i.e. milliseconds since the epoch)  <p><code>SetTransaction</code> is created when:</p> <ul> <li><code>DeltaSink</code> is requested to add a streaming micro-batch (for <code>STREAMING UPDATE</code> operation idempotence at query restart)</li> </ul>","title":"SetTransaction"},{"location":"SetTransaction/#demo","text":"<pre><code>val path = \"/tmp/delta/users\"\n\nimport org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, path)\n\nimport org.apache.spark.sql.delta.actions.SetTransaction\nassert(deltaLog.snapshot.setTransactions.isInstanceOf[Seq[SetTransaction]])\n\ndeltaLog.snapshot.setTransactions\n</code></pre>","title":"Demo"},{"location":"SingleAction/","text":"<p><code>SingleAction</code> is...FIXME</p>","title":"SingleAction"},{"location":"Snapshot/","text":"<p><code>Snapshot</code> is an immutable snapshot of the state of a Delta table at the version.</p>","title":"Snapshot"},{"location":"Snapshot/#creating-instance","text":"<p><code>Snapshot</code> takes the following to be created:</p> <ul> <li> Hadoop Path to the log directory <li> Version <li> LogSegment <li> <code>minFileRetentionTimestamp</code> (that is exactly DeltaLog.minFileRetentionTimestamp) <li> DeltaLog <li> Timestamp <li> <code>VersionChecksum</code>  <p>While being created, <code>Snapshot</code> prints out the following INFO message to the logs and initialize:</p> <pre><code>Created snapshot [this]\n</code></pre> <p><code>Snapshot</code> is created when:</p> <ul> <li><code>SnapshotManagement</code> is requested for a Snapshot</li> </ul>","title":"Creating Instance"},{"location":"Snapshot/#initializing","text":"","title":"Initializing <pre><code>init(): Unit\n</code></pre> <p><code>init</code> requests the DeltaLog for the protocolRead for the Protocol.</p>"},{"location":"Snapshot/#demo","text":"<ul> <li>Demo: DeltaTable, DeltaLog And Snapshots</li> </ul>","title":"Demo"},{"location":"Snapshot/#computed-state","text":"","title":"Computed State <pre><code>computedState: State\n</code></pre>  Lazy Value <p><code>computedState</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>computedState</code> takes the current cached set of actions and reads the latest state (executes a <code>state.select(...).first()</code> query).</p>  <p>Note</p> <p>The <code>state.select(...).first()</code> query uses aggregate standard functions (e.g. <code>last</code>, <code>collect_set</code>, <code>sum</code>, <code>count</code>) and so uses <code>groupBy</code> over the whole dataset indirectly.</p>  <p></p> <p><code>computedState</code> assumes that the protocol and metadata (actions) are defined. <code>computedState</code> throws an <code>IllegalStateException</code> when the actions are not defined and spark.databricks.delta.stateReconstructionValidation.enabled configuration property is enabled.</p> <pre><code>The [action] of your Delta table couldn't be recovered while Reconstructing\nversion: [version]. Did you manually delete files in the _delta_log directory?\n</code></pre>  <p>Note</p> <p>The <code>state.select(...).first()</code> query uses <code>last</code> with <code>ignoreNulls</code> flag <code>true</code> and so may give no rows for <code>first()</code>.</p>  <p><code>computedState</code> makes sure that the <code>State</code> to be returned has at least the default protocol and metadata (actions) defined.</p>"},{"location":"Snapshot/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"Snapshot/#sparkdatabricksdeltasnapshotpartitions","text":"","title":"spark.databricks.delta.snapshotPartitions <p><code>Snapshot</code> uses the spark.databricks.delta.snapshotPartitions configuration property for the number of partitions to use for state reconstruction.</p>"},{"location":"Snapshot/#sparkdatabricksdeltastatereconstructionvalidationenabled","text":"","title":"spark.databricks.delta.stateReconstructionValidation.enabled <p><code>Snapshot</code> uses the spark.databricks.delta.stateReconstructionValidation.enabled configuration property for reconstructing state.</p>"},{"location":"Snapshot/#state-dataset-of-actions","text":"","title":"State Dataset (of Actions) <pre><code>state: Dataset[SingleAction]\n</code></pre> <p><code>state</code> requests the cached delta table state for the current state (from the cache).</p> <p><code>state</code> is used when:</p> <ul> <li><code>Checkpoints</code> utility is used to writeCheckpoint</li> <li><code>Snapshot</code> is requested for computedState, all files and files removed (tombstones)</li> <li><code>VacuumCommand</code> utility is requested for garbage collection</li> </ul>"},{"location":"Snapshot/#cached-delta-table-state","text":"","title":"Cached Delta Table State <pre><code>cachedState: CachedDS[SingleAction]\n</code></pre>  Lazy Value <p><code>cachedState</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>cachedState</code> creates a Cached Delta State with the following:</p> <ul> <li> <p>The dataset part is the stateReconstruction dataset of SingleActions</p> </li> <li> <p>The name in the format Delta Table State #version - [redactedPath] (with the version and the path redacted)</p> </li> </ul>"},{"location":"Snapshot/#all-addfiles","text":"","title":"All AddFiles <pre><code>allFiles: Dataset[AddFile]\n</code></pre> <p><code>allFiles</code> simply takes the state dataset and selects AddFiles (adds <code>where</code> clause for <code>add IS NOT NULL</code> and <code>select</code> over the fields of AddFiles).</p>  <p>Note</p> <p><code>allFiles</code> simply adds <code>where</code> and <code>select</code> clauses. No computation happens yet as it is (a description of) a distributed computation as a <code>Dataset[AddFile]</code>.</p>  <pre><code>import org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, \"/tmp/delta/users\")\nval files = deltaLog.snapshot.allFiles\n\nscala&gt; :type files\norg.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.AddFile]\n\nscala&gt; files.show(truncate = false)\n+-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+\n|path                                                               |partitionValues|size|modificationTime|dataChange|stats|tags|\n+-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+\n|part-00000-4050db39-e0f5-485d-ab3b-3ca72307f621-c000.snappy.parquet|[]             |262 |1578083748000   |false     |null |null|\n|part-00000-ba39f292-2970-4528-a40c-8f0aa5f796de-c000.snappy.parquet|[]             |262 |1578083570000   |false     |null |null|\n|part-00003-99f9d902-24a7-4f76-a15a-6971940bc245-c000.snappy.parquet|[]             |429 |1578083748000   |false     |null |null|\n|part-00007-03d987f1-5bb3-4b5b-8db9-97b6667107e2-c000.snappy.parquet|[]             |429 |1578083748000   |false     |null |null|\n|part-00011-a759a8c2-507d-46dd-9da7-dc722316214b-c000.snappy.parquet|[]             |429 |1578083748000   |false     |null |null|\n|part-00015-2e685d29-25ed-4262-90a7-5491847fd8d0-c000.snappy.parquet|[]             |429 |1578083748000   |false     |null |null|\n|part-00015-ee0ac1af-e1e0-4422-8245-12da91ced0a2-c000.snappy.parquet|[]             |429 |1578083570000   |false     |null |null|\n+-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+\n</code></pre> <p><code>allFiles</code> is used when:</p> <ul> <li> <p><code>PartitionFiltering</code> is requested for the files to scan (matching projection attributes and predicates)</p> </li> <li> <p><code>DeltaSourceSnapshot</code> is requested for the initial files (indexed AddFiles)</p> </li> <li> <p><code>GenerateSymlinkManifestImpl</code> is requested to generateIncrementalManifest and generateFullManifest</p> </li> <li> <p><code>DeltaDataSource</code> is requested for an Insertable HadoopFsRelation</p> </li> </ul>"},{"location":"Snapshot/#statereconstruction-dataset-of-actions","text":"","title":"stateReconstruction Dataset of Actions <pre><code>stateReconstruction: Dataset[SingleAction]\n</code></pre>  <p>Note</p> <p><code>stateReconstruction</code> returns a <code>Dataset[SingleAction]</code> and so does not do any computation per se.</p>  <p><code>stateReconstruction</code> is a <code>Dataset</code> of SingleActions (that is the dataset part) of the cachedState.</p> <p><code>stateReconstruction</code> loads the log file indices (that gives a <code>Dataset[SingleAction]</code>).</p> <p><code>stateReconstruction</code> maps over partitions (using <code>Dataset.mapPartitions</code>) and canonicalize the paths for AddFile and RemoveFile actions.</p> <p><code>stateReconstruction</code> adds <code>file</code> column that uses a UDF to assert that <code>input_file_name()</code> belongs to the Delta table.</p>  <p>Note</p> <p>This UDF-based check is very clever.</p>  <p><code>stateReconstruction</code> repartitions the <code>Dataset</code> using the path of add or remove actions (with the configurable number of partitions) and <code>Dataset.sortWithinPartitions</code> by the <code>file</code> column.</p> <p>In the end, <code>stateReconstruction</code> maps over partitions (using <code>Dataset.mapPartitions</code>) that creates a InMemoryLogReplay, requests it to append the actions (as version <code>0</code>) and checkpoint.</p> <p><code>stateReconstruction</code> is used when:</p> <ul> <li><code>Snapshot</code> is requested for a cached Delta table state</li> </ul>"},{"location":"Snapshot/#loading-actions","text":"","title":"Loading Actions <pre><code>loadActions: Dataset[SingleAction]\n</code></pre> <p><code>loadActions</code> creates a union of <code>Dataset[SingleAction]</code>s for the indices (as LogicalRelations over a <code>HadoopFsRelation</code>) or defaults to an empty dataset.</p>"},{"location":"Snapshot/#indextorelation","text":"","title":"indexToRelation <pre><code>indexToRelation(\n  index: DeltaLogFileIndex,\n  schema: StructType = logSchema): LogicalRelation\n</code></pre> <p><code>indexToRelation</code> converts the DeltaLogFileIndex to a <code>LogicalRelation</code> (Spark SQL) leaf logical operator (using the logSchema).</p> <p><code>indexToRelation</code> creates a <code>LogicalRelation</code> over a <code>HadoopFsRelation</code> (Spark SQL) with the given index and the schema.</p>"},{"location":"Snapshot/#emptyactions-dataset-of-actions","text":"","title":"emptyActions Dataset (of Actions) <pre><code>emptyActions: Dataset[SingleAction]\n</code></pre> <p><code>emptyActions</code> is an empty dataset of SingleActions for loadActions (and <code>InitialSnapshot</code>'s <code>state</code>).</p>"},{"location":"Snapshot/#table-properties","text":"","title":"Table Properties <pre><code>getProperties: mutable.HashMap[String, String]\n</code></pre> <p><code>getProperties</code> returns the following:</p> <ul> <li>Configuration (of the Metadata) without <code>path</code></li> <li>delta.minReaderVersion to be the minReaderVersion (of the Protocol)</li> <li>delta.minWriterVersion to be the minWriterVersion (of the Protocol)</li> </ul> <p><code>getProperties</code> is used when:</p> <ul> <li><code>DeltaTableV2</code> is requested for the table properties</li> </ul>"},{"location":"Snapshot/#fileindices","text":"","title":"fileIndices <pre><code>fileIndices: Seq[DeltaLogFileIndex]\n</code></pre>  <p>Scala lazy value</p> <p><code>fileIndices</code> is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the <code>Snapshot</code> instance.</p> <pre><code>lazy val fileIndices: Seq[DeltaLogFileIndex]\n</code></pre>  <p><code>fileIndices</code> is a collection of the checkpointFileIndexOpt and the deltaFileIndexOpt (if they are available).</p>"},{"location":"Snapshot/#commit-file-index","text":"","title":"Commit File Index <pre><code>deltaFileIndexOpt: Option[DeltaLogFileIndex]\n</code></pre>  <p>Scala lazy value</p> <p><code>deltaFileIndexOpt</code> is a Scala lazy value and is initialized once when first accessed. Once computed, it stays unchanged for the <code>Snapshot</code> instance.</p> <pre><code>lazy val deltaFileIndexOpt: Option[DeltaLogFileIndex]\n</code></pre>  <p><code>deltaFileIndexOpt</code> is a DeltaLogFileIndex (in <code>JsonFileFormat</code>) for the checkpoint file of the LogSegment.</p>"},{"location":"Snapshot/#checkpoint-file-index","text":"","title":"Checkpoint File Index <pre><code>checkpointFileIndexOpt: Option[DeltaLogFileIndex]\n</code></pre>  <p>Scala lazy value</p> <p><code>checkpointFileIndexOpt</code> is a Scala lazy value and is initialized once when first accessed. Once computed, it stays unchanged for the <code>Snapshot</code> instance.</p> <pre><code>lazy val checkpointFileIndexOpt: Option[DeltaLogFileIndex]\n</code></pre>  <p><code>checkpointFileIndexOpt</code> is a DeltaLogFileIndex (in <code>ParquetFileFormat</code>) for the delta files of the LogSegment.</p>"},{"location":"Snapshot/#transaction-version-by-app-id","text":"","title":"Transaction Version By App ID <pre><code>transactions: Map[String, Long]\n</code></pre> <p><code>transactions</code> takes the SetTransaction actions (from the state dataset) and makes them a lookup table of transaction version by appId.</p>  <p>Scala lazy value</p> <p><code>transactions</code> is a Scala lazy value and is initialized once at the first access. Once computed it stays unchanged for the <code>Snapshot</code> instance.</p> <pre><code>lazy val transactions: Map[String, Long]\n</code></pre>  <p><code>transactions</code> is used when <code>OptimisticTransactionImpl</code> is requested for the transaction version for a given (streaming query) id.</p>"},{"location":"Snapshot/#all-removefile-actions-tombstones","text":"","title":"All RemoveFile Actions (Tombstones) <pre><code>tombstones: Dataset[RemoveFile]\n</code></pre> <p><code>tombstones</code>...FIXME</p> <pre><code>scala&gt; deltaLog.snapshot.tombstones.show(false)\n+----+-----------------+----------+\n|path|deletionTimestamp|dataChange|\n+----+-----------------+----------+\n+----+-----------------+----------+\n</code></pre>"},{"location":"SnapshotIterator/","text":"<p><code>SnapshotIterator</code> is an abstraction of iterators over indexed AddFile actions in a Delta log for DeltaSourceSnapshots.</p>","title":"SnapshotIterator"},{"location":"SnapshotIterator/#iterator-of-indexed-addfiles","text":"","title":"Iterator of Indexed AddFiles <pre><code>iterator(): Iterator[IndexedFile]\n</code></pre> <p><code>iterator</code> returns an <code>Iterator</code> (Scala) of <code>IndexedFile</code>s (AddFile actions in a Delta log with extra metadata) of filterFileList.</p> <p><code>iterator</code>\u00a0is used when:</p> <ul> <li><code>DeltaSource</code> is requested for the snapshot of a delta table at a given version</li> </ul>"},{"location":"SnapshotIterator/#closing-iterator-cleaning-up-internal-resources","text":"","title":"Closing Iterator (Cleaning Up Internal Resources) <pre><code>close(\n  unpersistSnapshot: Boolean): Unit\n</code></pre> <p><code>close</code> is a no-op (and leaves proper operation to implementations).</p> <p><code>close</code>\u00a0is used when:</p> <ul> <li><code>DeltaSource</code> is requested to cleanUpSnapshotResources</li> </ul>"},{"location":"SnapshotIterator/#implementations","text":"","title":"Implementations <ul> <li>DeltaSourceSnapshot</li> </ul>"},{"location":"SnapshotManagement/","text":"<p><code>SnapshotManagement</code> is an extension for DeltaLog to manage Snapshots.</p>","title":"SnapshotManagement"},{"location":"SnapshotManagement/#current-snapshot","text":"","title":"Current Snapshot <p><code>SnapshotManagement</code> manages <code>currentSnapshot</code> registry with the recently-loaded Snapshot (of a Delta table).</p> <p><code>currentSnapshot</code> is initialized as the latest available Snapshot right when DeltaLog is created and updated on demand.</p> <p><code>currentSnapshot</code>...FIXME</p> <p><code>currentSnapshot</code> is used when:</p> <ul> <li><code>SnapshotManagement</code> is requested to...FIXME</li> </ul>"},{"location":"SnapshotManagement/#loading-latest-snapshot-at-initialization","text":"","title":"Loading Latest Snapshot at Initialization <pre><code>getSnapshotAtInit: Snapshot\n</code></pre> <p><code>getSnapshotAtInit</code> getLogSegmentFrom for the last checkpoint.</p> <p><code>getSnapshotAtInit</code> prints out the following INFO message to the logs:</p> <pre><code>Loading version [version][startCheckpoint]\n</code></pre> <p><code>getSnapshotAtInit</code> creates a Snapshot for the log segment.</p> <p><code>getSnapshotAtInit</code> records the current time in lastUpdateTimestamp registry.</p> <p><code>getSnapshotAtInit</code> prints out the following INFO message to the logs:</p> <pre><code>Returning initial snapshot [snapshot]\n</code></pre>"},{"location":"SnapshotManagement/#fetching-log-files-for-version-checkpointed","text":"","title":"Fetching Log Files for Version Checkpointed <pre><code>getLogSegmentFrom(\n  startingCheckpoint: Option[CheckpointMetaData]): LogSegment\n</code></pre> <p><code>getLogSegmentFrom</code> fetches log files for the version (based on the optional <code>CheckpointMetaData</code> as the starting checkpoint version to start listing log files from).</p>"},{"location":"SnapshotManagement/#fetching-latest-checkpoint-and-delta-log-files-for-version","text":"","title":"Fetching Latest Checkpoint and Delta Log Files for Version <pre><code>getLogSegmentForVersion(\n  startCheckpoint: Option[Long],\n  versionToLoad: Option[Long] = None): LogSegment\n</code></pre> <p><code>getLogSegmentForVersion</code> list all the files (in a transaction log) from the given <code>startCheckpoint</code> (or defaults to <code>0</code>).</p> <p><code>getLogSegmentForVersion</code> filters out unnecessary files and leaves checkpoint and delta files only.</p> <p><code>getLogSegmentForVersion</code> filters out checkpoint files of size <code>0</code>.</p> <p><code>getLogSegmentForVersion</code> takes all the files that are older than the requested <code>versionToLoad</code>.</p> <p><code>getLogSegmentForVersion</code> splits the files into checkpoint and delta files.</p> <p><code>getLogSegmentForVersion</code> finds the latest checkpoint from the list.</p> <p>In the end, <code>getLogSegmentForVersion</code> creates a LogSegment with the (checkpoint and delta) files.</p> <p><code>getLogSegmentForVersion</code> is used when:</p> <ul> <li><code>SnapshotManagement</code> is requested for getLogSegmentFrom, updateInternal and getSnapshotAt</li> </ul>"},{"location":"SnapshotManagement/#listing-files-from-version-upwards","text":"","title":"Listing Files from Version Upwards <pre><code>listFrom(\n  startVersion: Long): Iterator[FileStatus]\n</code></pre> <p><code>listFrom</code>...FIXME</p>"},{"location":"SnapshotManagement/#creating-snapshot","text":"","title":"Creating Snapshot <pre><code>createSnapshot(\n  segment: LogSegment,\n  minFileRetentionTimestamp: Long,\n  timestamp: Long): Snapshot\n</code></pre> <p><code>createSnapshot</code> readChecksum (for the version of the given LogSegment) and creates a Snapshot.</p> <p><code>createSnapshot</code> is used when:</p> <ul> <li><code>SnapshotManagement</code> is requested for getSnapshotAtInit, updateInternal and getSnapshotAt</li> </ul>"},{"location":"SnapshotManagement/#last-successful-update-timestamp","text":"","title":"Last Successful Update Timestamp <p><code>SnapshotManagement</code> uses <code>lastUpdateTimestamp</code> internal registry for the timestamp of the last successful update.</p>"},{"location":"SnapshotManagement/#updating-current-snapshot","text":"","title":"Updating Current Snapshot <pre><code>update(\n  stalenessAcceptable: Boolean = false): Snapshot\n</code></pre> <p><code>update</code> determines whether to do update asynchronously or not based on the input <code>stalenessAcceptable</code> flag and isSnapshotStale.</p> <p>With <code>stalenessAcceptable</code> flag turned off (the default value) and the state snapshot is not stale, <code>update</code> updates (with <code>isAsync</code> flag turned off).</p> <p><code>update</code>...FIXME</p>"},{"location":"SnapshotManagement/#usage","text":"","title":"Usage <p><code>update</code> is used when:</p> <ul> <li><code>DeltaHistoryManager</code> is requested to getHistory, getActiveCommitAtTime, checkVersionExists</li> <li><code>DeltaLog</code> is requested to start a transaction</li> <li><code>OptimisticTransactionImpl</code> is requested to doCommit and getNextAttemptVersion</li> <li><code>DeltaTableV2</code> is requested for a Snapshot</li> <li><code>TahoeLogFileIndex</code> is requested for a Snapshot</li> <li><code>DeltaSource</code> is requested for the getStartingVersion</li> <li>In Delta commands...</li> </ul>"},{"location":"SnapshotManagement/#issnapshotstale","text":"","title":"isSnapshotStale <pre><code>isSnapshotStale: Boolean\n</code></pre> <p><code>isSnapshotStale</code> reads spark.databricks.delta.stalenessLimit configuration property.</p> <p><code>isSnapshotStale</code> is enabled (<code>true</code>) when any of the following holds:</p> <ol> <li>spark.databricks.delta.stalenessLimit configuration property is <code>0</code> (the default)</li> <li>Internal lastUpdateTimestamp has never been updated (and is below <code>0</code>) or is at least spark.databricks.delta.stalenessLimit configuration property old</li> </ol>"},{"location":"SnapshotManagement/#tryupdate","text":"","title":"tryUpdate <pre><code>tryUpdate(\n  isAsync: Boolean = false): Snapshot\n</code></pre> <p><code>tryUpdate</code>...FIXME</p>"},{"location":"SnapshotManagement/#updateinternal","text":"","title":"updateInternal <pre><code>updateInternal(\n  isAsync: Boolean): Snapshot // (1)\n</code></pre> <ol> <li><code>isAsync</code> flag is not used</li> </ol> <p><code>updateInternal</code> requests the current Snapshot for the LogSegment that is in turn requested for the checkpointVersion. <code>updateInternal</code> gets the LogSegment for the <code>checkpointVersion</code>.</p> <p>If the <code>LogSegment</code>s are equal (and so no new files have been added), <code>updateInternal</code> updates the lastUpdateTimestamp registry to the current timestamp and returns the currentSnapshot.</p> <p>Otherwise, if the fetched <code>LogSegment</code> is different than the current Snapshot's, <code>updateInternal</code> prints out the following INFO message to the logs:</p> <pre><code>Loading version [version][ starting from checkpoint version [v]]\n</code></pre> <p><code>updateInternal</code> creates a new Snapshot with the fetched <code>LogSegment</code>.</p> <p><code>updateInternal</code> replaces Snapshots and prints out the following INFO message to the logs:</p> <pre><code>Updated snapshot to [newSnapshot]\n</code></pre>"},{"location":"SnapshotManagement/#replacing-snapshots","text":"","title":"Replacing Snapshots <pre><code>replaceSnapshot(\n  newSnapshot: Snapshot): Unit\n</code></pre> <p><code>replaceSnapshot</code> requests the currentSnapshot to uncache (and drop any cached data) and makes the given <code>newSnapshot</code> the current one.</p>"},{"location":"SnapshotManagement/#demo","text":"","title":"Demo <pre><code>import org.apache.spark.sql.delta.DeltaLog\nval log = DeltaLog.forTable(spark, dataPath)\n\nimport org.apache.spark.sql.delta.SnapshotManagement\nassert(log.isInstanceOf[SnapshotManagement], \"DeltaLog is a SnapshotManagement\")\n</code></pre> <pre><code>val snapshot = log.update(stalenessAcceptable = false)\n</code></pre> <pre><code>scala&gt; :type snapshot\norg.apache.spark.sql.delta.Snapshot\n\nassert(snapshot.version == 0)\n</code></pre>"},{"location":"SnapshotManagement/#logging","text":"","title":"Logging <p>As an extension of DeltaLog, use DeltaLog logging to see what happens inside.</p>"},{"location":"StagedDeltaTableV2/","text":"<p><code>StagedDeltaTableV2</code> is a <code>StagedTable</code> (Spark SQL) and a <code>SupportsWrite</code> (Spark SQL).</p>","title":"StagedDeltaTableV2"},{"location":"StagedDeltaTableV2/#creating-instance","text":"<p><code>StagedDeltaTableV2</code> takes the following to be created:</p> <ul> <li> Identifier <li> Schema <li> Partitions (<code>Array[Transform]</code>) <li> Properties <li> Operation (one of <code>Create</code>, <code>CreateOrReplace</code>, <code>Replace</code>)  <p><code>StagedDeltaTableV2</code> is created when <code>DeltaCatalog</code> is requested to stageReplace, stageCreateOrReplace or stageCreate.</p>","title":"Creating Instance"},{"location":"StagedDeltaTableV2/#commitstagedchanges","text":"","title":"commitStagedChanges <pre><code>commitStagedChanges(): Unit\n</code></pre> <p><code>commitStagedChanges</code>...FIXME</p> <p><code>commitStagedChanges</code> is part of the <code>StagedTable</code> (Spark SQL) abstraction.</p>"},{"location":"StagedDeltaTableV2/#abortstagedchanges","text":"","title":"abortStagedChanges <pre><code>abortStagedChanges(): Unit\n</code></pre> <p><code>abortStagedChanges</code> does nothing.</p> <p><code>abortStagedChanges</code> is part of the <code>StagedTable</code> (Spark SQL) abstraction.</p>"},{"location":"StagedDeltaTableV2/#creating-writebuilder","text":"","title":"Creating WriteBuilder <pre><code>newWriteBuilder(\n  info: LogicalWriteInfo): V1WriteBuilder\n</code></pre> <p><code>newWriteBuilder</code>...FIXME</p> <p><code>newWriteBuilder</code> is part of the <code>SupportsWrite</code> (Spark SQL) abstraction.</p>"},{"location":"StateCache/","text":"<p><code>StateCache</code> is an abstraction of state caches that can cache a Dataset and uncache them all.</p>","title":"StateCache"},{"location":"StateCache/#contract","text":"","title":"Contract"},{"location":"StateCache/#sparksession","text":"","title":"SparkSession <pre><code>spark: SparkSession\n</code></pre> <p><code>SparkSession</code> the cached RDDs belong to</p>"},{"location":"StateCache/#implementations","text":"<ul> <li>DeltaSourceSnapshot</li> <li>Snapshot</li> </ul>","title":"Implementations"},{"location":"StateCache/#cached-rdds","text":"","title":"Cached RDDs <pre><code>cached: ArrayBuffer[RDD[_]]\n</code></pre> <p><code>StateCache</code> tracks cached RDDs in <code>cached</code> internal registry.</p> <p><code>cached</code> is given a new <code>RDD</code> when <code>StateCache</code> is requested to cache a Dataset.</p> <p><code>cached</code> is used when <code>StateCache</code> is requested to get a cached Dataset and uncache.</p>"},{"location":"StateCache/#caching-dataset","text":"","title":"Caching Dataset <pre><code>cacheDS[A](\n  ds: Dataset[A],\n  name: String): CachedDS[A]\n</code></pre> <p><code>cacheDS</code> creates a new CachedDS.</p> <p><code>cacheDS</code> is used when:</p> <ul> <li><code>Snapshot</code> is requested for a cached state)</li> <li><code>DeltaSourceSnapshot</code> is requested to initialFiles</li> </ul>"},{"location":"StateCache/#uncaching-all-cached-datasets","text":"","title":"Uncaching All Cached Datasets <pre><code>uncache[A](\n  ds: Dataset[A],\n  name: String): CachedDS[A]\n</code></pre> <p><code>uncache</code> uses the isCached internal flag to avoid multiple executions.</p> <p><code>uncache</code> is used when:</p> <ul> <li><code>DeltaLog</code> utility is used to access deltaLogCache and a cached entry expires</li> <li><code>SnapshotManagement</code> is requested to update state of a Delta table</li> <li><code>DeltaSourceSnapshot</code> is requested to close</li> </ul>"},{"location":"TahoeBatchFileIndex/","text":"<p><code>TahoeBatchFileIndex</code> is a file index of a delta table at a given version.</p>","title":"TahoeBatchFileIndex"},{"location":"TahoeBatchFileIndex/#creating-instance","text":"<p><code>TahoeBatchFileIndex</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li>Action Type</li> <li> AddFiles <li> DeltaLog <li> Data directory (as Hadoop Path) <li> Snapshot  <p><code>TahoeBatchFileIndex</code> is created when:</p> <ul> <li><code>DeltaLog</code> is requested for a DataFrame for given AddFiles</li> <li>DeleteCommand and UpdateCommand are executed (and <code>DeltaCommand</code> is requested for a HadoopFsRelation)</li> </ul>","title":"Creating Instance"},{"location":"TahoeBatchFileIndex/#action-type","text":"","title":"Action Type <p><code>TahoeBatchFileIndex</code> is given an Action Type identifier when created:</p> <ul> <li>batch or streaming when <code>DeltaLog</code> is requested for a batch or streaming DataFrame for given AddFiles, respectively</li> <li>delete for DeleteCommand</li> <li>update for UpdateCommand</li> </ul>  <p>Important</p> <p>Action Type seems not to be used ever.</p>"},{"location":"TahoeBatchFileIndex/#tableversion","text":"","title":"tableVersion <pre><code>tableVersion: Long\n</code></pre> <p><code>tableVersion</code> is part of the TahoeFileIndex abstraction.</p> <p><code>tableVersion</code> is always the version of the Snapshot.</p>"},{"location":"TahoeBatchFileIndex/#matchingfiles","text":"","title":"matchingFiles <pre><code>matchingFiles(\n  partitionFilters: Seq[Expression],\n  dataFilters: Seq[Expression],\n  keepStats: Boolean = false): Seq[AddFile]\n</code></pre> <p><code>matchingFiles</code> is part of the TahoeFileIndex abstraction.</p> <p><code>matchingFiles</code> filterFileList (that gives a <code>DataFrame</code>) and collects the AddFiles (using <code>Dataset.collect</code>).</p>"},{"location":"TahoeBatchFileIndex/#input-files","text":"","title":"Input Files <pre><code>inputFiles: Array[String]\n</code></pre> <p><code>inputFiles</code> is part of the <code>FileIndex</code> (Spark SQL) abstraction.</p> <p><code>inputFiles</code> returns the paths of all the given AddFiles.</p>"},{"location":"TahoeBatchFileIndex/#partitions","text":"","title":"Partitions <pre><code>partitionSchema: StructType\n</code></pre> <p><code>partitionSchema</code> is part of the <code>FileIndex</code> (Spark SQL) abstraction.</p> <p><code>partitionSchema</code> requests the Snapshot for the metadata that is in turn requested for the partitionSchema.</p>"},{"location":"TahoeBatchFileIndex/#estimated-size-of-relation","text":"","title":"Estimated Size of Relation <pre><code>sizeInBytes: Long\n</code></pre> <p><code>sizeInBytes</code> is part of the <code>FileIndex</code> (Spark SQL) abstraction.</p> <p><code>sizeInBytes</code> is a sum of the sizes of all the given AddFiles.</p>"},{"location":"TahoeFileIndex/","text":"<p><code>TahoeFileIndex</code>\u00a0is an extension of the <code>FileIndex</code> abstraction (Spark SQL) for file indices of delta tables that can list data files to scan (based on partition and data filters).</p> <p>The aim of <code>TahoeFileIndex</code> (and <code>FileIndex</code> in general) is to reduce usage of very expensive disk access for file-related information using Hadoop FileSystem API.</p>","title":"TahoeFileIndex"},{"location":"TahoeFileIndex/#contract","text":"","title":"Contract"},{"location":"TahoeFileIndex/#matchingfiles","text":"","title":"matchingFiles <pre><code>matchingFiles(\n  partitionFilters: Seq[Expression],\n  dataFilters: Seq[Expression]): Seq[AddFile]\n</code></pre> <p>AddFiles matching given partition and data filters (predicates)</p> <p>Used for listing data files</p>"},{"location":"TahoeFileIndex/#implementations","text":"<ul> <li>PinnedTahoeFileIndex</li> <li>TahoeBatchFileIndex</li> <li>TahoeLogFileIndex</li> </ul>","title":"Implementations"},{"location":"TahoeFileIndex/#creating-instance","text":"<p><code>TahoeFileIndex</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> <li> DeltaLog <li> Hadoop Path   Abstract Class <p><code>TahoeFileIndex</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete TahoeFileIndexes.</p>","title":"Creating Instance"},{"location":"TahoeFileIndex/#root-paths","text":"","title":"Root Paths <pre><code>rootPaths: Seq[Path]\n</code></pre> <p><code>rootPaths</code> is the path only.</p> <p><code>rootPaths</code>\u00a0is part of the <code>FileIndex</code> abstraction (Spark SQL).</p>"},{"location":"TahoeFileIndex/#listing-files","text":"","title":"Listing Files <pre><code>listFiles(\n  partitionFilters: Seq[Expression],\n  dataFilters: Seq[Expression]): Seq[PartitionDirectory]\n</code></pre> <p><code>listFiles</code> is the path only.</p> <p><code>listFiles</code>\u00a0is part of the <code>FileIndex</code> abstraction (Spark SQL).</p>"},{"location":"TahoeFileIndex/#partitions","text":"","title":"Partitions <pre><code>partitionSchema: StructType\n</code></pre> <p><code>partitionSchema</code> is the partition schema of (the Metadata of the Snapshot) of the DeltaLog.</p> <p><code>partitionSchema</code>\u00a0is part of the <code>FileIndex</code> abstraction (Spark SQL).</p>"},{"location":"TahoeFileIndex/#version-of-delta-table","text":"","title":"Version of Delta Table <pre><code>tableVersion: Long\n</code></pre> <p><code>tableVersion</code> is the version of (the snapshot of) the DeltaLog.</p> <p><code>tableVersion</code>\u00a0is used when <code>TahoeFileIndex</code> is requested for the human-friendly textual representation.</p>"},{"location":"TahoeFileIndex/#textual-representation","text":"","title":"Textual Representation <pre><code>toString: String\n</code></pre> <p><code>toString</code> returns the following text (using the version and the path of the Delta table):</p> <pre><code>Delta[version=[tableVersion], [truncatedPath]]\n</code></pre> <p><code>toString</code> is part of the <code>java.lang.Object</code> contract for a string representation of the object.</p>"},{"location":"TahoeLogFileIndex/","text":"<p><code>TahoeLogFileIndex</code> is a file index.</p>","title":"TahoeLogFileIndex"},{"location":"TahoeLogFileIndex/#creating-instance","text":"<p><code>TahoeLogFileIndex</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> DeltaLog <li> Data directory of the Delta table (as a Hadoop Path) <li> Snapshot at analysis <li> Partition Filters (as Catalyst expressions; default: empty) <li>isTimeTravelQuery flag (default: <code>false</code>)</li>  <p><code>TahoeLogFileIndex</code> is created when:</p> <ul> <li><code>DeltaLog</code> is requested for an Insertable HadoopFsRelation</li> </ul>","title":"Creating Instance"},{"location":"TahoeLogFileIndex/#sparkdatabricksdeltachecklatestschemaonread","text":"","title":"spark.databricks.delta.checkLatestSchemaOnRead <p><code>TahoeLogFileIndex</code> uses the spark.databricks.delta.checkLatestSchemaOnRead configuration property when requested for a Snapshot.</p>"},{"location":"TahoeLogFileIndex/#istimetravelquery-flag","text":"","title":"isTimeTravelQuery flag <p><code>TahoeLogFileIndex</code> is given a <code>isTimeTravelQuery</code> flag when created.</p> <p><code>isTimeTravelQuery</code> flag is <code>false</code> by default and can be different when <code>DeltaLog</code> is requested to create a BaseRelation (when <code>DeltaTableV2</code> is requested for a BaseRelation based on DeltaTimeTravelSpec).</p>"},{"location":"TahoeLogFileIndex/#demo","text":"","title":"Demo <pre><code>val q = spark.read.format(\"delta\").load(\"/tmp/delta/users\")\nval plan = q.queryExecution.executedPlan\n\nimport org.apache.spark.sql.execution.FileSourceScanExec\nval scan = plan.collect { case e: FileSourceScanExec =&gt; e }.head\n\nimport org.apache.spark.sql.delta.files.TahoeLogFileIndex\nval index = scan.relation.location.asInstanceOf[TahoeLogFileIndex]\nscala&gt; println(index)\nDelta[version=1, file:/tmp/delta/users]\n</code></pre>"},{"location":"TahoeLogFileIndex/#matchingfiles-method","text":"","title":"matchingFiles Method <pre><code>matchingFiles(\n  partitionFilters: Seq[Expression],\n  dataFilters: Seq[Expression],\n  keepStats: Boolean = false): Seq[AddFile]\n</code></pre> <p><code>matchingFiles</code> gets the snapshot (with <code>stalenessAcceptable</code> flag off) and requests it for the files to scan (for the index's partition filters, the given <code>partitionFilters</code> and <code>dataFilters</code>).</p>  <p>Note</p> <p>inputFiles and matchingFiles are similar. Both get the snapshot (of the delta table), but they use different filtering expressions and return value types.</p>  <p><code>matchingFiles</code> is part of the TahoeFileIndex abstraction.</p>"},{"location":"TahoeLogFileIndex/#inputfiles-method","text":"","title":"inputFiles Method <pre><code>inputFiles: Array[String]\n</code></pre> <p><code>inputFiles</code> gets the snapshot (with <code>stalenessAcceptable</code> flag off) and requests it for the files to scan (for the index's partition filters only).</p>  <p>Note</p> <p>inputFiles and matchingFiles are similar. Both get the snapshot, but they use different filtering expressions and return value types.</p>  <p><code>inputFiles</code> is part of the <code>FileIndex</code> contract (Spark SQL).</p>"},{"location":"TahoeLogFileIndex/#snapshot","text":"","title":"Snapshot <pre><code>getSnapshot: Snapshot\n</code></pre> <p><code>getSnapshot</code> returns the Snapshot to scan.</p>  <p>With checkSchemaOnRead enabled or the DeltaColumnMappingMode (of the Metadata of the Snapshot) set (different from <code>NoMapping</code>), <code>getSnapshot</code> makes sure that the schemas are read-compatible (and hasn't changed in an incompatible manner since analysis time)</p>  <p><code>getSnapshot</code> is used when:</p> <ul> <li><code>TahoeLogFileIndex</code> is requested for the matching files and the input files</li> </ul>"},{"location":"TahoeLogFileIndex/#getsnapshottoscan","text":"","title":"getSnapshotToScan <pre><code>getSnapshotToScan: Snapshot\n</code></pre> <p><code>getSnapshot</code> returns the Snapshot with isTimeTravelQuery enabled or requests the DeltaLog to update and give one.</p>"},{"location":"TahoeLogFileIndex/#internal-properties","text":"","title":"Internal Properties"},{"location":"TahoeLogFileIndex/#historicalsnapshotopt","text":"","title":"historicalSnapshotOpt <p>Historical snapshot that is the Snapshot for the versionToUse if defined.</p> <p>Used when <code>TahoeLogFileIndex</code> is requested for the (historical or latest) snapshot and the schema of the partition columns</p>"},{"location":"TransactionalWrite/","text":"<p><code>TransactionalWrite</code> is an abstraction of optimistic transactional writers that can write a structured query out to a Delta table.</p>","title":"TransactionalWrite"},{"location":"TransactionalWrite/#contract","text":"","title":"Contract"},{"location":"TransactionalWrite/#deltalog","text":"","title":"DeltaLog <pre><code>deltaLog: DeltaLog\n</code></pre> <p>DeltaLog (of a delta table) that this transaction is changing</p> <p>Used when:</p> <ul> <li>ActiveOptimisticTransactionRule logical rule is executed</li> <li><code>OptimisticTransactionImpl</code> is requested to prepare a commit, doCommit, checkAndRetry, and perform post-commit operations (and execute delta log checkpoint)</li> <li>ConvertToDeltaCommand is executed</li> <li><code>DeltaCommand</code> is requested to buildBaseRelation and commitLarge</li> <li>MergeIntoCommand is executed</li> <li><code>TransactionalWrite</code> is requested to write a structured query out to a delta table</li> <li>GenerateSymlinkManifest post-commit hook is executed</li> <li><code>ImplicitMetadataOperation</code> is requested to updateMetadata</li> <li><code>DeltaSink</code> is requested to addBatch</li> </ul>"},{"location":"TransactionalWrite/#metadata","text":"","title":"Metadata <pre><code>metadata: Metadata\n</code></pre> <p>Metadata (of the delta table) that this transaction is changing</p>"},{"location":"TransactionalWrite/#protocol","text":"","title":"Protocol <pre><code>protocol: Protocol\n</code></pre> <p>Protocol (of the delta table) that this transaction is changing</p> <p>Used when:</p> <ul> <li><code>OptimisticTransactionImpl</code> is requested to updateMetadata, verifyNewMetadata and prepareCommit</li> <li>ConvertToDeltaCommand is executed</li> </ul>"},{"location":"TransactionalWrite/#snapshot","text":"","title":"Snapshot <pre><code>snapshot: Snapshot\n</code></pre> <p>Snapshot (of the delta table) that this transaction is reading at</p>"},{"location":"TransactionalWrite/#implementations","text":"<ul> <li>OptimisticTransaction</li> </ul>","title":"Implementations"},{"location":"TransactionalWrite/#sparkdatabricksdeltahistorymetricsenabled","text":"","title":"spark.databricks.delta.history.metricsEnabled <p>With spark.databricks.delta.history.metricsEnabled configuration property enabled, <code>TransactionalWrite</code> creates a <code>BasicWriteJobStatsTracker</code> and registers SQL metrics (when requested to writeFiles).</p>"},{"location":"TransactionalWrite/#haswritten-flag","text":"","title":"hasWritten Flag <pre><code>hasWritten: Boolean = false\n</code></pre> <p><code>TransactionalWrite</code> uses <code>hasWritten</code> internal registry to prevent <code>OptimisticTransactionImpl</code> from updating metadata after having written out files.</p> <p><code>hasWritten</code> is initially <code>false</code> and changes to <code>true</code> after having written out files.</p>"},{"location":"TransactionalWrite/#writing-data-out-result-of-structured-query","text":"","title":"Writing Data Out (Result Of Structured Query) <pre><code>writeFiles(\n  data: Dataset[_]): Seq[FileAction]\nwriteFiles(\n  data: Dataset[_],\n  writeOptions: Option[DeltaOptions]): Seq[FileAction]\n</code></pre> <p><code>writeFiles</code> creates a DeltaInvariantCheckerExec and a DelayedCommitProtocol to write out files to the data path (of the DeltaLog).</p>  FileFormatWriter <p><code>writeFiles</code> uses Spark SQL's <code>FileFormatWriter</code> utility to write out a result of a streaming query.</p> <p>Learn about FileFormatWriter in The Internals of Spark SQL online book.</p>  <p><code>writeFiles</code> is executed within <code>SQLExecution.withNewExecutionId</code>.</p>  SQLAppStatusListener <p><code>writeFiles</code> can be tracked using web UI or <code>SQLAppStatusListener</code> (using <code>SparkListenerSQLExecutionStart</code> and <code>SparkListenerSQLExecutionEnd</code> events).</p> <p>Learn about SQLAppStatusListener in The Internals of Spark SQL online book.</p>  <p>In the end, <code>writeFiles</code> returns the addedStatuses of the DelayedCommitProtocol committer.</p> <p>Internally, <code>writeFiles</code> turns the hasWritten flag on (<code>true</code>).</p>  <p>Note</p> <p>After <code>writeFiles</code>, no metadata updates in the transaction are permitted.</p>  <p><code>writeFiles</code> normalize the given <code>data</code> dataset (based on the partitionColumns of the Metadata).</p> <p><code>writeFiles</code> getPartitioningColumns based on the partitionSchema of the Metadata.</p>"},{"location":"TransactionalWrite/#delayedcommitprotocol-committer","text":"","title":"DelayedCommitProtocol Committer <p><code>writeFiles</code> creates a DelayedCommitProtocol committer for the data path (of the DeltaLog).</p>"},{"location":"TransactionalWrite/#constraints","text":"","title":"Constraints <p><code>writeFiles</code> collects constraintss from the table metadata and the generated columns.</p>"},{"location":"TransactionalWrite/#deltainvariantcheckerexec","text":"","title":"DeltaInvariantCheckerExec <p><code>writeFiles</code> requests a new Execution ID (that is used to track all Spark jobs of <code>FileFormatWriter.write</code> in Spark SQL) with a physical query plan of a new DeltaInvariantCheckerExec unary physical operator (with the executed plan of the normalized query execution as the child operator)</p>"},{"location":"TransactionalWrite/#creating-committer","text":"","title":"Creating Committer <pre><code>getCommitter(\n  outputPath: Path): DelayedCommitProtocol\n</code></pre> <p><code>getCommitter</code> creates a new DelayedCommitProtocol with the delta job ID and the given <code>outputPath</code> (and no random prefix).</p>"},{"location":"TransactionalWrite/#getpartitioningcolumns","text":"","title":"getPartitioningColumns <pre><code>getPartitioningColumns(\n  partitionSchema: StructType,\n  output: Seq[Attribute],\n  colsDropped: Boolean): Seq[Attribute]\n</code></pre> <p><code>getPartitioningColumns</code>...FIXME</p>"},{"location":"TransactionalWrite/#normalizedata","text":"","title":"normalizeData <pre><code>normalizeData(\n  data: Dataset[_],\n  partitionCols: Seq[String]): (QueryExecution, Seq[Attribute])\n</code></pre> <p><code>normalizeData</code>...FIXME</p>"},{"location":"TransactionalWrite/#makeoutputnullable","text":"","title":"makeOutputNullable <pre><code>makeOutputNullable(\n  output: Seq[Attribute]): Seq[Attribute]\n</code></pre> <p><code>makeOutputNullable</code>...FIXME</p>"},{"location":"TransactionalWrite/#usage","text":"","title":"Usage <p><code>writeFiles</code> is used when:</p> <ul> <li>DeleteCommand, MergeIntoCommand, UpdateCommand, and WriteIntoDelta commands are executed</li> <li><code>DeltaSink</code> is requested to add a streaming micro-batch</li> </ul>"},{"location":"VerifyChecksum/","text":"<p>= VerifyChecksum</p> <p><code>VerifyChecksum</code> is...FIXME</p> <p>== [[validateChecksum]] <code>validateChecksum</code> Method</p>","title":"VerifyChecksum"},{"location":"VerifyChecksum/#source-scala","text":"","title":"[source, scala]"},{"location":"VerifyChecksum/#validatechecksumsnapshot-snapshot-unit","text":"<p><code>validateChecksum</code>...FIXME</p> <p>NOTE: <code>validateChecksum</code> is used when...FIXME</p>","title":"validateChecksum(snapshot: Snapshot): Unit"},{"location":"WriteIntoDeltaBuilder/","text":"<p><code>WriteIntoDeltaBuilder</code> is a <code>WriteBuilder</code> (Spark SQL) with support for the following capabilities:</p> <ul> <li><code>SupportsOverwrite</code> (Spark SQL)</li> <li><code>SupportsTruncate</code> (Spark SQL)</li> <li><code>V1WriteBuilder</code> (Spark SQL)</li> </ul>","title":"WriteIntoDeltaBuilder"},{"location":"WriteIntoDeltaBuilder/#creating-instance","text":"<p><code>WriteIntoDeltaBuilder</code> takes the following to be created:</p> <ul> <li> DeltaLog <li> Write-Specific Options  <p><code>WriteIntoDeltaBuilder</code> is created\u00a0when:</p> <ul> <li><code>DeltaTableV2</code> is requested for a WriteBuilder</li> </ul>","title":"Creating Instance"},{"location":"WriteIntoDeltaBuilder/#buildforv1write","text":"","title":"buildForV1Write <pre><code>buildForV1Write(): InsertableRelation\n</code></pre> <p><code>buildForV1Write</code>\u00a0is part of the <code>V1WriteBuilder</code> (Spark SQL) abstraction.</p> <p><code>buildForV1Write</code> creates an <code>InsertableRelation</code> (Spark SQL) that does the following when requested to <code>insert</code>:</p> <ol> <li>Creates and executes a WriteIntoDelta command</li> <li>Re-cache all cached plans (by requesting the <code>CacheManager</code> to <code>recacheByPlan</code> for a <code>LogicalRelation</code> over the BaseRelation of the DeltaLog)</li> </ol>"},{"location":"configuration-properties/","text":"","title":"Configuration Properties"},{"location":"configuration-properties/#sparkdeltalogstoreclass","text":"","title":"spark.delta.logStore.class <p>The fully-qualified class name of a LogStore</p> <p>Default: HDFSLogStore</p> <p>Used when:</p> <ul> <li><code>LogStoreProvider</code> is requested for a LogStore</li> </ul>"},{"location":"installation/","text":"<p>Installation of Delta Lake boils down to using spark-submit's <code>--packages</code> command-line option with the following configuration properties for DeltaSparkSessionExtension and DeltaCatalog:</p> <ul> <li><code>spark.sql.extensions</code> (Spark SQL)</li> <li><code>spark.sql.catalog.spark_catalog</code> (Spark SQL)</li> </ul> <p>Make sure that the version of Scala in Apache Spark should match Delta Lake's.</p>","title":"Installation"},{"location":"installation/#spark-sql-application","text":"","title":"Spark SQL Application <pre><code>import org.apache.spark.sql.SparkSession\nval spark = SparkSession\n  .builder\n  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n  .getOrCreate\n</code></pre>"},{"location":"installation/#spark-shell","text":"","title":"Spark Shell <pre><code>$ ./bin/spark-shell --version\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.2.1\n      /_/\n\nUsing Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.14\n</code></pre> <pre><code>./bin/spark-shell \\\n  --packages io.delta:delta-core_2.12:1.2.0 \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n</code></pre>"},{"location":"installation/#version","text":"","title":"Version <p><code>io.delta.VERSION</code> can be used to show the version of Delta Lake installed.</p> <pre><code>scala&gt; println(io.delta.VERSION)\n1.2.0\n</code></pre> <p>It is also possible to use DESCRIBE HISTORY and check out the engineInfo column.</p>"},{"location":"options/","text":"<p>Delta Lake comes with options to fine-tune its uses. They can be defined using <code>option</code> method of the following:</p> <ul> <li><code>DataFrameReader</code> (Spark SQL) and <code>DataFrameWriter</code> (Spark SQL) for batch queries</li> <li><code>DataStreamReader</code> (Spark Structured Streaming) and <code>DataStreamWriter</code> (Spark Structured Streaming) for streaming queries</li> <li>SQL queries</li> </ul>","title":"Options"},{"location":"options/#checkpointlocation","text":"","title":"checkpointLocation <p>Checkpoint directory for storing checkpoint data of streaming queries (Spark Structured Streaming).</p>"},{"location":"options/#datachange","text":"","title":"dataChange <p>Whether to write new data to the table or just rearrange data that is already part of the table. This option declares that the data being written by this job does not change any data in the table and merely rearranges existing data. This makes sure streaming queries reading from this table will not see any new changes</p> <p>Used when:</p> <ul> <li><code>DeltaWriteOptionsImpl</code> is requested for rearrangeOnly</li> </ul>  Demo <p>Learn more in Demo: dataChange.</p>"},{"location":"options/#excluderegex","text":"","title":"excludeRegex"},{"location":"options/#ignorechanges","text":"","title":"ignoreChanges"},{"location":"options/#ignoredeletes","text":"","title":"ignoreDeletes"},{"location":"options/#ignorefiledeletion","text":"","title":"ignoreFileDeletion"},{"location":"options/#maxbytespertrigger","text":"","title":"maxBytesPerTrigger"},{"location":"options/#maxfilespertrigger","text":"","title":"maxFilesPerTrigger <p>Maximum number of files (AddFiles) that DeltaSource is supposed to scan (read) in a streaming micro-batch (trigger)</p> <p>Default: <code>1000</code></p> <p>Must be at least <code>1</code></p>"},{"location":"options/#mergeschema","text":"","title":"mergeSchema <p>Enables schema migration (and allows automatic schema merging during a write operation for WriteIntoDelta and DeltaSink)</p> <p>Equivalent SQL Session configuration: spark.databricks.delta.schema.autoMerge.enabled</p>"},{"location":"options/#optimizewrite","text":"","title":"optimizeWrite <p>Enables...FIXME</p>"},{"location":"options/#overwriteschema","text":"","title":"overwriteSchema"},{"location":"options/#path","text":"","title":"path <p>(required) Directory on a Hadoop DFS-compliant file system with an optional time travel identifier</p> <p>Default: (undefined)</p>  <p>Note</p> <p>Can also be specified using <code>load</code> method of <code>DataFrameReader</code> and <code>DataStreamReader</code>.</p>"},{"location":"options/#queryname","text":"","title":"queryName"},{"location":"options/#replacewhere","text":"","title":"replaceWhere <p>Available as DeltaWriteOptions.replaceWhere</p>  <p>Demo</p> <p>Learn more in Demo: replaceWhere.</p>"},{"location":"options/#timestampasof","text":"","title":"timestampAsOf <p>Timestamp of the version of a Delta table for Time Travel</p> <p>Mutually exclusive with versionAsOf option and the time travel identifier of the path option.</p>"},{"location":"options/#usermetadata","text":"","title":"userMetadata <p>Defines a user-defined commit metadata</p> <p>Take precedence over spark.databricks.delta.commitInfo.userMetadata</p> <p>Available by inspecting CommitInfos using DESCRIBE HISTORY or DeltaTable.history.</p>  <p>Demo</p> <p>Learn more in Demo: User Metadata for Labelling Commits.</p>"},{"location":"options/#versionasof","text":"","title":"versionAsOf <p>Version of a Delta table for Time Travel</p> <p>Mutually exclusive with timestampAsOf option and the time travel identifier of the path option.</p> <p>Used when:</p> <ul> <li><code>DeltaDataSource</code> is requested for a relation</li> </ul>"},{"location":"overview/","text":"<p>Delta Lake is an open-source Apache Spark-based storage layer with ACID transactions and time travel.</p> <p>As it was well said: \"Delta is a storage format while Spark is an execution engine...to separate storage from compute.\"</p>","title":"Delta Lake"},{"location":"overview/#spark-sql","text":"<p>Delta Lake is yet another data source for Spark SQL (and uses Spark SQL's infrastructure for its work).</p> <p>Delta Lake 1.2.0 supports Apache Spark 3.2.0 (cf. build.sbt).</p>","title":"Spark SQL"},{"location":"overview/#delta-tables","text":"<p>Delta tables are parquet tables with a transactional log.</p> <p>Changes to (the state of) a delta table are reflected as actions and persisted to the transactional log (in JSON format).</p>","title":"Delta Tables"},{"location":"overview/#optimistictransaction","text":"<p>Delta Lake uses OptimisticTransaction for transactional writes. A commit is successful when the transaction can write the actions to a delta file (in the transactional log). In case the delta file for the commit version already exists, the transaction is retried.</p> <p>Structured queries can write (transactionally) to a delta table using the following interfaces:</p> <ul> <li> <p>WriteIntoDelta command for batch queries (Spark SQL)</p> </li> <li> <p>DeltaSink for streaming queries (Spark Structured Streaming)</p> </li> </ul> <p>More importantly, multiple queries can write to the same delta table simultaneously (at exactly the same time).</p>","title":"OptimisticTransaction"},{"location":"overview/#programmatic-apis","text":"<p>Delta Lake provides the following programmatic APIs:</p> <ul> <li>DeltaTable</li> <li>DeltaTableBuilder</li> <li>DeltaColumnBuilder</li> </ul>","title":"Programmatic APIs"},{"location":"overview/#structured-queries","text":"<p>Delta Lake supports batch and streaming queries (Spark SQL and Structured Streaming, respectively) using delta format.</p> <p>In order to fine tune queries over data in Delta Lake use options.</p>","title":"Structured Queries"},{"location":"overview/#batch-queries","text":"<p>Delta Lake supports reading and writing in batch queries:</p> <ul> <li> <p>Batch reads (as a <code>RelationProvider</code>)</p> </li> <li> <p>Batch writes (as a <code>CreatableRelationProvider</code>)</p> </li> </ul>","title":"Batch Queries"},{"location":"overview/#streaming-queries","text":"<p>Delta Lake supports reading and writing in streaming queries:</p> <ul> <li> <p>Stream reads (as a <code>Source</code>)</p> </li> <li> <p>Stream writes (as a <code>Sink</code>)</p> </li> </ul>","title":"Streaming Queries"},{"location":"overview/#logstore","text":"<p>Delta Lake uses LogStore abstraction for reading and writing physical log files and checkpoints (using Hadoop FileSystem API).</p>","title":"LogStore"},{"location":"overview/#delta-tables-in-logical-query-plans","text":"<p>Delta Table defines <code>DeltaTable</code> Scala extractor to find delta tables in a logical query plan. The extractor finds <code>LogicalRelation</code>s (Spark SQL) with <code>HadoopFsRelation</code> (Spark SQL) and TahoeFileIndex.</p> <p>Put simply, delta tables are <code>LogicalRelation</code>s with <code>HadoopFsRelation</code> with TahoeFileIndex in logical query plans.</p>","title":"Delta Tables in Logical Query Plans"},{"location":"overview/#concurrent-blind-append-transactions","text":"<p>A transaction can be blind append when simply appends new data to a table with no reliance on existing data (and without reading or modifying it).</p> <p>Blind append transactions are marked in the commit info to distinguish them from read-modify-appends (deletes, merges or updates) and assume no conflict between concurrent transactions.</p> <p>Blind Append Transactions allow for concurrent updates.</p> <pre><code>df.format(\"delta\").mode(\"append\").save(...)\n</code></pre>","title":"Concurrent Blind Append Transactions"},{"location":"overview/#generated-columns","text":"<p>Delta Lake supports Generated Columns.</p>","title":"Generated Columns"},{"location":"overview/#table-constraints","text":"<p>Delta Lake introduces table constraints to ensure data quality and integrity (during writes).</p>","title":"Table Constraints"},{"location":"overview/#exception-public-api","text":"<p>Delta Lake introduces exceptions due to conflicts between concurrent operations as a public API.</p>","title":"Exception Public API"},{"location":"overview/#simplified-storage-configuration","text":"<p>Storage</p>","title":"Simplified Storage Configuration"},{"location":"spark-logging/","text":"<p>Spark uses log4j for logging.</p>  <p>Note</p> <p>Learn more on Spark Logging in The Internals of Apache Spark online book.</p>","title":"Logging"},{"location":"table-properties/","text":"<p>Delta Lake allows setting up table properties for a custom behaviour of a delta table.</p>","title":"Table Properties"},{"location":"table-properties/#show-tblproperties","text":"<p>Table properties can be displayed using <code>SHOW TBLPROPERTIES</code> SQL command:</p> <pre><code>SHOW TBLPROPERTIES &lt;table_name&gt; [(comma-separated properties)]\n</code></pre>  <pre><code>sql(\"SHOW TBLPROPERTIES delta.`/tmp/delta/t1`\").show(truncate = false)\n</code></pre> <pre><code>+----------------------+-----+\n|key                   |value|\n+----------------------+-----+\n|delta.minReaderVersion|1    |\n|delta.minWriterVersion|2    |\n+----------------------+-----+\n</code></pre> <pre><code>sql(\"SHOW TBLPROPERTIES delta.`/tmp/delta/t1` (delta.minReaderVersion)\").show(truncate = false)\n</code></pre> <pre><code>+----------------------+-----+\n|key                   |value|\n+----------------------+-----+\n|delta.minReaderVersion|1    |\n+----------------------+-----+\n</code></pre>","title":"SHOW TBLPROPERTIES"},{"location":"table-properties/#alter-table-set-tblproperties","text":"<p>Table properties can be set a value or unset using <code>ALTER TABLE</code> SQL command:</p> <pre><code>ALTER TABLE &lt;table_name&gt; SET TBLPROPERTIES (&lt;key&gt;=&lt;value&gt;)\n</code></pre> <pre><code>ALTER TABLE table1 UNSET TBLPROPERTIES [IF EXISTS] ('key1', 'key2', ...);\n</code></pre>  <pre><code>sql(\"ALTER TABLE delta.`/tmp/delta/t1` SET TBLPROPERTIES (delta.enableExpiredLogCleanup=true)\")\n</code></pre> <pre><code>sql(\"SHOW TBLPROPERTIES delta.`/tmp/delta/t1` (delta.enableExpiredLogCleanup)\").show(truncate = false)\n</code></pre> <pre><code>+-----------------------------+-----+\n|key                          |value|\n+-----------------------------+-----+\n|delta.enableExpiredLogCleanup|true |\n+-----------------------------+-----+\n</code></pre>","title":"ALTER TABLE SET TBLPROPERTIES"},{"location":"time-travel/","text":"<p>Delta Lake supports time travelling which is loading a Delta table at a given version or timestamp (defined by path, versionAsOf or timestampAsOf options).</p> <p>Delta Lake allows <code>path</code> option to include time travel patterns (<code>@v123</code> and <code>@yyyyMMddHHmmssSSS</code>) unless the internal spark.databricks.delta.timeTravel.resolveOnIdentifier.enabled configuration property is turned off.</p> <p>Time Travel cannot be specified for catalog delta tables.</p> <p>Time travel is described using DeltaTimeTravelSpec.</p> <p>Demo: Time Travel</p>","title":"Time Travel"},{"location":"commands/","text":"","title":"Commands"},{"location":"commands/CreateDeltaTableCommand/","text":"<p><code>CreateDeltaTableCommand</code> is a <code>RunnableCommand</code> (Spark SQL).</p>","title":"CreateDeltaTableCommand"},{"location":"commands/CreateDeltaTableCommand/#creating-instance","text":"<p><code>CreateDeltaTableCommand</code> takes the following to be created:</p> <ul> <li> <code>CatalogTable</code> (Spark SQL) <li> Existing <code>CatalogTable</code> (if available) <li> <code>SaveMode</code> <li> Optional Data Query (<code>LogicalPlan</code>) <li> <code>CreationMode</code> (default: <code>TableCreationModes.Create</code>) <li> <code>tableByPath</code> flag (default: <code>false</code>)  <p><code>CreateDeltaTableCommand</code> is created when:</p> <ul> <li><code>DeltaCatalog</code> is requested to create a Delta table</li> </ul>","title":"Creating Instance"},{"location":"commands/CreateDeltaTableCommand/#executing-command","text":"","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> creates a DeltaLog (for the given table based on a table location) and a DeltaOptions.</p> <p><code>run</code> starts a transaction.</p> <p><code>run</code> branches off based on the optional data query. For data query defined, <code>run</code> creates a WriteIntoDelta and requests it to write. Otherwise, <code>run</code> creates an empty table.</p>  <p>Note</p> <p><code>run</code> does a bit more, but I don't think it's of much interest.</p>  <p><code>run</code> commits the transaction.</p> <p>In the end, <code>run</code> updateCatalog.</p> <p><code>run</code> is part of the <code>RunnableCommand</code> abstraction.</p>"},{"location":"commands/CreateDeltaTableCommand/#updatecatalog","text":"","title":"updateCatalog <pre><code>updateCatalog(\n  spark: SparkSession,\n  table: CatalogTable): Unit\n</code></pre> <p><code>updateCatalog</code> uses the given <code>SparkSession</code> to access <code>SessionCatalog</code> to <code>createTable</code> or <code>alterTable</code> when the tableByPath flag is off. Otherwise, <code>updateCatalog</code> does nothing.</p>"},{"location":"commands/CreateDeltaTableCommand/#getoperation","text":"","title":"getOperation <pre><code>getOperation(\n  metadata: Metadata,\n  isManagedTable: Boolean,\n  options: Option[DeltaOptions]): DeltaOperations.Operation\n</code></pre> <p><code>getOperation</code>...FIXME</p>"},{"location":"commands/DeltaCommand/","text":"<p>DeltaCommand is a marker interface for commands to work with data in delta tables.</p>","title":"DeltaCommand"},{"location":"commands/DeltaCommand/#implementations","text":"<ul> <li>AlterDeltaTableCommand</li> <li>ConvertToDeltaCommand</li> <li>DeleteCommand</li> <li>MergeIntoCommand</li> <li>UpdateCommand</li> <li>VacuumCommandImpl</li> <li>WriteIntoDelta</li> </ul>","title":"Implementations"},{"location":"commands/DeltaCommand/#parsepartitionpredicates-method","text":"","title":"parsePartitionPredicates Method <pre><code>parsePartitionPredicates(\n  spark: SparkSession,\n  predicate: String): Seq[Expression]\n</code></pre> <p><code>parsePartitionPredicates</code>...FIXME</p> <p><code>parsePartitionPredicates</code> is used when...FIXME</p>"},{"location":"commands/DeltaCommand/#verifypartitionpredicates-method","text":"","title":"verifyPartitionPredicates Method <pre><code>verifyPartitionPredicates(\n  spark: SparkSession,\n  partitionColumns: Seq[String],\n  predicates: Seq[Expression]): Unit\n</code></pre> <p><code>verifyPartitionPredicates</code>...FIXME</p> <p><code>verifyPartitionPredicates</code> is used when...FIXME</p>"},{"location":"commands/DeltaCommand/#generatecandidatefilemap-method","text":"","title":"generateCandidateFileMap Method <pre><code>generateCandidateFileMap(\n  basePath: Path,\n  candidateFiles: Seq[AddFile]): Map[String, AddFile]\n</code></pre> <p><code>generateCandidateFileMap</code>...FIXME</p> <p><code>generateCandidateFileMap</code> is used when...FIXME</p>"},{"location":"commands/DeltaCommand/#removefilesfrompaths-method","text":"","title":"removeFilesFromPaths Method <pre><code>removeFilesFromPaths(\n  deltaLog: DeltaLog,\n  nameToAddFileMap: Map[String, AddFile],\n  filesToRewrite: Seq[String],\n  operationTimestamp: Long): Seq[RemoveFile]\n</code></pre> <p><code>removeFilesFromPaths</code>...FIXME</p> <p><code>removeFilesFromPaths</code> is used when:</p> <ul> <li>DeleteCommand and UpdateCommand commands are executed</li> </ul>"},{"location":"commands/DeltaCommand/#creating-hadoopfsrelation-with-tahoebatchfileindex","text":"","title":"Creating HadoopFsRelation (with TahoeBatchFileIndex) <pre><code>buildBaseRelation(\n  spark: SparkSession,\n  txn: OptimisticTransaction,\n  actionType: String,\n  rootPath: Path,\n  inputLeafFiles: Seq[String],\n  nameToAddFileMap: Map[String, AddFile]): HadoopFsRelation\n</code></pre> <p><code>buildBaseRelation</code> converts the given <code>inputLeafFiles</code> to AddFiles (with the given <code>rootPath</code> and <code>nameToAddFileMap</code>).</p> <p><code>buildBaseRelation</code> creates a TahoeBatchFileIndex for the <code>AddFile</code>s (with the input <code>actionType</code> and <code>rootPath</code>).</p> <p>In the end, <code>buildBaseRelation</code> creates a <code>HadoopFsRelation</code> (Spark SQL) with the <code>TahoeBatchFileIndex</code> (and the other properties based on the metadata of the given OptimisticTransaction).</p> <p><code>buildBaseRelation</code> is used when:</p> <ul> <li>DeleteCommand and UpdateCommand commands are executed (with <code>delete</code> and <code>update</code> action types, respectively)</li> </ul>"},{"location":"commands/DeltaCommand/#gettouchedfile-method","text":"","title":"getTouchedFile Method <pre><code>getTouchedFile(\n  basePath: Path,\n  filePath: String,\n  nameToAddFileMap: Map[String, AddFile]): AddFile\n</code></pre> <p><code>getTouchedFile</code>...FIXME</p> <p><code>getTouchedFile</code> is used when:</p> <ul> <li> <p><code>DeltaCommand</code> is requested to removeFilesFromPaths and create a HadoopFsRelation (for DeleteCommand and UpdateCommand commands)</p> </li> <li> <p>MergeIntoCommand is executed</p> </li> </ul>"},{"location":"commands/DeltaCommand/#iscatalogtable-method","text":"","title":"isCatalogTable Method <pre><code>isCatalogTable(\n  analyzer: Analyzer,\n  tableIdent: TableIdentifier): Boolean\n</code></pre> <p><code>isCatalogTable</code>...FIXME</p> <p><code>isCatalogTable</code> is used when...FIXME</p>"},{"location":"commands/DeltaCommand/#ispathidentifier-method","text":"","title":"isPathIdentifier Method <pre><code>isPathIdentifier(\n  tableIdent: TableIdentifier): Boolean\n</code></pre> <p><code>isPathIdentifier</code>...FIXME</p> <p><code>isPathIdentifier</code> is used when...FIXME</p>"},{"location":"commands/DeltaCommand/#commitlarge","text":"","title":"commitLarge <pre><code>commitLarge(\n  spark: SparkSession,\n  txn: OptimisticTransaction,\n  actions: Iterator[Action],\n  op: DeltaOperations.Operation,\n  context: Map[String, String],\n  metrics: Map[String, String]): Long\n</code></pre> <p><code>commitLarge</code>...FIXME</p> <p><code>commitLarge</code> is used when:</p> <ul> <li>ConvertToDeltaCommand command is executed</li> </ul>"},{"location":"commands/DeltaCommand/#updateandcheckpoint","text":"","title":"updateAndCheckpoint <pre><code>updateAndCheckpoint(\n  spark: SparkSession,\n  deltaLog: DeltaLog,\n  commitSize: Int,\n  attemptVersion: Long): Unit\n</code></pre> <p><code>updateAndCheckpoint</code>...FIXME</p>"},{"location":"commands/WriteIntoDelta/","text":"<p><code>WriteIntoDelta</code> is a Delta command that can write data(frame) transactionally into a delta table.</p> <p><code>WriteIntoDelta</code> is a <code>RunnableCommand</code> (Spark SQL) logical operator.</p>","title":"WriteIntoDelta Command"},{"location":"commands/WriteIntoDelta/#creating-instance","text":"<p><code>WriteIntoDelta</code> takes the following to be created:</p> <ul> <li> DeltaLog <li> <code>SaveMode</code> <li> DeltaOptions <li> Names of the partition columns <li> Configuration <li> Data (<code>DataFrame</code>)  <p><code>WriteIntoDelta</code> is created\u00a0when:</p> <ul> <li><code>DeltaLog</code> is requested to create an insertable HadoopFsRelation (when <code>DeltaDataSource</code> is requested to create a relation as a CreatableRelationProvider or a RelationProvider)</li> <li><code>DeltaCatalog</code> is requested to createDeltaTable</li> <li><code>WriteIntoDeltaBuilder</code> is requested to buildForV1Write</li> <li><code>CreateDeltaTableCommand</code> command is executed</li> <li><code>DeltaDataSource</code> is requested to create a relation (for writing) (as a CreatableRelationProvider)</li> </ul>","title":"Creating Instance"},{"location":"commands/WriteIntoDelta/#implicitmetadataoperation","text":"<p><code>WriteIntoDelta</code> is an operation that can update metadata (schema and partitioning) of the delta table.</p>","title":"ImplicitMetadataOperation"},{"location":"commands/WriteIntoDelta/#executing-command","text":"","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code>\u00a0is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p> <p><code>run</code> requests the DeltaLog to start a new transaction.</p> <p><code>run</code> writes and requests the <code>OptimisticTransaction</code> to commit (with <code>DeltaOperations.Write</code> operation with the SaveMode, partition columns, replaceWhere and userMetadata).</p>"},{"location":"commands/WriteIntoDelta/#write","text":"","title":"write <pre><code>write(\n  txn: OptimisticTransaction,\n  sparkSession: SparkSession): Seq[Action]\n</code></pre> <p><code>write</code> checks out whether the write operation is to a delta table that already exists. If so (i.e. the readVersion of the transaction is above <code>-1</code>), <code>write</code> branches per the SaveMode:</p> <ul> <li> <p>For <code>ErrorIfExists</code>, <code>write</code> throws an <code>AnalysisException</code>.</p> <pre><code>[path] already exists.\n</code></pre> </li> <li> <p>For <code>Ignore</code>, <code>write</code> does nothing and returns back with no Actions.</p> </li> <li> <p>For <code>Overwrite</code>, <code>write</code> requests the DeltaLog to assert being removable</p> </li> </ul> <p><code>write</code> updateMetadata (with rearrangeOnly option).</p> <p><code>write</code>...FIXME</p> <p><code>write</code>\u00a0is used when:</p> <ul> <li>CreateDeltaTableCommand is executed</li> <li><code>WriteIntoDelta</code> is executed</li> </ul>"},{"location":"commands/WriteIntoDelta/#demo","text":"","title":"Demo <pre><code>import org.apache.spark.sql.delta.commands.WriteIntoDelta\nimport org.apache.spark.sql.delta.DeltaLog\nimport org.apache.spark.sql.SaveMode\nimport org.apache.spark.sql.delta.DeltaOptions\nval tableName = \"/tmp/delta/t1\"\nval data = spark.range(5).toDF\nval writeCmd = WriteIntoDelta(\n  deltaLog = DeltaLog.forTable(spark, tableName),\n  mode = SaveMode.Overwrite,\n  options = new DeltaOptions(Map.empty[String, String], spark.sessionState.conf),\n  partitionColumns = Seq.empty[String],\n  configuration = Map.empty[String, String],\n  data)\n\n// Review web UI @ http://localhost:4040\n\nwriteCmd.run(spark)\n</code></pre>"},{"location":"commands/alter/","text":"<p>Delta Lake supports altering delta tables using <code>ALTER TABLE</code> high-level operators:</p> <ul> <li>ALTER TABLE ADD CONSTRAINT</li> <li>ALTER TABLE DROP CONSTRAINT</li> <li>others</li> </ul>","title":"Alter Command"},{"location":"commands/alter/AlterDeltaTableCommand/","text":"<p><code>AlterDeltaTableCommand</code> is an extension of the DeltaCommand abstraction for Delta commands that alter a DeltaTableV2.</p>","title":"AlterDeltaTableCommand"},{"location":"commands/alter/AlterDeltaTableCommand/#contract","text":"","title":"Contract"},{"location":"commands/alter/AlterDeltaTableCommand/#table","text":"","title":"table <pre><code>table: DeltaTableV2\n</code></pre> <p>DeltaTableV2</p> <p>Used when:</p> <ul> <li><code>AlterDeltaTableCommand</code> is requested to startTransaction</li> </ul>"},{"location":"commands/alter/AlterDeltaTableCommand/#implementations","text":"<ul> <li>AlterTableAddColumnsDeltaCommand</li> <li>AlterTableAddConstraintDeltaCommand</li> <li>AlterTableChangeColumnDeltaCommand</li> <li>AlterTableDropConstraintDeltaCommand</li> <li>AlterTableReplaceColumnsDeltaCommand</li> <li>AlterTableSetLocationDeltaCommand</li> <li>AlterTableSetPropertiesDeltaCommand</li> <li>AlterTableUnsetPropertiesDeltaCommand</li> </ul>","title":"Implementations"},{"location":"commands/alter/AlterDeltaTableCommand/#starttransaction","text":"","title":"startTransaction <pre><code>startTransaction(): OptimisticTransaction\n</code></pre> <p><code>startTransaction</code> simply requests the DeltaTableV2 for the DeltaLog that in turn is requested to startTransaction.</p>"},{"location":"commands/alter/AlterTableAddColumnsDeltaCommand/","text":"<p><code>AlterTableAddColumnsDeltaCommand</code> is a <code>LeafRunnableCommand</code> (Spark SQL) that represents <code>ALTER TABLE ADD COLUMNS</code> SQL command.</p>  <p>Note</p> <p><code>AlterTableAddColumnsDeltaCommand</code> is a variant of Spark SQL's AlterTableAddColumnsCommand for Delta Lake to support <code>ALTER TABLE ADD COLUMNS</code> command.</p> <p>Otherwise, Spark SQL would throw an AnalysisException.</p>  <p><code>AlterTableAddColumnsDeltaCommand</code> is an AlterDeltaTableCommand.</p>","title":"AlterTableAddColumnsDeltaCommand Leaf Runnable Command"},{"location":"commands/alter/AlterTableAddColumnsDeltaCommand/#creating-instance","text":"<p><code>AlterTableAddColumnsDeltaCommand</code> takes the following to be created:</p> <ul> <li> DeltaTableV2 <li> Columns to Add  <p><code>AlterTableAddColumnsDeltaCommand</code> is created when:</p> <ul> <li><code>DeltaCatalog</code> is requested to alter a table</li> </ul>","title":"Creating Instance"},{"location":"commands/alter/AlterTableAddColumnsDeltaCommand/#ignorecacheddata","text":"","title":"IgnoreCachedData <p><code>AlterTableAddColumnsDeltaCommand</code> is an <code>IgnoreCachedData</code> (Spark SQL) logical operator.</p>"},{"location":"commands/alter/AlterTableAddColumnsDeltaCommand/#executing-command","text":"","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> starts a transaction and requests it for the current Metadata (of the DeltaTableV2).</p> <p><code>run</code> alters the current schema (creates a new metadata) and notifies the transaction.</p> <p>In the end, <code>run</code> commits the transaction (as ADD COLUMNS operation).</p>  <p><code>run</code> is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p>"},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/","text":"<p><code>AlterTableAddConstraintDeltaCommand</code> is an AlterDeltaTableCommand.</p> <p><code>AlterTableAddConstraintDeltaCommand</code> is a <code>RunnableCommand</code> (Spark SQL) and <code>IgnoreCachedData</code> (Spark SQL).</p>","title":"AlterTableAddConstraintDeltaCommand"},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#creating-instance","text":"<p><code>AlterTableAddConstraintDeltaCommand</code> takes the following to be created:</p> <ul> <li> DeltaTableV2 <li> Constraint Name <li> Constraint SQL Expression (as a text)  <p><code>AlterTableAddConstraintDeltaCommand</code> is created\u00a0when:</p> <ul> <li><code>DeltaCatalog</code> is requested to alter a delta table (with AddConstraint table changes)</li> </ul>","title":"Creating Instance"},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#executing-command","text":"","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code>\u00a0is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p> <p><code>run</code>...FIXME</p> <p><code>run</code> prints out the following INFO message to the logs:</p> <pre><code>Checking that [constraint] is satisfied for existing data. This will require a full table scan.\n</code></pre> <p><code>run</code> requests the <code>OptimisticTransaction</code> for the DeltaLog to create a <code>DataFrame</code> (for the snapshot).</p> <p><code>run</code> uses <code>where</code> operator and counts the rows that do not match the constraint.</p> <p>With no rows violating the check constraint, <code>run</code> requests the <code>OptimisticTransaction</code> to commit (with the new <code>Metadata</code> and a new AddConstraint).</p>"},{"location":"commands/alter/AlterTableAddConstraintDeltaCommand/#analysisexception","text":"","title":"AnalysisException <p><code>run</code> throws an <code>AnalysisException</code> when one or more rows violate the new constraint:</p> <pre><code>[num] rows in [tableName] violate the new CHECK constraint ([expr])\n</code></pre>"},{"location":"commands/alter/AlterTableChangeColumnDeltaCommand/","text":"<p>AlterTableChangeColumnDeltaCommand is...FIXME</p>","title":"AlterTableChangeColumnDeltaCommand"},{"location":"commands/alter/AlterTableDropConstraintDeltaCommand/","text":"<p><code>AlterTableDropConstraintDeltaCommand</code> is...FIXME</p>","title":"AlterTableDropConstraintDeltaCommand"},{"location":"commands/alter/AlterTableReplaceColumnsDeltaCommand/","text":"<p>AlterTableReplaceColumnsDeltaCommand is...FIXME</p>","title":"AlterTableReplaceColumnsDeltaCommand"},{"location":"commands/alter/AlterTableSetLocationDeltaCommand/","text":"<p>AlterTableSetLocationDeltaCommand is...FIXME</p>","title":"AlterTableSetLocationDeltaCommand"},{"location":"commands/alter/AlterTableSetPropertiesDeltaCommand/","text":"<p><code>AlterTableSetPropertiesDeltaCommand</code> is a AlterDeltaTableCommand.</p> <p><code>AlterTableSetPropertiesDeltaCommand</code> is a <code>RunnableCommand</code> (Spark SQL) logical operator.</p>","title":"AlterTableSetPropertiesDeltaCommand"},{"location":"commands/alter/AlterTableSetPropertiesDeltaCommand/#creating-instance","text":"<p><code>AlterTableSetPropertiesDeltaCommand</code> takes the following to be created:</p> <ul> <li> DeltaTableV2 <li> Configuration (<code>Map[String, String]</code>)  <p><code>AlterTableSetPropertiesDeltaCommand</code> is created\u00a0when:</p> <ul> <li><code>DeltaCatalog</code> is requested to alterTable</li> </ul>","title":"Creating Instance"},{"location":"commands/alter/AlterTableUnsetPropertiesDeltaCommand/","text":"<p>AlterTableUnsetPropertiesDeltaCommand is...FIXME</p>","title":"AlterTableUnsetPropertiesDeltaCommand"},{"location":"commands/convert/","text":"<p>Delta Lake supports converting (importing) parquet tables to delta format using the following high-level operators:</p> <ul> <li>CONVERT TO DELTA SQL command</li> <li>DeltaTable.convertToDelta</li> </ul>","title":"Convert to Delta Command"},{"location":"commands/convert/ConvertToDeltaCommand/","text":"<p><code>ConvertToDeltaCommand</code> is a DeltaCommand that converts a parquet table to delta format (imports it into Delta).</p> <p><code>ConvertToDeltaCommand</code> is a <code>RunnableCommand</code> (Spark SQL).</p> <p><code>ConvertToDeltaCommand</code> requires that the partition schema matches the partitions of the parquet table (or an AnalysisException is thrown)</p>","title":"ConvertToDeltaCommand"},{"location":"commands/convert/ConvertToDeltaCommand/#creating-instance","text":"<p><code>ConvertToDeltaCommand</code> takes the following to be created:</p> <ul> <li> Parquet table (<code>TableIdentifier</code>) <li> Partition schema (<code>Option[StructType]</code>) <li> Delta Path (<code>Option[String]</code>)  <p><code>ConvertToDeltaCommand</code> is created when:</p> <ul> <li>CONVERT TO DELTA statement is used (and <code>DeltaSqlAstBuilder</code> is requested to visitConvert)</li> <li>DeltaTable.convertToDelta utility is used (and <code>DeltaConvert</code> utility is used to executeConvert)</li> </ul>","title":"Creating Instance"},{"location":"commands/convert/ConvertToDeltaCommand/#executing-command","text":"","title":"Executing Command <pre><code>run(\n  spark: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> is part of the <code>RunnableCommand</code> (Spark SQL) contract.</p> <p><code>run</code> creates a ConvertProperties from the TableIdentifier (with the given <code>SparkSession</code>).</p> <p><code>run</code> makes sure that the (data source) provider (the database part of the TableIdentifier) is either <code>delta</code> or <code>parquet</code>. For all other data source providers, <code>run</code> throws an <code>AnalysisException</code>:</p> <pre><code>CONVERT TO DELTA only supports parquet tables, but you are trying to convert a [sourceName] source: [ident]\n</code></pre> <p>For <code>delta</code> data source provider, <code>run</code> simply prints out the following message to standard output and returns.</p> <pre><code>The table you are trying to convert is already a delta table\n</code></pre> <p>For <code>parquet</code> data source provider, <code>run</code> uses <code>DeltaLog</code> utility to create a DeltaLog. <code>run</code> then requests <code>DeltaLog</code> to update and start a new transaction. In the end, <code>run</code> performConvert.</p> <p>In case the readVersion of the new transaction is greater than <code>-1</code>, <code>run</code> simply prints out the following message to standard output and returns.</p> <pre><code>The table you are trying to convert is already a delta table\n</code></pre>"},{"location":"commands/convert/ConvertToDeltaCommand/#performconvert","text":"","title":"performConvert <pre><code>performConvert(\n  spark: SparkSession,\n  txn: OptimisticTransaction,\n  convertProperties: ConvertTarget): Seq[Row]\n</code></pre> <p><code>performConvert</code> makes sure that the directory exists (from the given <code>ConvertProperties</code> which is the table part of the TableIdentifier of the command).</p> <p><code>performConvert</code> requests the <code>OptimisticTransaction</code> for the DeltaLog that is then requested to ensureLogDirectoryExist.</p> <p><code>performConvert</code> creates a Dataset to recursively list directories and files in the directory and leaves only files (by filtering out directories using <code>WHERE</code> clause).</p>  <p>Note</p> <p><code>performConvert</code> uses <code>Dataset</code> API to build a distributed computation to query files.</p>  <p> <code>performConvert</code> caches the <code>Dataset</code> of file names. <p> <code>performConvert</code> uses spark.databricks.delta.import.batchSize.schemaInference configuration property for the number of files per batch for schema inference. <code>performConvert</code> mergeSchemasInParallel for every batch of files and then mergeSchemas. <p><code>performConvert</code> constructTableSchema using the inferred table schema and the partitionSchema (if specified).</p> <p><code>performConvert</code> creates a new Metadata using the table schema and the partitionSchema (if specified).</p> <p><code>performConvert</code> requests the <code>OptimisticTransaction</code> to update the metadata.</p> <p> <code>performConvert</code> uses spark.databricks.delta.import.batchSize.statsCollection configuration property for the number of files per batch for stats collection. <code>performConvert</code> creates an AddFile (in the data path of the DeltaLog of the <code>OptimisticTransaction</code>) for every file in a batch. <p> In the end, <code>performConvert</code> streamWrite (with the <code>OptimisticTransaction</code>, the <code>AddFile</code>s, and Convert operation) and unpersists the <code>Dataset</code> of file names."},{"location":"commands/convert/ConvertToDeltaCommand/#streamwrite","text":"","title":"streamWrite <pre><code>streamWrite(\n  spark: SparkSession,\n  txn: OptimisticTransaction,\n  addFiles: Iterator[AddFile],\n  op: DeltaOperations.Operation,\n  numFiles: Long): Long\n</code></pre> <p><code>streamWrite</code>...FIXME</p>"},{"location":"commands/convert/ConvertToDeltaCommand/#createaddfile","text":"","title":"createAddFile <pre><code>createAddFile(\n  file: SerializableFileStatus,\n  basePath: Path,\n  fs: FileSystem,\n  conf: SQLConf): AddFile\n</code></pre> <p><code>createAddFile</code> creates an AddFile action.</p> <p>Internally, <code>createAddFile</code>...FIXME</p> <p> <code>createAddFile</code> throws an <code>AnalysisException</code> if the number of fields in the given partition schema does not match the number of partitions found (at partition discovery phase): <pre><code>Expecting [size] partition column(s): [expectedCols], but found [size] partition column(s): [parsedCols] from parsing the file name: [path]\n</code></pre>"},{"location":"commands/convert/ConvertToDeltaCommand/#mergeschemasinparallel","text":"","title":"mergeSchemasInParallel <pre><code>mergeSchemasInParallel(\n  sparkSession: SparkSession,\n  filesToTouch: Seq[FileStatus],\n  serializedConf: SerializableConfiguration): Option[StructType]\n</code></pre> <p><code>mergeSchemasInParallel</code>...FIXME</p>"},{"location":"commands/convert/ConvertToDeltaCommand/#constructtableschema","text":"","title":"constructTableSchema <pre><code>constructTableSchema(\n  spark: SparkSession,\n  dataSchema: StructType,\n  partitionFields: Seq[StructField]): StructType\n</code></pre> <p><code>constructTableSchema</code>...FIXME</p>"},{"location":"commands/convert/ConvertToDeltaCommand/#converttodeltacommandbase","text":"","title":"ConvertToDeltaCommandBase <p><code>ConvertToDeltaCommandBase</code> is the base of <code>ConvertToDeltaCommand</code>-like commands with the only known implementation being <code>ConvertToDeltaCommand</code> itself.</p>"},{"location":"commands/convert/DeltaConvert/","text":"","title":"DeltaConvert Utility"},{"location":"commands/convert/DeltaConvert/#executeconvert","text":"","title":"executeConvert <pre><code>executeConvert(\n  spark: SparkSession,\n  tableIdentifier: TableIdentifier,\n  partitionSchema: Option[StructType],\n  deltaPath: Option[String]): DeltaTable\n</code></pre> <p><code>executeConvert</code> converts a parquet table to a delta table.</p> <p><code>executeConvert</code> executes a new ConvertToDeltaCommand.</p> <p>In the end, <code>executeConvert</code> creates a DeltaTable.</p>  <p>Note</p> <p><code>executeConvert</code> can convert a Spark table (to Delta) that is registered in a metastore.</p>  <p><code>executeConvert</code> is used when:</p> <ul> <li>DeltaTable.convertToDelta utility is used</li> </ul>"},{"location":"commands/convert/DeltaConvert/#demo","text":"","title":"Demo <pre><code>import org.apache.spark.sql.SparkSession\nassert(spark.isInstanceOf[SparkSession])\n\n// CONVERT TO DELTA only supports parquet tables\n// TableIdentifier should be parquet.`users`\nimport org.apache.spark.sql.catalyst.TableIdentifier\nval table = TableIdentifier(table = \"users\", database = Some(\"parquet\"))\n\nimport org.apache.spark.sql.types.{StringType, StructField, StructType}\nval partitionSchema: Option[StructType] = Some(\n  new StructType().add(StructField(\"country\", StringType)))\n\nval deltaPath: Option[String] = None\n\n// Use web UI to monitor execution, e.g. http://localhost:4040\nimport io.delta.tables.execution.DeltaConvert\nDeltaConvert.executeConvert(spark, table, partitionSchema, deltaPath)\n</code></pre>"},{"location":"commands/convert/FileManifest/","text":"<p><code>FileManifest</code> is an abstraction of file manifests for ConvertToDeltaCommand.</p> <p><code>FileManifest</code> is <code>Closeable</code> (Java).</p>","title":"FileManifest"},{"location":"commands/convert/FileManifest/#contract","text":"","title":"Contract"},{"location":"commands/convert/FileManifest/#basepath","text":"","title":"basePath <pre><code>basePath: String\n</code></pre> <p>The base path of a delta table</p>"},{"location":"commands/convert/FileManifest/#getfiles","text":"","title":"getFiles <pre><code>getFiles: Iterator[SerializableFileStatus]\n</code></pre> <p>The active files of a delta table</p> <p>Used when:</p> <ul> <li><code>ConvertToDeltaCommand</code> is requested to createDeltaActions and performConvert</li> </ul>"},{"location":"commands/convert/FileManifest/#implementations","text":"<ul> <li>ManualListingFileManifest</li> <li>MetadataLogFileManifest</li> </ul>","title":"Implementations"},{"location":"commands/convert/ManualListingFileManifest/","text":"<p><code>ManualListingFileManifest</code> is a FileManifest.</p>","title":"ManualListingFileManifest"},{"location":"commands/convert/ManualListingFileManifest/#getfiles","text":"","title":"getFiles <pre><code>getFiles: Iterator[SerializableFileStatus]\n</code></pre> <p><code>getFiles</code>\u00a0is part of the FileManifest abstraction.</p> <p><code>getFiles</code>...FIXME</p>"},{"location":"commands/convert/ManualListingFileManifest/#close","text":"","title":"close <pre><code>close\n</code></pre> <p><code>close</code>\u00a0is part of the <code>Closeable</code> (Java) abstraction.</p> <p><code>close</code>...FIXME</p>"},{"location":"commands/convert/ManualListingFileManifest/#hdfs-filestatus-list-dataset","text":"","title":"HDFS FileStatus List Dataset <pre><code>list: Dataset[SerializableFileStatus]\n</code></pre> <p><code>list</code> creates a HDFS FileStatus dataset and marks it to be cached (once an action is executed).</p>  <p>Scala lazy value</p> <p><code>list</code> is a Scala lazy value and is initialized once when first accessed. Once computed it stays unchanged.</p>  <p><code>list</code>\u00a0is used when:</p> <ul> <li><code>ManualListingFileManifest</code> is requested to getFiles and close</li> </ul>"},{"location":"commands/convert/ManualListingFileManifest/#dolist","text":"","title":"doList <pre><code>doList(): Dataset[SerializableFileStatus]\n</code></pre> <p><code>doList</code>...FIXME</p> <p><code>doList</code>\u00a0is used when:</p> <ul> <li><code>ManualListingFileManifest</code> is requested for file status dataset</li> </ul>"},{"location":"commands/convert/MetadataLogFileManifest/","text":"<p><code>MetadataLogFileManifest</code> is...FIXME</p>","title":"MetadataLogFileManifest"},{"location":"commands/delete/","text":"<p>Delta Lake supports deleting records from delta tables using the following high-level operators:</p> <ul> <li>DELETE FROM SQL command</li> <li>DeltaTable.delete</li> </ul>","title":"Delete Command"},{"location":"commands/delete/DeleteCommand/","text":"<p><code>DeleteCommand</code> is a DeltaCommand that represents DeltaDelete logical command at execution.</p> <p><code>DeleteCommand</code> is a <code>RunnableCommand</code> (Spark SQL) logical operator.</p>","title":"DeleteCommand"},{"location":"commands/delete/DeleteCommand/#creating-instance","text":"<p><code>DeleteCommand</code> takes the following to be created:</p> <ul> <li> TahoeFileIndex <li> Target Data (LogicalPlan) <li> Condition (Expression)  <p><code>DeleteCommand</code> is created (also using apply factory utility) when:</p> <ul> <li>PreprocessTableDelete logical resolution rule is executed (and resolves a DeltaDelete logical command)</li> </ul>","title":"Creating Instance"},{"location":"commands/delete/DeleteCommand/#performance-metrics","text":"Name web UI     <code>numRemovedFiles</code> number of files removed.   <code>numAddedFiles</code> number of files added.   <code>numDeletedRows</code> number of rows deleted.","title":"Performance Metrics"},{"location":"commands/delete/DeleteCommand/#executing-command","text":"","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p> <p><code>run</code> requests the TahoeFileIndex for the DeltaLog (and asserts that the table is removable).</p> <p><code>run</code> requests the <code>DeltaLog</code> to start a new transaction for performDelete.</p> <p>In the end, <code>run</code> re-caches all cached plans (incl. this relation itself) by requesting the <code>CacheManager</code> (Spark SQL) to recache the target.</p>"},{"location":"commands/delete/DeleteCommand/#performdelete","text":"","title":"performDelete <pre><code>performDelete(\n  sparkSession: SparkSession,\n  deltaLog: DeltaLog,\n  txn: OptimisticTransaction): Unit\n</code></pre>"},{"location":"commands/delete/DeleteCommand/#number-of-table-files","text":"","title":"Number of Table Files <p><code>performDelete</code> requests the given DeltaLog for the current Snapshot that is in turn requested for the number of files in the delta table.</p>"},{"location":"commands/delete/DeleteCommand/#finding-delete-actions","text":"","title":"Finding Delete Actions <p><code>performDelete</code> branches off based on the optional condition:</p> <ol> <li>No condition to delete the whole table</li> <li>Condition defined on metadata only</li> <li>Other conditions</li> </ol>"},{"location":"commands/delete/DeleteCommand/#delete-condition-undefined","text":"","title":"Delete Condition Undefined <p><code>performDelete</code>...FIXME</p>"},{"location":"commands/delete/DeleteCommand/#metadata-only-delete-condition","text":"","title":"Metadata-Only Delete Condition <p><code>performDelete</code>...FIXME</p>"},{"location":"commands/delete/DeleteCommand/#other-delete-conditions","text":"","title":"Other Delete Conditions <p><code>performDelete</code>...FIXME</p>"},{"location":"commands/delete/DeleteCommand/#delete-actions-available","text":"","title":"Delete Actions Available <p><code>performDelete</code>...FIXME</p>"},{"location":"commands/delete/DeleteCommand/#creating-deletecommand","text":"","title":"Creating DeleteCommand <pre><code>apply(\n  delete: DeltaDelete): DeleteCommand\n</code></pre> <p><code>apply</code> creates a DeleteCommand.</p>"},{"location":"commands/delete/DeltaDelete/","text":"<p><code>DeltaDelete</code> is an unary logical operator (Spark SQL) that represents <code>DeleteFromTable</code>s (Spark SQL) at execution.</p> <p>As per the comment:</p>  <p>Needs to be compatible with DBR 6 and can't use the new class added in Spark 3.0: <code>DeleteFromTable</code>.</p>","title":"DeltaDelete Unary Logical Command"},{"location":"commands/delete/DeltaDelete/#creating-instance","text":"<p><code>DeltaDelete</code> takes the following to be created:</p> <ul> <li> Child <code>LogicalPlan</code> (Spark SQL) <li> Condition Expression (Spark SQL)  <p><code>DeltaDelete</code> is created\u00a0when:</p> <ul> <li>DeltaAnalysis logical resolution rule is executed and resolves <code>DeleteFromTable</code>s (Spark SQL)</li> </ul>","title":"Creating Instance"},{"location":"commands/delete/DeltaDelete/#logical-resolution","text":"<p><code>DeltaDelete</code> is resolved to a DeleteCommand when PreprocessTableDelete post-hoc logical resolution rule is executed.</p>","title":"Logical Resolution"},{"location":"commands/describe-detail/","text":"<p>Delta Lake supports displaying details of delta tables using the following high-level operator:</p> <ul> <li>DESCRIBE DETAIL SQL command</li> </ul>","title":"DESCRIBE DETAIL Command"},{"location":"commands/describe-detail/DescribeDeltaDetailCommand/","text":"<p><code>DescribeDeltaDetailCommand</code> is a <code>RunnableCommand</code> (Spark SQL) for DESCRIBE DETAIL SQL command.</p> <pre><code>(DESC | DESCRIBE) DETAIL (path | table)\n</code></pre>","title":"DescribeDeltaDetailCommand (DescribeDeltaDetailCommandBase)"},{"location":"commands/describe-detail/DescribeDeltaDetailCommand/#creating-instance","text":"<p><code>DescribeDeltaDetailCommand</code> takes the following to be created:</p> <ul> <li> (optional) Table Path <li> (optional) Table identifier  <p><code>DescribeDeltaDetailCommand</code> is created\u00a0when:</p> <ul> <li><code>DeltaSqlAstBuilder</code> is requested to parse DESCRIBE DETAIL SQL command)</li> </ul>","title":"Creating Instance"},{"location":"commands/describe-detail/DescribeDeltaDetailCommand/#run","text":"","title":"run <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code>\u00a0is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p> <p><code>run</code>...FIXME</p>"},{"location":"commands/describe-detail/DescribeDeltaDetailCommand/#demo","text":"","title":"Demo <pre><code>val q = sql(\"DESCRIBE DETAIL '/tmp/delta/users'\")\n</code></pre> <pre><code>scala&gt; q.show\n+------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+\n|format|                  id|name|description|            location|           createdAt|       lastModified|partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|\n+------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+\n| delta|3799b291-dbfa-4f8...|null|       null|file:/tmp/delta/u...|2020-01-06 17:08:...|2020-01-06 17:12:28| [city, country]|       4|       2581|        []|               1|               2|\n+------+--------------------+----+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+\n</code></pre>"},{"location":"commands/describe-history/","text":"<p>Delta Lake supports displaying versions (history) of delta tables using the following high-level operators:</p> <ul> <li>DESCRIBE HISTORY SQL command</li> <li>DeltaTable.history</li> </ul>","title":"Describe History Command"},{"location":"commands/describe-history/DescribeDeltaHistoryCommand/","text":"<p><code>DescribeDeltaHistoryCommand</code> is a <code>RunnableCommand</code> (Spark SQL) that uses DeltaHistoryManager for the commit history of a delta table.</p> <p><code>DescribeDeltaHistoryCommand</code> is used for DESCRIBE HISTORY SQL command.</p>","title":"DescribeDeltaHistoryCommand"},{"location":"commands/describe-history/DescribeDeltaHistoryCommand/#creating-instance","text":"<p><code>DescribeDeltaHistoryCommand</code> takes the following to be created:</p> <ul> <li> (optional) Directory <li> (optional) <code>TableIdentifier</code> <li> (optional) Number of commits to display <li> Output Attributes (default: CommitInfo)  <p><code>DescribeDeltaHistoryCommand</code> is created for:</p> <ul> <li>DESCRIBE HISTORY SQL command (that uses <code>DeltaSqlAstBuilder</code> to parse DESCRIBE HISTORY SQL command)</li> </ul>","title":"Creating Instance"},{"location":"commands/describe-history/DescribeDeltaHistoryCommand/#executing-command","text":"","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code>\u00a0is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p> <p><code>run</code> creates a Hadoop <code>Path</code> to (the location of) the delta table (based on DeltaTableIdentifier).</p> <p><code>run</code> creates a DeltaLog for the delta table.</p> <p><code>run</code> requests the <code>DeltaLog</code> for the DeltaHistoryManager that is requested for the commit history.</p>"},{"location":"commands/generate/","text":"<p>Delta Lake supports executing generator functions on delta tables using the following high-level operators:</p> <ul> <li>GENERATE SQL command</li> <li>DeltaTable.generate</li> </ul> <p>Only symlink_format_manifest mode is supported.</p>","title":"Generate Command"},{"location":"commands/generate/DeltaGenerateCommand/","text":"<p><code>DeltaGenerateCommand</code> is a <code>RunnableCommand</code> (Spark SQL) to execute a generate function on a delta table.</p> <p><code>DeltaGenerateCommand</code> is used for the following:</p> <ul> <li>GENERATE SQL command</li> <li>DeltaTable.generate operation</li> </ul> <p><code>DeltaGenerateCommand</code> supports symlink_format_manifest mode name only.</p>","title":"DeltaGenerateCommand"},{"location":"commands/generate/DeltaGenerateCommand/#demo","text":"<pre><code>val path = \"/tmp/delta/d01\"\nval tid = s\"delta.`$path`\"\nval q = s\"GENERATE symlink_format_manifest FOR TABLE $tid\"\nsql(q).collect\n</code></pre>","title":"Demo"},{"location":"commands/generate/DeltaGenerateCommand/#creating-instance","text":"<p><code>DeltaGenerateCommand</code> takes the following to be created:</p> <ul> <li>Mode Name</li> <li> <code>TableIdentifier</code> (Spark SQL)  <p><code>DeltaGenerateCommand</code> is created for:</p> <ul> <li>GENERATE SQL command (that uses <code>DeltaSqlAstBuilder</code> to parse GENERATE SQL command)</li> <li>DeltaTable.generate operator (that uses <code>DeltaTableOperations</code> to executeGenerate)</li> </ul>","title":"Creating Instance"},{"location":"commands/generate/DeltaGenerateCommand/#generate-mode-name","text":"","title":"Generate Mode Name <p><code>DeltaGenerateCommand</code> is given a mode name when created.</p> <p><code>DeltaGenerateCommand</code> uses a lookup table of the supported generation functions by mode name (yet supports just <code>symlink_format_manifest</code>).</p>    Mode Name Generation Function     <code>symlink_format_manifest</code> generateFullManifest"},{"location":"commands/generate/DeltaGenerateCommand/#executing-command","text":"","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code>\u00a0is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p> <p><code>run</code> creates a Hadoop <code>Path</code> to (the location of) the delta table (based on DeltaTableIdentifier).</p> <p><code>run</code> creates a DeltaLog for the delta table.</p> <p><code>run</code> executes the generation function for the mode name.</p> <p><code>run</code> returns no rows (an empty collection).</p>"},{"location":"commands/generate/DeltaGenerateCommand/#illegalargumentexception","text":"","title":"IllegalArgumentException <p><code>run</code> throws an <code>IllegalArgumentException</code> when executed with an unsupported mode name:</p> <pre><code>Specified mode '[modeName]' is not supported. Supported modes are: [supportedModes]\n</code></pre>"},{"location":"commands/generate/DeltaGenerateCommand/#analysisexception","text":"","title":"AnalysisException <p><code>run</code> throws an <code>AnalysisException</code> when executed for a non-delta table:</p> <pre><code>GENERATE is only supported for Delta tables.\n</code></pre>"},{"location":"commands/merge/","text":"<p>Delta Lake supports merging records into a delta table using the following high-level operators:</p> <ul> <li>MERGE INTO SQL command (Spark SQL)</li> <li>DeltaTable.merge</li> </ul>","title":"Merge Command"},{"location":"commands/merge/#demo","text":"<ul> <li>Demo: Merge Operation</li> </ul>","title":"Demo"},{"location":"commands/merge/DeltaMergeAction/","text":"<p><code>DeltaMergeAction</code> is...FIXME</p>","title":"DeltaMergeAction"},{"location":"commands/merge/DeltaMergeBuilder/","text":"<p><code>DeltaMergeBuilder</code> is a builder interface to describe how to merge data from a source DataFrame into the target delta table (using whenMatched and whenNotMatched conditions).</p> <p>In the end, <code>DeltaMergeBuilder</code> is supposed to be executed to take action. <code>DeltaMergeBuilder</code> creates a DeltaMergeInto logical command that is resolved to a MergeIntoCommand runnable logical command (using PreprocessTableMerge logical resolution rule).</p>","title":"DeltaMergeBuilder"},{"location":"commands/merge/DeltaMergeBuilder/#creating-instance","text":"<p><code>DeltaMergeBuilder</code> takes the following to be created:</p> <ul> <li> Target DeltaTable <li> Source <code>DataFrame</code> <li> Condition <code>Column</code> <li> When Clauses  <p><code>DeltaMergeBuilder</code> is created using DeltaTable.merge operator.</p>","title":"Creating Instance"},{"location":"commands/merge/DeltaMergeBuilder/#operators","text":"","title":"Operators"},{"location":"commands/merge/DeltaMergeBuilder/#whenmatched","text":"","title":"whenMatched <pre><code>whenMatched(): DeltaMergeMatchedActionBuilder\nwhenMatched(\n  condition: Column): DeltaMergeMatchedActionBuilder\nwhenMatched(\n  condition: String): DeltaMergeMatchedActionBuilder\n</code></pre> <p>Creates a DeltaMergeMatchedActionBuilder (for the <code>DeltaMergeBuilder</code> and a condition)</p>"},{"location":"commands/merge/DeltaMergeBuilder/#whennotmatched","text":"","title":"whenNotMatched <pre><code>whenNotMatched(): DeltaMergeNotMatchedActionBuilder\nwhenNotMatched(\n  condition: Column): DeltaMergeNotMatchedActionBuilder\nwhenNotMatched(\n  condition: String): DeltaMergeNotMatchedActionBuilder\n</code></pre> <p>Creates a DeltaMergeNotMatchedActionBuilder (for the <code>DeltaMergeBuilder</code> and a condition)</p>"},{"location":"commands/merge/DeltaMergeBuilder/#executing-merge","text":"","title":"Executing Merge <pre><code>execute(): Unit\n</code></pre> <p><code>execute</code> creates a merge plan (that is DeltaMergeInto logical command) and resolves column references.</p> <p><code>execute</code> runs PreprocessTableMerge logical resolution rule on the <code>DeltaMergeInto</code> logical command (that gives MergeIntoCommand runnable logical command).</p> <p>In the end, <code>execute</code> executes the MergeIntoCommand logical command.</p>"},{"location":"commands/merge/DeltaMergeBuilder/#creating-logical-plan-for-merge","text":"","title":"Creating Logical Plan for Merge <pre><code>mergePlan: DeltaMergeInto\n</code></pre> <p><code>mergePlan</code> creates a DeltaMergeInto logical command.</p> <p><code>mergePlan</code> is used when <code>DeltaMergeBuilder</code> is requested to execute.</p>"},{"location":"commands/merge/DeltaMergeBuilder/#creating-deltamergebuilder","text":"","title":"Creating DeltaMergeBuilder <pre><code>apply(\n  targetTable: DeltaTable,\n  source: DataFrame,\n  onCondition: Column): DeltaMergeBuilder\n</code></pre> <p><code>apply</code> utility creates a new <code>DeltaMergeBuilder</code> for the given parameters and no DeltaMergeIntoClauses.</p> <p><code>apply</code> is used for DeltaTable.merge operator.</p>"},{"location":"commands/merge/DeltaMergeBuilder/#adding-deltamergeintoclause","text":"","title":"Adding DeltaMergeIntoClause <pre><code>withClause(\n  clause: DeltaMergeIntoClause): DeltaMergeBuilder\n</code></pre> <p><code>withClause</code> creates a new <code>DeltaMergeBuilder</code> (based on the existing properties, e.g. the DeltaTable) with the given DeltaMergeIntoClause added to the existing DeltaMergeIntoClauses (to create a more refined <code>DeltaMergeBuilder</code>).</p> <p><code>withClause</code> is used when:</p> <ul> <li>DeltaMergeMatchedActionBuilder is requested to updateAll, delete and addUpdateClause</li> <li>DeltaMergeNotMatchedActionBuilder is requested to insertAll and addInsertClause</li> </ul>"},{"location":"commands/merge/DeltaMergeInto/","text":"<p><code>DeltaMergeInto</code> is a logical <code>Command</code> (Spark SQL).</p>","title":"DeltaMergeInto Logical Command"},{"location":"commands/merge/DeltaMergeInto/#creating-instance","text":"<p><code>DeltaMergeInto</code> takes the following to be created:</p> <ul> <li> Target Table (Spark SQL) <li> Source Table or Subquery (Spark SQL) <li> Condition Expression <li> DeltaMergeIntoMatchedClauses <li> Non-Matched DeltaMergeIntoInsertClauses <li>migrateSchema flag</li>  <p>When created, <code>DeltaMergeInto</code> verifies the actions in the matchedClauses and notMatchedClauses clauses.</p> <p><code>DeltaMergeInto</code> is created (using apply and resolveReferences utilities) when:</p> <ul> <li><code>DeltaMergeBuilder</code> is requested to execute</li> <li>DeltaAnalysis logical resolution rule is executed</li> </ul>","title":"Creating Instance"},{"location":"commands/merge/DeltaMergeInto/#preprocesstablemerge","text":"","title":"PreprocessTableMerge <p><code>DeltaMergeInto</code> is resolved to MergeIntoCommand by PreprocessTableMerge logical resolution rule.</p>"},{"location":"commands/merge/DeltaMergeInto/#migrateschema-flag","text":"","title":"migrateSchema Flag <p><code>DeltaMergeInto</code> is given <code>migrateSchema</code> flag when created:</p> <ul> <li>apply uses <code>false</code> always</li> <li>resolveReferences is <code>true</code> only with the spark.databricks.delta.schema.autoMerge.enabled configuration property enabled and <code>*</code>s only (in matched and not-matched clauses)</li> </ul> <p><code>migrateSchema</code> is used when:</p> <ul> <li>PreprocessTableMerge logical resolution rule is executed</li> </ul>"},{"location":"commands/merge/DeltaMergeInto/#supportssubquery","text":"","title":"SupportsSubquery <p><code>DeltaMergeInto</code> is a <code>SupportsSubquery</code> (Spark SQL)</p>"},{"location":"commands/merge/DeltaMergeInto/#creating-deltamergeinto","text":"","title":"Creating DeltaMergeInto <pre><code>apply(\n  target: LogicalPlan,\n  source: LogicalPlan,\n  condition: Expression,\n  whenClauses: Seq[DeltaMergeIntoClause]): DeltaMergeInto\n</code></pre> <p><code>apply</code> collects DeltaMergeIntoInsertClauses and DeltaMergeIntoMatchedClauses from the given <code>whenClauses</code> and creates a <code>DeltaMergeInto</code> command (with migrateSchema flag off).</p> <p><code>apply</code> throws an <code>AnalysisException</code> for the <code>whenClauses</code> empty:</p> <pre><code>There must be at least one WHEN clause in a MERGE statement\n</code></pre> <p><code>apply</code> throws an <code>AnalysisException</code> if there is a matched clause with no condition (except the last matched clause):</p> <pre><code>When there are more than one MATCHED clauses in a MERGE statement, only the last MATCHED clause can omit the condition.\n</code></pre> <p><code>apply</code> throws an <code>AnalysisException</code> if there is an insert clause with no condition (except the last matched clause):</p> <pre><code>When there are more than one NOT MATCHED clauses in a MERGE statement, only the last NOT MATCHED clause can omit the condition.\n</code></pre> <p><code>apply</code> is used when:</p> <ul> <li><code>DeltaMergeBuilder</code> is requested to execute (when mergePlan)</li> <li>DeltaAnalysis logical resolution rule is executed (and resolves <code>MergeIntoTable</code> logical command)</li> </ul>"},{"location":"commands/merge/DeltaMergeInto/#resolvereferences","text":"","title":"resolveReferences <pre><code>resolveReferences(\n  merge: DeltaMergeInto,\n  conf: SQLConf)(\n  resolveExpr: (Expression, LogicalPlan) =&gt; Expression): DeltaMergeInto\n</code></pre> <p><code>resolveReferences</code>...FIXME</p> <p><code>resolveReferences</code> is used when:</p> <ul> <li><code>DeltaMergeBuilder</code> is requested to execute</li> <li>DeltaAnalysis logical resolution rule is executed (and resolves <code>MergeIntoTable</code> logical command)</li> </ul>"},{"location":"commands/merge/DeltaMergeIntoClause/","text":"<p><code>DeltaMergeIntoClause</code>\u00a0is an extension of the <code>Expression</code> (Spark SQL) abstraction for WHEN clauses.</p>","title":"DeltaMergeIntoClause"},{"location":"commands/merge/DeltaMergeIntoClause/#contract","text":"","title":"Contract"},{"location":"commands/merge/DeltaMergeIntoClause/#actions","text":"","title":"Actions <pre><code>actions: Seq[Expression]\n</code></pre>"},{"location":"commands/merge/DeltaMergeIntoClause/#condition","text":"","title":"Condition <pre><code>condition: Option[Expression]\n</code></pre>"},{"location":"commands/merge/DeltaMergeIntoClause/#implementations","text":"<ul> <li>DeltaMergeIntoInsertClause</li> <li>DeltaMergeIntoMatchedClause</li> </ul>  Sealed Trait <p><code>DeltaMergeIntoClause</code> is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).</p>","title":"Implementations"},{"location":"commands/merge/DeltaMergeIntoClause/#verifing-actions","text":"","title":"Verifing Actions <pre><code>verifyActions(): Unit\n</code></pre> <p><code>verifyActions</code> goes over the actions and makes sure that they are either <code>UnresolvedStar</code>s (Spark SQL) or DeltaMergeActions.</p> <p>For unsupported actions, <code>verifyActions</code> throws an <code>IllegalArgumentException</code>:</p> <pre><code>Unexpected action expression [action] in [this]\n</code></pre> <p><code>verifyActions</code>\u00a0is used when:</p> <ul> <li><code>DeltaMergeInto</code> is created</li> </ul>"},{"location":"commands/merge/DeltaMergeIntoDeleteClause/","text":"<p><code>DeltaMergeIntoDeleteClause</code> is...FIXME</p>","title":"DeltaMergeIntoDeleteClause"},{"location":"commands/merge/DeltaMergeIntoInsertClause/","text":"<p><code>DeltaMergeIntoInsertClause</code> is a DeltaMergeIntoClause for the following:</p> <ul> <li><code>InsertAction</code> not-matched actions in <code>MergeIntoTable</code> (Spark SQL) logical command</li> <li>DeltaMergeNotMatchedActionBuilder.insertAll, DeltaMergeNotMatchedActionBuilder.insert and DeltaMergeNotMatchedActionBuilder.insertExpr operators</li> </ul>","title":"DeltaMergeIntoInsertClause"},{"location":"commands/merge/DeltaMergeIntoInsertClause/#creating-instance","text":"<p><code>DeltaMergeIntoInsertClause</code> takes the following to be created:</p> <ul> <li> (optional) Condition <code>Expression</code> (Spark SQL) <li> Action <code>Expression</code>s (Spark SQL)  <p><code>DeltaMergeIntoInsertClause</code> is created\u00a0when:</p> <ul> <li><code>DeltaMergeNotMatchedActionBuilder</code> is requested to insertAll and addInsertClause</li> <li>DeltaAnalysis logical resolution rule is executed (and resolves <code>MergeIntoTable</code> logical command with <code>InsertAction</code> not-matched actions)</li> </ul>","title":"Creating Instance"},{"location":"commands/merge/DeltaMergeIntoMatchedClause/","text":"<p><code>DeltaMergeIntoMatchedClause</code>\u00a0is an extension of the DeltaMergeIntoClause abstraction for WHEN MATCHED clauses.</p>","title":"DeltaMergeIntoMatchedClause"},{"location":"commands/merge/DeltaMergeIntoMatchedClause/#implementations","text":"<ul> <li>DeltaMergeIntoDeleteClause</li> <li>DeltaMergeIntoUpdateClause</li> </ul>  Sealed Trait <p><code>DeltaMergeIntoMatchedClause</code> is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).</p>","title":"Implementations"},{"location":"commands/merge/DeltaMergeIntoUpdateClause/","text":"<p><code>DeltaMergeIntoUpdateClause</code> is a DeltaMergeIntoMatchedClause for the following:</p> <ul> <li><code>UpdateAction</code> matched actions in <code>MergeIntoTable</code> (Spark SQL) logical command</li> <li>DeltaMergeMatchedActionBuilder.updateAll, DeltaMergeMatchedActionBuilder.update and DeltaMergeMatchedActionBuilder.updateExpr operators</li> </ul>","title":"DeltaMergeIntoUpdateClause"},{"location":"commands/merge/DeltaMergeIntoUpdateClause/#creating-instance","text":"<p><code>DeltaMergeIntoUpdateClause</code> takes the following to be created:</p> <ul> <li> (optional) Condition <code>Expression</code> (Spark SQL) <li> Action <code>Expression</code>s (Spark SQL)  <p><code>DeltaMergeIntoUpdateClause</code> is created\u00a0when:</p> <ul> <li><code>DeltaMergeMatchedActionBuilder</code> is requested to updateAll and addUpdateClause</li> <li>DeltaAnalysis logical resolution rule is executed (and resolves <code>MergeIntoTable</code> logical command with <code>UpdateAction</code> matched actions)</li> </ul>","title":"Creating Instance"},{"location":"commands/merge/DeltaMergeMatchedActionBuilder/","text":"<p><code>DeltaMergeMatchedActionBuilder</code> is a builder interface for DeltaMergeBuilder.whenMatched operator.</p>","title":"DeltaMergeMatchedActionBuilder"},{"location":"commands/merge/DeltaMergeMatchedActionBuilder/#creating-instance","text":"<p><code>DeltaMergeMatchedActionBuilder</code> takes the following to be created:</p> <ul> <li> DeltaMergeBuilder <li> Optional match condition  <p><code>DeltaMergeMatchedActionBuilder</code> is created when:</p> <ul> <li><code>DeltaMergeBuilder</code> is requested to whenMatched (using apply factory method)</li> </ul>","title":"Creating Instance"},{"location":"commands/merge/DeltaMergeMatchedActionBuilder/#operators","text":"","title":"Operators"},{"location":"commands/merge/DeltaMergeMatchedActionBuilder/#delete","text":"","title":"delete <pre><code>delete(): DeltaMergeBuilder\n</code></pre> <p>Adds a <code>DeltaMergeIntoDeleteClause</code> (with the matchCondition) to the DeltaMergeBuilder.</p>"},{"location":"commands/merge/DeltaMergeMatchedActionBuilder/#update","text":"","title":"update <pre><code>update(\n  set: Map[String, Column]): DeltaMergeBuilder\n</code></pre>"},{"location":"commands/merge/DeltaMergeMatchedActionBuilder/#updateall","text":"","title":"updateAll <pre><code>updateAll(): DeltaMergeBuilder\n</code></pre>"},{"location":"commands/merge/DeltaMergeMatchedActionBuilder/#updateexpr","text":"","title":"updateExpr <pre><code>updateExpr(\n  set: Map[String, String]): DeltaMergeBuilder\n</code></pre>"},{"location":"commands/merge/DeltaMergeMatchedActionBuilder/#creating-deltamergematchedactionbuilder","text":"","title":"Creating DeltaMergeMatchedActionBuilder <pre><code>apply(\n  mergeBuilder: DeltaMergeBuilder,\n  matchCondition: Option[Column]): DeltaMergeMatchedActionBuilder\n</code></pre> <p><code>apply</code> creates a <code>DeltaMergeMatchedActionBuilder</code> (for the given parameters).</p> <p><code>apply</code> is used when:</p> <ul> <li><code>DeltaMergeBuilder</code> is requested to whenMatched</li> </ul>"},{"location":"commands/merge/DeltaMergeMatchedActionBuilder/#addupdateclause","text":"","title":"addUpdateClause <pre><code>addUpdateClause(\n  set: Map[String, Column]): DeltaMergeBuilder\n</code></pre> <p><code>addUpdateClause</code>...FIXME</p> <p><code>addUpdateClause</code> is used when:</p> <ul> <li><code>DeltaMergeMatchedActionBuilder</code> is requested to update and updateExpr</li> </ul>"},{"location":"commands/merge/DeltaMergeNotMatchedActionBuilder/","text":"<p><code>DeltaMergeNotMatchedActionBuilder</code> is a builder interface for DeltaMergeBuilder.whenNotMatched operator.</p>","title":"DeltaMergeNotMatchedActionBuilder"},{"location":"commands/merge/DeltaMergeNotMatchedActionBuilder/#creating-instance","text":"<p><code>DeltaMergeNotMatchedActionBuilder</code> takes the following to be created:</p> <ul> <li> DeltaMergeBuilder <li> (optional) Not-Match Condition (Spark SQL)  <p><code>DeltaMergeNotMatchedActionBuilder</code> is created\u00a0when:</p> <ul> <li><code>DeltaMergeBuilder</code> is requested to whenNotMatched</li> </ul>","title":"Creating Instance"},{"location":"commands/merge/DeltaMergeNotMatchedActionBuilder/#operators","text":"","title":"Operators"},{"location":"commands/merge/DeltaMergeNotMatchedActionBuilder/#insert","text":"","title":"insert <pre><code>insert(\n  values: Map[String, Column]): DeltaMergeBuilder\n</code></pre> <p><code>insert</code> adds an insert clause (with the <code>values</code>).</p>"},{"location":"commands/merge/DeltaMergeNotMatchedActionBuilder/#insertall","text":"","title":"insertAll <pre><code>insertAll(): DeltaMergeBuilder\n</code></pre> <p><code>insertAll</code> requests the DeltaMergeBuilder to register a new DeltaMergeIntoInsertClause.</p>"},{"location":"commands/merge/DeltaMergeNotMatchedActionBuilder/#insertexpr","text":"","title":"insertExpr <pre><code>insertExpr(\n  values: Map[String, String]): DeltaMergeBuilder\n</code></pre> <p><code>insertExpr</code> adds an insert clause (with the <code>values</code>).</p>"},{"location":"commands/merge/DeltaMergeNotMatchedActionBuilder/#registering-new-deltamergeintoinsertclause","text":"","title":"Registering New DeltaMergeIntoInsertClause <pre><code>addInsertClause(\n  setValues: Map[String, Column]): DeltaMergeBuilder\n</code></pre> <p><code>addInsertClause</code> requests the DeltaMergeBuilder to register a new DeltaMergeIntoInsertClause (similarly to insertAll but with the given <code>setValues</code>).</p> <p><code>addInsertClause</code> is used when:</p> <ul> <li><code>DeltaMergeNotMatchedActionBuilder</code> is requested to insert and insertExpr</li> </ul>"},{"location":"commands/merge/JoinedRowProcessor/","text":"<p><code>JoinedRowProcessor</code> is...FIXME</p>","title":"JoinedRowProcessor"},{"location":"commands/merge/JoinedRowProcessor/#creating-instance","text":"<p><code>JoinedRowProcessor</code> takes the following to be created:</p> <ul> <li> <code>targetRowHasNoMatch</code> Expression <li> <code>sourceRowHasNoMatch</code> Expression <li> Optional <code>matchedCondition1</code> Expression <li> Optional <code>matchedOutput1</code> Expressions <li> Optional <code>matchedCondition2</code> Expression <li> Optional <code>matchedOutput2</code> Expressions <li> Optional <code>notMatchedCondition</code> Expression <li> Optional <code>notMatchedOutput</code> Expressions <li> Optional <code>noopCopyOutput</code> Expression <li> <code>deleteRowOutput</code> Expressions <li> <code>joinedAttributes</code> Attributes <li> <code>joinedRowEncoder</code> ExpressionEncoder <li> <code>outputRowEncoder</code> ExpressionEncoder  <p><code>JoinedRowProcessor</code> is created\u00a0when <code>MergeIntoCommand</code> is requested to writeAllChanges.</p>","title":"Creating Instance"},{"location":"commands/merge/JoinedRowProcessor/#processing-partition","text":"","title":"Processing Partition <pre><code>processPartition(\n  rowIterator: Iterator[Row]): Iterator[Row]\n</code></pre> <p><code>processPartition</code>...FIXME</p> <p><code>processPartition</code>\u00a0is used when <code>MergeIntoCommand</code> is requested to writeAllChanges.</p>"},{"location":"commands/merge/MergeIntoCommand/","text":"<p><code>MergeIntoCommand</code> is a DeltaCommand that represents a DeltaMergeInto logical command at execution.</p> <p><code>MergeIntoCommand</code> is a <code>RunnableCommand</code> (Spark SQL) logical operator.</p>  <p>Tip</p> <p>Learn more in Demo: Merge Operation.</p>","title":"MergeIntoCommand"},{"location":"commands/merge/MergeIntoCommand/#performance-metrics","text":"Name web UI     <code>numSourceRows</code> number of source rows   numTargetRowsCopied number of target rows rewritten unmodified   <code>numTargetRowsInserted</code> number of inserted rows   <code>numTargetRowsUpdated</code> number of updated rows   <code>numTargetRowsDeleted</code> number of deleted rows    <code>numTargetFilesBeforeSkipping</code> number of target files before skipping    <code>numTargetFilesAfterSkipping</code> number of target files after skipping    <code>numTargetFilesRemoved</code> number of files removed to target   <code>numTargetFilesAdded</code> number of files added to target","title":"Performance Metrics"},{"location":"commands/merge/MergeIntoCommand/#number-of-target-rows-rewritten-unmodified","text":"","title":"number of target rows rewritten unmodified <p><code>numTargetRowsCopied</code> performance metric (like the other metrics) is turned into a non-deterministic user-defined function (UDF).</p> <p><code>numTargetRowsCopied</code> becomes <code>incrNoopCountExpr</code> UDF.</p> <p><code>incrNoopCountExpr</code> UDF is resolved on a joined plan and used to create a JoinedRowProcessor for processing partitions of the joined plan <code>Dataset</code>.</p>"},{"location":"commands/merge/MergeIntoCommand/#creating-instance","text":"<p><code>MergeIntoCommand</code> takes the following to be created:</p> <ul> <li>Source Data</li> <li> Target Data (LogicalPlan) <li> TahoeFileIndex <li> Condition Expression <li> Matched Clauses (<code>Seq[DeltaMergeIntoMatchedClause]</code>) <li> Optional Non-Matched Clause (<code>Option[DeltaMergeIntoInsertClause]</code>) <li> Migrated Schema  <p><code>MergeIntoCommand</code> is created when:</p> <ul> <li>PreprocessTableMerge logical resolution rule is executed (on a DeltaMergeInto logical command)</li> </ul>","title":"Creating Instance"},{"location":"commands/merge/MergeIntoCommand/#source-data","text":"","title":"Source Data <p>When created, <code>MergeIntoCommand</code> is given a <code>LogicalPlan</code> (Spark SQL) for the source data to merge from (internally referred to as source).</p> <p>The source is used twice:</p> <ul> <li>Firstly, in one of the following:<ul> <li>An inner join (in findTouchedFiles) that is <code>count</code> in web UI</li> <li>A leftanti join (in writeInsertsOnlyWhenNoMatchedClauses)</li> </ul> </li> <li>Secondly, in right or full outer join (in writeAllChanges)</li> </ul>  <p>Tip</p> <p>Enable <code>DEBUG</code> logging level for org.apache.spark.sql.delta.commands.MergeIntoCommand logger to see the inner-workings of writeAllChanges.</p>"},{"location":"commands/merge/MergeIntoCommand/#target-deltalog","text":"","title":"Target DeltaLog <pre><code>targetDeltaLog: DeltaLog\n</code></pre> <p><code>targetDeltaLog</code> is the DeltaLog (of the TahoeFileIndex) of the target delta table.</p> <p><code>targetDeltaLog</code> is used for the following:</p> <ul> <li>Start a new transaction when executed</li> <li>To access the Data Path when finding files to rewrite</li> </ul>  Lazy Value <p><code>targetDeltaLog</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards.</p>"},{"location":"commands/merge/MergeIntoCommand/#executing-command","text":"","title":"Executing Command <pre><code>run(\n  spark: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p> <p><code>run</code> requests the target DeltaLog to start a new transaction.</p> <p>With spark.databricks.delta.schema.autoMerge.enabled configuration property enabled, <code>run</code> updates the metadata (of the transaction).</p> <p> <code>run</code> determines Delta actions (RemoveFiles and AddFiles).  <p>Describe <code>deltaActions</code> part</p>  <p>With spark.databricks.delta.history.metricsEnabled configuration property enabled, <code>run</code> requests the current transaction to register SQL metrics for the Delta operation.</p> <p><code>run</code> requests the current transaction to commit (with the Delta actions and <code>Merge</code> operation).</p> <p><code>run</code> records the Delta event.</p> <p><code>run</code> posts a <code>SparkListenerDriverAccumUpdates</code> Spark event (with the metrics).</p> <p>In the end, <code>run</code> requests the <code>CacheManager</code> to <code>recacheByPlan</code>.</p>"},{"location":"commands/merge/MergeIntoCommand/#finding-files-to-rewrite","text":"","title":"Finding Files to Rewrite <pre><code>findTouchedFiles(\n  spark: SparkSession,\n  deltaTxn: OptimisticTransaction): Seq[AddFile]\n</code></pre>  <p>Important</p> <p><code>findTouchedFiles</code> is such a fine piece of art (a gem). It uses a custom accumulator, a UDF (to use this accumulator to record touched file names) and <code>input_file_name()</code> standard function for the names of the files read.</p> <p>It is always worth keeping in mind that Delta Lake uses files for data storage and that is why <code>input_file_name()</code> standard function works. It would not work for non-file-based data sources.</p>   Example 1: Understanding the Internals of <code>findTouchedFiles</code> <p>The following query writes out a 10-element dataset using the default parquet data source to <code>/tmp/parquet</code> directory:</p> <pre><code>val target = \"/tmp/parquet\"\nspark.range(10).write.save(target)\n</code></pre> <p>The number of parquet part files varies based on the number of partitions (CPU cores).</p> <p>The following query loads the parquet dataset back alongside <code>input_file_name()</code> standard function to mimic <code>findTouchedFiles</code>'s behaviour.</p> <pre><code>val FILE_NAME_COL = \"_file_name_\"\nval dataFiles = spark.read.parquet(target).withColumn(FILE_NAME_COL, input_file_name())\n</code></pre> <pre><code>scala&gt; dataFiles.show(truncate = false)\n+---+---------------------------------------------------------------------------------------+\n|id |_file_name_                                                                            |\n+---+---------------------------------------------------------------------------------------+\n|4  |file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n|0  |file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n|3  |file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n|6  |file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n|1  |file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n|8  |file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n|2  |file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n|7  |file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n|5  |file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n|9  |file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n+---+---------------------------------------------------------------------------------------+\n</code></pre> <p>As you may have thought, not all part files have got data and so they are not included in the dataset. That is when <code>findTouchedFiles</code> uses <code>groupBy</code> operator and <code>count</code> action to calculate match frequency.</p> <pre><code>val counts = dataFiles.groupBy(FILE_NAME_COL).count()\nscala&gt; counts.show(truncate = false)\n+---------------------------------------------------------------------------------------+-----+\n|_file_name_                                                                            |count|\n+---------------------------------------------------------------------------------------+-----+\n|file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n|file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n|file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n|file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n|file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n|file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n|file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n|file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n|file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n|file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1    |\n+---------------------------------------------------------------------------------------+-----+\n</code></pre> <p>Let's load all the part files in the <code>/tmp/parquet</code> directory and find which file(s) have no data.</p> <pre><code>import scala.sys.process._\nval cmd = (s\"ls $target\" #| \"grep .parquet\").lineStream\nval allFiles = cmd.toArray.toSeq.toDF(FILE_NAME_COL)\n  .select(concat(lit(s\"file://$target/\"), col(FILE_NAME_COL)) as FILE_NAME_COL)\nval joinType = \"left_anti\" // MergeIntoCommand uses inner as it wants data file\nval noDataFiles = allFiles.join(dataFiles, Seq(FILE_NAME_COL), joinType)\n</code></pre> <p>Mind that the data vs non-data datasets could be different, but that should not \"interfere\" with the main reasoning flow.</p> <pre><code>scala&gt; noDataFiles.show(truncate = false)\n+---------------------------------------------------------------------------------------+\n|_file_name_                                                                            |\n+---------------------------------------------------------------------------------------+\n|file:///tmp/parquet/part-00000-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|\n+---------------------------------------------------------------------------------------+\n</code></pre>  <p><code>findTouchedFiles</code> registers an accumulator to collect all the distinct files that need to be rewritten (touched files).</p>  <p>Note</p> <p>The name of the accumulator is internal.metrics.MergeIntoDelta.touchedFiles and <code>internal.metrics</code> part is supposed to hide it from web UI as potentially large (set of file names to be rewritten).</p>  <p> <code>findTouchedFiles</code> defines a nondeterministic UDF that adds the file names to the accumulator (recordTouchedFileName). <p> <code>findTouchedFiles</code> splits conjunctive predicates (<code>And</code> binary expressions) in the condition expression and collects the predicates that use the target's columns (targetOnlyPredicates). <code>findTouchedFiles</code> requests the given OptimisticTransaction for the files that match the target-only predicates (and creates a <code>dataSkippedFiles</code> collection of AddFiles).  <p>Note</p> <p>This step looks similar to filter predicate pushdown.</p>  <p><code>findTouchedFiles</code> creates one <code>DataFrame</code> for the source data (using Spark SQL utility).</p> <p><code>findTouchedFiles</code> builds a logical query plan for the files (matching the predicates) and creates another <code>DataFrame</code> for the target data. <code>findTouchedFiles</code> adds two columns to the target dataframe:</p> <ol> <li><code>_row_id_</code> for <code>monotonically_increasing_id()</code> standard function</li> <li><code>_file_name_</code> for <code>input_file_name()</code> standard function</li> </ol> <p><code>findTouchedFiles</code> creates (a <code>DataFrame</code> that is) an INNER JOIN of the source and target <code>DataFrame</code>s using the condition expression.</p> <p><code>findTouchedFiles</code> takes the joined dataframe and selects <code>_row_id_</code> column and the recordTouchedFileName UDF on the <code>_file_name_</code> column as <code>one</code>. The <code>DataFrame</code> is internally known as <code>collectTouchedFiles</code>.</p> <p><code>findTouchedFiles</code> uses <code>groupBy</code> operator on <code>_row_id_</code> to calculate a sum of all the values in the <code>one</code> column (as <code>count</code> column) in the two-column <code>collectTouchedFiles</code> dataset. The <code>DataFrame</code> is internally known as <code>matchedRowCounts</code>.</p>  <p>Note</p> <p>No Spark job has been submitted yet. <code>findTouchedFiles</code> is still in \"query preparation\" mode.</p>  <p><code>findTouchedFiles</code> uses <code>filter</code> on the <code>count</code> column (in the <code>matchedRowCounts</code> dataset) with values greater than <code>1</code>. If there are any, <code>findTouchedFiles</code> throws an <code>UnsupportedOperationException</code> exception:</p> <pre><code>Cannot perform MERGE as multiple source rows matched and attempted to update the same\ntarget row in the Delta table. By SQL semantics of merge, when multiple source rows match\non the same target row, the update operation is ambiguous as it is unclear which source\nshould be used to update the matching target row.\nYou can preprocess the source table to eliminate the possibility of multiple matches.\n</code></pre>  <p>Note</p> <p>Since <code>findTouchedFiles</code> uses <code>count</code> action there should be a Spark SQL query reported (and possibly Spark jobs) in web UI.</p>  <p><code>findTouchedFiles</code> requests the <code>touchedFilesAccum</code> accumulator for the touched file names.</p>  Example 2: Understanding the Internals of <code>findTouchedFiles</code> <pre><code>val TOUCHED_FILES_ACCUM_NAME = \"MergeIntoDelta.touchedFiles\"\nval touchedFilesAccum = spark.sparkContext.collectionAccumulator[String](TOUCHED_FILES_ACCUM_NAME)\nval recordTouchedFileName = udf { (fileName: String) =&gt; {\n  touchedFilesAccum.add(fileName)\n  1\n}}.asNondeterministic()\n</code></pre> <pre><code>val target = \"/tmp/parquet\"\nspark.range(10).write.save(target)\n</code></pre> <pre><code>val FILE_NAME_COL = \"_file_name_\"\nval dataFiles = spark.read.parquet(target).withColumn(FILE_NAME_COL, input_file_name())\nval collectTouchedFiles = dataFiles.select(col(FILE_NAME_COL), recordTouchedFileName(col(FILE_NAME_COL)).as(\"one\"))\n</code></pre> <pre><code>scala&gt; collectTouchedFiles.show(truncate = false)\n+---------------------------------------------------------------------------------------+---+\n|_file_name_                                                                            |one|\n+---------------------------------------------------------------------------------------+---+\n|file:///tmp/parquet/part-00007-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n|file:///tmp/parquet/part-00001-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n|file:///tmp/parquet/part-00006-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n|file:///tmp/parquet/part-00011-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n|file:///tmp/parquet/part-00003-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n|file:///tmp/parquet/part-00014-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n|file:///tmp/parquet/part-00004-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n|file:///tmp/parquet/part-00012-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n|file:///tmp/parquet/part-00009-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n|file:///tmp/parquet/part-00015-76df546f-91f8-4cbb-8fcc-f51478e0db31-c000.snappy.parquet|1  |\n+---------------------------------------------------------------------------------------+---+    \n</code></pre> <pre><code>import scala.collection.JavaConverters._\nval touchedFileNames = touchedFilesAccum.value.asScala.toSeq\n</code></pre> <p>Use the Stages tab in web UI to review the accumulator values.</p>  <p><code>findTouchedFiles</code> prints out the following TRACE message to the logs:</p> <pre><code>findTouchedFiles: matched files:\n  [touchedFileNames]\n</code></pre> <p><code>findTouchedFiles</code> generateCandidateFileMap for the files that match the target-only predicates.</p> <p><code>findTouchedFiles</code> getTouchedFile for every touched file name.</p> <p><code>findTouchedFiles</code> updates the following performance metrics:</p> <ul> <li>numTargetFilesBeforeSkipping and adds the numOfFiles of the Snapshot of the given OptimisticTransaction</li> <li>numTargetFilesAfterSkipping and adds the number of the files that match the target-only predicates</li> <li>numTargetFilesRemoved and adds the number of the touched files</li> </ul> <p>In the end, <code>findTouchedFiles</code> gives the touched files (as AddFiles).</p>"},{"location":"commands/merge/MergeIntoCommand/#writing-all-changes","text":"","title":"Writing All Changes <pre><code>writeAllChanges(\n  spark: SparkSession,\n  deltaTxn: OptimisticTransaction,\n  filesToRewrite: Seq[AddFile]): Seq[AddFile]\n</code></pre> <p><code>writeAllChanges</code> builds the target output columns (possibly with some <code>null</code>s for the target columns that are not in the current schema).</p> <p> <code>writeAllChanges</code> builds a target logical query plan for the AddFiles. <p> <code>writeAllChanges</code> determines a join type to use (<code>rightOuter</code> or <code>fullOuter</code>). <p><code>writeAllChanges</code> prints out the following DEBUG message to the logs:</p> <pre><code>writeAllChanges using [joinType] join:\nsource.output: [outputSet]\ntarget.output: [outputSet]\ncondition: [condition]\nnewTarget.output: [outputSet]\n</code></pre> <p> <code>writeAllChanges</code> creates a <code>joinedDF</code> DataFrame that is a join of the DataFrames for the source and the new target logical plans with the given join condition and the join type. <p><code>writeAllChanges</code> creates a <code>JoinedRowProcessor</code> that is then used to map over partitions of the joined DataFrame.</p> <p><code>writeAllChanges</code> prints out the following DEBUG message to the logs:</p> <pre><code>writeAllChanges: join output plan:\n[outputDF.queryExecution]\n</code></pre> <p><code>writeAllChanges</code> requests the input OptimisticTransaction to writeFiles (possibly repartitioning by the partition columns if table is partitioned and spark.databricks.delta.merge.repartitionBeforeWrite.enabled configuration property is enabled).</p> <p><code>writeAllChanges</code> is used when <code>MergeIntoCommand</code> is requested to run.</p>"},{"location":"commands/merge/MergeIntoCommand/#building-target-logical-query-plan-for-addfiles","text":"","title":"Building Target Logical Query Plan for AddFiles <pre><code>buildTargetPlanWithFiles(\n  deltaTxn: OptimisticTransaction,\n  files: Seq[AddFile]): LogicalPlan\n</code></pre> <p><code>buildTargetPlanWithFiles</code> creates a DataFrame to represent the given AddFiles to access the analyzed logical query plan. <code>buildTargetPlanWithFiles</code> requests the given OptimisticTransaction for the DeltaLog to create a DataFrame (for the Snapshot and the given AddFiles).</p> <p>In the end, <code>buildTargetPlanWithFiles</code> creates a <code>Project</code> logical operator with <code>Alias</code> expressions so the output columns of the analyzed logical query plan (of the <code>DataFrame</code> of the <code>AddFiles</code>) reference the target's output columns (by name).</p>  <p>Note</p> <p>The output columns of the target delta table are associated with a OptimisticTransaction as the Metadata.</p> <pre><code>deltaTxn.metadata.schema\n</code></pre>"},{"location":"commands/merge/MergeIntoCommand/#writeinsertsonlywhennomatchedclauses","text":"","title":"writeInsertsOnlyWhenNoMatchedClauses <pre><code>writeInsertsOnlyWhenNoMatchedClauses(\n  spark: SparkSession,\n  deltaTxn: OptimisticTransaction): Seq[AddFile]\n</code></pre> <p><code>writeInsertsOnlyWhenNoMatchedClauses</code>...FIXME</p>"},{"location":"commands/merge/MergeIntoCommand/#exceptions","text":"","title":"Exceptions <p><code>run</code> throws an <code>AnalysisException</code> when the target schema is different than the delta table's (has changed after analysis phase):</p> <pre><code>The schema of your Delta table has changed in an incompatible way since your DataFrame or DeltaTable object was created. Please redefine your DataFrame or DeltaTable object. Changes:\n[schemaDiff]\nThis check can be turned off by setting the session configuration key spark.databricks.delta.checkLatestSchemaOnRead to false.\n</code></pre>"},{"location":"commands/merge/MergeIntoCommand/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.commands.MergeIntoCommand</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.delta.commands.MergeIntoCommand=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"commands/optimize/","text":"<p><code>OPTIMIZE</code> command compacts files together (that are smaller than <code>spark.databricks.delta.optimize.minFileSize</code> to files of <code>spark.databricks.delta.optimize.maxFileSize</code> size).</p> <p><code>OPTIMIZE</code> command uses <code>spark.databricks.delta.optimize.maxThreads</code> threads for compaction.</p> <p><code>OPTIMIZE</code> command can be executed using OPTIMIZE SQL command.</p>","title":"Optimize Command"},{"location":"commands/optimize/#demo","text":"<pre><code>// val q = sql(\"OPTIMIZE delta.`/tmp/delta/t1`\")\nval q = sql(\"OPTIMIZE delta_01\")\n</code></pre> <pre><code>scala&gt; q.show(truncate = false)\n+--------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n|path                                                    |metrics                                                                                           |\n+--------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n|file:/Users/jacek/dev/oss/spark/spark-warehouse/delta_01|{1, 9, {778, 778, 778.0, 1, 778}, {296, 692, 552.8888888888889, 9, 4976}, 1, null, 1, 9, 0, false}|\n+--------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n</code></pre>","title":"Demo"},{"location":"commands/optimize/OptimizeExecutor/","text":"<p><code>OptimizeExecutor</code> is a DeltaCommand with a SQLMetricsReporting.</p>","title":"OptimizeExecutor"},{"location":"commands/optimize/OptimizeExecutor/#creating-instance","text":"<p><code>OptimizeExecutor</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> DeltaLog (of the Delta table to be optimized) <li> Partition predicate expressions (Spark SQL)  <p><code>OptimizeExecutor</code> is created when:</p> <ul> <li><code>OptimizeTableCommand</code> is requested to run</li> </ul>","title":"Creating Instance"},{"location":"commands/optimize/OptimizeExecutor/#optimize","text":"","title":"optimize <pre><code>optimize(): Seq[Row]\n</code></pre> <p><code>optimize</code> reads the following configuration properties:</p> <ul> <li>spark.databricks.delta.optimize.minFileSize for the threshold of the size of files to be grouped together and rewritten as larger ones</li> <li>spark.databricks.delta.optimize.maxFileSize for the maximum desired file size (in bytes) of the compaction output files</li> </ul> <p><code>optimize</code> requests the DeltaLog to startTransaction.</p> <p><code>optimize</code> requests the <code>OptimisticTransaction</code> for the files matching the partition predicates.</p> <p><code>optimize</code> finds the files of the size below the spark.databricks.delta.optimize.minFileSize threshold (that are the files considered for compacting) and groups them by partition values.</p> <p><code>optimize</code> group the files into bins (of the <code>spark.databricks.delta.optimize.maxFileSize</code> size).</p>  <p>Note</p> <p>A bin is a group of files, whose total size does not exceed the desired size. They will be coalesced into a single output file.</p>  <p><code>optimize</code> creates a <code>ForkJoinPool</code> with spark.databricks.delta.optimize.maxThreads threads (with the <code>OptimizeJob</code> thread prefix). The task pool is then used to parallelize the submission of runCompactBinJob optimization jobs to Spark.</p> <p>Once the compaction jobs are done, <code>optimize</code> tries to commit the transaction (the given actions to the log) when there were any AddFiles.</p> <p>In the end, <code>optimize</code> returns a <code>Row</code> with the data path (of the Delta table) and the optimize statistics.</p> <p><code>optimize</code> is used when:</p> <ul> <li><code>OptimizeTableCommand</code> is requested to run</li> </ul>"},{"location":"commands/optimize/OptimizeTableCommand/","text":"<p><code>OptimizeTableCommand</code> is a <code>LeafRunnableCommand</code> (Spark SQL).</p>","title":"OptimizeTableCommand"},{"location":"commands/optimize/OptimizeTableCommand/#creating-instance","text":"<p><code>OptimizeTableCommand</code> takes the following to be created:</p> <ul> <li> Table Path <li> <code>TableIdentifier</code> <li> Optional partition predicate  <p><code>OptimizeTableCommand</code> is created when:</p> <ul> <li><code>DeltaSqlAstBuilder</code> is requested to parse OPTIMIZE SQL statement</li> </ul>","title":"Creating Instance"},{"location":"commands/optimize/OptimizeTableCommand/#executing-command","text":"","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> gets the DeltaLog of the Delta table (by the path or tableId).</p> <p>In the end, <code>run</code> creates an OptimizeExecutor that is in turn requested to optimize.</p>  <p><code>run</code> is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p>"},{"location":"commands/update/","text":"<p>Delta Lake supports updating (records in) delta tables using the following high-level operators:</p> <ul> <li>UPDATE TABLE SQL command</li> <li>DeltaTable.update or DeltaTable.updateExpr</li> </ul>","title":"Update Command"},{"location":"commands/update/DeltaUpdateTable/","text":"<p><code>DeltaUpdateTable</code> is an unary logical operator (Spark SQL) that represents <code>UpdateTable</code> (Spark SQL) at execution.</p>","title":"DeltaUpdateTable Unary Logical Operator"},{"location":"commands/update/DeltaUpdateTable/#creating-instance","text":"<p><code>DeltaUpdateTable</code> takes the following to be created:</p> <ul> <li> Child <code>LogicalPlan</code> (Spark SQL) <li> Update Column Expressions (Spark SQL) <li> Update Expressions (Spark SQL) <li> (optional) Condition Expression (Spark SQL)  <p><code>DeltaUpdateTable</code> is created\u00a0when:</p> <ul> <li>DeltaAnalysis logical resolution rule is executed (on an <code>UpdateTable</code>)</li> </ul>","title":"Creating Instance"},{"location":"commands/update/DeltaUpdateTable/#logical-resolution","text":"<p><code>DeltaUpdateTable</code> is resolved to a UpdateCommand when PreprocessTableUpdate post-hoc logical resolution rule is executed.</p>","title":"Logical Resolution"},{"location":"commands/update/UpdateCommand/","text":"<p><code>UpdateCommand</code> is a DeltaCommand that represents DeltaUpdateTable logical command at execution.</p> <p><code>UpdateCommand</code> is a <code>RunnableCommand</code> (Spark SQL) logical operator.</p>","title":"UpdateCommand"},{"location":"commands/update/UpdateCommand/#creating-instance","text":"<p><code>UpdateCommand</code> takes the following to be created:</p> <ul> <li> TahoeFileIndex <li> Target Data (LogicalPlan) <li> Update Expressions (Spark SQL) <li> (optional) Condition Expression (Spark SQL)  <p><code>UpdateCommand</code> is created when:</p> <ul> <li>PreprocessTableUpdate logical resolution rule is executed (and resolves a DeltaUpdateTable logical command)</li> </ul>","title":"Creating Instance"},{"location":"commands/update/UpdateCommand/#performance-metrics","text":"Name web UI     <code>numAddedFiles</code> number of files added.   <code>numRemovedFiles</code> number of files removed.   <code>numUpdatedRows</code> number of rows updated.   <code>executionTimeMs</code> time taken to execute the entire operation   <code>scanTimeMs</code> time taken to scan the files for matches   <code>rewriteTimeMs</code> time taken to rewrite the matched files","title":"Performance Metrics"},{"location":"commands/update/UpdateCommand/#executing-command","text":"","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p> <p><code>run</code>...FIXME</p>"},{"location":"commands/update/UpdateCommand/#performupdate","text":"","title":"performUpdate <pre><code>performUpdate(\n  sparkSession: SparkSession,\n  deltaLog: DeltaLog,\n  txn: OptimisticTransaction): Unit\n</code></pre> <p><code>performUpdate</code>...FIXME</p>"},{"location":"commands/update/UpdateCommand/#rewritefiles","text":"","title":"rewriteFiles <pre><code>rewriteFiles(\n  spark: SparkSession,\n  txn: OptimisticTransaction,\n  rootPath: Path,\n  inputLeafFiles: Seq[String],\n  nameToAddFileMap: Map[String, AddFile],\n  condition: Expression): Seq[FileAction]\n</code></pre> <p><code>rewriteFiles</code>...FIXME</p>"},{"location":"commands/update/UpdateCommand/#buildupdatedcolumns","text":"","title":"buildUpdatedColumns <pre><code>buildUpdatedColumns(\n  condition: Expression): Seq[Column]\n</code></pre> <p><code>buildUpdatedColumns</code>...FIXME</p>"},{"location":"commands/vacuum/","text":"<p>Vacuum command allows for garbage collection of a delta table.</p> <p>Vacuum command can be executed as VACUUM SQL command or DeltaTable.vacuum operator.</p>","title":"Vacuum Command"},{"location":"commands/vacuum/#demo","text":"","title":"Demo"},{"location":"commands/vacuum/#vacuum-sql-command","text":"<pre><code>val q = sql(\"VACUUM delta.`/tmp/delta/t1`\")\n</code></pre> <pre><code>scala&gt; q.show\nDeleted 0 files and directories in a total of 2 directories.\n+------------------+\n|              path|\n+------------------+\n|file:/tmp/delta/t1|\n+------------------+\n</code></pre>","title":"VACUUM SQL Command"},{"location":"commands/vacuum/#deltatablevacuum","text":"<pre><code>import io.delta.tables.DeltaTable\nDeltaTable.forPath(\"/tmp/delta/t1\").vacuum\n</code></pre>","title":"DeltaTable.vacuum"},{"location":"commands/vacuum/#dry-run","text":"<p>Visit Demo: Vacuum.</p>","title":"Dry Run"},{"location":"commands/vacuum/VacuumCommand/","text":"<p><code>VacuumCommand</code> is a concrete VacuumCommandImpl for garbage collection of a delta table for the following:</p> <ul> <li> <p>DeltaTable.vacuum operator (as <code>DeltaTableOperations</code> is requested to execute vacuum command)</p> </li> <li> <p>VACUUM SQL command (as VacuumTableCommand is executed)</p> </li> </ul>","title":"VacuumCommand Utility"},{"location":"commands/vacuum/VacuumCommand/#garbage-collection-of-delta-table","text":"","title":"Garbage Collection of Delta Table <pre><code>gc(\n  spark: SparkSession,\n  deltaLog: DeltaLog,\n  dryRun: Boolean = true,\n  retentionHours: Option[Double] = None,\n  clock: Clock = new SystemClock): DataFrame\n</code></pre> <p><code>gc</code> requests the given <code>DeltaLog</code> to update (and hence give the latest Snapshot of the delta table).</p>"},{"location":"commands/vacuum/VacuumCommand/#retentionmillis","text":"","title":"retentionMillis <p><code>gc</code> converts the retention hours to milliseconds and checkRetentionPeriodSafety (with deletedFileRetentionDuration table configuration).</p>"},{"location":"commands/vacuum/VacuumCommand/#timestamp-to-delete-files-before","text":"","title":"Timestamp to Delete Files Before <p><code>gc</code> determines the timestamp to delete files before based on the retentionMillis (if defined) or defaults to minFileRetentionTimestamp table configuration.</p> <p><code>gc</code> prints out the following INFO message to the logs (with the path of the given DeltaLog):</p> <pre><code>Starting garbage collection (dryRun = [dryRun]) of untracked files older than [deleteBeforeTimestamp] in [path]\n</code></pre>"},{"location":"commands/vacuum/VacuumCommand/#valid-files","text":"","title":"Valid Files <p><code>gc</code> requests the <code>Snapshot</code> for the state dataset and maps over partitions (<code>Dataset.mapPartitions</code>) with a map function that does the following (for every Action in a partition of SingleActions):</p> <ol> <li>Skips RemoveFiles with the deletion timestamp after the timestamp to delete files before</li> <li>Adds the path of FileActions (that live inside the directory of the table) with all subdirectories</li> <li>Skips other actions</li> </ol> <p><code>gc</code> converts the mapped state dataset into a <code>DataFrame</code> with a single <code>path</code> column.</p>  <p>Note</p> <p>There is no DataFrame action executed so no processing yet (using Spark).</p>"},{"location":"commands/vacuum/VacuumCommand/#all-files-and-directories-dataset","text":"","title":"All Files and Directories Dataset <p><code>gc</code> finds all the files and directories (recursively) in the data path (with <code>spark.sql.sources.parallelPartitionDiscovery.parallelism</code> number of file listing tasks).</p>"},{"location":"commands/vacuum/VacuumCommand/#caching-all-files-and-directories-dataset","text":"","title":"Caching All Files and Directories Dataset <p><code>gc</code> caches the allFilesAndDirs dataset.</p>"},{"location":"commands/vacuum/VacuumCommand/#number-of-directories","text":"","title":"Number of Directories <p><code>gc</code> counts the number of directories (as the count of the rows with <code>isDir</code> column being <code>true</code> in the allFilesAndDirs dataset).</p>  <p>Note</p> <p>This step submits a Spark job for <code>Dataset.count</code>.</p>"},{"location":"commands/vacuum/VacuumCommand/#paths-dataset","text":"","title":"Paths Dataset <p><code>gc</code> creates a Spark SQL query to count <code>path</code>s of the allFilesAndDirs with files with the <code>modificationTime</code> ealier than the deleteBeforeTimestamp and the directories (<code>isDir</code>s). That creates a <code>DataFrame</code> of <code>path</code> and <code>count</code> columns.</p> <p><code>gc</code> uses left-anti join of the counted path <code>DataFrame</code> with the validFiles on <code>path</code>.</p> <p><code>gc</code> filters out paths with <code>count</code> more than <code>1</code> and selects <code>path</code>.</p>"},{"location":"commands/vacuum/VacuumCommand/#dry-run","text":"","title":"Dry Run <p><code>gc</code> counts the rows in the paths Dataset for the number of files and directories that are safe to delete (numFiles).</p>  <p>Note</p> <p>This step submits a Spark job for <code>Dataset.count</code>.</p>  <p><code>gc</code> prints out the following message to the console (with the dirCounts):</p> <pre><code>Found [numFiles] files and directories in a total of [dirCounts] directories that are safe to delete.\n</code></pre> <p>In the end, <code>gc</code> converts the paths to Hadoop DFS format and creates a <code>DataFrame</code> with a single <code>path</code> column.</p>"},{"location":"commands/vacuum/VacuumCommand/#deleting-files-and-directories","text":"","title":"Deleting Files and Directories <p><code>gc</code> prints out the following INFO message to the logs:</p> <pre><code>Deleting untracked files and empty directories in [path]\n</code></pre> <p><code>gc</code> deletes the untracked files and empty directories (with parallel delete enabled flag based on spark.databricks.delta.vacuum.parallelDelete.enabled configuration property).</p> <p><code>gc</code> prints out the following message to standard output (with the dirCounts):</p> <pre><code>Deleted [filesDeleted] files and directories in a total of [dirCounts] directories.\n</code></pre> <p>In the end, <code>gc</code> creates a <code>DataFrame</code> with a single <code>path</code> column with just the data path of the delta table to vacuum.</p>"},{"location":"commands/vacuum/VacuumCommand/#unpersist-all-files-and-directories-dataset","text":"","title":"Unpersist All Files and Directories Dataset <p><code>gc</code> unpersists the allFilesAndDirs dataset.</p>"},{"location":"commands/vacuum/VacuumCommand/#checkretentionperiodsafety","text":"","title":"checkRetentionPeriodSafety <pre><code>checkRetentionPeriodSafety(\n  spark: SparkSession,\n  retentionMs: Option[Long],\n  configuredRetention: Long): Unit\n</code></pre> <p><code>checkRetentionPeriodSafety</code>...FIXME</p>"},{"location":"commands/vacuum/VacuumCommand/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.commands.VacuumCommand</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.delta.commands.VacuumCommand=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"commands/vacuum/VacuumCommandImpl/","text":"<p><code>VacuumCommandImpl</code>\u00a0is a DeltaCommand.</p>  <p>Note</p> <p><code>VacuumCommandImpl</code>\u00a0is a Scala trait just to let Databricks provide a commercial version of vacuum command.</p>","title":"VacuumCommandImpl"},{"location":"commands/vacuum/VacuumCommandImpl/#delete","text":"","title":"delete <pre><code>delete(\n  diff: Dataset[String],\n  spark: SparkSession,\n  basePath: String,\n  hadoopConf: Broadcast[SerializableConfiguration],\n  parallel: Boolean): Long\n</code></pre> <p><code>delete</code>...FIXME</p> <p><code>delete</code>\u00a0is used when:</p> <ul> <li><code>VacuumCommand</code> is requested to gc</li> </ul>"},{"location":"commands/vacuum/VacuumTableCommand/","text":"<p><code>VacuumTableCommand</code> is a runnable command (Spark SQL) for VACUUM SQL command.</p>","title":"VacuumTableCommand"},{"location":"commands/vacuum/VacuumTableCommand/#creating-instance","text":"<p><code>VacuumTableCommand</code> takes the following to be created:</p> <ul> <li> Path <li> <code>TableIdentifier</code> <li> Optional Horizon Hours <li> <code>dryRun</code> flag  <p><code>VacuumTableCommand</code> requires that either the table or the path is defined and it is the root directory of a delta table. Partition directories are not supported.</p> <p><code>VacuumTableCommand</code> is created\u00a0when:</p> <ul> <li><code>DeltaSqlAstBuilder</code> is requested to parse VACUUM SQL command</li> </ul>","title":"Creating Instance"},{"location":"commands/vacuum/VacuumTableCommand/#executing-command","text":"","title":"Executing Command <pre><code>run(\n  sparkSession: SparkSession): Seq[Row]\n</code></pre> <p><code>run</code> is part of the <code>RunnableCommand</code> (Spark SQL) abstraction.</p> <p><code>run</code> takes the path to vacuum (either the table or the path) and finds the root directory of the delta table.</p> <p><code>run</code> creates a DeltaLog instance for the delta table and gc it (passing in the <code>DeltaLog</code> instance, the dryRun and the horizonHours options).</p> <p><code>run</code> throws an <code>AnalysisException</code> when executed for a non-root directory of a delta table:</p> <pre><code>Please provide the base path ([baseDeltaPath]) when Vacuuming Delta tables. Vacuuming specific partitions is currently not supported.\n</code></pre> <p><code>run</code> throws an <code>AnalysisException</code> when executed for a <code>DeltaLog</code> with the snapshot version being <code>-1</code>:</p> <pre><code>[deltaTableIdentifier] is not a Delta table. VACUUM is only supported for Delta tables.\n</code></pre>"},{"location":"commands/vacuum/VacuumTableCommand/#output-schema","text":"","title":"Output Schema <p>The output schema of <code>VacuumTableCommand</code> is a single <code>path</code> column (of type <code>StringType</code>).</p>"},{"location":"constraints/","text":"<p>Delta Lake allows for table constraints using the following SQL statements:</p> <ul> <li>ALTER TABLE ADD CONSTRAINT</li> <li>ALTER TABLE DROP CONSTRAINT</li> </ul> <p>Table constraints can be one of the following:</p> <ul> <li>Column-Level Invariants</li> <li>Table-Level Check Constraints</li> <li>Generated Columns Constraints</li> </ul> <p>Column-level invariants require Protocol to be at least <code>2</code> for the writer version.</p> <p>DeltaCatalog is used to add or remove constraints of a delta table (using AddConstraint and DropConstraint).</p>","title":"Table Constraints"},{"location":"constraints/#references","text":"<ul> <li>Constraints</li> </ul>","title":"References"},{"location":"constraints/AddConstraint/","text":"<p><code>AddConstraint</code> is a <code>TableChange</code> (Spark SQL).</p>","title":"AddConstraint"},{"location":"constraints/AddConstraint/#creating-instance","text":"<p><code>AddConstraint</code> takes the following to be created:</p> <ul> <li> Constraint Name <li> Expression  <p><code>AddConstraint</code> is created\u00a0when:</p> <ul> <li><code>DeltaAnalysis</code> logical resolution rule is executed (on a logical query plan with AlterTableAddConstraintStatement)</li> </ul>","title":"Creating Instance"},{"location":"constraints/AddConstraint/#query-execution","text":"<p><code>AddConstraint</code> is resolved to AlterTableAddConstraintDeltaCommand and immediately executed when <code>DeltaCatalog</code> is requested to alter a delta table.</p>","title":"Query Execution"},{"location":"constraints/AlterTableAddConstraintStatement/","text":"<p><code>AlterTableAddConstraintStatement</code> is a <code>ParsedStatement</code> (Spark SQL) for ALTER TABLE ADD CONSTRAINT SQL statement.</p>","title":"AlterTableAddConstraintStatement"},{"location":"constraints/AlterTableAddConstraintStatement/#creating-instance","text":"<p><code>AlterTableAddConstraintStatement</code> takes the following to be created:</p> <ul> <li> Table Name <li> Constraint Name <li> Expression","title":"Creating Instance"},{"location":"constraints/AlterTableAddConstraintStatement/#analysis-phase","text":"<p><code>AlterTableAddConstraintStatement</code> is resolved by DeltaAnalysis logical resolution rule.</p>","title":"Analysis Phase"},{"location":"constraints/AlterTableDropConstraintStatement/","text":"<p><code>AlterTableDropConstraintStatement</code> is...FIXME</p>","title":"AlterTableDropConstraintStatement"},{"location":"constraints/CheckDeltaInvariant/","text":"<p><code>CheckDeltaInvariant</code> is a <code>UnaryExpression</code> (Spark SQL) for DeltaInvariantCheckerExec physical operator.</p>","title":"CheckDeltaInvariant"},{"location":"constraints/CheckDeltaInvariant/#creating-instance","text":"<p><code>CheckDeltaInvariant</code> takes the following to be created:</p> <ul> <li> Child Expression (Spark SQL) <li> Column Extractors (<code>Map[String, Expression]</code>) <li> Constraint  <p><code>CheckDeltaInvariant</code> is created using withBoundReferences and\u00a0when:</p> <ul> <li><code>DeltaInvariantCheckerExec</code> physical operator is executed</li> </ul>","title":"Creating Instance"},{"location":"constraints/CheckDeltaInvariant/#evaluating-expression","text":"","title":"Evaluating Expression <pre><code>eval(\n  input: InternalRow): Any\n</code></pre> <p><code>eval</code>\u00a0is part of the <code>Expression</code> (Spark SQL) abstraction.</p> <p><code>eval</code> asserts the constraint on the input <code>InternalRow</code>.</p>"},{"location":"constraints/CheckDeltaInvariant/#asserts-constraint","text":"","title":"Asserts Constraint <pre><code>assertRule(\n  input: InternalRow): Unit\n</code></pre> <p><code>assertRule</code>...FIXME</p>"},{"location":"constraints/CheckDeltaInvariant/#creating-checkdeltainvariant-with-boundreferences","text":"","title":"Creating CheckDeltaInvariant with BoundReferences <pre><code>withBoundReferences(\n  input: AttributeSeq): CheckDeltaInvariant\n</code></pre> <p><code>withBoundReferences</code>...FIXME</p> <p><code>withBoundReferences</code>\u00a0is used when:</p> <ul> <li><code>DeltaInvariantCheckerExec</code> physical operator is executed</li> </ul>"},{"location":"constraints/Constraint/","text":"<p><code>Constraint</code> is an abstraction of table constraints that writers have to assert before writing.</p>","title":"Constraint"},{"location":"constraints/Constraint/#contract","text":"","title":"Contract"},{"location":"constraints/Constraint/#name","text":"","title":"Name <pre><code>name: String\n</code></pre> <p>Used when:</p> <ul> <li><code>InvariantViolationException</code> utility is used to create an InvariantViolationException for a check constraint</li> </ul>"},{"location":"constraints/Constraint/#implementations","text":"Sealed Trait <p><code>Constraint</code> is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).</p>","title":"Implementations"},{"location":"constraints/Constraint/#check","text":"","title":"Check <p>A constraint with a SQL expression (Spark SQL) to check for when writing out data</p>"},{"location":"constraints/Constraint/#notnull","text":"","title":"NotNull <p>A constraint on a column to be not null</p> <p>Name: <code>NOT NULL</code></p>"},{"location":"constraints/Constraints/","text":"","title":"Constraints Utility"},{"location":"constraints/Constraints/#extracting-constraints-from-table-metadata","text":"","title":"Extracting Constraints from Table Metadata <pre><code>getAll(\n  metadata: Metadata,\n  spark: SparkSession): Seq[Constraint]\n</code></pre> <p><code>getAll</code> extracts Constraints from the given metadata and the associated schema.</p> <p><code>getAll</code>\u00a0is used when:</p> <ul> <li><code>TransactionalWrite</code> is requested to write data out</li> </ul>"},{"location":"constraints/Constraints/#extracting-check-constraints-from-table-metadata","text":"","title":"Extracting Check Constraints from Table Metadata <pre><code>getCheckConstraints(\n  metadata: Metadata,\n  spark: SparkSession): Seq[Constraint]\n</code></pre> <p><code>getCheckConstraints</code> extracts Check constraints from the <code>delta.constraints.</code>-keyed entries in the configuration of the given Metadata:</p> <ul> <li>The name is the key without the <code>delta.constraints.</code> prefix</li> <li>The expression is the value parsed</li> </ul> <p><code>getCheckConstraints</code>\u00a0is used when:</p> <ul> <li><code>Protocol</code> utility is used to requiredMinimumProtocol</li> <li><code>Constraints</code> utility is used to getAll</li> </ul>"},{"location":"constraints/DeltaInvariantChecker/","text":"<p><code>DeltaInvariantChecker</code> is a <code>UnaryNode</code> (Spark SQL).</p>","title":"DeltaInvariantChecker Unary Logical Operator"},{"location":"constraints/DeltaInvariantChecker/#creating-instance","text":"<p><code>DeltaInvariantChecker</code> takes the following to be created:</p> <ul> <li> Child <code>LogicalPlan</code> (Spark SQL) <li> Constraints   <p>Note</p> <p><code>DeltaInvariantChecker</code> does not seem to be created\u00a0at all.</p>","title":"Creating Instance"},{"location":"constraints/DeltaInvariantChecker/#execution-planning","text":"<p><code>DeltaInvariantChecker</code> is planned as DeltaInvariantCheckerExec unary physical operator by DeltaInvariantCheckerStrategy execution planning strategy.</p>","title":"Execution Planning"},{"location":"constraints/DeltaInvariantCheckerExec/","text":"<p><code>DeltaInvariantCheckerExec</code> is an <code>UnaryExecNode</code> (Spark SQL) to assert constraints.</p>","title":"DeltaInvariantCheckerExec Unary Physical Operator"},{"location":"constraints/DeltaInvariantCheckerExec/#creating-instance","text":"<p><code>DeltaInvariantCheckerExec</code> takes the following to be created:</p> <ul> <li> Child <code>SparkPlan</code> (Spark SQL) <li> Constraints  <p><code>DeltaInvariantCheckerExec</code> is created\u00a0when:</p> <ul> <li><code>TransactionalWrite</code> is requested to write data out</li> <li>DeltaInvariantCheckerStrategy execution planning strategy is executed</li> </ul>","title":"Creating Instance"},{"location":"constraints/DeltaInvariantCheckerExec/#executing-physical-operator","text":"","title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code>\u00a0is part of the <code>SparkPlan</code> (Spark SQL) abstraction.</p> <p><code>doExecute</code> builds invariants for the given constraints and applies (evaluates) them to every row from the child physical operator.</p> <p><code>doExecute</code> simply requests the child physical operator to execute (and becomes a noop) for no constraints.</p>"},{"location":"constraints/DeltaInvariantCheckerExec/#building-invariants","text":"","title":"Building Invariants <pre><code>buildInvariantChecks(\n  output: Seq[Attribute],\n  constraints: Seq[Constraint],\n  spark: SparkSession): Seq[CheckDeltaInvariant]\n</code></pre> <p><code>buildInvariantChecks</code> converts the given Constraints into CheckDeltaInvariants.</p>"},{"location":"constraints/DeltaInvariantCheckerStrategy/","text":"<p><code>DeltaInvariantCheckerStrategy</code> is a <code>SparkStrategy</code> (Spark SQL) to plan a DeltaInvariantChecker unary logical operator (with constraints attached) into a DeltaInvariantCheckerExec for execution.</p>  <p>Danger</p> <p><code>DeltaInvariantCheckerStrategy</code> does not seem to be used at all.</p>","title":"DeltaInvariantCheckerStrategy Execution Planning Strategy"},{"location":"constraints/DeltaInvariantCheckerStrategy/#creating-instance","text":"<p><code>DeltaInvariantCheckerStrategy</code> is a Scala object and takes no input arguments to be created.</p>","title":"Creating Instance"},{"location":"constraints/DeltaInvariantCheckerStrategy/#executing-rule","text":"","title":"Executing Rule <pre><code>apply(\n  plan: LogicalPlan): Seq[SparkPlan]\n</code></pre> <p><code>apply</code>\u00a0is part of the <code>SparkStrategy</code> (Spark SQL) abstraction.</p> <p>For a given DeltaInvariantChecker unary logical operator with constraints attached, <code>apply</code> creates a DeltaInvariantCheckerExec unary physical operator. Otherwise, <code>apply</code> does nothing (noop).</p>"},{"location":"constraints/DropConstraint/","text":"<p><code>DropConstraint</code> is...FIXME</p>","title":"DropConstraint"},{"location":"constraints/Invariant/","text":"<p><code>Invariant</code> represents a Rule associated with a column.</p>","title":"Invariant"},{"location":"constraints/Invariant/#creating-instance","text":"<p><code>Invariant</code> takes the following to be created:</p> <ul> <li> Column Name <li> <code>Rule</code>","title":"Creating Instance"},{"location":"constraints/InvariantViolationException/","text":"<p><code>InvariantViolationException</code> is a <code>RuntimeException</code> (Java) that is reported when data does not match the rules of a table (using Constraints).</p>","title":"InvariantViolationException"},{"location":"constraints/InvariantViolationException/#creating-instance","text":"<p><code>InvariantViolationException</code> takes the following to be created:</p> <ul> <li> Error Message  <p><code>InvariantViolationException</code> is created\u00a0when:</p> <ul> <li><code>DeltaErrors</code> utility is used to notNullColumnMissingException</li> <li><code>InvariantViolationException</code> utility is used to apply</li> </ul>","title":"Creating Instance"},{"location":"constraints/InvariantViolationException/#creating-invariantviolationexception","text":"","title":"Creating InvariantViolationException <p><code>apply</code> creates a InvariantViolationException for the given constraint: Check or NotNull.</p>"},{"location":"constraints/InvariantViolationException/#check","text":"","title":"Check <pre><code>apply(\n  constraint: Constraints.Check,\n  values: Map[String, Any]): InvariantViolationException\n</code></pre> <p>Check</p> <pre><code>CHECK constraint [name] [sql] violated by row with values:\n - [column] : [value]\n</code></pre>"},{"location":"constraints/InvariantViolationException/#notnull","text":"","title":"NotNull <pre><code>apply(\n  constraint: Constraints.NotNull): InvariantViolationException\n</code></pre> <p>NotNull</p> <pre><code>NOT NULL constraint violated for column: [name]\n</code></pre>"},{"location":"constraints/InvariantViolationException/#usage","text":"","title":"Usage <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>CheckDeltaInvariant</code> is used to eval (and assertRule)</li> </ul>"},{"location":"constraints/Invariants/","text":"","title":"Invariants Utility"},{"location":"constraints/Invariants/#deltainvariants","text":"","title":"delta.invariants <p><code>Invariants</code> defines <code>delta.invariants</code> for the invariants of a delta table.</p> <p><code>delta.invariants</code> contains a JSON-encoded SQL expression.</p>"},{"location":"constraints/Invariants/#extracting-constraints-from-schema","text":"","title":"Extracting Constraints from Schema <pre><code>getFromSchema(\n  schema: StructType,\n  spark: SparkSession): Seq[Constraint]\n</code></pre> <p><code>getFromSchema</code> finds columns (top-level or nested) that are non-nullable and have delta.invariants metadata.</p> <p>For every parent of the columns, <code>getFromSchema</code> creates NotNull constraints.</p> <p>For the columns themselves, <code>getFromSchema</code> creates Check constraints.</p> <p><code>getFromSchema</code>\u00a0is used when:</p> <ul> <li><code>Protocol</code> utility is used to determine the required minimum protocol</li> <li><code>Constraints</code> utility is used to extract constraints from a table metadata</li> </ul>"},{"location":"constraints/Invariants/#rule","text":"","title":"Rule <p><code>Invariants</code> utility defines a <code>Rule</code> abstraction.</p> <p><code>Rule</code> has a name.</p>"},{"location":"constraints/Invariants/#arbitraryexpression","text":"","title":"ArbitraryExpression <p><code>ArbitraryExpression</code> is a Rule with the following:</p> <ul> <li><code>EXPRESSION([expression])</code> name</li> <li>An <code>Expression</code> (Spark SQL)</li> </ul>"},{"location":"contenders/","text":"<p>As it happens in the open source software world, Delta Lake is not alone in the area of Data Lakes on top of Apache Spark. The following is a list of some other open source projects that seems to compete or cover the same use cases.</p>","title":"Contenders"},{"location":"contenders/#apache-iceberg","text":"<p>Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to Presto and Spark that use a high-performance format that works just like a SQL table.</p>","title":"Apache Iceberg"},{"location":"contenders/#videos","text":"<ul> <li>ACID ORC, Iceberg, and Delta Lake\u2014An Overview of Table Formats for Large Scale Storage and Analytics</li> <li>Introducing Iceberg Tables designed for object stores</li> <li>Introducing Apache Iceberg: Tables Designed for Object Stores</li> <li>Iceberg: a fast table format for S3</li> </ul>","title":"Videos"},{"location":"contenders/#apache-hudi","text":"<p>Apache Hudi ingests and manages storage of large analytical datasets over DFS (HDFS or cloud stores) and provides three logical views for query access.</p>","title":"Apache Hudi"},{"location":"contenders/#videos_1","text":"<ul> <li>Hoodie: An Open Source Incremental Processing Framework From Uber</li> <li>Powering Uber's global network analytics pipelines in real-time with Apache Hudi</li> </ul>","title":"Videos"},{"location":"demo/","text":"<p>The following demos are available:</p> <ul> <li>Time Travel</li> <li>Vacuum</li> <li>dataChange</li> <li>replaceWhere</li> <li>Merge Operation</li> <li>Converting Parquet Dataset Into Delta Format</li> <li>Stream Processing of Delta Table</li> <li>Using Delta Lake as Streaming Sink in Structured Streaming</li> <li>Debugging Delta Lake Using IntelliJ IDEA</li> <li>Observing Transaction Retries</li> <li>DeltaTable, DeltaLog And Snapshots</li> <li>Schema Evolution</li> <li>User Metadata for Labelling Commits</li> </ul>","title":"Demos"},{"location":"demo/Converting-Parquet-Dataset-Into-Delta-Format/","text":"<pre><code>/*\nspark-shell \\\n  --packages io.delta:delta-core_2.12:1.2.0 \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.databricks.delta.snapshotPartitions=1\n*/\nassert(spark.version.matches(\"2.4.[2-6]\"), \"Delta Lake supports Spark 2.4.2+\")\n\nimport org.apache.spark.sql.SparkSession\nassert(spark.isInstanceOf[SparkSession])\n\nval deltaLake = \"/tmp/delta\"\n\n// Create parquet table\nval users = s\"$deltaLake/users\"\nimport spark.implicits._\nval data = Seq(\n  (0L, \"Agata\", \"Warsaw\", \"Poland\"),\n  (1L, \"Jacek\", \"Warsaw\", \"Poland\"),\n  (2L, \"Bartosz\", \"Paris\", \"France\")\n).toDF(\"id\", \"name\", \"city\", \"country\")\ndata\n  .write\n  .format(\"parquet\")\n  .partitionBy(\"city\", \"country\")\n  .mode(\"overwrite\")\n  .save(users)\n\n// TIP: Use git to version the users directory\n//      to track the changes for import\n\n// CONVERT TO DELTA only supports parquet tables\n// TableIdentifier should be parquet.`users`\n\n// Use TableIdentifier to refer to the parquet table\n// The path itself would work too\nval tableId = s\"parquet.`$users`\"\nval partitionSchema = \"city STRING, country STRING\"\n\n// Import users table into Delta Lake\n// Well, convert the parquet table into delta table\n// Use web UI to monitor execution, e.g. http://localhost:4040\n\nimport io.delta.tables.DeltaTable\nval dt = DeltaTable.convertToDelta(spark, tableId, partitionSchema)\nassert(dt.isInstanceOf[DeltaTable])\n\n// users table is now in delta format\nassert(DeltaTable.isDeltaTable(users))\n</code></pre>","title":"Demo: Converting Parquet Dataset Into Delta Format"},{"location":"demo/Debugging-Delta-Lake-Using-IntelliJ-IDEA/","text":"<p>Import Delta Lake's sources to IntelliJ IDEA.</p> <p>Configure a new Remote debug configuration in IntelliJ IDEA (e.g. Run &gt; Debug &gt; Edit Configurations...) and simply give it a name and save.</p>  <p>Tip</p> <p>Use <code>Option+Ctrl+D</code> to access Debug menu on mac OS.</p>  <p></p> <p>Run <code>spark-shell</code> as follows to enable remote JVM for debugging.</p> <pre><code>export SPARK_SUBMIT_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005\n</code></pre> <pre><code>spark-shell \\\n  --packages io.delta:delta-core_2.12:1.2.0 \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\\n  --conf spark.databricks.delta.snapshotPartitions=1\n</code></pre>","title":"Demo: Debugging Delta Lake Using IntelliJ IDEA"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/","text":"","title":"Demo: DeltaTable, DeltaLog And Snapshots"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#create-delta-table","text":"<pre><code>import org.apache.spark.sql.SparkSession\nassert(spark.isInstanceOf[SparkSession])\n</code></pre> <pre><code>val tableName = \"users\"\n</code></pre> <pre><code>sql(s\"DROP TABLE IF EXISTS $tableName\")\nsql(s\"\"\"\n    | CREATE TABLE $tableName (id bigint, name string, city string, country string)\n    | USING delta\n    \"\"\".stripMargin)\n</code></pre> <pre><code>scala&gt; spark.catalog.listTables.show\n+-----+--------+-----------+---------+-----------+\n| name|database|description|tableType|isTemporary|\n+-----+--------+-----------+---------+-----------+\n|users| default|       null|  MANAGED|      false|\n+-----+--------+-----------+---------+-----------+\n</code></pre>","title":"Create Delta Table"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#access-transaction-log-deltalog","text":"<pre><code>import org.apache.spark.sql.catalyst.TableIdentifier\nval tid = TableIdentifier(tableName)\n\nimport org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, tid)\n</code></pre> <p>Update the state of the delta table to the most recent version.</p> <pre><code>val snapshot = deltaLog.update()\nassert(snapshot.version == 0)\n</code></pre> <pre><code>val state = snapshot.state\n</code></pre> <pre><code>scala&gt; :type state\norg.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.SingleAction]\n</code></pre> <p>Review the cached RDD for the state snapshot in the Storage tab of the web UI (e.g. http://localhost:4040/storage/).</p> <p></p> <p>The \"version\" part of Delta Table State name of the cached RDD should match the version of the snapshot.</p> <p>Show the changes (actions).</p> <pre><code>scala&gt; state.show\n+----+----+------+--------------------+--------+----+----------+\n| txn| add|remove|            metaData|protocol| cdc|commitInfo|\n+----+----+------+--------------------+--------+----+----------+\n|null|null|  null|                null|  {1, 2}|null|      null|\n|null|null|  null|{90316970-5bf1-45...|    null|null|      null|\n+----+----+------+--------------------+--------+----+----------+\n</code></pre>","title":"Access Transaction Log (DeltaLog)"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#deltatable-as-dataframe","text":"","title":"DeltaTable as DataFrame"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#deltatable","text":"<pre><code>import io.delta.tables.DeltaTable\nval dt = DeltaTable.forName(tableName)\n</code></pre> <pre><code>val h = dt.history.select('version, 'operation, 'operationParameters, 'operationMetrics)\n</code></pre> <pre><code>scala&gt; h.show(truncate = false)\n+-------+------------+-----------------------------------------------------------------------------+----------------+\n|version|operation   |operationParameters                                                          |operationMetrics|\n+-------+------------+-----------------------------------------------------------------------------+----------------+\n|0      |CREATE TABLE|{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}|{}              |\n+-------+------------+-----------------------------------------------------------------------------+----------------+\n</code></pre>","title":"DeltaTable"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#converting-deltatable-into-dataframe","text":"<pre><code>val users = dt.toDF\n</code></pre> <pre><code>scala&gt; users.show\n+---+----+----+-------+\n| id|name|city|country|\n+---+----+----+-------+\n+---+----+----+-------+\n</code></pre>","title":"Converting DeltaTable into DataFrame"},{"location":"demo/DeltaTable-DeltaLog-And-Snapshots/#add-new-users","text":"<pre><code>val newUsers = Seq(\n  (0L, \"Agata\", \"Warsaw\", \"Poland\"),\n  (1L, \"Bartosz\", \"Paris\", \"France\")\n).toDF(\"id\", \"name\", \"city\", \"country\")\n</code></pre> <pre><code>scala&gt; newUsers.show\n+---+-------+------+-------+\n| id|   name|  city|country|\n+---+-------+------+-------+\n|  0|  Agata|Warsaw| Poland|\n|  1|Bartosz| Paris| France|\n+---+-------+------+-------+\n</code></pre> <pre><code>// newUsers.write.format(\"delta\").mode(\"append\").saveAsTable(name)\nnewUsers.writeTo(tableName).append\nassert(deltaLog.snapshot.version == 1)\n</code></pre> <p>Review the cached RDD for the state snapshot in the Storage tab of the web UI (e.g. http://localhost:4040/storage/).</p> <p>Note that the <code>DataFrame</code> variant of the delta table has automatically been refreshed (making <code>REFRESH TABLE</code> unnecessary).</p> <pre><code>scala&gt; users.show\n+---+-------+------+-------+\n| id|   name|  city|country|\n+---+-------+------+-------+\n|  1|Bartosz| Paris| France|\n|  0|  Agata|Warsaw| Poland|\n+---+-------+------+-------+\n</code></pre> <pre><code>val h = dt.history.select('version, 'operation, 'operationParameters, 'operationMetrics)\n</code></pre> <pre><code>scala&gt; h.show(truncate = false)\n+-------+------------+-----------------------------------------------------------------------------+-----------------------------------------------------------+\n|version|operation   |operationParameters                                                          |operationMetrics                                           |\n+-------+------------+-----------------------------------------------------------------------------+-----------------------------------------------------------+\n|1      |WRITE       |{mode -&gt; Append, partitionBy -&gt; []}                                          |{numFiles -&gt; 2, numOutputBytes -&gt; 2299, numOutputRows -&gt; 2}|\n|0      |CREATE TABLE|{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}|{}                                                         |\n+-------+------------+-----------------------------------------------------------------------------+-----------------------------------------------------------+\n</code></pre>","title":"Add new users"},{"location":"demo/Observing-Transaction-Retries/","text":"<p>Enable <code>ALL</code> logging level for org.apache.spark.sql.delta.OptimisticTransaction logger. You'll be looking for the following DEBUG message in the logs:</p> <pre><code>Attempting to commit version [version] with 13 actions with Serializable isolation level\n</code></pre> <p>Start with Debugging Delta Lake Using IntelliJ IDEA and place the following line breakpoints in <code>OptimisticTransactionImpl</code>:</p> <p>. In <code>OptimisticTransactionImpl.doCommit</code> when a transaction is about to <code>deltaLog.store.write</code> (line 388)</p> <p>. In <code>OptimisticTransactionImpl.doCommit</code> when a transaction is about to <code>checkAndRetry</code> after a <code>FileAlreadyExistsException</code> (line 433)</p> <p>. In <code>OptimisticTransactionImpl.checkAndRetry</code> when a transaction calculates <code>nextAttemptVersion</code> (line 453)</p> <p>In order to interfere with a transaction about to be committed, you will use ROOT:WriteIntoDelta.md[WriteIntoDelta] action (it is simple and does the work).</p> <p>Run the command (copy and paste the ROOT:WriteIntoDelta.md#demo[demo code] to <code>spark-shell</code> using paste mode). You should see the following messages in the logs:</p> <pre><code>scala&gt; writeCmd.run(spark)\nDeltaLog: DELTA: Updating the Delta table's state\nOptimisticTransaction: Attempting to commit version 6 with 13 actions with Serializable isolation level\n</code></pre> <p>That's when you \"commit\" another transaction (to simulate two competing transactional writes). Simply create a delta file for the transaction. The commit version in the message above is <code>6</code> so the name of the delta file should be <code>00000000000000000006.json</code>:</p> <pre><code>$ touch /tmp/delta/t1/_delta_log/00000000000000000006.json\n</code></pre> <p><code>F9</code> in IntelliJ IDEA to resume the <code>WriteIntoDelta</code> command. It should stop at <code>checkAndRetry</code> due to <code>FileAlreadyExistsException</code>. Press <code>F9</code> twice to resume.</p> <p>You should see the following messages in the logs:</p> <pre><code>OptimisticTransaction: No logical conflicts with deltas [6, 7), retrying.\nOptimisticTransaction: Attempting to commit version 7 with 13 actions with Serializable isolation level\n</code></pre> <p>Rinse and repeat. You know the drill already. Happy debugging!</p>","title":"Demo: Observing Transaction Retries"},{"location":"demo/Using-Delta-Lake-as-Streaming-Sink-in-Structured-Streaming/","text":"<pre><code>assert(spark.isInstanceOf[org.apache.spark.sql.SparkSession])\nassert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\")\n\n// Input data \"format\"\ncase class User(id: Long, name: String, city: String)\n\n// Any streaming data source would work\n// Using memory data source\n// Gives control over the input stream\nimplicit val ctx = spark.sqlContext\nimport org.apache.spark.sql.execution.streaming.MemoryStream\nval usersIn = MemoryStream[User]\nval users = usersIn.toDF\n\nval deltaLake = \"/tmp/delta-lake\"\nval checkpointLocation = \"/tmp/delta-checkpointLocation\"\nval path = s\"$deltaLake/users\"\nval partitionBy = \"city\"\n\n// The streaming query that writes out to Delta Lake\nval sq = users\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", checkpointLocation)\n  .option(\"path\", path)\n  .partitionBy(partitionBy)\n  .start\n\n// TIP: You could use git to version the path directory\n//      and track the changes of every micro-batch\n\n// TIP: Use web UI to monitor execution, e.g. http://localhost:4040\n\n// FIXME: Use DESCRIBE HISTORY every micro-batch\n\nval batch1 = Seq(\n  User(0, \"Agata\", \"Warsaw\"),\n  User(1, \"Jacek\", \"Warsaw\"))\nval offset = usersIn.addData(batch1)\nsq.processAllAvailable()\n\nval history = s\"DESCRIBE HISTORY delta.`$path`\"\nval clmns = Seq($\"version\", $\"timestamp\", $\"operation\", $\"operationParameters\", $\"isBlindAppend\")\n\nval h = sql(history).select(clmns: _*).orderBy($\"version\".asc)\nscala&gt; h.show(truncate = false)\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+\n|version|timestamp          |operation       |operationParameters                                                                  |isBlindAppend|\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+\n|0      |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -&gt; Append, queryId -&gt; f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -&gt; 0]|true         |\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+\n\nval batch2 = Seq(\n  User(2, \"Bartek\", \"Paris\"),\n  User(3, \"Jacek\", \"Paris\"))\nval offset = usersIn.addData(batch2)\nsq.processAllAvailable()\n\n// You have to execute the history SQL command again\n// It materializes immediately with whatever data is available at the time\nval h = sql(history).select(clmns: _*).orderBy($\"version\".asc)\nscala&gt; h.show(truncate = false)\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+\n|version|timestamp          |operation       |operationParameters                                                                  |isBlindAppend|\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+\n|0      |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -&gt; Append, queryId -&gt; f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -&gt; 0]|true         |\n|1      |2019-12-06 10:13:27|STREAMING UPDATE|[outputMode -&gt; Append, queryId -&gt; f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -&gt; 1]|true         |\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+\n\nval batch3 = Seq(\n  User(4, \"Gorazd\", \"Ljubljana\"))\nval offset = usersIn.addData(batch3)\nsq.processAllAvailable()\n\n// Let's use DeltaTable API instead\n\nimport io.delta.tables.DeltaTable\nval usersDT = DeltaTable.forPath(path)\n\nval h = usersDT.history.select(clmns: _*).orderBy($\"version\".asc)\nscala&gt; h.show(truncate = false)\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+\n|version|timestamp          |operation       |operationParameters                                                                  |isBlindAppend|\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+\n|0      |2019-12-06 10:06:20|STREAMING UPDATE|[outputMode -&gt; Append, queryId -&gt; f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -&gt; 0]|true         |\n|1      |2019-12-06 10:13:27|STREAMING UPDATE|[outputMode -&gt; Append, queryId -&gt; f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -&gt; 1]|true         |\n|2      |2019-12-06 10:20:56|STREAMING UPDATE|[outputMode -&gt; Append, queryId -&gt; f3990048-f0b7-48b6-9bf6-397004c36e53, epochId -&gt; 2]|true         |\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-------------+\n</code></pre>","title":"Demo: Using Delta Lake as Streaming Sink in Structured Streaming"},{"location":"demo/dataChange/","text":"<p>This demo shows dataChange option in action.</p> <p>In combination with <code>Overwrite</code> mode, <code>dataChange</code> option can be used to transactionally rearrange data in a delta table.</p>","title":"Demo: dataChange"},{"location":"demo/dataChange/#start-spark-shell","text":"<pre><code>./bin/spark-shell \\\n  --packages io.delta:delta-core_2.12:1.2.0 \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n</code></pre>","title":"Start Spark Shell"},{"location":"demo/dataChange/#create-delta-table","text":"<pre><code>val path = \"/tmp/delta/d01\"\n</code></pre> <p>Make sure that there is no delta table at the location. Remove it if exists and start over.</p> <pre><code>import org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, path)\nassert(deltaLog.tableExists == false)\n</code></pre> <p>Create the demo delta table (using SQL).</p> <pre><code>sql(s\"\"\"\n  CREATE TABLE delta.`$path`\n  USING delta\n  VALUES ((0, 'Jacek'), (1, 'Agata')) AS (id, name)\n  \"\"\")\n</code></pre>","title":"Create Delta Table"},{"location":"demo/dataChange/#show-history-before","text":"<pre><code>import io.delta.tables.DeltaTable\nval dt = DeltaTable.forPath(path)\nassert(dt.history.count == 1)\n</code></pre>","title":"Show History (Before)"},{"location":"demo/dataChange/#repartition-table","text":"<p>The following <code>dataChange</code> example shows a batch query that repartitions a delta table (perhaps while other queries could be using the delta table).</p> <p>Let's check out the number of partitions.</p> <pre><code>spark.read.format(\"delta\").load(path).rdd.getNumPartitions\n</code></pre> <p>The key items to pay attention to are:</p> <ol> <li>The batch query is independent from any other running streaming or batch queries over the delta table</li> <li>The batch query reads from the same delta table it saves data to</li> <li>The save mode is overwrite</li> <li>dataChange option is disabled</li> </ol> <pre><code>spark\n  .read\n  .format(\"delta\")\n  .load(path)\n  .repartition(10)\n  .write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .option(\"dataChange\", false)\n  .save(path)\n</code></pre> <p>Let's check out the number of partitions after the repartition.</p> <pre><code>spark.read.format(\"delta\").load(path).rdd.getNumPartitions\n</code></pre>","title":"Repartition Table"},{"location":"demo/dataChange/#show-history-after","text":"<pre><code>assert(dt.history.count == 2)\n</code></pre> <pre><code>dt.history\n  .select('version, 'operation, 'operationParameters, 'operationMetrics)\n  .orderBy('version.asc)\n  .show(truncate = false)\n</code></pre> <pre><code>+-------+----------------------+------------------------------------------------------------------------------+-----------------------------------------------------------+\n|version|operation             |operationParameters                                                           |operationMetrics                                           |\n+-------+----------------------+------------------------------------------------------------------------------+-----------------------------------------------------------+\n|0      |CREATE TABLE AS SELECT|{isManaged -&gt; false, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}|{numFiles -&gt; 1, numOutputBytes -&gt; 1273, numOutputRows -&gt; 1}|\n|1      |WRITE                 |{mode -&gt; Overwrite, partitionBy -&gt; []}                                        |{numFiles -&gt; 2, numOutputBytes -&gt; 1992, numOutputRows -&gt; 1}|\n+-------+----------------------+------------------------------------------------------------------------------+-----------------------------------------------------------+\n</code></pre>","title":"Show History (After)"},{"location":"demo/merge-operation/","text":"<p>This demo shows DeltaTable.merge operation (and the underlying MergeIntoCommand) in action.</p>  <p>Tip</p> <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.delta.commands.MergeIntoCommand</code> logger as described in Logging.</p>","title":"Demo: Merge Operation"},{"location":"demo/merge-operation/#create-delta-table-target-data","text":"","title":"Create Delta Table (Target Data)"},{"location":"demo/merge-operation/#modern-delta-lake-own-create-table","text":"<pre><code>val path = \"/tmp/delta/demo\"\nimport io.delta.tables.DeltaTable\nval target = DeltaTable.create.addColumn(\"id\", \"long\").location(path).execute\n</code></pre> <pre><code>assert(target.isInstanceOf[io.delta.tables.DeltaTable])\nassert(target.history.count == 1, \"There must be version 0 only\")\n</code></pre> <p>Unfortunately, the above leaves us with an empty Delta table. Let's fix it.</p> <pre><code>import org.apache.spark.sql.SaveMode\nspark.range(5).write.format(\"delta\").mode(SaveMode.Append).save(path)\n</code></pre> <pre><code>assert(target.history.count == 2)\n</code></pre>","title":"Modern Delta Lake-Own Create Table"},{"location":"demo/merge-operation/#legacy-spark-based-create-table","text":"<p>Note</p> <p>This legacy Spark-based <code>CREATE TABLE</code> is left for comparison purposes only.</p>  <pre><code>val path = \"/tmp/delta/demo\"\nval data = spark.range(5)\ndata.write.format(\"delta\").save(path)\n\nimport io.delta.tables.DeltaTable\nval target = DeltaTable.forPath(path)\n\nassert(target.isInstanceOf[io.delta.tables.DeltaTable])\nassert(target.history.count == 1, \"There must be version 0 only\")\n</code></pre>","title":"Legacy Spark-Based Create Table"},{"location":"demo/merge-operation/#source-data","text":"<pre><code>case class Person(id: Long, name: String)\nval source = Seq(Person(0, \"Zero\"), Person(1, \"One\")).toDF\n</code></pre> <p>Note the difference in the schema of the <code>target</code> and <code>source</code> datasets.</p> <pre><code>target.toDF.printSchema\n</code></pre> <pre><code>root\n |-- id: long (nullable = true)\n</code></pre> <pre><code>source.printSchema\n</code></pre> <pre><code>root\n |-- id: long (nullable = false)\n |-- name: string (nullable = true)\n</code></pre>","title":"Source Data"},{"location":"demo/merge-operation/#merge-with-schema-evolution","text":"<p>Not only do we update the matching rows, but also update the schema (schema evolution)</p> <pre><code>val mergeBuilder = target.as(\"to\")\n  .merge(\n    source = source.as(\"from\"),\n    condition = $\"to.id\" === $\"from.id\")\n</code></pre> <pre><code>assert(mergeBuilder.isInstanceOf[io.delta.tables.DeltaMergeBuilder])\n</code></pre> <pre><code>scala&gt; mergeBuilder.execute\norg.apache.spark.sql.AnalysisException: There must be at least one WHEN clause in a MERGE statement\n  at org.apache.spark.sql.catalyst.plans.logical.DeltaMergeInto$.apply(deltaMerge.scala:253)\n  at io.delta.tables.DeltaMergeBuilder.mergePlan(DeltaMergeBuilder.scala:268)\n  at io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:215)\n  at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:87)\n  at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:73)\n  at io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:120)\n  at io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:204)\n  ... 47 elided\n</code></pre> <pre><code>val mergeMatchedBuilder = mergeBuilder.whenMatched()\nassert(mergeMatchedBuilder.isInstanceOf[io.delta.tables.DeltaMergeMatchedActionBuilder])\n\nval mergeBuilderDeleteMatched = mergeMatchedBuilder.delete()\nassert(mergeBuilderDeleteMatched.isInstanceOf[io.delta.tables.DeltaMergeBuilder])\n\nmergeBuilderDeleteMatched.execute()\n</code></pre> <pre><code>assert(target.history.count == 3)\n</code></pre>","title":"Merge with Schema Evolution"},{"location":"demo/merge-operation/#update-all-columns-except-one","text":"<p>This demo shows how to update all columns except one on a match.</p> <pre><code>val targetDF = target\n  .toDF\n  .withColumn(\"num\", lit(1))\n  .withColumn(\"updated\", lit(false))\n</code></pre> <pre><code>targetDF.sort('id.asc).show\n</code></pre> <pre><code>+---+---+-------+\n| id|num|updated|\n+---+---+-------+\n|  2|  1|  false|\n|  3|  1|  false|\n|  4|  1|  false|\n+---+---+-------+\n</code></pre> <p>Write the modified data out to the delta table (that will create a new version with the schema changed).</p> <pre><code>targetDF\n  .write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .option(\"overwriteSchema\", true)\n  .save(path)\n</code></pre> <p>Reload the delta table (with the new column changes).</p> <pre><code>val target = DeltaTable.forPath(path)\nval targetDF = target.toDF\n</code></pre> <pre><code>val sourceDF = Seq(0, 1, 2).toDF(\"num\")\n</code></pre> <p>Create an update map (with the columns of the target delta table and the new values).</p> <pre><code>val updates = Map(\n  \"updated\" -&gt; lit(true))\n</code></pre> <pre><code>target.as(\"to\")\n  .merge(\n    source = sourceDF.as(\"from\"),\n    condition = $\"to.id\" === $\"from.num\")\n  .whenMatched.update(updates)\n  .execute()\n</code></pre> <p>Reload the delta table (with the merge changes).</p> <pre><code>val target = DeltaTable.forPath(path)\ntarget.toDF.sort('id.asc).show\n</code></pre> <pre><code>+---+---+-------+\n| id|num|updated|\n+---+---+-------+\n|  2|  1|   true|\n|  3|  1|  false|\n|  4|  1|  false|\n+---+---+-------+\n</code></pre> <pre><code>assert(target.history.count == 5)\n</code></pre>","title":"Update All Columns Except One"},{"location":"demo/replaceWhere/","text":"<p>This demo shows replaceWhere predicate option.</p> <p>In combination with <code>Overwrite</code> mode, a <code>replaceWhere</code> option can be used to transactionally replace data that matches a predicate.</p>","title":"Demo: replaceWhere"},{"location":"demo/replaceWhere/#create-delta-table","text":"<pre><code>val table = \"d1\"\n</code></pre> <pre><code>sql(s\"\"\"\n  CREATE TABLE $table (`id` LONG, p STRING)\n  USING delta\n  PARTITIONED BY (p)\n  COMMENT 'Delta table'\n\"\"\")\n</code></pre> <pre><code>spark.catalog.listTables.show\n</code></pre> <pre><code>+----+--------+-----------+---------+-----------+\n|name|database|description|tableType|isTemporary|\n+----+--------+-----------+---------+-----------+\n|  d1| default|Delta table|  MANAGED|      false|\n+----+--------+-----------+---------+-----------+\n</code></pre> <pre><code>import org.apache.spark.sql.delta.DeltaLog\nval dt = DeltaLog.forTable(spark, table)\nval m = dt.snapshot.metadata\nval partitionSchema = m.partitionSchema\n</code></pre>","title":"Create Delta Table"},{"location":"demo/replaceWhere/#write-data","text":"<pre><code>Seq((0L, \"a\"),\n    (1L, \"a\"))\n  .toDF(\"id\", \"p\")\n  .write\n  .format(\"delta\")\n  .option(\"replaceWhere\", \"p = 'a'\")\n  .mode(\"overwrite\")\n  .saveAsTable(table)\n</code></pre>","title":"Write Data"},{"location":"demo/schema-evolution/","text":"<pre><code>/*\nspark-shell \\\n  --packages io.delta:delta-core_2.12:1.2.0 \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.databricks.delta.snapshotPartitions=1\n*/\n\ncase class PersonV1(id: Long, name: String)\nimport org.apache.spark.sql.Encoders\nval schemaV1 = Encoders.product[PersonV1].schema\nscala&gt; schemaV1.printTreeString\nroot\n |-- id: long (nullable = false)\n |-- name: string (nullable = true)\n\nval dataPath = \"/tmp/delta/people\"\n\n// Write data\nSeq(PersonV1(0, \"Zero\"), PersonV1(1, \"One\"))\n  .toDF\n  .write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .option(\"overwriteSchema\", \"true\")\n  .save(dataPath)\n\n// Committed delta #0 to file:/tmp/delta/people/_delta_log\n\nimport org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, dataPath)\nassert(deltaLog.snapshot.version == 0)\n\nscala&gt; deltaLog.snapshot.dataSchema.printTreeString\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n\nimport io.delta.tables.DeltaTable\nval dt = DeltaTable.forPath(dataPath)\nscala&gt; dt.toDF.show\n+---+----+\n| id|name|\n+---+----+\n|  0|Zero|\n|  1| One|\n+---+----+\n\nval main = dt.as(\"main\")\n\ncase class PersonV2(id: Long, name: String, newField: Boolean)\nval schemaV2 = Encoders.product[PersonV2].schema\nscala&gt; schemaV2.printTreeString\nroot\n |-- id: long (nullable = false)\n |-- name: string (nullable = true)\n |-- newField: boolean (nullable = false)\n\nval updates = Seq(\n  PersonV2(0, \"ZERO\", newField = true),\n  PersonV2(2, \"TWO\", newField = false)).toDF\n\n// Merge two datasets and create a new version\n// Schema evolution in play\nmain.merge(\n    source = updates.as(\"updates\"),\n    condition = $\"main.id\" === $\"updates.id\")\n  .whenMatched.updateAll\n  .execute\n\nval latestPeople = spark\n  .read\n  .format(\"delta\")\n  .load(dataPath)\nscala&gt; latestPeople.show\n+---+----+\n| id|name|\n+---+----+\n|  0|ZERO|\n|  1| One|\n+---+----+\n</code></pre>","title":"Demo: Schema Evolution"},{"location":"demo/stream-processing-of-delta-table/","text":"<pre><code>/*\nspark-shell \\\n  --packages io.delta:delta-core_2.12:1.2.0 \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.databricks.delta.snapshotPartitions=1\n*/\nassert(spark.version.matches(\"2.4.[2-4]\"), \"Delta Lake supports Spark 2.4.2+\")\n\nimport org.apache.spark.sql.SparkSession\nassert(spark.isInstanceOf[SparkSession])\n\nval deltaTableDir = \"/tmp/delta/users\"\nval checkpointLocation = \"/tmp/checkpointLocation\"\n\n// Initialize the delta table\n// - No data\n// - Schema only\ncase class Person(id: Long, name: String, city: String)\nspark.emptyDataset[Person].write.format(\"delta\").save(deltaTableDir)\n\nimport org.apache.spark.sql.streaming.Trigger\nimport scala.concurrent.duration._\nval sq = spark\n  .readStream\n  .format(\"delta\")\n  .option(\"maxFilesPerTrigger\", 1) // Maximum number of files to scan per micro-batch\n  .load(deltaTableDir)\n  .writeStream\n  .format(\"console\")\n  .option(\"truncate\", false)\n  .option(\"checkpointLocation\", checkpointLocation)\n  .trigger(Trigger.ProcessingTime(10.seconds)) // Useful for debugging\n  .start\n\n// The streaming query over delta table\n// should display the 0th version as Batch 0\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n+---+----+----+\n|id |name|city|\n+---+----+----+\n+---+----+----+\n\n// Let's write to the delta table\nval users = Seq(\n    Person(0, \"Jacek\", \"Warsaw\"),\n    Person(1, \"Agata\", \"Warsaw\"),\n    Person(2, \"Jacek\", \"Paris\"),\n    Person(3, \"Domas\", \"Vilnius\")).toDF\n\n// More partitions are more file added\n// And per maxFilesPerTrigger as 1 file addition per micro-batch\n// You should see more micro-batches\nscala&gt; println(users.rdd.getNumPartitions)\n4\n\n// Change the default SaveMode.ErrorIfExists to more meaningful save mode\nimport org.apache.spark.sql.SaveMode\nusers\n  .write\n  .format(\"delta\")\n  .mode(SaveMode.Append) // Appending rows\n  .save(deltaTableDir)\n\n// Immediately after the above write finishes\n// New batches should be printed out to the console\n// Per the number of partitions of users dataset\n// And per maxFilesPerTrigger as 1 file addition\n// You should see as many micro-batches as files\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+---+-----+------+\n|id |name |city  |\n+---+-----+------+\n|0  |Jacek|Warsaw|\n+---+-----+------+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+---+-----+------+\n|id |name |city  |\n+---+-----+------+\n|1  |Agata|Warsaw|\n+---+-----+------+\n\n-------------------------------------------\nBatch: 3\n-------------------------------------------\n+---+-----+-----+\n|id |name |city |\n+---+-----+-----+\n|2  |Jacek|Paris|\n+---+-----+-----+\n\n-------------------------------------------\nBatch: 4\n-------------------------------------------\n+---+-----+-------+\n|id |name |city   |\n+---+-----+-------+\n|3  |Domas|Vilnius|\n+---+-----+-------+\n</code></pre>","title":"Demo: Stream Processing of Delta Table"},{"location":"demo/time-travel/","text":"<p>This demo shows Time Travel in action.</p> <pre><code>./bin/spark-shell \\\n  --packages io.delta:delta-core_2.12:1.2.0 \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n</code></pre> <pre><code>import io.delta.implicits._\n\nSeq((\"r1\", 500), (\"r2\", 600)).toDF(\"id\", \"value\").write.delta(\"/tmp/delta/demo\")\n\nsql(\"DESCRIBE HISTORY delta.`/tmp/delta/demo`\").select('version, 'operation).show(truncate = false)\n\n// +-------+---------+\n// |version|operation|\n// +-------+---------+\n// |0      |WRITE    |\n// +-------+---------+\n\nsql(\"UPDATE delta.`/tmp/delta/demo` SET value = '700' WHERE id = 'r1'\")\n\nsql(\"DESCRIBE HISTORY delta.`/tmp/delta/demo`\").select('version, 'operation).show(truncate = false)\n\n// +-------+---------+\n// |version|operation|\n// +-------+---------+\n// |1      |UPDATE   |\n// |0      |WRITE    |\n// +-------+---------+\n\nspark.read.delta(\"/tmp/delta/demo\").show\n\n// +---+-----+\n// | id|value|\n// +---+-----+\n// | r1|  700|\n// | r2|  600|\n// +---+-----+\n\nspark.read.option(\"versionAsOf\", 0).delta(\"/tmp/delta/demo\").show\n\n// +---+-----+\n// | id|value|\n// +---+-----+\n// | r1|  500|\n// | r2|  600|\n// +---+-----+\n</code></pre> <p>The above query over a previous version is also possible using version pattern.</p> <pre><code>spark.read.delta(\"/tmp/delta/demo@v1\").show\n</code></pre>","title":"Demo: Time Travel"},{"location":"demo/user-metadata-for-labelling-commits/","text":"<p>The demo shows how to differentiate commits of a write batch query using userMetadata option.</p>  <p>Tip</p> <p>A fine example could be for distinguishing between two or more separate streaming write queries.</p>","title":"Demo: User Metadata for Labelling Commits"},{"location":"demo/user-metadata-for-labelling-commits/#creating-delta-table","text":"<pre><code>val tableName = \"/tmp/delta-demo-userMetadata\"\n</code></pre> <pre><code>spark.range(5)\n  .write\n  .format(\"delta\")\n  .save(tableName)\n</code></pre>","title":"Creating Delta Table"},{"location":"demo/user-metadata-for-labelling-commits/#describing-history","text":"<pre><code>import io.delta.tables.DeltaTable\nval d = DeltaTable.forPath(tableName)\n</code></pre> <p>We are interested in a subset of the available history metadata.</p> <pre><code>d.history\n  .select('version, 'operation, 'operationParameters, 'userMetadata)\n  .show(truncate = false)\n</code></pre> <pre><code>+-------+---------+------------------------------------------+------------+\n|version|operation|operationParameters                       |userMetadata|\n+-------+---------+------------------------------------------+------------+\n|0      |WRITE    |[mode -&gt; ErrorIfExists, partitionBy -&gt; []]|null        |\n+-------+---------+------------------------------------------+------------+\n</code></pre>","title":"Describing History"},{"location":"demo/user-metadata-for-labelling-commits/#appending-data","text":"<p>In this step, you're going to append new data to the existing Delta table.</p> <p>You're going to use userMetadata option for a custom user-defined historical marker (e.g. to know when this extra append happended in the life of the Delta table).</p> <pre><code>val userMetadata = \"two more rows for demo\"\n</code></pre> <p>Since you're appending new rows, it is required to use <code>Append</code> mode.</p> <pre><code>import org.apache.spark.sql.SaveMode.Append\n</code></pre> <p>The whole append write is as follows:</p> <pre><code>spark.range(start = 5, end = 7)\n  .write\n  .format(\"delta\")\n  .option(\"userMetadata\", userMetadata)\n  .mode(Append)\n  .save(tableName)\n</code></pre> <p>That write query creates another version of the Delta table.</p>","title":"Appending Data"},{"location":"demo/user-metadata-for-labelling-commits/#listing-versions-with-usermetadata","text":"<p>For the sake of the demo, you are going to show the versions of the Delta table with <code>userMetadata</code> defined.</p> <pre><code>d.history\n  .select('version, 'operation, 'operationParameters, 'userMetadata)\n  .where('userMetadata.isNotNull)\n  .show(truncate = false)\n</code></pre> <pre><code>+-------+---------+-----------------------------------+----------------------+\n|version|operation|operationParameters                |userMetadata          |\n+-------+---------+-----------------------------------+----------------------+\n|1      |WRITE    |[mode -&gt; Append, partitionBy -&gt; []]|two more rows for demo|\n+-------+---------+-----------------------------------+----------------------+\n</code></pre>","title":"Listing Versions with userMetadata"},{"location":"demo/vacuum/","text":"<p>This demo shows vacuum command in action.</p>","title":"Demo: Vacuum"},{"location":"demo/vacuum/#start-spark-shell","text":"<pre><code>./bin/spark-shell \\\n  --packages io.delta:delta-core_2.12:1.2.0 \\\n  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n</code></pre>","title":"Start Spark Shell"},{"location":"demo/vacuum/#create-delta-table","text":"<pre><code>val path = \"/tmp/delta/t1\"\n</code></pre> <p>Make sure that there is no delta table at the location. Remove it if exists and start over.</p> <pre><code>import org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, path)\nassert(deltaLog.tableExists == false)\n</code></pre> <p>Create a demo delta table (using Scala API). Write some data to the delta table, effectively creating the first version.</p> <pre><code>spark.range(4)\n  .withColumn(\"p\", 'id % 2)\n  .write\n  .format(\"delta\")\n  .partitionBy(\"p\")\n  .save(path)\n</code></pre> <p>Display the available versions of the delta table.</p> <pre><code>import io.delta.tables.DeltaTable\nval dt = DeltaTable.forPath(path)\n</code></pre> <pre><code>val history = dt.history.select('version, 'operation, 'operationMetrics)\nhistory.show(truncate = false)\n</code></pre> <pre><code>+-------+---------+-----------------------------------------------------------+\n|version|operation|operationMetrics                                           |\n+-------+---------+-----------------------------------------------------------+\n|0      |WRITE    |[numFiles -&gt; 4, numOutputBytes -&gt; 1852, numOutputRows -&gt; 4]|\n+-------+---------+-----------------------------------------------------------+\n</code></pre>","title":"Create Delta Table"},{"location":"demo/vacuum/#delete-all","text":"<p>Delete all data in the delta table, effectively creating the second version.</p> <pre><code>import io.delta.tables.DeltaTable\nDeltaTable.forPath(path).delete\n</code></pre> <p>Display the available versions of the delta table.</p> <pre><code>val history = dt.history.select('version, 'operation, 'operationMetrics)\nhistory.show(truncate = false)\n</code></pre> <pre><code>+-------+---------+-----------------------------------------------------------+\n|version|operation|operationMetrics                                           |\n+-------+---------+-----------------------------------------------------------+\n|1      |DELETE   |[numRemovedFiles -&gt; 4]                                     |\n|0      |WRITE    |[numFiles -&gt; 4, numOutputBytes -&gt; 1852, numOutputRows -&gt; 4]|\n+-------+---------+-----------------------------------------------------------+\n</code></pre>","title":"Delete All"},{"location":"demo/vacuum/#vacuum-dry-run-illegalargumentexception","text":"<p>Let's vacuum the delta table (in <code>DRY RUN</code> mode).</p> <pre><code>sql(s\"VACUUM delta.`$path` RETAIN 0 HOURS DRY RUN\")\n</code></pre> <pre><code>java.lang.IllegalArgumentException: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n</code></pre> <p>Attempting to vacuum the delta table (even with <code>DRY RUN</code>) gives an <code>IllegalArgumentException</code> because of the default values of the following:</p> <ul> <li>spark.databricks.delta.retentionDurationCheck.enabled configuration property</li> <li>deletedFileRetentionDuration table property</li> </ul>","title":"Vacuum DRY RUN (IllegalArgumentException)"},{"location":"demo/vacuum/#vacuum-dry-run","text":"","title":"Vacuum DRY RUN"},{"location":"demo/vacuum/#retentiondurationcheckenabled-configuration-property","text":"<p>Turn the spark.databricks.delta.retentionDurationCheck.enabled configuration property off and give the <code>VACUUM</code> command a go again.</p> <pre><code>spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", false)\n</code></pre> <pre><code>val q = sql(s\"VACUUM delta.`$path` RETAIN 0 HOURS DRY RUN\")\n</code></pre> <p>You should see the following message in the console:</p> <pre><code>Found 4 files and directories in a total of 3 directories that are safe to delete.\n</code></pre> <p>The result <code>DataFrame</code> is the paths that are safe to delete which are all of the data files in the delta table.</p> <pre><code>q.show(truncate = false)\n</code></pre> <pre><code>+------------------------------------------------------------------------------------------+\n|path                                                                                      |\n+------------------------------------------------------------------------------------------+\n|file:/tmp/delta/t1/p=0/part-00011-40983cc5-18bf-4d91-8b7b-eb805b8c862d.c000.snappy.parquet|\n|file:/tmp/delta/t1/p=1/part-00015-9576ff56-28f7-410e-8ea3-e43352a5083c.c000.snappy.parquet|\n|file:/tmp/delta/t1/p=0/part-00003-8e300857-ad6a-4889-b944-d31624a8024f.c000.snappy.parquet|\n|file:/tmp/delta/t1/p=1/part-00007-4fba265a-0884-44c3-84ed-4e9aee524e3a.c000.snappy.parquet|\n+------------------------------------------------------------------------------------------+\n</code></pre>","title":"retentionDurationCheck.enabled Configuration Property"},{"location":"demo/vacuum/#deletedfileretentionduration-table-property","text":"<p>Let's <code>DESCRIBE DETAIL</code> to review the current table properties (incl. deletedFileRetentionDuration).</p> <pre><code>val tid = s\"delta.`$path`\"\n</code></pre> <pre><code>val detail = sql(s\"DESCRIBE DETAIL $tid\").select('format, 'location, 'properties)\ndetail.show(truncate = false)\n</code></pre> <pre><code>+------+------------------+----------+\n|format|location          |properties|\n+------+------------------+----------+\n|delta |file:/tmp/delta/t1|[]        |\n+------+------------------+----------+\n</code></pre> <p>Prefix the <code>deletedFileRetentionDuration</code> table property with <code>delta.</code> for <code>ALTER TABLE</code> to accept it as a Delta property.</p> <pre><code>sql(s\"ALTER TABLE $tid SET TBLPROPERTIES (delta.deletedFileRetentionDuration = '0 hours')\")\n</code></pre> <pre><code>val detail = sql(s\"DESCRIBE DETAIL $tid\").select('format, 'location, 'properties)\ndetail.show(truncate = false)\n</code></pre> <pre><code>+------+------------------+-----------------------------------------------+\n|format|location          |properties                                     |\n+------+------------------+-----------------------------------------------+\n|delta |file:/tmp/delta/t1|[delta.deletedFileRetentionDuration -&gt; 0 hours]|\n+------+------------------+-----------------------------------------------+\n</code></pre> <p>Display the available versions of the delta table and note that <code>ALTER TABLE</code> gives a new version. This time you include <code>operationParameters</code> column (not <code>operationMetrics</code> as less important).</p> <pre><code>val history = dt.history.select('version, 'operation, 'operationParameters)\nhistory.show(truncate = false)\n</code></pre> <pre><code>+-------+-----------------+----------------------------------------------------------------+\n|version|operation        |operationParameters                                             |\n+-------+-----------------+----------------------------------------------------------------+\n|2      |SET TBLPROPERTIES|[properties -&gt; {\"delta.deletedFileRetentionDuration\":\"0 hours\"}]|\n|1      |DELETE           |[predicate -&gt; []]                                               |\n|0      |WRITE            |[mode -&gt; ErrorIfExists, partitionBy -&gt; [\"p\"]]                   |\n+-------+-----------------+----------------------------------------------------------------+\n</code></pre> <p>You can access the table properties (table configuration) using DeltaLog Scala API.</p> <pre><code>import org.apache.spark.sql.delta.DeltaLog\nval log = DeltaLog.forTable(spark, path)\nlog.snapshot.metadata.configuration\n</code></pre> <p>Let's revert the latest change to <code>spark.databricks.delta.retentionDurationCheck.enabled</code> and turn it on back.</p> <pre><code>spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", true)\n</code></pre> <pre><code>val q = sql(s\"VACUUM delta.`$path` RETAIN 0 HOURS DRY RUN\")\n</code></pre> <p>You should see the following message in the console:</p> <pre><code>Found 4 files and directories in a total of 3 directories that are safe to delete.\n</code></pre> <p>The result <code>DataFrame</code> is the paths that are safe to delete which are all of the data files in the delta table.</p> <pre><code>q.show(truncate = false)\n</code></pre> <pre><code>+------------------------------------------------------------------------------------------+\n|path                                                                                      |\n+------------------------------------------------------------------------------------------+\n|file:/tmp/delta/t1/p=0/part-00011-40983cc5-18bf-4d91-8b7b-eb805b8c862d.c000.snappy.parquet|\n|file:/tmp/delta/t1/p=1/part-00015-9576ff56-28f7-410e-8ea3-e43352a5083c.c000.snappy.parquet|\n|file:/tmp/delta/t1/p=0/part-00003-8e300857-ad6a-4889-b944-d31624a8024f.c000.snappy.parquet|\n|file:/tmp/delta/t1/p=1/part-00007-4fba265a-0884-44c3-84ed-4e9aee524e3a.c000.snappy.parquet|\n+------------------------------------------------------------------------------------------+\n</code></pre>","title":"deletedFileRetentionDuration Table Property"},{"location":"demo/vacuum/#tree-delta-table-directory","text":"<p>In a terminal (outside <code>spark-shell</code>) run <code>tree</code> or a similar command to review what the directory of the delta table looks like.</p> <pre><code>tree /tmp/delta/t1\n</code></pre> <pre><code>/tmp/delta/t1\n\u251c\u2500\u2500 _delta_log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 00000000000000000000.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 00000000000000000001.json\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 00000000000000000002.json\n\u251c\u2500\u2500 p=0\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 part-00003-8e300857-ad6a-4889-b944-d31624a8024f.c000.snappy.parquet\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 part-00011-40983cc5-18bf-4d91-8b7b-eb805b8c862d.c000.snappy.parquet\n\u2514\u2500\u2500 p=1\n    \u251c\u2500\u2500 part-00007-4fba265a-0884-44c3-84ed-4e9aee524e3a.c000.snappy.parquet\n    \u2514\u2500\u2500 part-00015-9576ff56-28f7-410e-8ea3-e43352a5083c.c000.snappy.parquet\n\n3 directories, 7 files\n</code></pre>","title":"Tree Delta Table Directory"},{"location":"demo/vacuum/#retain-0-hours","text":"<p>Let's clean up (vacuum) the delta table entirely, effectively deleting all the data files physically from disk.</p> <p>Back in <code>spark-shell</code>, run <code>VACUUM</code> SQL command. Note that you're going to use it with no <code>DRY RUN</code>.</p> <pre><code>sql(s\"VACUUM delta.`$path` RETAIN 0 HOURS\").show(truncate = false)\n</code></pre> <pre><code>Deleted 4 files and directories in a total of 3 directories.\n+------------------+\n|path              |\n+------------------+\n|file:/tmp/delta/t1|\n+------------------+\n</code></pre> <p>In a terminal (outside <code>spark-shell</code>), run <code>tree</code> or a similar command to review what the directory of the delta table looks like.</p> <pre><code>tree /tmp/delta/t1\n</code></pre> <pre><code>/tmp/delta/t1\n\u251c\u2500\u2500 _delta_log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 00000000000000000000.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 00000000000000000001.json\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 00000000000000000002.json\n\u251c\u2500\u2500 p=0\n\u2514\u2500\u2500 p=1\n\n3 directories, 3 files\n</code></pre> <p>Switch to <code>spark-shell</code> and display the available versions of the delta table. There should really be no change compared to the last time you executed it.</p> <pre><code>val history = dt.history.select('version, 'operation, 'operationParameters)\nhistory.show(truncate = false)\n</code></pre> <pre><code>+-------+-----------------+----------------------------------------------------------------+\n|version|operation        |operationParameters                                             |\n+-------+-----------------+----------------------------------------------------------------+\n|2      |SET TBLPROPERTIES|[properties -&gt; {\"delta.deletedFileRetentionDuration\":\"0 hours\"}]|\n|1      |DELETE           |[predicate -&gt; []]                                               |\n|0      |WRITE            |[mode -&gt; ErrorIfExists, partitionBy -&gt; [\"p\"]]                   |\n+-------+-----------------+----------------------------------------------------------------+\n</code></pre>","title":"Retain 0 Hours"},{"location":"exceptions/","text":"<p>Among the new features of Delta Lake 1.0.0 are public DeltaConcurrentModificationExceptions for conflicts between concurrent operations. This allows you to catch such exceptions and retry write operations.</p>","title":"Delta Exceptions"},{"location":"exceptions/ConcurrentAppendException/","text":"<p><code>ConcurrentAppendException</code> is a DeltaConcurrentModificationException.</p>","title":"ConcurrentAppendException"},{"location":"exceptions/ConcurrentAppendException/#creating-instance","text":"<p><code>ConcurrentAppendException</code> takes the following to be created:</p> <ul> <li> Error Message  <p><code>ConcurrentAppendException</code> is created\u00a0when:</p> <ul> <li><code>DeltaErrors</code> utility is used to concurrentAppendException</li> </ul>","title":"Creating Instance"},{"location":"exceptions/ConcurrentDeleteDeleteException/","text":"<p><code>ConcurrentDeleteDeleteException</code> is a DeltaConcurrentModificationException.</p>","title":"ConcurrentDeleteDeleteException"},{"location":"exceptions/ConcurrentDeleteDeleteException/#creating-instance","text":"<p><code>ConcurrentDeleteDeleteException</code> takes the following to be created:</p> <ul> <li> Error Message  <p><code>ConcurrentDeleteDeleteException</code> is created\u00a0when:</p> <ul> <li><code>DeltaErrors</code> utility is used to concurrentDeleteDeleteException</li> </ul>","title":"Creating Instance"},{"location":"exceptions/ConcurrentDeleteReadException/","text":"<p><code>ConcurrentDeleteReadException</code> is a DeltaConcurrentModificationException.</p>","title":"ConcurrentDeleteReadException"},{"location":"exceptions/ConcurrentDeleteReadException/#creating-instance","text":"<p><code>ConcurrentDeleteReadException</code> takes the following to be created:</p> <ul> <li> Error Message  <p><code>ConcurrentDeleteReadException</code> is created\u00a0when:</p> <ul> <li><code>DeltaErrors</code> utility is used to concurrentDeleteReadException</li> </ul>","title":"Creating Instance"},{"location":"exceptions/ConcurrentTransactionException/","text":"<p><code>ConcurrentTransactionException</code> is a DeltaConcurrentModificationException.</p>","title":"ConcurrentTransactionException"},{"location":"exceptions/ConcurrentTransactionException/#creating-instance","text":"<p><code>ConcurrentTransactionException</code> takes the following to be created:</p> <ul> <li> Error Message  <p><code>ConcurrentTransactionException</code> is created\u00a0when:</p> <ul> <li><code>DeltaErrors</code> utility is used to concurrentTransactionException</li> </ul>","title":"Creating Instance"},{"location":"exceptions/ConcurrentWriteException/","text":"<p><code>ConcurrentWriteException</code> is a DeltaConcurrentModificationException.</p>","title":"ConcurrentWriteException"},{"location":"exceptions/ConcurrentWriteException/#creating-instance","text":"<p><code>ConcurrentWriteException</code> takes the following to be created:</p> <ul> <li> Error Message  <p><code>ConcurrentWriteException</code> is created\u00a0when:</p> <ul> <li><code>DeltaErrors</code> utility is used to concurrentWriteException</li> </ul>","title":"Creating Instance"},{"location":"exceptions/DeltaConcurrentModificationException/","text":"<p><code>DeltaConcurrentModificationException</code>\u00a0is an extension of the <code>ConcurrentModificationException</code> (Java) abstraction for commit conflict exceptions.</p>  <p>Note</p> <p>There are two <code>DeltaConcurrentModificationException</code> abstractions in two different packages:</p> <ul> <li><code>io.delta.exceptions</code></li> <li><code>org.apache.spark.sql.delta</code> (obsolete since 1.0.0)</li> </ul>","title":"DeltaConcurrentModificationException"},{"location":"exceptions/DeltaConcurrentModificationException/#implementations","text":"<ul> <li>ConcurrentAppendException</li> <li>ConcurrentDeleteDeleteException</li> <li>ConcurrentDeleteReadException</li> <li>ConcurrentTransactionException</li> <li>ConcurrentWriteException</li> <li>MetadataChangedException</li> <li>ProtocolChangedException</li> </ul>","title":"Implementations"},{"location":"exceptions/DeltaConcurrentModificationException/#creating-instance","text":"<p><code>DeltaConcurrentModificationException</code> takes the following to be created:</p> <ul> <li> Error Message   Abstract Class <p><code>DeltaConcurrentModificationException</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete DeltaConcurrentModificationExceptions.</p>","title":"Creating Instance"},{"location":"exceptions/MetadataChangedException/","text":"<p><code>MetadataChangedException</code> is a DeltaConcurrentModificationException.</p>","title":"MetadataChangedException"},{"location":"exceptions/MetadataChangedException/#creating-instance","text":"<p><code>MetadataChangedException</code> takes the following to be created:</p> <ul> <li> Error Message  <p><code>MetadataChangedException</code> is created\u00a0when:</p> <ul> <li><code>DeltaErrors</code> utility is used to metadataChangedException</li> </ul>","title":"Creating Instance"},{"location":"exceptions/ProtocolChangedException/","text":"<p><code>ProtocolChangedException</code> is a DeltaConcurrentModificationException.</p>","title":"ProtocolChangedException"},{"location":"exceptions/ProtocolChangedException/#creating-instance","text":"<p><code>ProtocolChangedException</code> takes the following to be created:</p> <ul> <li> Error Message  <p><code>ProtocolChangedException</code> is created\u00a0when:</p> <ul> <li><code>DeltaErrors</code> utility is used to protocolChangedException</li> </ul>","title":"Creating Instance"},{"location":"generated-columns/","text":"<p>Generated Columns are columns of a delta table with generation expressions.</p> <p>Generation Expression is a SQL expression to generate values at write time (unless provided by a query). Generation expressions are attached to a column using delta.generationExpression metadata key.</p> <p>Generated Columns can be defined using DeltaColumnBuilder.generatedAlwaysAs operator.</p> <p>Generated Columns is a new feature in Delta Lake 1.0.0.</p>","title":"Generated Columns"},{"location":"generated-columns/GeneratedColumn/","text":"","title":"GeneratedColumn Utility"},{"location":"generated-columns/GeneratedColumn/#isgeneratedcolumn","text":"","title":"isGeneratedColumn <pre><code>isGeneratedColumn(\n  protocol: Protocol,\n  field: StructField): Boolean\nisGeneratedColumn(\n  field: StructField): Boolean\n</code></pre> <p><code>isGeneratedColumn</code> is <code>true</code> when the metadata of the <code>StructField</code> contains generation expression.</p>"},{"location":"generated-columns/GeneratedColumn/#getgeneratedcolumns","text":"","title":"getGeneratedColumns <pre><code>getGeneratedColumns(\n  snapshot: Snapshot): Seq[StructField]\n</code></pre> <p><code>getGeneratedColumns</code>...FIXME</p> <p><code>getGeneratedColumns</code>\u00a0is used when:</p> <ul> <li>PreprocessTableUpdate logical resolution rule is executed (and toCommand)</li> </ul>"},{"location":"generated-columns/GeneratedColumn/#enforcesgeneratedcolumns","text":"","title":"enforcesGeneratedColumns <pre><code>enforcesGeneratedColumns(\n  protocol: Protocol,\n  metadata: Metadata): Boolean\n</code></pre> <p><code>enforcesGeneratedColumns</code> is <code>true</code> when the following all hold:</p> <ul> <li>satisfyGeneratedColumnProtocol with the Protocol</li> <li>There is at least one generated column in the schema of the Metadata</li> </ul> <p><code>enforcesGeneratedColumns</code>\u00a0is used when:</p> <ul> <li><code>TransactionalWrite</code> is requested to write data out (and normalizeData)</li> </ul>"},{"location":"generated-columns/GeneratedColumn/#removegenerationexpressions","text":"","title":"removeGenerationExpressions <pre><code>removeGenerationExpressions(\n  schema: StructType): StructType\n</code></pre> <p><code>removeGenerationExpressions</code>...FIXME</p> <p><code>removeGenerationExpressions</code>\u00a0is used when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"generated-columns/GeneratedColumn/#satisfygeneratedcolumnprotocol","text":"","title":"satisfyGeneratedColumnProtocol <pre><code>satisfyGeneratedColumnProtocol(\n  protocol: Protocol): Boolean\n</code></pre> <p><code>satisfyGeneratedColumnProtocol</code> is <code>true</code> when the minWriterVersion of the Protocol is at least <code>4</code>.</p> <p><code>satisfyGeneratedColumnProtocol</code>\u00a0is used when:</p> <ul> <li><code>GeneratedColumn</code> utility is used to isGeneratedColumn, getGeneratedColumns and enforcesGeneratedColumns</li> <li><code>OptimisticTransactionImpl</code> is requested to updateMetadata</li> <li><code>ImplicitMetadataOperation</code> is requested to updateMetadata</li> </ul>"},{"location":"generated-columns/GeneratedColumn/#addgeneratedcolumnsorreturnconstraints","text":"","title":"addGeneratedColumnsOrReturnConstraints <pre><code>addGeneratedColumnsOrReturnConstraints(\n  deltaLog: DeltaLog,\n  queryExecution: QueryExecution,\n  schema: StructType,\n  df: DataFrame): (DataFrame, Seq[Constraint])\n</code></pre> <p><code>addGeneratedColumnsOrReturnConstraints</code> returns a <code>DataFrame</code> with generated columns (missing in the schema) and constraints for generated columns (existing in the schema).</p> <p><code>addGeneratedColumnsOrReturnConstraints</code> finds generated columns (among the top-level columns in the given schema (StructType)).</p> <p>For every generated column, <code>addGeneratedColumnsOrReturnConstraints</code> creates a Check constraint with the following:</p> <ul> <li><code>Generated Column</code> name</li> <li><code>EqualNullSafe</code> expression that compares the generated column expression with the value provided by the user</li> </ul> <p>In the end, <code>addGeneratedColumnsOrReturnConstraints</code> uses <code>select</code> operator on the given <code>DataFrame</code>.</p> <p><code>addGeneratedColumnsOrReturnConstraints</code>\u00a0is used when:</p> <ul> <li><code>TransactionalWrite</code> is requested to write data out (and normalizeData)</li> </ul>"},{"location":"generated-columns/demo/","text":"","title":"Demo: Generated Columns"},{"location":"generated-columns/demo/#create-delta-table-with-generated-column","text":"<pre><code>val dataPath = \"/tmp/delta/values\"\n</code></pre> <pre><code>import io.delta.tables.DeltaTable\nimport org.apache.spark.sql.types.DataTypes\n</code></pre> <pre><code>DeltaTable.create\n  .addColumn(\"id\", DataTypes.LongType, nullable = false)\n  .addColumn(\n    DeltaTable.columnBuilder(\"value\")\n      .dataType(DataTypes.BooleanType)\n      .generatedAlwaysAs(\"true\")\n      .build)\n  .location(dataPath)\n  .execute\n</code></pre>","title":"Create Delta Table with Generated Column"},{"location":"generated-columns/demo/#review-metadata","text":"<pre><code>import org.apache.spark.sql.delta.DeltaLog\nval deltaLog = DeltaLog.forTable(spark, dataPath)\n</code></pre> <pre><code>println(deltaLog.snapshot.metadata.dataSchema(\"value\").metadata.json)\n</code></pre> <pre><code>{\"delta.generationExpression\":\"true\"}\n</code></pre>","title":"Review Metadata"},{"location":"generated-columns/demo/#write-to-delta-table","text":"<pre><code>import io.delta.implicits._\nimport org.apache.spark.sql.SaveMode\n</code></pre> <pre><code>spark.range(5)\n  .write\n  .mode(SaveMode.Append)\n  .delta(dataPath)\n</code></pre>","title":"Write to Delta Table"},{"location":"generated-columns/demo/#show-table","text":"<pre><code>DeltaTable.forPath(dataPath).toDF.orderBy('id).show\n</code></pre> <pre><code>+---+-----+\n| id|value|\n+---+-----+\n|  0| true|\n|  1| true|\n|  2| true|\n|  3| true|\n|  4| true|\n+---+-----+\n</code></pre>","title":"Show Table"},{"location":"generated-columns/demo/#invariantviolationexception","text":"<p>The following one-row query will break the CHECK constraint on the generated column since the value is not <code>true</code>.</p> <pre><code>spark.range(5, 6)\n  .withColumn(\"value\", lit(false))\n  .write\n  .mode(SaveMode.Append)\n  .delta(dataPath)\n</code></pre> <pre><code>org.apache.spark.sql.delta.schema.InvariantViolationException: CHECK constraint Generated Column (`value` &lt;=&gt; true) violated by row with values:\n - value : false\n  at org.apache.spark.sql.delta.schema.InvariantViolationException$.apply(InvariantViolationException.scala:50)\n  at org.apache.spark.sql.delta.schema.InvariantViolationException$.apply(InvariantViolationException.scala:60)\n  at org.apache.spark.sql.delta.schema.InvariantViolationException.apply(InvariantViolationException.scala)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n  at org.apache.spark.sql.delta.constraints.DeltaInvariantCheckerExec.$anonfun$doExecute$3(DeltaInvariantCheckerExec.scala:86)\n  at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:278)\n  at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n  at org.apache.spark.scheduler.Task.run(Task.scala:131)\n  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n  at java.base/java.lang.Thread.run(Thread.java:829)\n</code></pre>","title":"InvariantViolationException"},{"location":"sql/","text":"<p>Delta Lake registers custom SQL statements (using DeltaSparkSessionExtension to inject DeltaSqlParser with DeltaSqlAstBuilder).</p> <p>The SQL statements support <code>table</code> of the format <code>delta.`path`</code> (with backticks), e.g. <code>delta.`/tmp/delta/t1`</code> while <code>path</code> is between single quotes, e.g. <code>'/tmp/delta/t1'</code>.</p> <p>The SQL statements can also refer to a table registered in a metastore.</p>","title":"Delta SQL"},{"location":"sql/#alter-table-add-constraint","text":"","title":"ALTER TABLE ADD CONSTRAINT <pre><code>ALTER TABLE table\nADD CONSTRAINT name constraint\n</code></pre> <p>Creates a AlterTableAddConstraintStatement</p>"},{"location":"sql/#alter-table-drop-constraint","text":"","title":"ALTER TABLE DROP CONSTRAINT <pre><code>ALTER TABLE table\nDROP CONSTRAINT (IF EXISTS)? name\n</code></pre> <p>Creates a AlterTableDropConstraintStatement</p>"},{"location":"sql/#convert-to-delta","text":"","title":"CONVERT TO DELTA <pre><code>CONVERT TO DELTA table\n  (PARTITIONED BY '(' colTypeList ')')?\n</code></pre> <p>Executes ConvertToDeltaCommand</p>"},{"location":"sql/#describe-detail","text":"","title":"DESCRIBE DETAIL <pre><code>(DESC | DESCRIBE) DETAIL (path | table)\n</code></pre> <p>Executes DescribeDeltaDetailCommand</p>"},{"location":"sql/#describe-history","text":"","title":"DESCRIBE HISTORY <pre><code>(DESC | DESCRIBE) HISTORY (path | table)\n  (LIMIT limit)?\n</code></pre> <p>Executes DescribeDeltaHistoryCommand</p>"},{"location":"sql/#generate","text":"","title":"GENERATE <pre><code>GENERATE modeName FOR TABLE table\n</code></pre> <p>Executes DeltaGenerateCommand</p>"},{"location":"sql/#optimize","text":"","title":"OPTIMIZE <pre><code>OPTIMIZE (path | table)\n  (WHERE partitionPredicate)?\n</code></pre> <p>Executes OptimizeTableCommand (on the Delta table identified by a directory path or a table name)</p>"},{"location":"sql/#vacuum","text":"","title":"VACUUM <pre><code>VACUUM (path | table)\n  (RETAIN number HOURS)? (DRY RUN)?\n</code></pre> <p>Executes VacuumTableCommand</p>"},{"location":"sql/DeltaSqlAstBuilder/","text":"<p><code>DeltaSqlAstBuilder</code> is a command builder for the Delta SQL statements (described in DeltaSqlBase.g4 ANTLR grammar).</p> <p><code>DeltaSqlParser</code> is used by DeltaSqlParser.</p>    SQL Statement Logical Command     ALTER TABLE ADD CONSTRAINT AlterTableAddConstraintStatement   ALTER TABLE DROP CONSTRAINT AlterTableDropConstraintStatement   CONVERT TO DELTA ConvertToDeltaCommand   DESCRIBE DETAIL DescribeDeltaDetailCommand   DESCRIBE HISTORY DescribeDeltaHistoryCommand   GENERATE DeltaGenerateCommand   OPTIMIZE OptimizeTableCommand   VACUUM VacuumTableCommand","title":"DeltaSqlAstBuilder"},{"location":"sql/DeltaSqlParser/","text":"<p><code>DeltaSqlParser</code> is a SQL parser (Spark SQL's ParserInterface) for Delta SQL.</p> <p><code>DeltaSqlParser</code> is registered in a Spark SQL application using DeltaSparkSessionExtension.</p>","title":"DeltaSqlParser"},{"location":"sql/DeltaSqlParser/#creating-instance","text":"<p><code>DeltaSqlParser</code> takes the following to be created:</p> <ul> <li> <code>ParserInterface</code> (to fall back to for unsupported SQL)  <p><code>DeltaSqlParser</code> is created when:</p> <ul> <li><code>DeltaSparkSessionExtension</code> is requested to register Delta SQL support</li> </ul>","title":"Creating Instance"},{"location":"sql/DeltaSqlParser/#deltasqlastbuilder","text":"","title":"DeltaSqlAstBuilder <p><code>DeltaSqlParser</code> uses DeltaSqlAstBuilder to convert SQL statements to their runtime representation (as a <code>LogicalPlan</code>).</p> <p>In case an AST could not be converted to a <code>LogicalPlan</code>, <code>DeltaSqlAstBuilder</code> requests the delegate ParserInterface to parse it.</p>"},{"location":"storage/","text":"<p>Delta Lake can now automatically load the correct LogStore needed for common storage systems hosting the Delta table being read or written to.</p> <p>LogStoreProvider uses DelegatingLogStore unless spark.delta.logStore.class configuration property is defined.</p> <p>The scheme of the Delta table path is used to dynamically load the necessary LogStore implementation. This also allows the same application to simultaneously read and write to Delta tables on different cloud storage systems.</p> <p>DelegatingLogStore allows for custom LogStores per URI scheme before using the default LogStores.</p>    Schemes Default LogStore     <code>s3</code>, <code>s3a</code>, <code>s3n</code> S3SingleDriverLogStore   <code>abfs</code>, <code>abfss</code>, <code>adl</code>, <code>wasb</code>, <code>wasbs</code> <code>AzureLogStore</code>    <p><code>DelegatingLogStore</code> uses HDFSLogStore as the default LogStore.</p>","title":"Storage"},{"location":"storage/DelegatingLogStore/","text":"<p><code>DelegatingLogStore</code> is the default LogStore.</p>","title":"DelegatingLogStore"},{"location":"storage/DelegatingLogStore/#creating-instance","text":"<p><code>DelegatingLogStore</code> takes the following to be created:</p> <ul> <li> <code>Configuration</code> (Apache Hadoop)  <p><code>DelegatingLogStore</code> is created\u00a0when:</p> <ul> <li><code>LogStore</code> utility is used to createLogStoreWithClassName</li> </ul>","title":"Creating Instance"},{"location":"storage/DelegatingLogStore/#default-logstore","text":"","title":"Default LogStore <pre><code>defaultLogStore: LogStore\n</code></pre> <p><code>DelegatingLogStore</code> creates a LogStore (lazily) that is used when requested to schemeBasedLogStore.</p>  Lazy Value <p><code>defaultLogStore</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p>"},{"location":"storage/DelegatingLogStore/#logstore-by-scheme-lookup-table","text":"","title":"LogStore by Scheme Lookup Table <pre><code>schemeToLogStoreMap: Map[String, LogStore]\n</code></pre> <p><code>DelegatingLogStore</code> uses an internal registry of LogStores by scheme for looking them up once created.</p>"},{"location":"storage/DelegatingLogStore/#looking-up-logstore-delegate-by-path","text":"","title":"Looking Up LogStore Delegate by Path <pre><code>getDelegate(\n  path: Path): LogStore\n</code></pre> <p><code>getDelegate</code> is a mere alias of schemeBasedLogStore.</p>"},{"location":"storage/DelegatingLogStore/#schemebasedlogstore","text":"","title":"schemeBasedLogStore <pre><code>schemeBasedLogStore(\n  path: Path): LogStore\n</code></pre> <p><code>schemeBasedLogStore</code> takes the scheme component (of the URI) of the given path.</p> <p>If undefined, <code>schemeBasedLogStore</code> gives the defaultLogStore.</p> <p>For a scheme defined, <code>schemeBasedLogStore</code> looks it up in the schemeToLogStoreMap registry and returns it when found.</p> <p>Otherwise, <code>schemeBasedLogStore</code> creates a LogStore based on the following (in the order):</p> <ol> <li>Scheme-specific configuration key to look up the class name of the <code>LogStore</code> in the <code>SparkConf</code></li> <li>Default LogStore class name for the scheme</li> <li>Uses the defaultLogStore</li> </ol> <p><code>schemeBasedLogStore</code> registers the <code>LogStore</code> in the schemeToLogStoreMap registry for future lookups.</p> <p><code>schemeBasedLogStore</code> prints out the following INFO message to the logs:</p> <pre><code>LogStore [className] is used for scheme [scheme]\n</code></pre>"},{"location":"storage/DelegatingLogStore/#default-logstore-class-name-for-scheme","text":"","title":"Default LogStore (Class Name) for Scheme <pre><code>getDefaultLogStoreClassName(\n  scheme: String): Option[String]\n</code></pre> <p><code>getDefaultLogStoreClassName</code> returns the class name of the LogStore for a given <code>scheme</code> or <code>None</code> (undefined).</p>    Schemes Class Name     <code>s3</code>, <code>s3a</code>, <code>s3n</code> S3SingleDriverLogStore   <code>abfs</code>, <code>abfss</code>, <code>adl</code>, <code>wasb</code>, <code>wasbs</code> <code>AzureLogStore</code>"},{"location":"storage/DelegatingLogStore/#creating-logstore","text":"","title":"Creating LogStore <pre><code>createLogStore(\n  className: String): LogStore\n</code></pre> <p><code>createLogStore</code> creates a LogStore for the given class name.</p> <p><code>createLogStore</code>\u00a0is used when:</p> <ul> <li><code>DelegatingLogStore</code> is requested to schemeBasedLogStore and defaultLogStore</li> </ul>"},{"location":"storage/HDFSLogStore/","text":"<p><code>HDFSLogStore</code> is a HadoopFileSystemLogStore.</p> <p><code>HDFSLogStore</code> is the default LogStore of DelegatingLogStore.</p>","title":"HDFSLogStore"},{"location":"storage/HDFSLogStore/#creating-instance","text":"<p><code>HDFSLogStore</code> takes the following to be created:</p> <ul> <li> <code>SparkConf</code> (Apache Spark) <li> Hadoop Configuration  <p><code>HDFSLogStore</code> is created\u00a0when:</p> <ul> <li><code>DelegatingLogStore</code> is requested for the default LogStore</li> </ul>","title":"Creating Instance"},{"location":"storage/HadoopFileSystemLogStore/","text":"<p><code>HadoopFileSystemLogStore</code>\u00a0is an extension of the LogStore abstraction for Hadoop DFS-based log stores.</p>","title":"HadoopFileSystemLogStore"},{"location":"storage/HadoopFileSystemLogStore/#implementations","text":"<ul> <li>AzureLogStore</li> <li>HDFSLogStore</li> <li>LocalLogStore</li> <li>S3SingleDriverLogStore</li> </ul>","title":"Implementations"},{"location":"storage/HadoopFileSystemLogStore/#creating-instance","text":"<p><code>HadoopFileSystemLogStore</code> takes the following to be created:</p> <ul> <li> <code>SparkConf</code> (Apache Spark) <li> Hadoop Configuration   Abstract Class <p><code>HadoopFileSystemLogStore</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete HadoopFileSystemLogStores.</p>","title":"Creating Instance"},{"location":"storage/LogStore/","text":"<p><code>LogStore</code> is an abstraction of transaction log stores (to read and write physical log files and checkpoints).</p> <p><code>LogStore</code> is created using LogStoreProvider based on spark.delta.logStore.class configuration property.</p>","title":"LogStore"},{"location":"storage/LogStore/#contract","text":"","title":"Contract"},{"location":"storage/LogStore/#invalidatecache","text":"","title":"invalidateCache <pre><code>invalidateCache(): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>DelegatingLogStore</code> is requested to invalidateCache</li> </ul>"},{"location":"storage/LogStore/#ispartialwritevisible","text":"","title":"isPartialWriteVisible <pre><code>isPartialWriteVisible(\n  path: Path): Boolean\n</code></pre> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>Checkpoints</code> utility is used to writeCheckpoint</li> <li><code>DelegatingLogStore</code> is requested to isPartialWriteVisible</li> </ul>"},{"location":"storage/LogStore/#listfrom","text":"","title":"listFrom <pre><code>listFrom(\n  path: Path): Iterator[FileStatus]\nlistFrom(\n  path: String): Iterator[FileStatus]  \n</code></pre> <p>Used when:</p> <ul> <li><code>Checkpoints</code> is requested to findLastCompleteCheckpoint</li> <li><code>DeltaHistoryManager</code> is requested to getEarliestDeltaFile, getEarliestReproducibleCommit and getCommits</li> <li><code>DeltaLog</code> is requested to getChanges</li> <li><code>MetadataCleanup</code> is requested to listExpiredDeltaLogs</li> <li><code>SnapshotManagement</code> is requested to listFrom</li> <li><code>DelegatingLogStore</code> is requested to listFrom</li> <li><code>DeltaFileOperations</code> utility is used to listUsingLogStore</li> </ul>"},{"location":"storage/LogStore/#read","text":"","title":"read <pre><code>read(\n  path: Path): Seq[String]\nread(\n  path: String): Seq[String]\n</code></pre> <p>Used when:</p> <ul> <li><code>Checkpoints</code> is requested to loadMetadataFromFile</li> <li><code>ReadChecksum</code> is requested to readChecksum</li> <li><code>DeltaLog</code> is requested to getChanges</li> <li><code>OptimisticTransactionImpl</code> is requested to checkForConflicts</li> <li><code>DelegatingLogStore</code> is requested to read</li> <li><code>LogStore</code> is requested to readAsIterator</li> </ul>"},{"location":"storage/LogStore/#write","text":"","title":"write <pre><code>write(\n  path: Path,\n  actions: Iterator[String],\n  overwrite: Boolean = false): Unit\nwrite(\n  path: String,\n  actions: Iterator[String]): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>Checkpoints</code> is requested to checkpoint</li> <li><code>OptimisticTransactionImpl</code> is requested to doCommit</li> <li><code>DeltaCommand</code> is requested to commitLarge</li> <li><code>GenerateSymlinkManifestImpl</code> is requested to writeManifestFiles</li> <li><code>DelegatingLogStore</code> is requested to write</li> </ul>"},{"location":"storage/LogStore/#implementations","text":"<ul> <li>DelegatingLogStore</li> <li>HadoopFileSystemLogStore</li> <li>LogStoreAdaptor</li> </ul>","title":"Implementations"},{"location":"storage/LogStore/#creating-logstore","text":"","title":"Creating LogStore <pre><code>apply(\n  sc: SparkContext): LogStore\napply(\n  sparkConf: SparkConf,\n  hadoopConf: Configuration): LogStore\n</code></pre> <p><code>apply</code> creates a LogStore.</p> <p><code>apply</code>\u00a0is used when:</p> <ul> <li><code>GenerateSymlinkManifestImpl</code> is requested to writeManifestFiles and writeSingleManifestFile</li> <li><code>DeltaHistoryManager</code> is requested to getHistory and getActiveCommitAtTime</li> <li><code>DeltaFileOperations</code> is requested to recursiveListDirs, localListDirs, and localListFrom</li> </ul>"},{"location":"storage/LogStore/#createlogstorewithclassname","text":"","title":"createLogStoreWithClassName <pre><code>createLogStoreWithClassName(\n  className: String,\n  sparkConf: SparkConf,\n  hadoopConf: Configuration): LogStore\n</code></pre> <p><code>createLogStoreWithClassName</code> branches off based on the given <code>className</code>.</p> <p><code>createLogStoreWithClassName</code> creates a DelegatingLogStore when the <code>className</code> is the fully-qualified class name of <code>DelegatingLogStore</code>.</p> <p>Otherwise, <code>createLogStoreWithClassName</code> loads the class and braches off based on the class type.</p> <p>For io.delta.storage.LogStore, <code>createLogStoreWithClassName</code> creates an instance thereof (with the given <code>Configuration</code>) and wraps it up in a LogStoreAdaptor.</p> <p>For all other cases, <code>createLogStoreWithClassName</code> creates an instance thereof (with the given <code>SparkConf</code> and <code>Configuration</code>).</p> <p><code>createLogStoreWithClassName</code>\u00a0is used when:</p> <ul> <li><code>DelegatingLogStore</code> is requested to createLogStore</li> <li><code>LogStoreProvider</code> is requested to createLogStore</li> </ul>"},{"location":"storage/LogStore/#logstoreschemeconfkey","text":"","title":"logStoreSchemeConfKey <pre><code>logStoreSchemeConfKey(\n  scheme: String): String\n</code></pre> <p><code>logStoreSchemeConfKey</code> simply returns the following text for the given <code>scheme</code>:</p> <pre><code>spark.delta.logStore.[scheme].impl\n</code></pre> <p><code>logStoreSchemeConfKey</code>\u00a0is used when:</p> <ul> <li><code>DelegatingLogStore</code> is requested to schemeBasedLogStore</li> </ul>"},{"location":"storage/LogStoreAdaptor/","text":"<p><code>LogStoreAdaptor</code> is a LogStore.</p>","title":"LogStoreAdaptor"},{"location":"storage/LogStoreAdaptor/#creating-instance","text":"<p><code>LogStoreAdaptor</code> takes the following to be created:</p> <ul> <li> io.delta.storage.LogStore  <p><code>LogStoreAdaptor</code> is created\u00a0when:</p> <ul> <li><code>LogStore</code> utility is used to createLogStoreWithClassName</li> </ul>","title":"Creating Instance"},{"location":"storage/LogStoreProvider/","text":"<p><code>LogStoreProvider</code> is an abstraction of providers of LogStores.</p>","title":"LogStoreProvider"},{"location":"storage/LogStoreProvider/#sparkdeltalogstoreclass","text":"","title":"spark.delta.logStore.class <p><code>LogStoreProvider</code> uses the spark.delta.logStore.class configuration property for the LogStore to create (for a DeltaLog, a DeltaHistoryManager, and DeltaFileOperations).</p>"},{"location":"storage/LogStoreProvider/#creating-logstore","text":"","title":"Creating LogStore <pre><code>createLogStore(\n  spark: SparkSession): LogStore\ncreateLogStore(\n  sparkConf: SparkConf,\n  hadoopConf: Configuration): LogStore\n</code></pre> <p><code>createLogStore</code> creates a LogStore based on spark.delta.logStore.class configuration property (if defined) or defaults to DelegatingLogStore.</p> <p><code>createLogStore</code> is used when:</p> <ul> <li><code>DeltaLog</code> is requested for the LogStore</li> <li>LogStore.apply utility is used</li> </ul>"},{"location":"storage/S3SingleDriverLogStore/","text":"<p><code>S3SingleDriverLogStore</code> is a HadoopFileSystemLogStore.</p>","title":"S3SingleDriverLogStore"},{"location":"storage/S3SingleDriverLogStore/#creating-instance","text":"<p><code>S3SingleDriverLogStore</code> takes the following to be created:</p> <ul> <li> <code>SparkConf</code> (Apache Spark) <li> Hadoop Configuration","title":"Creating Instance"}]}